{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\"\"\"Melody RNN model.\"\"\"\n\nimport copy\n\n internal imports\nimport tensorflow as tf\n\nimport magenta\nfrom magenta.models.shared import events_rnn_model\nimport magenta.music as mm\n\nDEFAULT_MIN_NOTE = 48\nDEFAULT_MAX_NOTE = 84\nDEFAULT_TRANSPOSE_TO_KEY = 0\n\n\nclass MelodyRnnModel(events_rnn_model.EventSequenceRnnModel):\n  \"\"\"Class for RNN melody generation models.\"\"\"\n\n  def generate_melody(self, num_steps, primer_melody, temperature=1.0,\n                      beam_size=1, branch_factor=1, steps_per_iteration=1):\n    \"\"\"Generate a melody from a primer melody.\n\n    Args:\n      num_steps: The integer length in steps of the final melody, after\n          generation. Includes the primer.\n      primer_melody: The primer melody, a Melody object.\n      temperature: A float specifying how much to divide the logits by\n         before computing the softmax. Greater than 1.0 makes melodies more\n         random, less than 1.0 makes melodies less random.\n      beam_size: An integer, beam size to use when generating melodies via beam\n          search.\n      branch_factor: An integer, beam search branch factor to use.\n      steps_per_iteration: An integer, number of melody steps to take per beam\n          search iteration.\n\n    Returns:\n      The generated Melody object (which begins with the provided primer\n          melody).\n    \"\"\"\n    melody = copy.deepcopy(primer_melody)\n\n    transpose_amount = melody.squash(\n        self._config.min_note,\n        self._config.max_note,\n        self._config.transpose_to_key)\n\n    melody = self._generate_events(num_steps, melody, temperature, beam_size,\n                                   branch_factor, steps_per_iteration)\n\n    melody.transpose(-transpose_amount)\n\n    return melody\n\n  def melody_log_likelihood(self, melody):\n    \"\"\"Evaluate the log likelihood of a melody under the model.\n\n    Args:\n      melody: The Melody object for which to evaluate the log likelihood.\n\n    Returns:\n      The log likelihood of `melody` under this model.\n    \"\"\"\n    melody_copy = copy.deepcopy(melody)\n\n    melody_copy.squash(\n        self._config.min_note,\n        self._config.max_note,\n        self._config.transpose_to_key)\n\n    return self._evaluate_log_likelihood([melody_copy])[0]\n\n\nclass MelodyRnnConfig(events_rnn_model.EventSequenceRnnConfig):\n  \"\"\"Stores a configuration for a MelodyRnn.\n\n  You can change `min_note` and `max_note` to increase/decrease the melody\n  range. Since melodies are transposed into this range to be run through\n  the model and then transposed back into their original range after the\n  melodies have been extended, the location of the range is somewhat\n  arbitrary, but the size of the range determines the possible size of the\n  generated melodies range. `transpose_to_key` should be set to the key\n  that if melodies were transposed into that key, they would best sit\n  between `min_note` and `max_note` with having as few notes outside that\n  range.\n\n  Attributes:\n    details: The GeneratorDetails message describing the config.\n    encoder_decoder: The EventSequenceEncoderDecoder object to use.\n    hparams: The HParams containing hyperparameters to use.\n    min_note: The minimum midi pitch the encoded melodies can have.\n    max_note: The maximum midi pitch (exclusive) the encoded melodies can have.\n    transpose_to_key: The key that encoded melodies will be transposed into, or\n        None if it should not be transposed.\n  \"\"\"\n\n  def __init__(self, details, encoder_decoder, hparams,\n               min_note=DEFAULT_MIN_NOTE, max_note=DEFAULT_MAX_NOTE,\n               transpose_to_key=DEFAULT_TRANSPOSE_TO_KEY):\n    super(MelodyRnnConfig, self).__init__(details, encoder_decoder, hparams)\n\n    if min_note < mm.MIN_MIDI_PITCH:\n      raise ValueError('min_note must be >= 0. min_note is %d.' % min_note)\n    if max_note > mm.MAX_MIDI_PITCH + 1:\n      raise ValueError('max_note must be <= 128. max_note is %d.' % max_note)\n    if max_note - min_note < mm.NOTES_PER_OCTAVE:\n      raise ValueError('max_note - min_note must be >= 12. min_note is %d. '\n                       'max_note is %d. max_note - min_note is %d.' %\n                       (min_note, max_note, max_note - min_note))\n    if (transpose_to_key is not None and\n        (transpose_to_key < 0 or transpose_to_key > mm.NOTES_PER_OCTAVE - 1)):\n      raise ValueError('transpose_to_key must be >= 0 and <= 11. '\n                       'transpose_to_key is %d.' % transpose_to_key)\n\n    self.min_note = min_note\n    self.max_note = max_note\n    self.transpose_to_key = transpose_to_key\n\n\n Default configurations.\ndefault_configs = {\n    'basic_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='basic_rnn',\n            description='Melody RNN with one-hot encoding.'),\n        magenta.music.OneHotEventSequenceEncoderDecoder(\n            magenta.music.MelodyOneHotEncoding(\n                min_note=DEFAULT_MIN_NOTE,\n                max_note=DEFAULT_MAX_NOTE)),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            clip_norm=5,\n            learning_rate=0.001)),\n\n    'lookback_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='lookback_rnn',\n            description='Melody RNN with lookback encoding.'),\n        magenta.music.LookbackEventSequenceEncoderDecoder(\n            magenta.music.MelodyOneHotEncoding(\n                min_note=DEFAULT_MIN_NOTE,\n                max_note=DEFAULT_MAX_NOTE)),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            clip_norm=5,\n            learning_rate=0.001)),\n\n    'attention_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='attention_rnn',\n            description='Melody RNN with lookback encoding and attention.'),\n        magenta.music.KeyMelodyEncoderDecoder(\n            min_note=DEFAULT_MIN_NOTE,\n            max_note=DEFAULT_MAX_NOTE),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            attn_length=40,\n            clip_norm=3,\n            learning_rate=0.001))\n}\n", "comments": "   melody rnn model      import copy    internal imports import tensorflow tf  import magenta magenta models shared import events rnn model import magenta music mm  default min note   48 default max note   84 default transpose to key   0   class melodyrnnmodel(events rnn model eventsequencernnmodel)       class rnn melody generation models        def generate melody(self  num steps  primer melody  temperature 1 0                        beam size 1  branch factor 1  steps per iteration 1)         generate melody primer melody       args        num steps  the integer length steps final melody            generation  includes primer        primer melody  the primer melody  melody object        temperature  a float specifying much divide logits          computing softmax  greater 1 0 makes melodies          random  less 1 0 makes melodies less random        beam size  an integer  beam size use generating melodies via beam           search        branch factor  an integer  beam search branch factor use        steps per iteration  an integer  number melody steps take per beam           search iteration       returns        the generated melody object (which begins provided primer           melody)              melody   copy deepcopy(primer melody)      transpose amount   melody squash(         self  config min note          self  config max note          self  config transpose key)      melody   self  generate events(num steps  melody  temperature  beam size                                     branch factor  steps per iteration)      melody transpose( transpose amount)      return melody    def melody log likelihood(self  melody)         evaluate log likelihood melody model       args        melody  the melody object evaluate log likelihood       returns        the log likelihood  melody  model              melody copy   copy deepcopy(melody)      melody copy squash(         self  config min note          self  config max note          self  config transpose key)      return self  evaluate log likelihood( melody copy ) 0    class melodyrnnconfig(events rnn model eventsequencernnconfig)       stores configuration melodyrnn     you change  min note   max note  increase decrease melody   range  since melodies transposed range run   model transposed back original range   melodies extended  location range somewhat   arbitrary  size range determines possible size   generated melodies range   transpose key  set key   melodies transposed key  would best sit    min note   max note  notes outside   range     attributes      details  the generatordetails message describing config      encoder decoder  the eventsequenceencoderdecoder object use      hparams  the hparams containing hyperparameters use      min note  the minimum midi pitch encoded melodies      max note  the maximum midi pitch (exclusive) encoded melodies      transpose key  the key encoded melodies transposed          none transposed           copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     internal imports    default configurations  ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Melody RNN model.\"\"\"\n\nimport copy\n\n# internal imports\nimport tensorflow as tf\n\nimport magenta\nfrom magenta.models.shared import events_rnn_model\nimport magenta.music as mm\n\nDEFAULT_MIN_NOTE = 48\nDEFAULT_MAX_NOTE = 84\nDEFAULT_TRANSPOSE_TO_KEY = 0\n\n\nclass MelodyRnnModel(events_rnn_model.EventSequenceRnnModel):\n  \"\"\"Class for RNN melody generation models.\"\"\"\n\n  def generate_melody(self, num_steps, primer_melody, temperature=1.0,\n                      beam_size=1, branch_factor=1, steps_per_iteration=1):\n    \"\"\"Generate a melody from a primer melody.\n\n    Args:\n      num_steps: The integer length in steps of the final melody, after\n          generation. Includes the primer.\n      primer_melody: The primer melody, a Melody object.\n      temperature: A float specifying how much to divide the logits by\n         before computing the softmax. Greater than 1.0 makes melodies more\n         random, less than 1.0 makes melodies less random.\n      beam_size: An integer, beam size to use when generating melodies via beam\n          search.\n      branch_factor: An integer, beam search branch factor to use.\n      steps_per_iteration: An integer, number of melody steps to take per beam\n          search iteration.\n\n    Returns:\n      The generated Melody object (which begins with the provided primer\n          melody).\n    \"\"\"\n    melody = copy.deepcopy(primer_melody)\n\n    transpose_amount = melody.squash(\n        self._config.min_note,\n        self._config.max_note,\n        self._config.transpose_to_key)\n\n    melody = self._generate_events(num_steps, melody, temperature, beam_size,\n                                   branch_factor, steps_per_iteration)\n\n    melody.transpose(-transpose_amount)\n\n    return melody\n\n  def melody_log_likelihood(self, melody):\n    \"\"\"Evaluate the log likelihood of a melody under the model.\n\n    Args:\n      melody: The Melody object for which to evaluate the log likelihood.\n\n    Returns:\n      The log likelihood of `melody` under this model.\n    \"\"\"\n    melody_copy = copy.deepcopy(melody)\n\n    melody_copy.squash(\n        self._config.min_note,\n        self._config.max_note,\n        self._config.transpose_to_key)\n\n    return self._evaluate_log_likelihood([melody_copy])[0]\n\n\nclass MelodyRnnConfig(events_rnn_model.EventSequenceRnnConfig):\n  \"\"\"Stores a configuration for a MelodyRnn.\n\n  You can change `min_note` and `max_note` to increase/decrease the melody\n  range. Since melodies are transposed into this range to be run through\n  the model and then transposed back into their original range after the\n  melodies have been extended, the location of the range is somewhat\n  arbitrary, but the size of the range determines the possible size of the\n  generated melodies range. `transpose_to_key` should be set to the key\n  that if melodies were transposed into that key, they would best sit\n  between `min_note` and `max_note` with having as few notes outside that\n  range.\n\n  Attributes:\n    details: The GeneratorDetails message describing the config.\n    encoder_decoder: The EventSequenceEncoderDecoder object to use.\n    hparams: The HParams containing hyperparameters to use.\n    min_note: The minimum midi pitch the encoded melodies can have.\n    max_note: The maximum midi pitch (exclusive) the encoded melodies can have.\n    transpose_to_key: The key that encoded melodies will be transposed into, or\n        None if it should not be transposed.\n  \"\"\"\n\n  def __init__(self, details, encoder_decoder, hparams,\n               min_note=DEFAULT_MIN_NOTE, max_note=DEFAULT_MAX_NOTE,\n               transpose_to_key=DEFAULT_TRANSPOSE_TO_KEY):\n    super(MelodyRnnConfig, self).__init__(details, encoder_decoder, hparams)\n\n    if min_note < mm.MIN_MIDI_PITCH:\n      raise ValueError('min_note must be >= 0. min_note is %d.' % min_note)\n    if max_note > mm.MAX_MIDI_PITCH + 1:\n      raise ValueError('max_note must be <= 128. max_note is %d.' % max_note)\n    if max_note - min_note < mm.NOTES_PER_OCTAVE:\n      raise ValueError('max_note - min_note must be >= 12. min_note is %d. '\n                       'max_note is %d. max_note - min_note is %d.' %\n                       (min_note, max_note, max_note - min_note))\n    if (transpose_to_key is not None and\n        (transpose_to_key < 0 or transpose_to_key > mm.NOTES_PER_OCTAVE - 1)):\n      raise ValueError('transpose_to_key must be >= 0 and <= 11. '\n                       'transpose_to_key is %d.' % transpose_to_key)\n\n    self.min_note = min_note\n    self.max_note = max_note\n    self.transpose_to_key = transpose_to_key\n\n\n# Default configurations.\ndefault_configs = {\n    'basic_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='basic_rnn',\n            description='Melody RNN with one-hot encoding.'),\n        magenta.music.OneHotEventSequenceEncoderDecoder(\n            magenta.music.MelodyOneHotEncoding(\n                min_note=DEFAULT_MIN_NOTE,\n                max_note=DEFAULT_MAX_NOTE)),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            clip_norm=5,\n            learning_rate=0.001)),\n\n    'lookback_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='lookback_rnn',\n            description='Melody RNN with lookback encoding.'),\n        magenta.music.LookbackEventSequenceEncoderDecoder(\n            magenta.music.MelodyOneHotEncoding(\n                min_note=DEFAULT_MIN_NOTE,\n                max_note=DEFAULT_MAX_NOTE)),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            clip_norm=5,\n            learning_rate=0.001)),\n\n    'attention_rnn': MelodyRnnConfig(\n        magenta.protobuf.generator_pb2.GeneratorDetails(\n            id='attention_rnn',\n            description='Melody RNN with lookback encoding and attention.'),\n        magenta.music.KeyMelodyEncoderDecoder(\n            min_note=DEFAULT_MIN_NOTE,\n            max_note=DEFAULT_MAX_NOTE),\n        tf.contrib.training.HParams(\n            batch_size=128,\n            rnn_layer_sizes=[128, 128],\n            dropout_keep_prob=0.5,\n            attn_length=40,\n            clip_norm=3,\n            learning_rate=0.001))\n}\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "melody_rnn_model.py", "id": "43e109f0bf3b8334212529927a36fa26", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-magenta/tensorflow-magenta-ca73164/magenta/models/melody_rnn/melody_rnn_model.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:00:14Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}