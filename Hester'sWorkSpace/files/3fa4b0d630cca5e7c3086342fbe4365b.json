{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport re\nimport time\n\n internal imports\n\nimport numpy as np\nimport scipy\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom magenta.common import tf_utils\nfrom magenta.models.onsets_frames_transcription import constants\nfrom magenta.models.onsets_frames_transcription import data\nfrom magenta.models.onsets_frames_transcription import infer_util\nfrom magenta.models.onsets_frames_transcription import model\nfrom magenta.music import midi_io\n\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    'acoustic_run_dir', None,\n    'Path to look for acoustic checkpoints. Should contain subdir `train`.')\ntf.app.flags.DEFINE_string(\n    'acoustic_checkpoint_filename', None,\n    'Filename of the checkpoint to use. If not specified, will use the latest '\n    'checkpoint')\ntf.app.flags.DEFINE_string(\n    'examples_path', None,\n    'Path to TFRecord of test examples.')\ntf.app.flags.DEFINE_string(\n    'run_dir', '~/tmp/onsets_frames/infer',\n    'Path to store output midi files and summary events.')\ntf.app.flags.DEFINE_string(\n    'hparams',\n    'onset_mode=length_ms,onset_length=32',\n    'A comma-separated list of `name=value` hyperparameter values.')\ntf.app.flags.DEFINE_float(\n    'frame_threshold', 0.5,\n    'Threshold to use when sampling from the acoustic model.')\ntf.app.flags.DEFINE_float(\n    'onset_threshold', 0.5,\n    'Threshold to use when sampling from the acoustic model.')\ntf.app.flags.DEFINE_integer(\n    'max_seconds_per_sequence', 0,\n    'If set, will truncate sequences to be at most this many seconds long.')\ntf.app.flags.DEFINE_integer(\n    'min_note_duration_ms', 0,\n    'Notes shorter than this duration will be ignored when computing metrics.')\ntf.app.flags.DEFINE_boolean(\n    'require_onset', True,\n    'If set, require an onset prediction for a new note to start.')\ntf.app.flags.DEFINE_string(\n    'log', 'INFO',\n    'The threshold for what messages will be logged: '\n    'DEBUG, INFO, WARN, ERROR, or FATAL.')\n\n\ndef model_inference(acoustic_checkpoint, hparams, examples_path, run_dir):\n  tf.logging.info('acoustic_checkpoint=%s', acoustic_checkpoint)\n  tf.logging.info('examples_path=%s', examples_path)\n  tf.logging.info('run_dir=%s', run_dir)\n\n  with tf.Graph().as_default():\n    num_dims = constants.MIDI_PITCHES\n\n     Build the acoustic model within an 'acoustic' scope to isolate its\n     variables from the other models.\n    with tf.variable_scope('acoustic'):\n      truncated_length = 0\n      if FLAGS.max_seconds_per_sequence:\n        truncated_length = int(\n            math.ceil((FLAGS.max_seconds_per_sequence *\n                       data.hparams_frames_per_second(hparams))))\n      acoustic_data_provider, _ = data.provide_batch(\n          batch_size=1,\n          examples=examples_path,\n          hparams=hparams,\n          is_training=False,\n          truncated_length=truncated_length)\n\n      _, _, data_labels, _, _ = model.get_model(\n          acoustic_data_provider, hparams, is_training=False)\n\n     The checkpoints won't have the new scopes.\n    acoustic_variables = {\n        re.sub(r'^acoustic/', '', var.op.name): var\n        for var in slim.get_variables(scope='acoustic/')\n    }\n    acoustic_restore = tf.train.Saver(acoustic_variables)\n\n    onset_probs_flat = tf.get_default_graph().get_tensor_by_name(\n        'acoustic/onsets/onset_probs_flat:0')\n    frame_probs_flat = tf.get_default_graph().get_tensor_by_name(\n        'acoustic/frame_probs_flat:0')\n\n     Define some metrics.\n    (metrics_to_updates,\n     metric_note_precision,\n     metric_note_recall,\n     metric_note_f1,\n     metric_note_precision_with_offsets,\n     metric_note_recall_with_offsets,\n     metric_note_f1_with_offsets,\n     metric_frame_labels,\n     metric_frame_predictions) = infer_util.define_metrics(num_dims)\n\n    summary_op = tf.summary.merge_all()\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    global_step_increment = global_step.assign_add(1)\n\n     Use a custom init function to restore the acoustic and language models\n     from their separate checkpoints.\n    def init_fn(unused_self, sess):\n      acoustic_restore.restore(sess, acoustic_checkpoint)\n\n    scaffold = tf.train.Scaffold(init_fn=init_fn)\n    session_creator = tf.train.ChiefSessionCreator(\n        scaffold=scaffold)\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n      tf.logging.info('running session')\n      summary_writer = tf.summary.FileWriter(\n          logdir=run_dir, graph=sess.graph)\n\n      tf.logging.info('Inferring for %d batches',\n                      acoustic_data_provider.num_batches)\n      infer_times = []\n      num_frames = []\n      for unused_i in range(acoustic_data_provider.num_batches):\n        start_time = time.time()\n        labels, filenames, note_sequences, logits, onset_logits = sess.run([\n            data_labels,\n            acoustic_data_provider.filenames,\n            acoustic_data_provider.note_sequences,\n            frame_probs_flat,\n            onset_probs_flat,\n        ])\n         We expect these all to be length 1 because batch size is 1.\n        assert len(filenames) == len(note_sequences) == 1\n         These should be the same length and have been flattened.\n        assert len(labels) == len(logits) == len(onset_logits)\n\n        frame_predictions = logits > FLAGS.frame_threshold\n        if FLAGS.require_onset:\n          onset_predictions = onset_logits > FLAGS.onset_threshold\n        else:\n          onset_predictions = None\n\n        sequence_prediction = infer_util.pianoroll_to_note_sequence(\n            frame_predictions,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            min_duration_ms=FLAGS.min_note_duration_ms,\n            onset_predictions=onset_predictions)\n\n        end_time = time.time()\n        infer_time = end_time - start_time\n        infer_times.append(infer_time)\n        num_frames.append(logits.shape[0])\n        tf.logging.info(\n            'Infer time %f, frames %d, frames/sec %f, running average %f',\n            infer_time, logits.shape[0],\n            logits.shape[0] / infer_time,\n            np.sum(num_frames) / np.sum(infer_times))\n\n        tf.logging.info('Scoring sequence %s', filenames[0])\n        sequence_label = infer_util.score_sequence(\n            sess,\n            global_step_increment,\n            summary_op,\n            summary_writer,\n            metrics_to_updates,\n            metric_note_precision,\n            metric_note_recall,\n            metric_note_f1,\n            metric_note_precision_with_offsets,\n            metric_note_recall_with_offsets,\n            metric_note_f1_with_offsets,\n            metric_frame_labels,\n            metric_frame_predictions,\n            frame_labels=labels,\n            sequence_prediction=sequence_prediction,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            note_sequence_str_label=note_sequences[0],\n            min_duration_ms=FLAGS.min_note_duration_ms,\n            sequence_id=filenames[0])\n\n         Make filenames UNIX-friendly.\n        filename = filenames[0].replace('/', '_').replace(':', '.')\n        output_file = os.path.join(run_dir, filename + '.mid')\n        tf.logging.info('Writing inferred midi file to %s', output_file)\n        midi_io.sequence_proto_to_midi_file(sequence_prediction, output_file)\n\n        label_from_frames_output_file = os.path.join(\n            run_dir, filename + '_label_from_frames.mid')\n        tf.logging.info('Writing label from frames midi file to %s',\n                        label_from_frames_output_file)\n        sequence_label_from_frames = infer_util.pianoroll_to_note_sequence(\n            labels,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            min_duration_ms=FLAGS.min_note_duration_ms)\n        midi_io.sequence_proto_to_midi_file(sequence_label_from_frames,\n                                            label_from_frames_output_file)\n\n        label_output_file = os.path.join(run_dir, filename + '_label.mid')\n        tf.logging.info('Writing label midi file to %s', label_output_file)\n        midi_io.sequence_proto_to_midi_file(sequence_label, label_output_file)\n\n         Also write a pianoroll showing acoustic model output vs labels.\n        pianoroll_output_file = os.path.join(run_dir,\n                                             filename + '_pianoroll.png')\n        tf.logging.info('Writing acoustic logit/label file to %s',\n                        pianoroll_output_file)\n        with tf.gfile.GFile(pianoroll_output_file, mode='w') as f:\n          scipy.misc.imsave(\n              f, infer_util.posterior_pianoroll_image(\n                  logits, sequence_prediction, labels, overlap=True,\n                  frames_per_second=data.hparams_frames_per_second(\n                      hparams)))\n\n        summary_writer.flush()\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if FLAGS.acoustic_checkpoint_filename:\n    acoustic_checkpoint = os.path.join(\n        os.path.expanduser(FLAGS.acoustic_run_dir), 'train',\n        FLAGS.acoustic_checkpoint_filename)\n  else:\n    acoustic_checkpoint = tf.train.latest_checkpoint(\n        os.path.join(os.path.expanduser(FLAGS.acoustic_run_dir), 'train'))\n\n  run_dir = os.path.expanduser(FLAGS.run_dir)\n\n  hparams = tf_utils.merge_hparams(\n      constants.DEFAULT_HPARAMS, model.get_default_hparams())\n  hparams.parse(FLAGS.hparams)\n\n  tf.gfile.MakeDirs(run_dir)\n\n  model_inference(\n      acoustic_checkpoint=acoustic_checkpoint,\n      hparams=hparams,\n      examples_path=FLAGS.examples_path,\n      run_dir=run_dir)\n\n\ndef console_entry_point():\n  tf.app.run(main)\n\nif __name__ == '__main__':\n  console_entry_point()\n", "comments": "   inference onset conditioned model        copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     internal imports    build acoustic model within  acoustic  scope isolate    variables models     the checkpoints new scopes     define metrics     use custom init function restore acoustic language models    separate checkpoints     we expect length 1 batch size 1     these length flattened     make filenames unix friendly     also write pianoroll showing acoustic model output vs labels  ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Inference for onset conditioned model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport re\nimport time\n\n# internal imports\n\nimport numpy as np\nimport scipy\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom magenta.common import tf_utils\nfrom magenta.models.onsets_frames_transcription import constants\nfrom magenta.models.onsets_frames_transcription import data\nfrom magenta.models.onsets_frames_transcription import infer_util\nfrom magenta.models.onsets_frames_transcription import model\nfrom magenta.music import midi_io\n\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    'acoustic_run_dir', None,\n    'Path to look for acoustic checkpoints. Should contain subdir `train`.')\ntf.app.flags.DEFINE_string(\n    'acoustic_checkpoint_filename', None,\n    'Filename of the checkpoint to use. If not specified, will use the latest '\n    'checkpoint')\ntf.app.flags.DEFINE_string(\n    'examples_path', None,\n    'Path to TFRecord of test examples.')\ntf.app.flags.DEFINE_string(\n    'run_dir', '~/tmp/onsets_frames/infer',\n    'Path to store output midi files and summary events.')\ntf.app.flags.DEFINE_string(\n    'hparams',\n    'onset_mode=length_ms,onset_length=32',\n    'A comma-separated list of `name=value` hyperparameter values.')\ntf.app.flags.DEFINE_float(\n    'frame_threshold', 0.5,\n    'Threshold to use when sampling from the acoustic model.')\ntf.app.flags.DEFINE_float(\n    'onset_threshold', 0.5,\n    'Threshold to use when sampling from the acoustic model.')\ntf.app.flags.DEFINE_integer(\n    'max_seconds_per_sequence', 0,\n    'If set, will truncate sequences to be at most this many seconds long.')\ntf.app.flags.DEFINE_integer(\n    'min_note_duration_ms', 0,\n    'Notes shorter than this duration will be ignored when computing metrics.')\ntf.app.flags.DEFINE_boolean(\n    'require_onset', True,\n    'If set, require an onset prediction for a new note to start.')\ntf.app.flags.DEFINE_string(\n    'log', 'INFO',\n    'The threshold for what messages will be logged: '\n    'DEBUG, INFO, WARN, ERROR, or FATAL.')\n\n\ndef model_inference(acoustic_checkpoint, hparams, examples_path, run_dir):\n  tf.logging.info('acoustic_checkpoint=%s', acoustic_checkpoint)\n  tf.logging.info('examples_path=%s', examples_path)\n  tf.logging.info('run_dir=%s', run_dir)\n\n  with tf.Graph().as_default():\n    num_dims = constants.MIDI_PITCHES\n\n    # Build the acoustic model within an 'acoustic' scope to isolate its\n    # variables from the other models.\n    with tf.variable_scope('acoustic'):\n      truncated_length = 0\n      if FLAGS.max_seconds_per_sequence:\n        truncated_length = int(\n            math.ceil((FLAGS.max_seconds_per_sequence *\n                       data.hparams_frames_per_second(hparams))))\n      acoustic_data_provider, _ = data.provide_batch(\n          batch_size=1,\n          examples=examples_path,\n          hparams=hparams,\n          is_training=False,\n          truncated_length=truncated_length)\n\n      _, _, data_labels, _, _ = model.get_model(\n          acoustic_data_provider, hparams, is_training=False)\n\n    # The checkpoints won't have the new scopes.\n    acoustic_variables = {\n        re.sub(r'^acoustic/', '', var.op.name): var\n        for var in slim.get_variables(scope='acoustic/')\n    }\n    acoustic_restore = tf.train.Saver(acoustic_variables)\n\n    onset_probs_flat = tf.get_default_graph().get_tensor_by_name(\n        'acoustic/onsets/onset_probs_flat:0')\n    frame_probs_flat = tf.get_default_graph().get_tensor_by_name(\n        'acoustic/frame_probs_flat:0')\n\n    # Define some metrics.\n    (metrics_to_updates,\n     metric_note_precision,\n     metric_note_recall,\n     metric_note_f1,\n     metric_note_precision_with_offsets,\n     metric_note_recall_with_offsets,\n     metric_note_f1_with_offsets,\n     metric_frame_labels,\n     metric_frame_predictions) = infer_util.define_metrics(num_dims)\n\n    summary_op = tf.summary.merge_all()\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    global_step_increment = global_step.assign_add(1)\n\n    # Use a custom init function to restore the acoustic and language models\n    # from their separate checkpoints.\n    def init_fn(unused_self, sess):\n      acoustic_restore.restore(sess, acoustic_checkpoint)\n\n    scaffold = tf.train.Scaffold(init_fn=init_fn)\n    session_creator = tf.train.ChiefSessionCreator(\n        scaffold=scaffold)\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n      tf.logging.info('running session')\n      summary_writer = tf.summary.FileWriter(\n          logdir=run_dir, graph=sess.graph)\n\n      tf.logging.info('Inferring for %d batches',\n                      acoustic_data_provider.num_batches)\n      infer_times = []\n      num_frames = []\n      for unused_i in range(acoustic_data_provider.num_batches):\n        start_time = time.time()\n        labels, filenames, note_sequences, logits, onset_logits = sess.run([\n            data_labels,\n            acoustic_data_provider.filenames,\n            acoustic_data_provider.note_sequences,\n            frame_probs_flat,\n            onset_probs_flat,\n        ])\n        # We expect these all to be length 1 because batch size is 1.\n        assert len(filenames) == len(note_sequences) == 1\n        # These should be the same length and have been flattened.\n        assert len(labels) == len(logits) == len(onset_logits)\n\n        frame_predictions = logits > FLAGS.frame_threshold\n        if FLAGS.require_onset:\n          onset_predictions = onset_logits > FLAGS.onset_threshold\n        else:\n          onset_predictions = None\n\n        sequence_prediction = infer_util.pianoroll_to_note_sequence(\n            frame_predictions,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            min_duration_ms=FLAGS.min_note_duration_ms,\n            onset_predictions=onset_predictions)\n\n        end_time = time.time()\n        infer_time = end_time - start_time\n        infer_times.append(infer_time)\n        num_frames.append(logits.shape[0])\n        tf.logging.info(\n            'Infer time %f, frames %d, frames/sec %f, running average %f',\n            infer_time, logits.shape[0],\n            logits.shape[0] / infer_time,\n            np.sum(num_frames) / np.sum(infer_times))\n\n        tf.logging.info('Scoring sequence %s', filenames[0])\n        sequence_label = infer_util.score_sequence(\n            sess,\n            global_step_increment,\n            summary_op,\n            summary_writer,\n            metrics_to_updates,\n            metric_note_precision,\n            metric_note_recall,\n            metric_note_f1,\n            metric_note_precision_with_offsets,\n            metric_note_recall_with_offsets,\n            metric_note_f1_with_offsets,\n            metric_frame_labels,\n            metric_frame_predictions,\n            frame_labels=labels,\n            sequence_prediction=sequence_prediction,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            note_sequence_str_label=note_sequences[0],\n            min_duration_ms=FLAGS.min_note_duration_ms,\n            sequence_id=filenames[0])\n\n        # Make filenames UNIX-friendly.\n        filename = filenames[0].replace('/', '_').replace(':', '.')\n        output_file = os.path.join(run_dir, filename + '.mid')\n        tf.logging.info('Writing inferred midi file to %s', output_file)\n        midi_io.sequence_proto_to_midi_file(sequence_prediction, output_file)\n\n        label_from_frames_output_file = os.path.join(\n            run_dir, filename + '_label_from_frames.mid')\n        tf.logging.info('Writing label from frames midi file to %s',\n                        label_from_frames_output_file)\n        sequence_label_from_frames = infer_util.pianoroll_to_note_sequence(\n            labels,\n            frames_per_second=data.hparams_frames_per_second(hparams),\n            min_duration_ms=FLAGS.min_note_duration_ms)\n        midi_io.sequence_proto_to_midi_file(sequence_label_from_frames,\n                                            label_from_frames_output_file)\n\n        label_output_file = os.path.join(run_dir, filename + '_label.mid')\n        tf.logging.info('Writing label midi file to %s', label_output_file)\n        midi_io.sequence_proto_to_midi_file(sequence_label, label_output_file)\n\n        # Also write a pianoroll showing acoustic model output vs labels.\n        pianoroll_output_file = os.path.join(run_dir,\n                                             filename + '_pianoroll.png')\n        tf.logging.info('Writing acoustic logit/label file to %s',\n                        pianoroll_output_file)\n        with tf.gfile.GFile(pianoroll_output_file, mode='w') as f:\n          scipy.misc.imsave(\n              f, infer_util.posterior_pianoroll_image(\n                  logits, sequence_prediction, labels, overlap=True,\n                  frames_per_second=data.hparams_frames_per_second(\n                      hparams)))\n\n        summary_writer.flush()\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if FLAGS.acoustic_checkpoint_filename:\n    acoustic_checkpoint = os.path.join(\n        os.path.expanduser(FLAGS.acoustic_run_dir), 'train',\n        FLAGS.acoustic_checkpoint_filename)\n  else:\n    acoustic_checkpoint = tf.train.latest_checkpoint(\n        os.path.join(os.path.expanduser(FLAGS.acoustic_run_dir), 'train'))\n\n  run_dir = os.path.expanduser(FLAGS.run_dir)\n\n  hparams = tf_utils.merge_hparams(\n      constants.DEFAULT_HPARAMS, model.get_default_hparams())\n  hparams.parse(FLAGS.hparams)\n\n  tf.gfile.MakeDirs(run_dir)\n\n  model_inference(\n      acoustic_checkpoint=acoustic_checkpoint,\n      hparams=hparams,\n      examples_path=FLAGS.examples_path,\n      run_dir=run_dir)\n\n\ndef console_entry_point():\n  tf.app.run(main)\n\nif __name__ == '__main__':\n  console_entry_point()\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "onsets_frames_transcription_infer.py", "id": "3fa4b0d630cca5e7c3086342fbe4365b", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-magenta/tensorflow-magenta-ca73164/magenta/models/onsets_frames_transcription/onsets_frames_transcription_infer.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:00:14Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}