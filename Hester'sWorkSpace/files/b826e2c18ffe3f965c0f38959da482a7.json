{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Segmentation results visualization on a given set of images.\n\nSee model.py for more details and usage.\n\"\"\"\nimport math\nimport os.path\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom deeplab import common\nfrom deeplab import model\nfrom deeplab.datasets import segmentation_dataset\nfrom deeplab.utils import input_generator\nfrom deeplab.utils import save_annotation\n\nslim = tf.contrib.slim\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('master', '', 'BNS name of the tensorflow server')\n\n Settings for log directories.\n\nflags.DEFINE_string('vis_logdir', None, 'Where to write the event logs.')\n\nflags.DEFINE_string('checkpoint_dir', None, 'Directory of model checkpoints.')\n\n Settings for visualizing the model.\n\nflags.DEFINE_integer('vis_batch_size', 1,\n                     'The number of images in each batch during evaluation.')\n\nflags.DEFINE_multi_integer('vis_crop_size', [513, 513],\n                           'Crop size [height, width] for visualization.')\n\nflags.DEFINE_integer('eval_interval_secs', 60 * 5,\n                     'How often (in seconds) to run evaluation.')\n\n For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or\n rates = [6, 12, 18] if output_stride = 16. Note one could use different\n atrous_rates/output_stride during training/evaluation.\nflags.DEFINE_multi_integer('atrous_rates', None,\n                           'Atrous rates for atrous spatial pyramid pooling.')\n\nflags.DEFINE_integer('output_stride', 16,\n                     'The ratio of input to output spatial resolution.')\n\n Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale test.\nflags.DEFINE_multi_float('eval_scales', [1.0],\n                         'The scales to resize images for evaluation.')\n\n Change to True for adding flipped images during test.\nflags.DEFINE_bool('add_flipped_images', False,\n                  'Add flipped images for evaluation or not.')\n\n Dataset settings.\n\nflags.DEFINE_string('dataset', 'pascal_voc_seg',\n                    'Name of the segmentation dataset.')\n\nflags.DEFINE_string('vis_split', 'val',\n                    'Which split of the dataset used for visualizing results')\n\nflags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')\n\nflags.DEFINE_enum('colormap_type', 'pascal', ['pascal', 'cityscapes'],\n                  'Visualization colormap type.')\n\nflags.DEFINE_boolean('also_save_raw_predictions', False,\n                     'Also save raw predictions.')\n\nflags.DEFINE_integer('max_number_of_iterations', 0,\n                     'Maximum number of visualization iterations. Will loop '\n                     'indefinitely upon nonpositive values.')\n\n The folder where semantic segmentation predictions are saved.\n_SEMANTIC_PREDICTION_SAVE_FOLDER = 'segmentation_results'\n\n The folder where raw semantic segmentation predictions are saved.\n_RAW_SEMANTIC_PREDICTION_SAVE_FOLDER = 'raw_segmentation_results'\n\n The format to save image.\n_IMAGE_FORMAT = '%06d_image'\n\n The format to save prediction\n_PREDICTION_FORMAT = '%06d_prediction'\n\n To evaluate Cityscapes results on the evaluation server, the labels used\n during training should be mapped to the labels for evaluation.\n_CITYSCAPES_TRAIN_ID_TO_EVAL_ID = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n                                   23, 24, 25, 26, 27, 28, 31, 32, 33]\n\n\ndef _convert_train_id_to_eval_id(prediction, train_id_to_eval_id):\n  \"\"\"Converts the predicted label for evaluation.\n\n  There are cases where the training labels are not equal to the evaluation\n  labels. This function is used to perform the conversion so that we could\n  evaluate the results on the evaluation server.\n\n  Args:\n    prediction: Semantic segmentation prediction.\n    train_id_to_eval_id: A list mapping from train id to evaluation id.\n\n  Returns:\n    Semantic segmentation prediction whose labels have been changed.\n  \"\"\"\n  converted_prediction = prediction.copy()\n  for train_id, eval_id in enumerate(train_id_to_eval_id):\n    converted_prediction[prediction == train_id] = eval_id\n\n  return converted_prediction\n\n\ndef _process_batch(sess, original_images, semantic_predictions, image_names,\n                   image_heights, image_widths, image_id_offset, save_dir,\n                   raw_save_dir, train_id_to_eval_id=None):\n  \"\"\"Evaluates one single batch qualitatively.\n\n  Args:\n    sess: TensorFlow session.\n    original_images: One batch of original images.\n    semantic_predictions: One batch of semantic segmentation predictions.\n    image_names: Image names.\n    image_heights: Image heights.\n    image_widths: Image widths.\n    image_id_offset: Image id offset for indexing images.\n    save_dir: The directory where the predictions will be saved.\n    raw_save_dir: The directory where the raw predictions will be saved.\n    train_id_to_eval_id: A list mapping from train id to eval id.\n  \"\"\"\n  (original_images,\n   semantic_predictions,\n   image_names,\n   image_heights,\n   image_widths) = sess.run([original_images, semantic_predictions,\n                             image_names, image_heights, image_widths])\n\n  num_image = semantic_predictions.shape[0]\n  for i in range(num_image):\n    image_height = np.squeeze(image_heights[i])\n    image_width = np.squeeze(image_widths[i])\n    original_image = np.squeeze(original_images[i])\n    semantic_prediction = np.squeeze(semantic_predictions[i])\n    crop_semantic_prediction = semantic_prediction[:image_height, :image_width]\n\n     Save image.\n    save_annotation.save_annotation(\n        original_image, save_dir, _IMAGE_FORMAT % (image_id_offset + i),\n        add_colormap=False)\n\n     Save prediction.\n    save_annotation.save_annotation(\n        crop_semantic_prediction, save_dir,\n        _PREDICTION_FORMAT % (image_id_offset + i), add_colormap=True,\n        colormap_type=FLAGS.colormap_type)\n\n    if FLAGS.also_save_raw_predictions:\n      image_filename = image_names[i]\n\n      if train_id_to_eval_id is not None:\n        crop_semantic_prediction = _convert_train_id_to_eval_id(\n            crop_semantic_prediction,\n            train_id_to_eval_id)\n      save_annotation.save_annotation(\n          crop_semantic_prediction, raw_save_dir, image_filename,\n          add_colormap=False)\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(tf.logging.INFO)\n   Get dataset-dependent information.\n  dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset, FLAGS.vis_split, dataset_dir=FLAGS.dataset_dir)\n  train_id_to_eval_id = None\n  if dataset.name == segmentation_dataset.get_cityscapes_dataset_name():\n    tf.logging.info('Cityscapes requires converting train_id to eval_id.')\n    train_id_to_eval_id = _CITYSCAPES_TRAIN_ID_TO_EVAL_ID\n\n   Prepare for visualization.\n  tf.gfile.MakeDirs(FLAGS.vis_logdir)\n  save_dir = os.path.join(FLAGS.vis_logdir, _SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(save_dir)\n  raw_save_dir = os.path.join(\n      FLAGS.vis_logdir, _RAW_SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(raw_save_dir)\n\n  tf.logging.info('Visualizing on %s set', FLAGS.vis_split)\n\n  g = tf.Graph()\n  with g.as_default():\n    samples = input_generator.get(dataset,\n                                  FLAGS.vis_crop_size,\n                                  FLAGS.vis_batch_size,\n                                  min_resize_value=FLAGS.min_resize_value,\n                                  max_resize_value=FLAGS.max_resize_value,\n                                  resize_factor=FLAGS.resize_factor,\n                                  dataset_split=FLAGS.vis_split,\n                                  is_training=False,\n                                  model_variant=FLAGS.model_variant)\n\n    model_options = common.ModelOptions(\n        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},\n        crop_size=FLAGS.vis_crop_size,\n        atrous_rates=FLAGS.atrous_rates,\n        output_stride=FLAGS.output_stride)\n\n    if tuple(FLAGS.eval_scales) == (1.0,):\n      tf.logging.info('Performing single-scale test.')\n      predictions = model.predict_labels(\n          samples[common.IMAGE],\n          model_options=model_options,\n          image_pyramid=FLAGS.image_pyramid)\n    else:\n      tf.logging.info('Performing multi-scale test.')\n      predictions = model.predict_labels_multi_scale(\n          samples[common.IMAGE],\n          model_options=model_options,\n          eval_scales=FLAGS.eval_scales,\n          add_flipped_images=FLAGS.add_flipped_images)\n    predictions = predictions[common.OUTPUT_TYPE]\n\n    if FLAGS.min_resize_value and FLAGS.max_resize_value:\n       Only support batch_size = 1, since we assume the dimensions of original\n       image after tf.squeeze is [height, width, 3].\n      assert FLAGS.vis_batch_size == 1\n\n       Reverse the resizing and padding operations performed in preprocessing.\n       First, we slice the valid regions (i.e., remove padded region) and then\n       we reisze the predictions back.\n      original_image = tf.squeeze(samples[common.ORIGINAL_IMAGE])\n      original_image_shape = tf.shape(original_image)\n      predictions = tf.slice(\n          predictions,\n          [0, 0, 0],\n          [1, original_image_shape[0], original_image_shape[1]])\n      resized_shape = tf.to_int32([tf.squeeze(samples[common.HEIGHT]),\n                                   tf.squeeze(samples[common.WIDTH])])\n      predictions = tf.squeeze(\n          tf.image.resize_images(tf.expand_dims(predictions, 3),\n                                 resized_shape,\n                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n                                 align_corners=True), 3)\n\n    tf.train.get_or_create_global_step()\n    saver = tf.train.Saver(slim.get_variables_to_restore())\n    sv = tf.train.Supervisor(graph=g,\n                             logdir=FLAGS.vis_logdir,\n                             init_op=tf.global_variables_initializer(),\n                             summary_op=None,\n                             summary_writer=None,\n                             global_step=None,\n                             saver=saver)\n    num_batches = int(math.ceil(\n        dataset.num_samples / float(FLAGS.vis_batch_size)))\n    last_checkpoint = None\n\n     Loop to visualize the results when new checkpoint is created.\n    num_iters = 0\n    while (FLAGS.max_number_of_iterations <= 0 or\n           num_iters < FLAGS.max_number_of_iterations):\n      num_iters += 1\n      last_checkpoint = slim.evaluation.wait_for_new_checkpoint(\n          FLAGS.checkpoint_dir, last_checkpoint)\n      start = time.time()\n      tf.logging.info(\n          'Starting visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      tf.logging.info('Visualizing with model %s', last_checkpoint)\n\n      with sv.managed_session(FLAGS.master,\n                              start_standard_services=False) as sess:\n        sv.start_queue_runners(sess)\n        sv.saver.restore(sess, last_checkpoint)\n\n        image_id_offset = 0\n        for batch in range(num_batches):\n          tf.logging.info('Visualizing batch %d / %d', batch + 1, num_batches)\n          _process_batch(sess=sess,\n                         original_images=samples[common.ORIGINAL_IMAGE],\n                         semantic_predictions=predictions,\n                         image_names=samples[common.IMAGE_NAME],\n                         image_heights=samples[common.HEIGHT],\n                         image_widths=samples[common.WIDTH],\n                         image_id_offset=image_id_offset,\n                         save_dir=save_dir,\n                         raw_save_dir=raw_save_dir,\n                         train_id_to_eval_id=train_id_to_eval_id)\n          image_id_offset += FLAGS.vis_batch_size\n\n      tf.logging.info(\n          'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)\n\n\nif __name__ == '__main__':\n  flags.mark_flag_as_required('checkpoint_dir')\n  flags.mark_flag_as_required('vis_logdir')\n  flags.mark_flag_as_required('dataset_dir')\n  tf.app.run()\n", "comments": "   segmentation results visualization given set images   see model py details usage      import math import os path import time import numpy np import tensorflow tf deeplab import common deeplab import model deeplab datasets import segmentation dataset deeplab utils import input generator deeplab utils import save annotation  slim   tf contrib slim  flags   tf app flags  flags   flags flags  flags define string( master        bns name tensorflow server )    settings log directories   flags define string( vis logdir   none   where write event logs  )  flags define string( checkpoint dir   none   directory model checkpoints  )    settings visualizing model   flags define integer( vis batch size   1                        the number images batch evaluation  )  flags define multi integer( vis crop size    513  513                               crop size  height  width  visualization  )  flags define integer( eval interval secs   60   5                        how often (in seconds) run evaluation  )    for  xception 65   use atrous rates    12  24  36  output stride   8    rates    6  12  18  output stride   16  note one could use different   atrous rates output stride training evaluation  flags define multi integer( atrous rates   none                              atrous rates atrous spatial pyramid pooling  )  flags define integer( output stride   16                        the ratio input output spatial resolution  )    change  0 5  0 75  1 0  1 25  1 5  1 75  multi scale test  flags define multi float( eval scales    1 0                             the scales resize images evaluation  )    change true adding flipped images test  flags define bool( add flipped images   false                     add flipped images evaluation  )    dataset settings   flags define string( dataset    pascal voc seg                        name segmentation dataset  )  flags define string( vis split    val                        which split dataset used visualizing results )  flags define string( dataset dir   none   where dataset reside  )  flags define enum( colormap type    pascal     pascal    cityscapes                       visualization colormap type  )  flags define boolean( also save raw predictions   false                        also save raw predictions  )  flags define integer( max number iterations   0                        maximum number visualization iterations  will loop                         indefinitely upon nonpositive values  )    the folder semantic segmentation predictions saved   semantic prediction save folder    segmentation results     the folder raw semantic segmentation predictions saved   raw semantic prediction save folder    raw segmentation results     the format save image   image format     06d image     the format save prediction  prediction format     06d prediction     to evaluate cityscapes results evaluation server  labels used   training mapped labels evaluation   cityscapes train id to eval id    7  8  11  12  13  17  19  20  21  22                                     23  24  25  26  27  28  31  32  33    def  convert train id eval id(prediction  train id eval id)       converts predicted label evaluation     there cases training labels equal evaluation   labels  this function used perform conversion could   evaluate results evaluation server     args      prediction  semantic segmentation prediction      train id eval id  a list mapping train id evaluation id     returns      semantic segmentation prediction whose labels changed          converted prediction   prediction copy()   train id  eval id enumerate(train id eval id)      converted prediction prediction    train id    eval id    return converted prediction   def  process batch(sess  original images  semantic predictions  image names                     image heights  image widths  image id offset  save dir                     raw save dir  train id eval id none)       evaluates one single batch qualitatively     args      sess  tensorflow session      original images  one batch original images      semantic predictions  one batch semantic segmentation predictions      image names  image names      image heights  image heights      image widths  image widths      image id offset  image id offset indexing images      save dir  the directory predictions saved      raw save dir  the directory raw predictions saved      train id eval id  a list mapping train id eval id           copyright 2018 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       settings log directories     settings visualizing model     for  xception 65   use atrous rates    12  24  36  output stride   8     rates    6  12  18  output stride   16  note one could use different    atrous rates output stride training evaluation     change  0 5  0 75  1 0  1 25  1 5  1 75  multi scale test     change true adding flipped images test     dataset settings     the folder semantic segmentation predictions saved     the folder raw semantic segmentation predictions saved     the format save image     the format save prediction    to evaluate cityscapes results evaluation server  labels used    training mapped labels evaluation     save image     save prediction     get dataset dependent information     prepare visualization     only support batch size   1  since assume dimensions original    image tf squeeze  height  width  3      reverse resizing padding operations performed preprocessing     first  slice valid regions (i e   remove padded region)    reisze predictions back     loop visualize results new checkpoint created  ", "content": "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Segmentation results visualization on a given set of images.\n\nSee model.py for more details and usage.\n\"\"\"\nimport math\nimport os.path\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom deeplab import common\nfrom deeplab import model\nfrom deeplab.datasets import segmentation_dataset\nfrom deeplab.utils import input_generator\nfrom deeplab.utils import save_annotation\n\nslim = tf.contrib.slim\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('master', '', 'BNS name of the tensorflow server')\n\n# Settings for log directories.\n\nflags.DEFINE_string('vis_logdir', None, 'Where to write the event logs.')\n\nflags.DEFINE_string('checkpoint_dir', None, 'Directory of model checkpoints.')\n\n# Settings for visualizing the model.\n\nflags.DEFINE_integer('vis_batch_size', 1,\n                     'The number of images in each batch during evaluation.')\n\nflags.DEFINE_multi_integer('vis_crop_size', [513, 513],\n                           'Crop size [height, width] for visualization.')\n\nflags.DEFINE_integer('eval_interval_secs', 60 * 5,\n                     'How often (in seconds) to run evaluation.')\n\n# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or\n# rates = [6, 12, 18] if output_stride = 16. Note one could use different\n# atrous_rates/output_stride during training/evaluation.\nflags.DEFINE_multi_integer('atrous_rates', None,\n                           'Atrous rates for atrous spatial pyramid pooling.')\n\nflags.DEFINE_integer('output_stride', 16,\n                     'The ratio of input to output spatial resolution.')\n\n# Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale test.\nflags.DEFINE_multi_float('eval_scales', [1.0],\n                         'The scales to resize images for evaluation.')\n\n# Change to True for adding flipped images during test.\nflags.DEFINE_bool('add_flipped_images', False,\n                  'Add flipped images for evaluation or not.')\n\n# Dataset settings.\n\nflags.DEFINE_string('dataset', 'pascal_voc_seg',\n                    'Name of the segmentation dataset.')\n\nflags.DEFINE_string('vis_split', 'val',\n                    'Which split of the dataset used for visualizing results')\n\nflags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')\n\nflags.DEFINE_enum('colormap_type', 'pascal', ['pascal', 'cityscapes'],\n                  'Visualization colormap type.')\n\nflags.DEFINE_boolean('also_save_raw_predictions', False,\n                     'Also save raw predictions.')\n\nflags.DEFINE_integer('max_number_of_iterations', 0,\n                     'Maximum number of visualization iterations. Will loop '\n                     'indefinitely upon nonpositive values.')\n\n# The folder where semantic segmentation predictions are saved.\n_SEMANTIC_PREDICTION_SAVE_FOLDER = 'segmentation_results'\n\n# The folder where raw semantic segmentation predictions are saved.\n_RAW_SEMANTIC_PREDICTION_SAVE_FOLDER = 'raw_segmentation_results'\n\n# The format to save image.\n_IMAGE_FORMAT = '%06d_image'\n\n# The format to save prediction\n_PREDICTION_FORMAT = '%06d_prediction'\n\n# To evaluate Cityscapes results on the evaluation server, the labels used\n# during training should be mapped to the labels for evaluation.\n_CITYSCAPES_TRAIN_ID_TO_EVAL_ID = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n                                   23, 24, 25, 26, 27, 28, 31, 32, 33]\n\n\ndef _convert_train_id_to_eval_id(prediction, train_id_to_eval_id):\n  \"\"\"Converts the predicted label for evaluation.\n\n  There are cases where the training labels are not equal to the evaluation\n  labels. This function is used to perform the conversion so that we could\n  evaluate the results on the evaluation server.\n\n  Args:\n    prediction: Semantic segmentation prediction.\n    train_id_to_eval_id: A list mapping from train id to evaluation id.\n\n  Returns:\n    Semantic segmentation prediction whose labels have been changed.\n  \"\"\"\n  converted_prediction = prediction.copy()\n  for train_id, eval_id in enumerate(train_id_to_eval_id):\n    converted_prediction[prediction == train_id] = eval_id\n\n  return converted_prediction\n\n\ndef _process_batch(sess, original_images, semantic_predictions, image_names,\n                   image_heights, image_widths, image_id_offset, save_dir,\n                   raw_save_dir, train_id_to_eval_id=None):\n  \"\"\"Evaluates one single batch qualitatively.\n\n  Args:\n    sess: TensorFlow session.\n    original_images: One batch of original images.\n    semantic_predictions: One batch of semantic segmentation predictions.\n    image_names: Image names.\n    image_heights: Image heights.\n    image_widths: Image widths.\n    image_id_offset: Image id offset for indexing images.\n    save_dir: The directory where the predictions will be saved.\n    raw_save_dir: The directory where the raw predictions will be saved.\n    train_id_to_eval_id: A list mapping from train id to eval id.\n  \"\"\"\n  (original_images,\n   semantic_predictions,\n   image_names,\n   image_heights,\n   image_widths) = sess.run([original_images, semantic_predictions,\n                             image_names, image_heights, image_widths])\n\n  num_image = semantic_predictions.shape[0]\n  for i in range(num_image):\n    image_height = np.squeeze(image_heights[i])\n    image_width = np.squeeze(image_widths[i])\n    original_image = np.squeeze(original_images[i])\n    semantic_prediction = np.squeeze(semantic_predictions[i])\n    crop_semantic_prediction = semantic_prediction[:image_height, :image_width]\n\n    # Save image.\n    save_annotation.save_annotation(\n        original_image, save_dir, _IMAGE_FORMAT % (image_id_offset + i),\n        add_colormap=False)\n\n    # Save prediction.\n    save_annotation.save_annotation(\n        crop_semantic_prediction, save_dir,\n        _PREDICTION_FORMAT % (image_id_offset + i), add_colormap=True,\n        colormap_type=FLAGS.colormap_type)\n\n    if FLAGS.also_save_raw_predictions:\n      image_filename = image_names[i]\n\n      if train_id_to_eval_id is not None:\n        crop_semantic_prediction = _convert_train_id_to_eval_id(\n            crop_semantic_prediction,\n            train_id_to_eval_id)\n      save_annotation.save_annotation(\n          crop_semantic_prediction, raw_save_dir, image_filename,\n          add_colormap=False)\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(tf.logging.INFO)\n  # Get dataset-dependent information.\n  dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset, FLAGS.vis_split, dataset_dir=FLAGS.dataset_dir)\n  train_id_to_eval_id = None\n  if dataset.name == segmentation_dataset.get_cityscapes_dataset_name():\n    tf.logging.info('Cityscapes requires converting train_id to eval_id.')\n    train_id_to_eval_id = _CITYSCAPES_TRAIN_ID_TO_EVAL_ID\n\n  # Prepare for visualization.\n  tf.gfile.MakeDirs(FLAGS.vis_logdir)\n  save_dir = os.path.join(FLAGS.vis_logdir, _SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(save_dir)\n  raw_save_dir = os.path.join(\n      FLAGS.vis_logdir, _RAW_SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(raw_save_dir)\n\n  tf.logging.info('Visualizing on %s set', FLAGS.vis_split)\n\n  g = tf.Graph()\n  with g.as_default():\n    samples = input_generator.get(dataset,\n                                  FLAGS.vis_crop_size,\n                                  FLAGS.vis_batch_size,\n                                  min_resize_value=FLAGS.min_resize_value,\n                                  max_resize_value=FLAGS.max_resize_value,\n                                  resize_factor=FLAGS.resize_factor,\n                                  dataset_split=FLAGS.vis_split,\n                                  is_training=False,\n                                  model_variant=FLAGS.model_variant)\n\n    model_options = common.ModelOptions(\n        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},\n        crop_size=FLAGS.vis_crop_size,\n        atrous_rates=FLAGS.atrous_rates,\n        output_stride=FLAGS.output_stride)\n\n    if tuple(FLAGS.eval_scales) == (1.0,):\n      tf.logging.info('Performing single-scale test.')\n      predictions = model.predict_labels(\n          samples[common.IMAGE],\n          model_options=model_options,\n          image_pyramid=FLAGS.image_pyramid)\n    else:\n      tf.logging.info('Performing multi-scale test.')\n      predictions = model.predict_labels_multi_scale(\n          samples[common.IMAGE],\n          model_options=model_options,\n          eval_scales=FLAGS.eval_scales,\n          add_flipped_images=FLAGS.add_flipped_images)\n    predictions = predictions[common.OUTPUT_TYPE]\n\n    if FLAGS.min_resize_value and FLAGS.max_resize_value:\n      # Only support batch_size = 1, since we assume the dimensions of original\n      # image after tf.squeeze is [height, width, 3].\n      assert FLAGS.vis_batch_size == 1\n\n      # Reverse the resizing and padding operations performed in preprocessing.\n      # First, we slice the valid regions (i.e., remove padded region) and then\n      # we reisze the predictions back.\n      original_image = tf.squeeze(samples[common.ORIGINAL_IMAGE])\n      original_image_shape = tf.shape(original_image)\n      predictions = tf.slice(\n          predictions,\n          [0, 0, 0],\n          [1, original_image_shape[0], original_image_shape[1]])\n      resized_shape = tf.to_int32([tf.squeeze(samples[common.HEIGHT]),\n                                   tf.squeeze(samples[common.WIDTH])])\n      predictions = tf.squeeze(\n          tf.image.resize_images(tf.expand_dims(predictions, 3),\n                                 resized_shape,\n                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n                                 align_corners=True), 3)\n\n    tf.train.get_or_create_global_step()\n    saver = tf.train.Saver(slim.get_variables_to_restore())\n    sv = tf.train.Supervisor(graph=g,\n                             logdir=FLAGS.vis_logdir,\n                             init_op=tf.global_variables_initializer(),\n                             summary_op=None,\n                             summary_writer=None,\n                             global_step=None,\n                             saver=saver)\n    num_batches = int(math.ceil(\n        dataset.num_samples / float(FLAGS.vis_batch_size)))\n    last_checkpoint = None\n\n    # Loop to visualize the results when new checkpoint is created.\n    num_iters = 0\n    while (FLAGS.max_number_of_iterations <= 0 or\n           num_iters < FLAGS.max_number_of_iterations):\n      num_iters += 1\n      last_checkpoint = slim.evaluation.wait_for_new_checkpoint(\n          FLAGS.checkpoint_dir, last_checkpoint)\n      start = time.time()\n      tf.logging.info(\n          'Starting visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      tf.logging.info('Visualizing with model %s', last_checkpoint)\n\n      with sv.managed_session(FLAGS.master,\n                              start_standard_services=False) as sess:\n        sv.start_queue_runners(sess)\n        sv.saver.restore(sess, last_checkpoint)\n\n        image_id_offset = 0\n        for batch in range(num_batches):\n          tf.logging.info('Visualizing batch %d / %d', batch + 1, num_batches)\n          _process_batch(sess=sess,\n                         original_images=samples[common.ORIGINAL_IMAGE],\n                         semantic_predictions=predictions,\n                         image_names=samples[common.IMAGE_NAME],\n                         image_heights=samples[common.HEIGHT],\n                         image_widths=samples[common.WIDTH],\n                         image_id_offset=image_id_offset,\n                         save_dir=save_dir,\n                         raw_save_dir=raw_save_dir,\n                         train_id_to_eval_id=train_id_to_eval_id)\n          image_id_offset += FLAGS.vis_batch_size\n\n      tf.logging.info(\n          'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)\n\n\nif __name__ == '__main__':\n  flags.mark_flag_as_required('checkpoint_dir')\n  flags.mark_flag_as_required('vis_logdir')\n  flags.mark_flag_as_required('dataset_dir')\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "vis.py", "id": "b826e2c18ffe3f965c0f38959da482a7", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/deeplab/vis.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}