{"author": "rushter", "code": "from __future__ import print_function\n\nimport logging\nimport random\n\nimport numpy as np\nimport sys\n\nfrom mla.datasets import load_nietzsche\nfrom mla.neuralnet import NeuralNet\nfrom mla.neuralnet.constraints import SmallNorm\nfrom mla.neuralnet.layers import Activation, Dense\nfrom mla.neuralnet.layers.recurrent import LSTM, RNN\nfrom mla.neuralnet.optimizers import RMSprop\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\n\n\ndef sample(preds, temperature=1.0):\n    \n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n\nX, y, text, chars, char_indices, indices_char = load_nietzsche()\n\nitems_count = X.shape[0] - (X.shape[0] % 64)\nmaxlen = X.shape[1]\nX = X[0:items_count]\ny = y[0:items_count]\n\nprint(X.shape, y.shape)\n\n# rnn_layer = RNN(128, return_sequences=False)\nrnn_layer = LSTM(128, return_sequences=False, )\n\nmodel = NeuralNet(\n    layers=[\n        rnn_layer,\n        # Flatten(),\n        # TimeStepSlicer(-1),\n        Dense(X.shape[2]),\n        Activation('softmax'),\n    ],\n    loss='categorical_crossentropy',\n    optimizer=RMSprop(learning_rate=0.01),\n    metric='accuracy',\n    batch_size=64,\n    max_epochs=1,\n    shuffle=False,\n\n)\n\nfor _ in range(25):\n    model.fit(X, y)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    generated = ''\n    sentence = text[start_index: start_index + maxlen]\n    generated += sentence\n    print('----- Generating with seed: \"' + sentence + '\"')\n    sys.stdout.write(generated)\n    for i in range(100):\n        x = np.zeros((64, maxlen, len(chars)))\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1.\n        preds = model.predict(x)[0]\n        next_index = sample(preds, 0.5)\n        next_char = indices_char[next_index]\n\n        generated += next_char\n        sentence = sentence[1:] + next_char\n\n        sys.stdout.write(next_char)\n        sys.stdout.flush()\n    print()\n", "comments": "  example taken  https   github com fchollet keras blob master examples lstm text generation py    helper function sample index probability array    round number sequences batch processing    lstm or rnn    rnn layer   rnn(128  return sequences false)    flatten()     timestepslicer( 1)  ", "content": "from __future__ import print_function\n\nimport logging\nimport random\n\nimport numpy as np\nimport sys\n\nfrom mla.datasets import load_nietzsche\nfrom mla.neuralnet import NeuralNet\nfrom mla.neuralnet.constraints import SmallNorm\nfrom mla.neuralnet.layers import Activation, Dense\nfrom mla.neuralnet.layers.recurrent import LSTM, RNN\nfrom mla.neuralnet.optimizers import RMSprop\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\n# Example taken from: https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n\ndef sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n\nX, y, text, chars, char_indices, indices_char = load_nietzsche()\n# Round the number of sequences for batch processing\nitems_count = X.shape[0] - (X.shape[0] % 64)\nmaxlen = X.shape[1]\nX = X[0:items_count]\ny = y[0:items_count]\n\nprint(X.shape, y.shape)\n# LSTM OR RNN\n# rnn_layer = RNN(128, return_sequences=False)\nrnn_layer = LSTM(128, return_sequences=False, )\n\nmodel = NeuralNet(\n    layers=[\n        rnn_layer,\n        # Flatten(),\n        # TimeStepSlicer(-1),\n        Dense(X.shape[2]),\n        Activation('softmax'),\n    ],\n    loss='categorical_crossentropy',\n    optimizer=RMSprop(learning_rate=0.01),\n    metric='accuracy',\n    batch_size=64,\n    max_epochs=1,\n    shuffle=False,\n\n)\n\nfor _ in range(25):\n    model.fit(X, y)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    generated = ''\n    sentence = text[start_index: start_index + maxlen]\n    generated += sentence\n    print('----- Generating with seed: \"' + sentence + '\"')\n    sys.stdout.write(generated)\n    for i in range(100):\n        x = np.zeros((64, maxlen, len(chars)))\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1.\n        preds = model.predict(x)[0]\n        next_index = sample(preds, 0.5)\n        next_char = indices_char[next_index]\n\n        generated += next_char\n        sentence = sentence[1:] + next_char\n\n        sys.stdout.write(next_char)\n        sys.stdout.flush()\n    print()\n", "description": "Minimal and clean examples of machine learning algorithms", "file_name": "nnet_rnn_text_generation.py", "id": "df67de26ee6be23f15cb0064ec32b3a5", "language": "Python", "project_name": "MLAlgorithms", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/rushter-MLAlgorithms/rushter-MLAlgorithms-d398777/examples/nnet_rnn_text_generation.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:25:48Z", "url": "https://github.com/rushter/MLAlgorithms", "wiki": false}