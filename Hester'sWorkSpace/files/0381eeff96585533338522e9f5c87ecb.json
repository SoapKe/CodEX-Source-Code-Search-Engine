{"author": "yunjey", "code": "\n\n\n\nimport torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\ntransform = transforms.Compose([\n    transforms.Scale(40),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32),\n    transforms.ToTensor()])\n\n\ntrain_dataset = dsets.CIFAR10(root='./data/',\n                               train=True, \n                               transform=transform,\n                               download=True)\n\ntest_dataset = dsets.CIFAR10(root='./data/',\n                              train=False, \n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=100, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=100, \n                                          shuffle=False)\n\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(3, 16)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, layers[0])\n        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n        self.avg_pool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64, num_classes)\n        \n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(\n                conv3x3(self.in_channels, out_channels, stride=stride),\n                nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n    \nresnet = ResNet(ResidualBlock, [2, 2, 2, 2])\n\n\n\ncriterion = nn.CrossEntropyLoss()\nlr = 0.001\noptimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n\n\nfor epoch in range(80):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print (\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f\" %(epoch+1, 80, i+1, 500, loss.data[0]))\n\n    \n    if (epoch+1) % 20 == 0:\n        lr /= 3\n        optimizer = torch.optim.Adam(resnet.parameters(), lr=lr) \n\n\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images)\n    outputs = resnet(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n\n\ntorch.save(resnet.state_dict(), 'resnet.pkl')", "comments": "  implementation https   arxiv org pdf 1512 03385 pdf     see section 4 2 model architecture cifar 10     some part code referenced     https   github com pytorch vision blob master torchvision models resnet py    image preprocessing     cifar 10 dataset    data loader (input pipeline)    3x3 convolution    residual block    resnet module    loss optimizer    training     forward   backward   optimize    decaying learning rate    test    save model ", "content": "# Implementation of https://arxiv.org/pdf/1512.03385.pdf.\n# See section 4.2 for model architecture on CIFAR-10.\n# Some part of the code was referenced below.\n# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\nimport torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n# Image Preprocessing \ntransform = transforms.Compose([\n    transforms.Scale(40),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32),\n    transforms.ToTensor()])\n\n# CIFAR-10 Dataset\ntrain_dataset = dsets.CIFAR10(root='./data/',\n                               train=True, \n                               transform=transform,\n                               download=True)\n\ntest_dataset = dsets.CIFAR10(root='./data/',\n                              train=False, \n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=100, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=100, \n                                          shuffle=False)\n\n# 3x3 Convolution\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\n# Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n# ResNet Module\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(3, 16)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, layers[0])\n        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n        self.avg_pool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64, num_classes)\n        \n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(\n                conv3x3(self.in_channels, out_channels, stride=stride),\n                nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n    \nresnet = ResNet(ResidualBlock, [2, 2, 2, 2])\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\nlr = 0.001\noptimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n\n# Training \nfor epoch in range(80):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print (\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f\" %(epoch+1, 80, i+1, 500, loss.data[0]))\n\n    # Decaying Learning Rate\n    if (epoch+1) % 20 == 0:\n        lr /= 3\n        optimizer = torch.optim.Adam(resnet.parameters(), lr=lr) \n\n# Test\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images)\n    outputs = resnet(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n\n# Save the Model\ntorch.save(resnet.state_dict(), 'resnet.pkl')", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main.py", "id": "0381eeff96585533338522e9f5c87ecb", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/02-intermediate/deep_residual_network/main.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}