{"author": "tflearn", "code": "import tensorflow as tf\nimport tflearn\nimport unittest\nimport os\n\nclass TestLayers(unittest.TestCase):\n    \n\n    def test_core_layers(self):\n\n        X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n        Y_nand = [[1.], [1.], [1.], [0.]]\n        Y_or = [[0.], [1.], [1.], [1.]]\n\n        \n        with tf.Graph().as_default():\n            \n            g = tflearn.input_data(shape=[None, 2])\n\n            \n            g_nand = tflearn.fully_connected(g, 32, activation='linear')\n            g_nand = tflearn.fully_connected(g_nand, 32, activation='linear')\n            g_nand = tflearn.fully_connected(g_nand, 1, activation='sigmoid')\n            g_nand = tflearn.regression(g_nand, optimizer='sgd',\n                                        learning_rate=2.,\n                                        loss='binary_crossentropy')\n            \n            g_or = tflearn.fully_connected(g, 32, activation='linear')\n            g_or = tflearn.fully_connected(g_or, 32, activation='linear')\n            g_or = tflearn.fully_connected(g_or, 1, activation='sigmoid')\n            g_or = tflearn.regression(g_or, optimizer='sgd',\n                                      learning_rate=2.,\n                                      loss='binary_crossentropy')\n            \n            g_xor = tflearn.merge([g_nand, g_or], mode='elemwise_mul')\n\n            \n            m = tflearn.DNN(g_xor)\n            m.fit(X, [Y_nand, Y_or], n_epoch=400, snapshot_epoch=False)\n\n            \n            self.assertLess(m.predict([[0., 0.]])[0][0], 0.01)\n            self.assertGreater(m.predict([[0., 1.]])[0][0], 0.9)\n            self.assertGreater(m.predict([[1., 0.]])[0][0], 0.9)\n            self.assertLess(m.predict([[1., 1.]])[0][0], 0.01)\n\n        \n        with tf.Graph().as_default():\n            net = tflearn.input_data(shape=[None, 2])\n            net = tflearn.flatten(net)\n            net = tflearn.reshape(net, new_shape=[-1])\n            net = tflearn.activation(net, 'relu')\n            net = tflearn.dropout(net, 0.5)\n            net = tflearn.single_unit(net)\n\n    def test_conv_layers(self):\n\n        X = [[0., 0., 0., 0.], [1., 1., 1., 1.], [0., 0., 1., 0.], [1., 1., 1., 0.]]\n        Y = [[1., 0.], [0., 1.], [1., 0.], [0., 1.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2, activation='relu')\n            g = tflearn.max_pool_2d(g, 2)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n            m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n            \n            #self.assertGreater(m.predict([[1., 0., 0., 0.]])[0][0], 0.5)\n\n        \n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2)\n            g = tflearn.conv_2d(g, 4, 1)\n            g = tflearn.conv_2d_transpose(g, 4, 2, [2, 2])\n            g = tflearn.max_pool_2d(g, 2)\n\n    def test_recurrent_layers(self):\n\n        X = [[1, 3, 5, 7], [2, 4, 8, 10], [1, 5, 9, 11], [2, 6, 8, 0]]\n        Y = [[0., 1.], [1., 0.], [0., 1.], [1., 0.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.embedding(g, input_dim=12, output_dim=4)\n            g = tflearn.lstm(g, 6)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n            m.fit(X, Y, n_epoch=300, snapshot_epoch=False)\n            self.assertGreater(m.predict([[5, 9, 11, 1]])[0][1], 0.9)\n\n    def test_regression_placeholder(self):\n        \n\n        with tf.Graph().as_default():\n\n            g = tflearn.input_data(shape=[None, 2])\n            g_nand = tflearn.fully_connected(g, 1, activation='linear')\n            with tf.name_scope(\"Y\"):\n                Y_in = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"Y\")\n            tflearn.regression(g_nand, optimizer='sgd',\n                               placeholder=Y_in,\n                               learning_rate=2.,\n                               loss='binary_crossentropy', \n                               op_name=\"regression1\",\n                               name=\"Y\")\n            \n            \n            tflearn.regression(g_nand, optimizer='adam',\n                               placeholder=Y_in,\n                               learning_rate=2.,\n                               loss='binary_crossentropy', \n                               op_name=\"regression2\",\n                               name=\"Y\")\n\n            self.assertEqual(len(tf.get_collection(tf.GraphKeys.TARGETS)), 1)\n\n    def test_feed_dict_no_None(self):\n\n        X = [[0., 0., 0., 0.], [1., 1., 1., 1.], [0., 0., 1., 0.], [1., 1., 1., 0.]]\n        Y = [[1., 0.], [0., 1.], [1., 0.], [0., 1.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4], name=\"X_in\")\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2)\n            g = tflearn.conv_2d(g, 4, 1)\n            g = tflearn.max_pool_2d(g, 2)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n\n            def do_fit():\n                m.fit({\"X_in\": X, 'non_existent': X}, Y, n_epoch=30, snapshot_epoch=False)\n            self.assertRaisesRegexp(Exception, \"Feed dict asks for variable named 'non_existent' but no such variable is known to exist\", do_fit)\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "comments": "        testing layers tflearn layers                      check regression duplicate placeholders                graph definition    building network 2 optimizers    nand operator definition    or operator definition    xor merging nand or operators    training    testing    bulk tests    todo  fix test   self assertgreater(m predict(  1   0   0   0   ) 0  0   0 5)    bulk tests    test  use default trainable vars    practice  different two regressions ", "content": "import tensorflow as tf\nimport tflearn\nimport unittest\nimport os\n\nclass TestLayers(unittest.TestCase):\n    \"\"\"\n    Testing layers from tflearn/layers\n    \"\"\"\n\n    def test_core_layers(self):\n\n        X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n        Y_nand = [[1.], [1.], [1.], [0.]]\n        Y_or = [[0.], [1.], [1.], [1.]]\n\n        # Graph definition\n        with tf.Graph().as_default():\n            # Building a network with 2 optimizers\n            g = tflearn.input_data(shape=[None, 2])\n\n            # Nand operator definition\n            g_nand = tflearn.fully_connected(g, 32, activation='linear')\n            g_nand = tflearn.fully_connected(g_nand, 32, activation='linear')\n            g_nand = tflearn.fully_connected(g_nand, 1, activation='sigmoid')\n            g_nand = tflearn.regression(g_nand, optimizer='sgd',\n                                        learning_rate=2.,\n                                        loss='binary_crossentropy')\n            # Or operator definition\n            g_or = tflearn.fully_connected(g, 32, activation='linear')\n            g_or = tflearn.fully_connected(g_or, 32, activation='linear')\n            g_or = tflearn.fully_connected(g_or, 1, activation='sigmoid')\n            g_or = tflearn.regression(g_or, optimizer='sgd',\n                                      learning_rate=2.,\n                                      loss='binary_crossentropy')\n            # XOR merging Nand and Or operators\n            g_xor = tflearn.merge([g_nand, g_or], mode='elemwise_mul')\n\n            # Training\n            m = tflearn.DNN(g_xor)\n            m.fit(X, [Y_nand, Y_or], n_epoch=400, snapshot_epoch=False)\n\n            # Testing\n            self.assertLess(m.predict([[0., 0.]])[0][0], 0.01)\n            self.assertGreater(m.predict([[0., 1.]])[0][0], 0.9)\n            self.assertGreater(m.predict([[1., 0.]])[0][0], 0.9)\n            self.assertLess(m.predict([[1., 1.]])[0][0], 0.01)\n\n        # Bulk Tests\n        with tf.Graph().as_default():\n            net = tflearn.input_data(shape=[None, 2])\n            net = tflearn.flatten(net)\n            net = tflearn.reshape(net, new_shape=[-1])\n            net = tflearn.activation(net, 'relu')\n            net = tflearn.dropout(net, 0.5)\n            net = tflearn.single_unit(net)\n\n    def test_conv_layers(self):\n\n        X = [[0., 0., 0., 0.], [1., 1., 1., 1.], [0., 0., 1., 0.], [1., 1., 1., 0.]]\n        Y = [[1., 0.], [0., 1.], [1., 0.], [0., 1.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2, activation='relu')\n            g = tflearn.max_pool_2d(g, 2)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n            m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n            # TODO: Fix test\n            #self.assertGreater(m.predict([[1., 0., 0., 0.]])[0][0], 0.5)\n\n        # Bulk Tests\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2)\n            g = tflearn.conv_2d(g, 4, 1)\n            g = tflearn.conv_2d_transpose(g, 4, 2, [2, 2])\n            g = tflearn.max_pool_2d(g, 2)\n\n    def test_recurrent_layers(self):\n\n        X = [[1, 3, 5, 7], [2, 4, 8, 10], [1, 5, 9, 11], [2, 6, 8, 0]]\n        Y = [[0., 1.], [1., 0.], [0., 1.], [1., 0.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4])\n            g = tflearn.embedding(g, input_dim=12, output_dim=4)\n            g = tflearn.lstm(g, 6)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n            m.fit(X, Y, n_epoch=300, snapshot_epoch=False)\n            self.assertGreater(m.predict([[5, 9, 11, 1]])[0][1], 0.9)\n\n    def test_regression_placeholder(self):\n        '''\n        Check that regression does not duplicate placeholders\n        '''\n\n        with tf.Graph().as_default():\n\n            g = tflearn.input_data(shape=[None, 2])\n            g_nand = tflearn.fully_connected(g, 1, activation='linear')\n            with tf.name_scope(\"Y\"):\n                Y_in = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"Y\")\n            tflearn.regression(g_nand, optimizer='sgd',\n                               placeholder=Y_in,\n                               learning_rate=2.,\n                               loss='binary_crossentropy', \n                               op_name=\"regression1\",\n                               name=\"Y\")\n            # for this test, just use the same default trainable_vars\n            # in practice, this should be different for the two regressions\n            tflearn.regression(g_nand, optimizer='adam',\n                               placeholder=Y_in,\n                               learning_rate=2.,\n                               loss='binary_crossentropy', \n                               op_name=\"regression2\",\n                               name=\"Y\")\n\n            self.assertEqual(len(tf.get_collection(tf.GraphKeys.TARGETS)), 1)\n\n    def test_feed_dict_no_None(self):\n\n        X = [[0., 0., 0., 0.], [1., 1., 1., 1.], [0., 0., 1., 0.], [1., 1., 1., 0.]]\n        Y = [[1., 0.], [0., 1.], [1., 0.], [0., 1.]]\n\n        with tf.Graph().as_default():\n            g = tflearn.input_data(shape=[None, 4], name=\"X_in\")\n            g = tflearn.reshape(g, new_shape=[-1, 2, 2, 1])\n            g = tflearn.conv_2d(g, 4, 2)\n            g = tflearn.conv_2d(g, 4, 1)\n            g = tflearn.max_pool_2d(g, 2)\n            g = tflearn.fully_connected(g, 2, activation='softmax')\n            g = tflearn.regression(g, optimizer='sgd', learning_rate=1.)\n\n            m = tflearn.DNN(g)\n\n            def do_fit():\n                m.fit({\"X_in\": X, 'non_existent': X}, Y, n_epoch=30, snapshot_epoch=False)\n            self.assertRaisesRegexp(Exception, \"Feed dict asks for variable named 'non_existent' but no such variable is known to exist\", do_fit)\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "test_layers.py", "id": "17bdad11422e23a81dd6c03684d82bc0", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/tests/test_layers.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}