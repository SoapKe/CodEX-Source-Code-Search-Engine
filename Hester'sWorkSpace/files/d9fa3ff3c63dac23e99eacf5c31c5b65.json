{"author": "chiphuyen", "code": "\"\"\" Solution for simple linear regression example using tf.data\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\n\n\ndataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n\niterator = dataset.make_initializable_iterator()\nX, Y = iterator.get_next()\n\n\nw = tf.get_variable('weights', initializer=tf.constant(0.0))\nb = tf.get_variable('bias', initializer=tf.constant(0.0))\n\n\nY_predicted = X * w + b\n\n\nloss = tf.square(Y - Y_predicted, name='loss')\n# loss = utils.huber_loss(Y, Y_predicted)\n\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n\nstart = time.time()\nwith tf.Session() as sess:\n    \n    sess.run(tf.global_variables_initializer()) \n    writer = tf.summary.FileWriter('./graphs/linear_reg', sess.graph)\n    \n    \n    for i in range(100):\n        sess.run(iterator.initializer) \n        total_loss = 0\n        try:\n            while True:\n                _, l = sess.run([optimizer, loss]) \n                total_loss += l\n        except tf.errors.OutOfRangeError:\n            pass\n            \n        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n\n    \n    writer.close() \n    \n    \n    w_out, b_out = sess.run([w, b]) \n    print('w: %f, b: %f' %(w_out, b_out))\nprint('Took: %f seconds' %(time.time() - start))\n\n\nplt.plot(data[:,0], data[:,1], 'bo', label='Real data')\nplt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data with squared error')\n# plt.plot(data[:,0], data[:,0] * (-5.883589) + 85.124306, 'g', label='Predicted data with Huber loss')\nplt.legend()\nplt.show()", "comments": "    solution simple linear regression example using tf data created chip huyen (chiphuyen cs stanford edu) cs20   tensorflow deep learning research  cs20 stanford edu lecture 03        step 1  read data    step 2  create dataset iterator    step 3  create weight bias  initialized 0    step 4  build model predict y    step 5  use square error loss function    loss   utils huber loss(y  y predicted)    step 6  using gradient descent learning rate 0 001 minimize loss    step 7  initialize necessary variables  case  w b    step 8  train model 100 epochs    initialize iterator    close writer done using    step 9  output values w b    plot results    plt plot(data   0   data   0    ( 5 883589)   85 124306   g   label  predicted data huber loss ) ", "content": "\"\"\" Solution for simple linear regression example using tf.data\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n# Step 1: read in the data\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\n\n# Step 2: create Dataset and iterator\ndataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n\niterator = dataset.make_initializable_iterator()\nX, Y = iterator.get_next()\n\n# Step 3: create weight and bias, initialized to 0\nw = tf.get_variable('weights', initializer=tf.constant(0.0))\nb = tf.get_variable('bias', initializer=tf.constant(0.0))\n\n# Step 4: build model to predict Y\nY_predicted = X * w + b\n\n# Step 5: use the square error as the loss function\nloss = tf.square(Y - Y_predicted, name='loss')\n# loss = utils.huber_loss(Y, Y_predicted)\n\n# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n\nstart = time.time()\nwith tf.Session() as sess:\n    # Step 7: initialize the necessary variables, in this case, w and b\n    sess.run(tf.global_variables_initializer()) \n    writer = tf.summary.FileWriter('./graphs/linear_reg', sess.graph)\n    \n    # Step 8: train the model for 100 epochs\n    for i in range(100):\n        sess.run(iterator.initializer) # initialize the iterator\n        total_loss = 0\n        try:\n            while True:\n                _, l = sess.run([optimizer, loss]) \n                total_loss += l\n        except tf.errors.OutOfRangeError:\n            pass\n            \n        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n\n    # close the writer when you're done using it\n    writer.close() \n    \n    # Step 9: output the values of w and b\n    w_out, b_out = sess.run([w, b]) \n    print('w: %f, b: %f' %(w_out, b_out))\nprint('Took: %f seconds' %(time.time() - start))\n\n# plot the results\nplt.plot(data[:,0], data[:,1], 'bo', label='Real data')\nplt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data with squared error')\n# plt.plot(data[:,0], data[:,0] * (-5.883589) + 85.124306, 'g', label='Predicted data with Huber loss')\nplt.legend()\nplt.show()", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "03_linreg_dataset.py", "id": "d9fa3ff3c63dac23e99eacf5c31c5b65", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/03_linreg_dataset.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}