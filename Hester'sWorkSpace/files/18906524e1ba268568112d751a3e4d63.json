{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Script which evaluates model on adversarial examples.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport imagenet\nimport inception_resnet_v2\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.nets import inception\n\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    'batch_size', 50, 'The number of samples in each batch.')\n\ntf.app.flags.DEFINE_integer(\n    'max_num_batches', None,\n    'Max number of batches to evaluate by default use all.')\n\ntf.app.flags.DEFINE_string(\n    'master', '', 'The address of the TensorFlow master to use.')\n\ntf.app.flags.DEFINE_string(\n    'checkpoint_path', '/tmp/tfmodel/',\n    'The directory where the model was written to or an absolute path to a '\n    'checkpoint file.')\n\ntf.app.flags.DEFINE_integer(\n    'num_preprocessing_threads', 4,\n    'The number of threads used to create the batches.')\n\ntf.app.flags.DEFINE_string(\n    'split_name', 'validation', 'The name of the train/test split.')\n\ntf.app.flags.DEFINE_string(\n    'dataset_dir', None, 'The directory where the dataset files are stored.')\n\ntf.app.flags.DEFINE_string(\n    'model_name', 'inception_v3',\n    'Name of the model to use, either \"inception_v3\" or \"inception_resnet_v2\"')\n\ntf.app.flags.DEFINE_float(\n    'moving_average_decay', None,\n    'The decay to use for the moving average.'\n    'If left as None, then moving averages are not used.')\n\ntf.app.flags.DEFINE_string(\n    'adversarial_method', 'none',\n    'What kind of adversarial examples to use for evaluation. '\n    'Could be one of: \"none\", \"stepll\", \"stepllnoise\".')\n\ntf.app.flags.DEFINE_float(\n    'adversarial_eps', 0.0,\n    'Size of adversarial perturbation in range [0, 255].')\n\n\nFLAGS = tf.app.flags.FLAGS\n\n\nIMAGE_SIZE = 299\nNUM_CLASSES = 1001\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  \"\"\"Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  \"\"\"\n  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n     Crop the central region of the image with an area containing 87.5% of\n     the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n       Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef create_model(x, reuse=None):\n  \"\"\"Create model graph.\n\n  Args:\n    x: input images\n    reuse: reuse parameter which will be passed to underlying variable scopes.\n      Should be None first call and True every subsequent call.\n\n  Returns:\n    (logits, end_points) - tuple of model logits and enpoints\n\n  Raises:\n    ValueError: if model type specified by --model_name flag is invalid.\n  \"\"\"\n  if FLAGS.model_name == 'inception_v3':\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      return inception.inception_v3(\n          x, num_classes=NUM_CLASSES, is_training=False, reuse=reuse)\n  elif FLAGS.model_name == 'inception_resnet_v2':\n    with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n      return inception_resnet_v2.inception_resnet_v2(\n          x, num_classes=NUM_CLASSES, is_training=False, reuse=reuse)\n  else:\n    raise ValueError('Invalid model name: %s' % (FLAGS.model_name))\n\n\ndef step_target_class_adversarial_images(x, eps, one_hot_target_class):\n  \"\"\"Base code for one step towards target class methods.\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n    one_hot_target_class: one hot encoded target classes for all images\n\n  Returns:\n    tensor with adversarial images\n  \"\"\"\n  logits, end_points = create_model(x, reuse=True)\n  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                  logits,\n                                                  label_smoothing=0.1,\n                                                  weights=1.0)\n  cross_entropy += tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                   end_points['AuxLogits'],\n                                                   label_smoothing=0.1,\n                                                   weights=0.4)\n  x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n  x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n  return tf.stop_gradient(x_adv)\n\n\ndef stepll_adversarial_images(x, eps):\n  \"\"\"One step towards least likely class (Step L.L.) adversarial examples.\n\n  This method is an alternative to FGSM which does not use true classes.\n  Method is described in the \"Adversarial Machine Learning at Scale\" paper,\n  https://arxiv.org/abs/1611.01236\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n\n  Returns:\n    adversarial images\n  \"\"\"\n  logits, _ = create_model(x, reuse=True)\n  least_likely_class = tf.argmin(logits, 1)\n  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n  return step_target_class_adversarial_images(x, eps, one_hot_ll_class)\n\n\ndef stepllnoise_adversarial_images(x, eps):\n  \"\"\"Step L.L. with noise method.\n\n  This is an imporvement of Step L.L. method. This method is better against\n  adversarially trained models which learn to mask gradient.\n  Method is described in the section \"New randomized one shot attack\" of\n  \"Ensemble Adversarial Training: Attacks and Defenses\" paper,\n  https://arxiv.org/abs/1705.07204\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n\n  Returns:\n    adversarial images\n  \"\"\"\n  logits, _ = create_model(x, reuse=True)\n  least_likely_class = tf.argmin(logits, 1)\n  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n  x_noise = x + eps / 2 * tf.sign(tf.random_normal(x.shape))\n  return step_target_class_adversarial_images(x_noise, eps / 2,\n                                              one_hot_ll_class)\n\n\ndef get_input_images(dataset_images):\n  \"\"\"Gets input images for the evaluation.\n\n  Args:\n    dataset_images: tensor with dataset images\n\n  Returns:\n    tensor with input images, which is either dataset images or adversarial\n    images.\n\n  Raises:\n    ValueError: if adversarial method specified by --adversarial_method flag\n      is invalid.\n  \"\"\"\n   adversarial_eps defines max difference of values of pixels if\n   pixels are in range [0, 255]. However values of dataset pixels are\n   in range [-1, 1], so converting epsilon.\n  eps = FLAGS.adversarial_eps / 255 * 2.0\n\n  if FLAGS.adversarial_method == 'stepll':\n    return stepll_adversarial_images(dataset_images, eps)\n  elif FLAGS.adversarial_method == 'stepllnoise':\n    return stepllnoise_adversarial_images(dataset_images, eps)\n  elif FLAGS.adversarial_method == 'none':\n    return dataset_images\n  else:\n    raise ValueError('Invalid adversarial method: %s'\n                     % (FLAGS.adversarial_method))\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError('You must supply the dataset directory with --dataset_dir')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = tf.train.get_or_create_global_step()\n\n    \n     Prepare dataset \n    \n    dataset = imagenet.get_split(FLAGS.split_name, FLAGS.dataset_dir)\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=False,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size)\n    [dataset_image, label] = provider.get(['image', 'label'])\n    dataset_image = preprocess_for_eval(dataset_image, IMAGE_SIZE, IMAGE_SIZE)\n    dataset_images, labels = tf.train.batch(\n        [dataset_image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    \n     Define the model and input exampeles \n    \n    create_model(tf.placeholder(tf.float32, shape=dataset_images.shape))\n    input_images = get_input_images(dataset_images)\n    logits, _ = create_model(input_images, reuse=True)\n\n    if FLAGS.moving_average_decay > 0:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    \n     Define the metrics \n    \n    predictions = tf.argmax(logits, 1)\n    labels = tf.squeeze(labels)\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\n        'Recall_5': slim.metrics.streaming_sparse_recall_at_k(\n            logits, tf.reshape(labels, [-1, 1]), 5),\n    })\n\n    \n     Run evaluation     \n    \n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n       This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info('Evaluating %s' % checkpoint_path)\n\n    top1_accuracy, top5_accuracy = slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=None,\n        summary_op=None,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()),\n        final_op=[names_to_values['Accuracy'], names_to_values['Recall_5']],\n        variables_to_restore=variables_to_restore)\n\n    print('Top1 Accuracy: ', top1_accuracy)\n    print('Top5 Accuracy: ', top5_accuracy)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   script evaluates model adversarial examples        future   import absolute import   future   import division   future   import print function  import math import imagenet import inception resnet v2  import tensorflow tf tensorflow contrib slim nets import inception   slim   tf contrib slim  tf app flags define integer(      batch size   50   the number samples batch  )  tf app flags define integer(      max num batches   none       max number batches evaluate default use  )  tf app flags define string(      master        the address tensorflow master use  )  tf app flags define string(      checkpoint path     tmp tfmodel         the directory model written absolute path        checkpoint file  )  tf app flags define integer(      num preprocessing threads   4       the number threads used create batches  )  tf app flags define string(      split name    validation    the name train test split  )  tf app flags define string(      dataset dir   none   the directory dataset files stored  )  tf app flags define string(      model name    inception v3        name model use  either  inception v3   inception resnet v2  )  tf app flags define float(      moving average decay   none       the decay use moving average        if left none  moving averages used  )  tf app flags define string(      adversarial method    none        what kind adversarial examples use evaluation         could one   none    stepll    stepllnoise   )  tf app flags define float(      adversarial eps   0 0       size adversarial perturbation range  0  255   )   flags   tf app flags flags   image size   299 num classes   1001   def preprocess eval(image  height  width                          central fraction 0 875  scope none)       prepare one image evaluation     if height width specified would output image size   applying resize bilinear    if central fraction specified would crop central fraction   input image     args      image  3 d tensor image  if dtype tf float32 range        0  1   otherwise would converted tf float32 assuming range        0  max   max largest positive representable number       int(8 16 32) data type (see  tf image convert image dtype  details)     height  integer     width  integer     central fraction  optional float  fraction image crop      scope  optional scope name scope    returns      3 d float tensor prepared image          tf name scope(scope   eval image    image  height  width )      image dtype    tf float32        image   tf image convert image dtype(image  dtype tf float32)       crop central region image area containing 87 5        original image      central fraction        image   tf image central crop(image  central fraction central fraction)      height width          resize image specified height width        image   tf expand dims(image  0)       image   tf image resize bilinear(image   height  width                                          align corners false)       image   tf squeeze(image   0 )     image   tf subtract(image  0 5)     image   tf multiply(image  2 0)     return image   def create model(x  reuse none)       create model graph     args      x  input images     reuse  reuse parameter passed underlying variable scopes        should none first call true every subsequent call     returns      (logits  end points)   tuple model logits enpoints    raises      valueerror  model type specified   model name flag invalid          flags model name     inception v3       slim arg scope(inception inception v3 arg scope())        return inception inception v3(           x  num classes num classes  training false  reuse reuse)   elif flags model name     inception resnet v2       slim arg scope(inception resnet v2 inception resnet v2 arg scope())        return inception resnet v2 inception resnet v2(           x  num classes num classes  training false  reuse reuse)   else      raise valueerror( invalid model name      (flags model name))   def step target class adversarial images(x  eps  one hot target class)       base code one step towards target class methods     args      x  source images     eps  size adversarial perturbation     one hot target class  one hot encoded target classes images    returns      tensor adversarial images         logits  end points   create model(x  reuse true)   cross entropy   tf losses softmax cross entropy(one hot target class                                                    logits                                                    label smoothing 0 1                                                    weights 1 0)   cross entropy    tf losses softmax cross entropy(one hot target class                                                     end points  auxlogits                                                       label smoothing 0 1                                                     weights 0 4)   x adv   x   eps   tf sign(tf gradients(cross entropy  x) 0 )   x adv   tf clip value(x adv   1 0  1 0)   return tf stop gradient(x adv)   def stepll adversarial images(x  eps)       one step towards least likely class (step l l ) adversarial examples     this method alternative fgsm use true classes    method described  adversarial machine learning scale  paper    https   arxiv org abs 1611 01236    args      x  source images     eps  size adversarial perturbation    returns      adversarial images         logits      create model(x  reuse true)   least likely class   tf argmin(logits  1)   one hot class   tf one hot(least likely class  num classes)   return step target class adversarial images(x  eps  one hot class)   def stepllnoise adversarial images(x  eps)       step l l  noise method     this imporvement step l l  method  this method better   adversarially trained models learn mask gradient    method described section  new randomized one shot attack     ensemble adversarial training  attacks defenses  paper    https   arxiv org abs 1705 07204    args      x  source images     eps  size adversarial perturbation    returns      adversarial images         logits      create model(x  reuse true)   least likely class   tf argmin(logits  1)   one hot class   tf one hot(least likely class  num classes)   x noise   x   eps   2   tf sign(tf random normal(x shape))   return step target class adversarial images(x noise  eps   2                                                one hot class)   def get input images(dataset images)       gets input images evaluation     args      dataset images  tensor dataset images    returns      tensor input images  either dataset images adversarial     images     raises      valueerror  adversarial method specified   adversarial method flag       invalid           copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       crop central region image area containing 87 5     original image     resize image specified height width     adversarial eps defines max difference values pixels    pixels range  0  255   however values dataset pixels    range   1  1   converting epsilon                          prepare dataset                                                                     define model input exampeles                                                                        define metrics                                                      run evaluation                                  this ensures make single pass data  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Script which evaluates model on adversarial examples.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport imagenet\nimport inception_resnet_v2\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.nets import inception\n\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    'batch_size', 50, 'The number of samples in each batch.')\n\ntf.app.flags.DEFINE_integer(\n    'max_num_batches', None,\n    'Max number of batches to evaluate by default use all.')\n\ntf.app.flags.DEFINE_string(\n    'master', '', 'The address of the TensorFlow master to use.')\n\ntf.app.flags.DEFINE_string(\n    'checkpoint_path', '/tmp/tfmodel/',\n    'The directory where the model was written to or an absolute path to a '\n    'checkpoint file.')\n\ntf.app.flags.DEFINE_integer(\n    'num_preprocessing_threads', 4,\n    'The number of threads used to create the batches.')\n\ntf.app.flags.DEFINE_string(\n    'split_name', 'validation', 'The name of the train/test split.')\n\ntf.app.flags.DEFINE_string(\n    'dataset_dir', None, 'The directory where the dataset files are stored.')\n\ntf.app.flags.DEFINE_string(\n    'model_name', 'inception_v3',\n    'Name of the model to use, either \"inception_v3\" or \"inception_resnet_v2\"')\n\ntf.app.flags.DEFINE_float(\n    'moving_average_decay', None,\n    'The decay to use for the moving average.'\n    'If left as None, then moving averages are not used.')\n\ntf.app.flags.DEFINE_string(\n    'adversarial_method', 'none',\n    'What kind of adversarial examples to use for evaluation. '\n    'Could be one of: \"none\", \"stepll\", \"stepllnoise\".')\n\ntf.app.flags.DEFINE_float(\n    'adversarial_eps', 0.0,\n    'Size of adversarial perturbation in range [0, 255].')\n\n\nFLAGS = tf.app.flags.FLAGS\n\n\nIMAGE_SIZE = 299\nNUM_CLASSES = 1001\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  \"\"\"Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  \"\"\"\n  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef create_model(x, reuse=None):\n  \"\"\"Create model graph.\n\n  Args:\n    x: input images\n    reuse: reuse parameter which will be passed to underlying variable scopes.\n      Should be None first call and True every subsequent call.\n\n  Returns:\n    (logits, end_points) - tuple of model logits and enpoints\n\n  Raises:\n    ValueError: if model type specified by --model_name flag is invalid.\n  \"\"\"\n  if FLAGS.model_name == 'inception_v3':\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      return inception.inception_v3(\n          x, num_classes=NUM_CLASSES, is_training=False, reuse=reuse)\n  elif FLAGS.model_name == 'inception_resnet_v2':\n    with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n      return inception_resnet_v2.inception_resnet_v2(\n          x, num_classes=NUM_CLASSES, is_training=False, reuse=reuse)\n  else:\n    raise ValueError('Invalid model name: %s' % (FLAGS.model_name))\n\n\ndef step_target_class_adversarial_images(x, eps, one_hot_target_class):\n  \"\"\"Base code for one step towards target class methods.\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n    one_hot_target_class: one hot encoded target classes for all images\n\n  Returns:\n    tensor with adversarial images\n  \"\"\"\n  logits, end_points = create_model(x, reuse=True)\n  cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                  logits,\n                                                  label_smoothing=0.1,\n                                                  weights=1.0)\n  cross_entropy += tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                   end_points['AuxLogits'],\n                                                   label_smoothing=0.1,\n                                                   weights=0.4)\n  x_adv = x - eps * tf.sign(tf.gradients(cross_entropy, x)[0])\n  x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n  return tf.stop_gradient(x_adv)\n\n\ndef stepll_adversarial_images(x, eps):\n  \"\"\"One step towards least likely class (Step L.L.) adversarial examples.\n\n  This method is an alternative to FGSM which does not use true classes.\n  Method is described in the \"Adversarial Machine Learning at Scale\" paper,\n  https://arxiv.org/abs/1611.01236\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n\n  Returns:\n    adversarial images\n  \"\"\"\n  logits, _ = create_model(x, reuse=True)\n  least_likely_class = tf.argmin(logits, 1)\n  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n  return step_target_class_adversarial_images(x, eps, one_hot_ll_class)\n\n\ndef stepllnoise_adversarial_images(x, eps):\n  \"\"\"Step L.L. with noise method.\n\n  This is an imporvement of Step L.L. method. This method is better against\n  adversarially trained models which learn to mask gradient.\n  Method is described in the section \"New randomized one shot attack\" of\n  \"Ensemble Adversarial Training: Attacks and Defenses\" paper,\n  https://arxiv.org/abs/1705.07204\n\n  Args:\n    x: source images\n    eps: size of adversarial perturbation\n\n  Returns:\n    adversarial images\n  \"\"\"\n  logits, _ = create_model(x, reuse=True)\n  least_likely_class = tf.argmin(logits, 1)\n  one_hot_ll_class = tf.one_hot(least_likely_class, NUM_CLASSES)\n  x_noise = x + eps / 2 * tf.sign(tf.random_normal(x.shape))\n  return step_target_class_adversarial_images(x_noise, eps / 2,\n                                              one_hot_ll_class)\n\n\ndef get_input_images(dataset_images):\n  \"\"\"Gets input images for the evaluation.\n\n  Args:\n    dataset_images: tensor with dataset images\n\n  Returns:\n    tensor with input images, which is either dataset images or adversarial\n    images.\n\n  Raises:\n    ValueError: if adversarial method specified by --adversarial_method flag\n      is invalid.\n  \"\"\"\n  # adversarial_eps defines max difference of values of pixels if\n  # pixels are in range [0, 255]. However values of dataset pixels are\n  # in range [-1, 1], so converting epsilon.\n  eps = FLAGS.adversarial_eps / 255 * 2.0\n\n  if FLAGS.adversarial_method == 'stepll':\n    return stepll_adversarial_images(dataset_images, eps)\n  elif FLAGS.adversarial_method == 'stepllnoise':\n    return stepllnoise_adversarial_images(dataset_images, eps)\n  elif FLAGS.adversarial_method == 'none':\n    return dataset_images\n  else:\n    raise ValueError('Invalid adversarial method: %s'\n                     % (FLAGS.adversarial_method))\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError('You must supply the dataset directory with --dataset_dir')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = tf.train.get_or_create_global_step()\n\n    ###################\n    # Prepare dataset #\n    ###################\n    dataset = imagenet.get_split(FLAGS.split_name, FLAGS.dataset_dir)\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=False,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size)\n    [dataset_image, label] = provider.get(['image', 'label'])\n    dataset_image = preprocess_for_eval(dataset_image, IMAGE_SIZE, IMAGE_SIZE)\n    dataset_images, labels = tf.train.batch(\n        [dataset_image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    ########################################\n    # Define the model and input exampeles #\n    ########################################\n    create_model(tf.placeholder(tf.float32, shape=dataset_images.shape))\n    input_images = get_input_images(dataset_images)\n    logits, _ = create_model(input_images, reuse=True)\n\n    if FLAGS.moving_average_decay > 0:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    ######################\n    # Define the metrics #\n    ######################\n    predictions = tf.argmax(logits, 1)\n    labels = tf.squeeze(labels)\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\n        'Recall_5': slim.metrics.streaming_sparse_recall_at_k(\n            logits, tf.reshape(labels, [-1, 1]), 5),\n    })\n\n    ######################\n    # Run evaluation     #\n    ######################\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info('Evaluating %s' % checkpoint_path)\n\n    top1_accuracy, top5_accuracy = slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=None,\n        summary_op=None,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()),\n        final_op=[names_to_values['Accuracy'], names_to_values['Recall_5']],\n        variables_to_restore=variables_to_restore)\n\n    print('Top1 Accuracy: ', top1_accuracy)\n    print('Top5 Accuracy: ', top5_accuracy)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "eval_on_adversarial.py", "id": "18906524e1ba268568112d751a3e4d63", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/adv_imagenet_models/eval_on_adversarial.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}