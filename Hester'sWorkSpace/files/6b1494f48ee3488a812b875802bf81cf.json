{"author": "chiphuyen", "code": "\"\"\" Using convolutional net on MNIST dataset of handwritten digits\nMNIST dataset: http://yann.lecun.com/exdb/mnist/\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 07\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time \n\nimport tensorflow as tf\n\nimport utils\n\ndef conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n    '''\n    A method that does convolution + relu on inputs\n    '''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        in_channels = inputs.shape[-1]\n        kernel = tf.get_variable('kernel', \n                                [k_size, k_size, in_channels, filters], \n                                initializer=tf.truncated_normal_initializer())\n        biases = tf.get_variable('biases', \n                                [filters],\n                                initializer=tf.random_normal_initializer())\n        conv = tf.nn.conv2d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n    return tf.nn.relu(conv + biases, name=scope.name)\n\ndef maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n    '''A method that does max pooling on inputs'''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        pool = tf.nn.max_pool(inputs, \n                            ksize=[1, ksize, ksize, 1], \n                            strides=[1, stride, stride, 1],\n                            padding=padding)\n    return pool\n\ndef fully_connected(inputs, out_dim, scope_name='fc'):\n    '''\n    A fully connected linear layer on inputs\n    '''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        in_dim = inputs.shape[-1]\n        w = tf.get_variable('weights', [in_dim, out_dim],\n                            initializer=tf.truncated_normal_initializer())\n        b = tf.get_variable('biases', [out_dim],\n                            initializer=tf.constant_initializer(0.0))\n        out = tf.matmul(inputs, w) + b\n    return out\n\nclass ConvNet(object):\n    def __init__(self):\n        self.lr = 0.001\n        self.batch_size = 128\n        self.keep_prob = tf.constant(0.75)\n        self.gstep = tf.Variable(0, dtype=tf.int32, \n                                trainable=False, name='global_step')\n        self.n_classes = 10\n        self.skip_step = 20\n        self.n_test = 10000\n        self.training = True\n\n    def get_data(self):\n        with tf.name_scope('data'):\n            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n                                                   train_data.output_shapes)\n            img, self.label = iterator.get_next()\n            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n            \n\n            self.train_init = iterator.make_initializer(train_data)  \n            self.test_init = iterator.make_initializer(test_data)    \n\n    def inference(self):\n        conv1 = conv_relu(inputs=self.img,\n                        filters=32,\n                        k_size=5,\n                        stride=1,\n                        padding='SAME',\n                        scope_name='conv1')\n        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n        conv2 = conv_relu(inputs=pool1,\n                        filters=64,\n                        k_size=5,\n                        stride=1,\n                        padding='SAME',\n                        scope_name='conv2')\n        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n        pool2 = tf.reshape(pool2, [-1, feature_dim])\n        fc = fully_connected(pool2, 1024, 'fc')\n        dropout = tf.nn.dropout(tf.nn.relu(fc), self.keep_prob, name='relu_dropout')\n        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n\n    def loss(self):\n        '''\n        define loss function\n        use softmax cross entropy with logits as the loss function\n        compute mean cross entropy, softmax is applied internally\n        '''\n        \n        with tf.name_scope('loss'):\n            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n            self.loss = tf.reduce_mean(entropy, name='loss')\n    \n    def optimize(self):\n        '''\n        Define training op\n        using Adam Gradient Descent to minimize cost\n        '''\n        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n                                                global_step=self.gstep)\n\n    def summary(self):\n        '''\n        Create summaries to write on TensorBoard\n        '''\n        with tf.name_scope('summaries'):\n            tf.summary.scalar('loss', self.loss)\n            tf.summary.scalar('accuracy', self.accuracy)\n            tf.summary.histogram('histogram loss', self.loss)\n            self.summary_op = tf.summary.merge_all()\n    \n    def eval(self):\n        '''\n        Count the number of right predictions in a batch\n        '''\n        with tf.name_scope('predict'):\n            preds = tf.nn.softmax(self.logits)\n            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\n    def build(self):\n        '''\n        Build the computation graph\n        '''\n        self.get_data()\n        self.inference()\n        self.loss()\n        self.optimize()\n        self.eval()\n        self.summary()\n\n    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n        start_time = time.time()\n        sess.run(init) \n        self.training = True\n        total_loss = 0\n        n_batches = 0\n        try:\n            while True:\n                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n                writer.add_summary(summaries, global_step=step)\n                if (step + 1) % self.skip_step == 0:\n                    print('Loss at step {0}: {1}'.format(step, l))\n                step += 1\n                total_loss += l\n                n_batches += 1\n        except tf.errors.OutOfRangeError:\n            pass\n        saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n        print('Took: {0} seconds'.format(time.time() - start_time))\n        return step\n\n    def eval_once(self, sess, init, writer, epoch, step):\n        start_time = time.time()\n        sess.run(init)\n        self.training = False\n        total_correct_preds = 0\n        try:\n            while True:\n                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n                writer.add_summary(summaries, global_step=step)\n                total_correct_preds += accuracy_batch\n        except tf.errors.OutOfRangeError:\n            pass\n\n        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n        print('Took: {0} seconds'.format(time.time() - start_time))\n\n    def train(self, n_epochs):\n        '''\n        The train function alternates between training one epoch and evaluating\n        '''\n        utils.safe_mkdir('checkpoints')\n        utils.safe_mkdir('checkpoints/convnet_mnist')\n        writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            \n            step = self.gstep.eval()\n\n            for epoch in range(n_epochs):\n                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n                self.eval_once(sess, self.test_init, writer, epoch, step)\n        writer.close()\n\nif __name__ == '__main__':\n    model = ConvNet()\n    model.build()\n    model.train(n_epochs=30)\n", "comments": "    using convolutional net mnist dataset handwritten digits mnist dataset  http   yann lecun com exdb mnist  cs 20   tensorflow deep learning research  cs20 stanford edu chip huyen (chiphuyen cs stanford edu) lecture 07              a method convolution   relu inputs             tf variable scope(scope name  reuse tf auto reuse) scope          channels   inputs shape  1          kernel   tf get variable( kernel                                     k size  k size  channels  filters                                    initializer tf truncated normal initializer())         biases   tf get variable( biases                                     filters                                   initializer tf random normal initializer())         conv   tf nn conv2d(inputs  kernel  strides  1  stride  stride  1   padding padding)     return tf nn relu(conv   biases  name scope name)  def maxpool(inputs  ksize  stride  padding  valid   scope name  pool )         a method max pooling inputs        tf variable scope(scope name  reuse tf auto reuse) scope          pool   tf nn max pool(inputs                               ksize  1  ksize  ksize  1                                strides  1  stride  stride  1                               padding padding)     return pool  def fully connected(inputs  dim  scope name  fc )              a fully connected linear layer inputs             tf variable scope(scope name  reuse tf auto reuse) scope          dim   inputs shape  1          w   tf get variable( weights    dim  dim                               initializer tf truncated normal initializer())         b   tf get variable( biases    dim                               initializer tf constant initializer(0 0))           tf matmul(inputs  w)   b     return  class convnet(object)      def   init  (self)          self lr   0 001         self batch size   128         self keep prob   tf constant(0 75)         self gstep   tf variable(0  dtype tf int32                                   trainable false  name  global step )         self n classes   10         self skip step   20         self n test   10000         self training   true      def get data(self)          tf name scope( data )              train data  test data   utils get mnist dataset(self batch size)             iterator   tf data iterator structure(train data output types                                                      train data output shapes)             img  self label   iterator get next()             self img   tf reshape(img  shape   1  28  28  1 )               reshape image make work tf nn conv2d              self train init   iterator make initializer(train data)    initializer train data             self test init   iterator make initializer(test data)      initializer train data      def inference(self)          conv1   conv relu(inputs self img                          filters 32                          k size 5                          stride 1                          padding  same                           scope name  conv1 )         pool1   maxpool(conv1  2  2   valid    pool1 )         conv2   conv relu(inputs pool1                          filters 64                          k size 5                          stride 1                          padding  same                           scope name  conv2 )         pool2   maxpool(conv2  2  2   valid    pool2 )         feature dim   pool2 shape 1    pool2 shape 2    pool2 shape 3          pool2   tf reshape(pool2    1  feature dim )         fc   fully connected(pool2  1024   fc )         dropout   tf nn dropout(tf nn relu(fc)  self keep prob  name  relu dropout )         self logits   fully connected(dropout  self n classes   logits )      def loss(self)                      define loss function         use softmax cross entropy logits loss function         compute mean cross entropy  softmax applied internally                                tf name scope( loss )              entropy   tf nn softmax cross entropy logits(labels self label  logits self logits)             self loss   tf reduce mean(entropy  name  loss )          def optimize(self)                      define training op         using adam gradient descent minimize cost                     self opt   tf train adamoptimizer(self lr) minimize(self loss                                                   global step self gstep)      def summary(self)                      create summaries write tensorboard                     tf name scope( summaries )              tf summary scalar( loss   self loss)             tf summary scalar( accuracy   self accuracy)             tf summary histogram( histogram loss   self loss)             self summary op   tf summary merge all()          def eval(self)                      count number right predictions batch                     tf name scope( predict )              preds   tf nn softmax(self logits)             correct preds   tf equal(tf argmax(preds  1)  tf argmax(self label  1))             self accuracy   tf reduce sum(tf cast(correct preds  tf float32))      def build(self)                      build computation graph                     self get data()         self inference()         self loss()         self optimize()         self eval()         self summary()      def train one epoch(self  sess  saver  init  writer  epoch  step)          start time   time time()         sess run(init)          self training   true         total loss   0         n batches   0         try              true                     l  summaries   sess run( self opt  self loss  self summary op )                 writer add summary(summaries  global step step)                 (step   1)   self skip step    0                      print( loss step  0    1   format(step  l))                 step    1                 total loss    l                 n batches    1         except tf errors outofrangeerror              pass         saver save(sess   checkpoints convnet mnist mnist convnet   step)         print( average loss epoch  0    1   format(epoch  total loss n batches))         print( took   0  seconds  format(time time()   start time))         return step      def eval once(self  sess  init  writer  epoch  step)          start time   time time()         sess run(init)         self training   false         total correct preds   0         try              true                  accuracy batch  summaries   sess run( self accuracy  self summary op )                 writer add summary(summaries  global step step)                 total correct preds    accuracy batch         except tf errors outofrangeerror              pass          print( accuracy epoch  0    1    format(epoch  total correct preds self n test))         print( took   0  seconds  format(time time()   start time))      def train(self  n epochs)                      the train function alternates training one epoch evaluating                reshape image make work tf nn conv2d    initializer train data    initializer train data     ", "content": "\"\"\" Using convolutional net on MNIST dataset of handwritten digits\nMNIST dataset: http://yann.lecun.com/exdb/mnist/\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 07\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time \n\nimport tensorflow as tf\n\nimport utils\n\ndef conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n    '''\n    A method that does convolution + relu on inputs\n    '''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        in_channels = inputs.shape[-1]\n        kernel = tf.get_variable('kernel', \n                                [k_size, k_size, in_channels, filters], \n                                initializer=tf.truncated_normal_initializer())\n        biases = tf.get_variable('biases', \n                                [filters],\n                                initializer=tf.random_normal_initializer())\n        conv = tf.nn.conv2d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n    return tf.nn.relu(conv + biases, name=scope.name)\n\ndef maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n    '''A method that does max pooling on inputs'''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        pool = tf.nn.max_pool(inputs, \n                            ksize=[1, ksize, ksize, 1], \n                            strides=[1, stride, stride, 1],\n                            padding=padding)\n    return pool\n\ndef fully_connected(inputs, out_dim, scope_name='fc'):\n    '''\n    A fully connected linear layer on inputs\n    '''\n    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n        in_dim = inputs.shape[-1]\n        w = tf.get_variable('weights', [in_dim, out_dim],\n                            initializer=tf.truncated_normal_initializer())\n        b = tf.get_variable('biases', [out_dim],\n                            initializer=tf.constant_initializer(0.0))\n        out = tf.matmul(inputs, w) + b\n    return out\n\nclass ConvNet(object):\n    def __init__(self):\n        self.lr = 0.001\n        self.batch_size = 128\n        self.keep_prob = tf.constant(0.75)\n        self.gstep = tf.Variable(0, dtype=tf.int32, \n                                trainable=False, name='global_step')\n        self.n_classes = 10\n        self.skip_step = 20\n        self.n_test = 10000\n        self.training = True\n\n    def get_data(self):\n        with tf.name_scope('data'):\n            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n                                                   train_data.output_shapes)\n            img, self.label = iterator.get_next()\n            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n            # reshape the image to make it work with tf.nn.conv2d\n\n            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n\n    def inference(self):\n        conv1 = conv_relu(inputs=self.img,\n                        filters=32,\n                        k_size=5,\n                        stride=1,\n                        padding='SAME',\n                        scope_name='conv1')\n        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n        conv2 = conv_relu(inputs=pool1,\n                        filters=64,\n                        k_size=5,\n                        stride=1,\n                        padding='SAME',\n                        scope_name='conv2')\n        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n        pool2 = tf.reshape(pool2, [-1, feature_dim])\n        fc = fully_connected(pool2, 1024, 'fc')\n        dropout = tf.nn.dropout(tf.nn.relu(fc), self.keep_prob, name='relu_dropout')\n        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n\n    def loss(self):\n        '''\n        define loss function\n        use softmax cross entropy with logits as the loss function\n        compute mean cross entropy, softmax is applied internally\n        '''\n        # \n        with tf.name_scope('loss'):\n            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n            self.loss = tf.reduce_mean(entropy, name='loss')\n    \n    def optimize(self):\n        '''\n        Define training op\n        using Adam Gradient Descent to minimize cost\n        '''\n        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n                                                global_step=self.gstep)\n\n    def summary(self):\n        '''\n        Create summaries to write on TensorBoard\n        '''\n        with tf.name_scope('summaries'):\n            tf.summary.scalar('loss', self.loss)\n            tf.summary.scalar('accuracy', self.accuracy)\n            tf.summary.histogram('histogram loss', self.loss)\n            self.summary_op = tf.summary.merge_all()\n    \n    def eval(self):\n        '''\n        Count the number of right predictions in a batch\n        '''\n        with tf.name_scope('predict'):\n            preds = tf.nn.softmax(self.logits)\n            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\n    def build(self):\n        '''\n        Build the computation graph\n        '''\n        self.get_data()\n        self.inference()\n        self.loss()\n        self.optimize()\n        self.eval()\n        self.summary()\n\n    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n        start_time = time.time()\n        sess.run(init) \n        self.training = True\n        total_loss = 0\n        n_batches = 0\n        try:\n            while True:\n                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n                writer.add_summary(summaries, global_step=step)\n                if (step + 1) % self.skip_step == 0:\n                    print('Loss at step {0}: {1}'.format(step, l))\n                step += 1\n                total_loss += l\n                n_batches += 1\n        except tf.errors.OutOfRangeError:\n            pass\n        saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n        print('Took: {0} seconds'.format(time.time() - start_time))\n        return step\n\n    def eval_once(self, sess, init, writer, epoch, step):\n        start_time = time.time()\n        sess.run(init)\n        self.training = False\n        total_correct_preds = 0\n        try:\n            while True:\n                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n                writer.add_summary(summaries, global_step=step)\n                total_correct_preds += accuracy_batch\n        except tf.errors.OutOfRangeError:\n            pass\n\n        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n        print('Took: {0} seconds'.format(time.time() - start_time))\n\n    def train(self, n_epochs):\n        '''\n        The train function alternates between training one epoch and evaluating\n        '''\n        utils.safe_mkdir('checkpoints')\n        utils.safe_mkdir('checkpoints/convnet_mnist')\n        writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            \n            step = self.gstep.eval()\n\n            for epoch in range(n_epochs):\n                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n                self.eval_once(sess, self.test_init, writer, epoch, step)\n        writer.close()\n\nif __name__ == '__main__':\n    model = ConvNet()\n    model.build()\n    model.train(n_epochs=30)\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "07_convnet_mnist.py", "id": "6b1494f48ee3488a812b875802bf81cf", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/07_convnet_mnist.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}