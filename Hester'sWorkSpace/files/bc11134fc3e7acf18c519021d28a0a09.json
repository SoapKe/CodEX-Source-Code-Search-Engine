{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\"\"\"SketchRNN RNN definition.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n internal imports\nimport numpy as np\nimport tensorflow as tf\n\n\ndef orthogonal(shape):\n  \"\"\"Orthogonal initilaizer.\"\"\"\n  flat_shape = (shape[0], np.prod(shape[1:]))\n  a = np.random.normal(0.0, 1.0, flat_shape)\n  u, _, v = np.linalg.svd(a, full_matrices=False)\n  q = u if u.shape == flat_shape else v\n  return q.reshape(shape)\n\n\ndef orthogonal_initializer(scale=1.0):\n  \"\"\"Orthogonal initializer.\"\"\"\n  def _initializer(shape, dtype=tf.float32,\n                   partition_info=None):   pylint: disable=unused-argument\n    return tf.constant(orthogonal(shape) * scale, dtype)\n\n  return _initializer\n\n\ndef lstm_ortho_initializer(scale=1.0):\n  \"\"\"LSTM orthogonal initializer.\"\"\"\n  def _initializer(shape, dtype=tf.float32,\n                   partition_info=None):   pylint: disable=unused-argument\n    size_x = shape[0]\n    size_h = shape[1] // 4   assumes lstm.\n    t = np.zeros(shape)\n    t[:, :size_h] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h:size_h * 2] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h * 2:size_h * 3] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h * 3:] = orthogonal([size_x, size_h]) * scale\n    return tf.constant(t, dtype)\n\n  return _initializer\n\n\nclass LSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"Vanilla LSTM cell.\n\n  Uses ortho initializer, and also recurrent dropout without memory loss\n  (https://arxiv.org/abs/1603.05118)\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.9):\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n\n  @property\n  def state_size(self):\n    return 2 * self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def get_output(self, state):\n    unused_c, h = tf.split(state, 2, 1)\n    return h\n\n  def __call__(self, x, state, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      c, h = tf.split(state, 2, 1)\n\n      x_size = x.get_shape().as_list()[1]\n\n      w_init = None   uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n       Keep W_xh and W_hh separate here as well to use different init methods.\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n      bias = tf.get_variable(\n          'bias', [4 * self.num_units],\n          initializer=tf.constant_initializer(0.0))\n\n      concat = tf.concat([x, h], 1)\n      w_full = tf.concat([w_xh, w_hh], 0)\n      hidden = tf.matmul(concat, w_full) + bias\n\n      i, j, f, o = tf.split(hidden, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(new_c) * tf.sigmoid(o)\n\n      return new_h, tf.concat([new_c, new_h], 1)   fuk tuples.\n\n\ndef layer_norm_all(h,\n                   batch_size,\n                   base,\n                   num_units,\n                   scope='layer_norm',\n                   reuse=False,\n                   gamma_start=1.0,\n                   epsilon=1e-3,\n                   use_bias=True):\n  \"\"\"Layer Norm (faster version, but not using defun).\"\"\"\n   Performs layer norm on multiple base at once (ie, i, g, j, o for lstm)\n   Reshapes h in to perform layer norm in parallel\n  h_reshape = tf.reshape(h, [batch_size, base, num_units])\n  mean = tf.reduce_mean(h_reshape, [2], keep_dims=True)\n  var = tf.reduce_mean(tf.square(h_reshape - mean), [2], keep_dims=True)\n  epsilon = tf.constant(epsilon)\n  rstd = tf.rsqrt(var + epsilon)\n  h_reshape = (h_reshape - mean) * rstd\n   reshape back to original\n  h = tf.reshape(h_reshape, [batch_size, base * num_units])\n  with tf.variable_scope(scope):\n    if reuse:\n      tf.get_variable_scope().reuse_variables()\n    gamma = tf.get_variable(\n        'ln_gamma', [4 * num_units],\n        initializer=tf.constant_initializer(gamma_start))\n    if use_bias:\n      beta = tf.get_variable(\n          'ln_beta', [4 * num_units], initializer=tf.constant_initializer(0.0))\n  if use_bias:\n    return gamma * h + beta\n  return gamma * h\n\n\ndef layer_norm(x,\n               num_units,\n               scope='layer_norm',\n               reuse=False,\n               gamma_start=1.0,\n               epsilon=1e-3,\n               use_bias=True):\n  \"\"\"Calculate layer norm.\"\"\"\n  axes = [1]\n  mean = tf.reduce_mean(x, axes, keep_dims=True)\n  x_shifted = x - mean\n  var = tf.reduce_mean(tf.square(x_shifted), axes, keep_dims=True)\n  inv_std = tf.rsqrt(var + epsilon)\n  with tf.variable_scope(scope):\n    if reuse is True:\n      tf.get_variable_scope().reuse_variables()\n    gamma = tf.get_variable(\n        'ln_gamma', [num_units],\n        initializer=tf.constant_initializer(gamma_start))\n    if use_bias:\n      beta = tf.get_variable(\n          'ln_beta', [num_units], initializer=tf.constant_initializer(0.0))\n  output = gamma * (x_shifted) * inv_std\n  if use_bias:\n    output += beta\n  return output\n\n\ndef raw_layer_norm(x, epsilon=1e-3):\n  axes = [1]\n  mean = tf.reduce_mean(x, axes, keep_dims=True)\n  std = tf.sqrt(\n      tf.reduce_mean(tf.square(x - mean), axes, keep_dims=True) + epsilon)\n  output = (x - mean) / (std)\n  return output\n\n\ndef super_linear(x,\n                 output_size,\n                 scope=None,\n                 reuse=False,\n                 init_w='ortho',\n                 weight_start=0.0,\n                 use_bias=True,\n                 bias_start=0.0,\n                 input_size=None):\n  \"\"\"Performs linear operation. Uses ortho init defined earlier.\"\"\"\n  shape = x.get_shape().as_list()\n  with tf.variable_scope(scope or 'linear'):\n    if reuse is True:\n      tf.get_variable_scope().reuse_variables()\n\n    w_init = None   uniform\n    if input_size is None:\n      x_size = shape[1]\n    else:\n      x_size = input_size\n    if init_w == 'zeros':\n      w_init = tf.constant_initializer(0.0)\n    elif init_w == 'constant':\n      w_init = tf.constant_initializer(weight_start)\n    elif init_w == 'gaussian':\n      w_init = tf.random_normal_initializer(stddev=weight_start)\n    elif init_w == 'ortho':\n      w_init = lstm_ortho_initializer(1.0)\n\n    w = tf.get_variable(\n        'super_linear_w', [x_size, output_size], tf.float32, initializer=w_init)\n    if use_bias:\n      b = tf.get_variable(\n          'super_linear_b', [output_size],\n          tf.float32,\n          initializer=tf.constant_initializer(bias_start))\n      return tf.matmul(x, w) + b\n    return tf.matmul(x, w)\n\n\nclass LayerNormLSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"Layer-Norm, with Ortho Init. and Recurrent Dropout without Memory Loss.\n\n  https://arxiv.org/abs/1607.06450 - Layer Norm\n  https://arxiv.org/abs/1603.05118 - Recurrent Dropout without Memory Loss\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.90):\n    \"\"\"Initialize the Layer Norm LSTM cell.\n\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (default 1.0).\n      use_recurrent_dropout: Whether to use Recurrent Dropout (default False)\n      dropout_keep_prob: float, dropout keep probability (default 0.90)\n    \"\"\"\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n\n  @property\n  def input_size(self):\n    return self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  @property\n  def state_size(self):\n    return 2 * self.num_units\n\n  def get_output(self, state):\n    h, unused_c = tf.split(state, 2, 1)\n    return h\n\n  def __call__(self, x, state, timestep=0, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      h, c = tf.split(state, 2, 1)\n\n      h_size = self.num_units\n      x_size = x.get_shape().as_list()[1]\n      batch_size = x.get_shape().as_list()[0]\n\n      w_init = None   uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n\n      concat = tf.concat([x, h], 1)   concat for speed.\n      w_full = tf.concat([w_xh, w_hh], 0)\n      concat = tf.matmul(concat, w_full)  + bias  live life without garbage.\n\n       i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      concat = layer_norm_all(concat, batch_size, 4, h_size, 'ln_all')\n      i, j, f, o = tf.split(concat, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(layer_norm(new_c, h_size, 'ln_c')) * tf.sigmoid(o)\n\n    return new_h, tf.concat([new_h, new_c], 1)\n\n\nclass HyperLSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"HyperLSTM with Ortho Init, Layer Norm, Recurrent Dropout, no Memory Loss.\n\n  https://arxiv.org/abs/1609.09106\n  http://blog.otoro.net/2016/09/28/hyper-networks/\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.90,\n               use_layer_norm=True,\n               hyper_num_units=256,\n               hyper_embedding_size=32,\n               hyper_use_recurrent_dropout=False):\n    \"\"\"Initialize the Layer Norm HyperLSTM cell.\n\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (default 1.0).\n      use_recurrent_dropout: Whether to use Recurrent Dropout (default False)\n      dropout_keep_prob: float, dropout keep probability (default 0.90)\n      use_layer_norm: boolean. (default True)\n        Controls whether we use LayerNorm layers in main LSTM & HyperLSTM cell.\n      hyper_num_units: int, number of units in HyperLSTM cell.\n        (default is 128, recommend experimenting with 256 for larger tasks)\n      hyper_embedding_size: int, size of signals emitted from HyperLSTM cell.\n        (default is 16, recommend trying larger values for large datasets)\n      hyper_use_recurrent_dropout: boolean. (default False)\n        Controls whether HyperLSTM cell also uses recurrent dropout.\n        Recommend turning this on only if hyper_num_units becomes large (>= 512)\n    \"\"\"\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n    self.use_layer_norm = use_layer_norm\n    self.hyper_num_units = hyper_num_units\n    self.hyper_embedding_size = hyper_embedding_size\n    self.hyper_use_recurrent_dropout = hyper_use_recurrent_dropout\n\n    self.total_num_units = self.num_units + self.hyper_num_units\n\n    if self.use_layer_norm:\n      cell_fn = LayerNormLSTMCell\n    else:\n      cell_fn = LSTMCell\n    self.hyper_cell = cell_fn(\n        hyper_num_units,\n        use_recurrent_dropout=hyper_use_recurrent_dropout,\n        dropout_keep_prob=dropout_keep_prob)\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  @property\n  def state_size(self):\n    return 2 * self.total_num_units\n\n  def get_output(self, state):\n    total_h, unused_total_c = tf.split(state, 2, 1)\n    h = total_h[:, 0:self.num_units]\n    return h\n\n  def hyper_norm(self, layer, scope='hyper', use_bias=True):\n    num_units = self.num_units\n    embedding_size = self.hyper_embedding_size\n     recurrent batch norm init trick (https://arxiv.org/abs/1603.09025).\n    init_gamma = 0.10   cooijmans' da man.\n    with tf.variable_scope(scope):\n      zw = super_linear(\n          self.hyper_output,\n          embedding_size,\n          init_w='constant',\n          weight_start=0.00,\n          use_bias=True,\n          bias_start=1.0,\n          scope='zw')\n      alpha = super_linear(\n          zw,\n          num_units,\n          init_w='constant',\n          weight_start=init_gamma / embedding_size,\n          use_bias=False,\n          scope='alpha')\n      result = tf.multiply(alpha, layer)\n      if use_bias:\n        zb = super_linear(\n            self.hyper_output,\n            embedding_size,\n            init_w='gaussian',\n            weight_start=0.01,\n            use_bias=False,\n            bias_start=0.0,\n            scope='zb')\n        beta = super_linear(\n            zb,\n            num_units,\n            init_w='constant',\n            weight_start=0.00,\n            use_bias=False,\n            scope='beta')\n        result += beta\n    return result\n\n  def __call__(self, x, state, timestep=0, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      total_h, total_c = tf.split(state, 2, 1)\n      h = total_h[:, 0:self.num_units]\n      c = total_c[:, 0:self.num_units]\n      self.hyper_state = tf.concat(\n          [total_h[:, self.num_units:], total_c[:, self.num_units:]], 1)\n\n      batch_size = x.get_shape().as_list()[0]\n      x_size = x.get_shape().as_list()[1]\n      self._input_size = x_size\n\n      w_init = None   uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n      bias = tf.get_variable(\n          'bias', [4 * self.num_units],\n          initializer=tf.constant_initializer(0.0))\n\n       concatenate the input and hidden states for hyperlstm input\n      hyper_input = tf.concat([x, h], 1)\n      hyper_output, hyper_new_state = self.hyper_cell(hyper_input,\n                                                      self.hyper_state)\n      self.hyper_output = hyper_output\n      self.hyper_state = hyper_new_state\n\n      xh = tf.matmul(x, w_xh)\n      hh = tf.matmul(h, w_hh)\n\n       split Wxh contributions\n      ix, jx, fx, ox = tf.split(xh, 4, 1)\n      ix = self.hyper_norm(ix, 'hyper_ix', use_bias=False)\n      jx = self.hyper_norm(jx, 'hyper_jx', use_bias=False)\n      fx = self.hyper_norm(fx, 'hyper_fx', use_bias=False)\n      ox = self.hyper_norm(ox, 'hyper_ox', use_bias=False)\n\n       split Whh contributions\n      ih, jh, fh, oh = tf.split(hh, 4, 1)\n      ih = self.hyper_norm(ih, 'hyper_ih', use_bias=True)\n      jh = self.hyper_norm(jh, 'hyper_jh', use_bias=True)\n      fh = self.hyper_norm(fh, 'hyper_fh', use_bias=True)\n      oh = self.hyper_norm(oh, 'hyper_oh', use_bias=True)\n\n       split bias\n      ib, jb, fb, ob = tf.split(bias, 4, 0)   bias is to be broadcasted.\n\n       i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i = ix + ih + ib\n      j = jx + jh + jb\n      f = fx + fh + fb\n      o = ox + oh + ob\n\n      if self.use_layer_norm:\n        concat = tf.concat([i, j, f, o], 1)\n        concat = layer_norm_all(concat, batch_size, 4, self.num_units, 'ln_all')\n        i, j, f, o = tf.split(concat, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(layer_norm(new_c, self.num_units, 'ln_c')) * tf.sigmoid(o)\n\n      hyper_h, hyper_c = tf.split(hyper_new_state, 2, 1)\n      new_total_h = tf.concat([new_h, hyper_h], 1)\n      new_total_c = tf.concat([new_c, hyper_c], 1)\n      new_total_state = tf.concat([new_total_h, new_total_c], 1)\n    return new_h, new_total_state\n", "comments": "   sketchrnn rnn definition        future   import absolute import   future   import division   future   import print function    internal imports import numpy np import tensorflow tf   def orthogonal(shape)       orthogonal initilaizer       flat shape   (shape 0   np prod(shape 1  ))     np random normal(0 0  1 0  flat shape)   u     v   np linalg svd(a  full matrices false)   q   u u shape    flat shape else v   return q reshape(shape)   def orthogonal initializer(scale 1 0)       orthogonal initializer       def  initializer(shape  dtype tf float32                     partition info none)     pylint  disable unused argument     return tf constant(orthogonal(shape)   scale  dtype)    return  initializer   def lstm ortho initializer(scale 1 0)       lstm orthogonal initializer       def  initializer(shape  dtype tf float32                     partition info none)     pylint  disable unused argument     size x   shape 0      size h   shape 1     4    assumes lstm        np zeros(shape)         size h    orthogonal( size x  size h )   scale        size h size h   2    orthogonal( size x  size h )   scale        size h   2 size h   3    orthogonal( size x  size h )   scale        size h   3     orthogonal( size x  size h )   scale     return tf constant(t  dtype)    return  initializer   class lstmcell(tf contrib rnn rnncell)       vanilla lstm cell     uses ortho initializer  also recurrent dropout without memory loss   (https   arxiv org abs 1603 05118)          def   init  (self                 num units                 forget bias 1 0                 use recurrent dropout false                 dropout keep prob 0 9)      self num units   num units     self forget bias   forget bias     self use recurrent dropout   use recurrent dropout     self dropout keep prob   dropout keep prob     property   def state size(self)      return 2   self num units     property   def output size(self)      return self num units    def get output(self  state)      unused c  h   tf split(state  2  1)     return h    def   call  (self  x  state  scope none)      tf variable scope(scope type(self)   name  )        c  h   tf split(state  2  1)        x size   x get shape() list() 1         w init   none    uniform        h init   lstm ortho initializer(1 0)          keep w xh w hh separate well use different init methods        w xh   tf get variable(            w xh    x size  4   self num units   initializer w init)       w hh   tf get variable(            w hh    self num units  4   self num units   initializer h init)       bias   tf get variable(            bias    4   self num units             initializer tf constant initializer(0 0))        concat   tf concat( x  h   1)       w full   tf concat( w xh  w hh   0)       hidden   tf matmul(concat  w full)   bias         j  f    tf split(hidden  4  1)        self use recurrent dropout          g   tf nn dropout(tf tanh(j)  self dropout keep prob)       else          g   tf tanh(j)        new c   c   tf sigmoid(f   self forget bias)   tf sigmoid(i)   g       new h   tf tanh(new c)   tf sigmoid(o)        return new h  tf concat( new c  new h   1)    fuk tuples    def layer norm all(h                     batch size                     base                     num units                     scope  layer norm                      reuse false                     gamma start 1 0                     epsilon 1e 3                     use bias true)       layer norm (faster version  using defun)         performs layer norm multiple base (ie   g  j  lstm)     reshapes h perform layer norm parallel   h reshape   tf reshape(h   batch size  base  num units )   mean   tf reduce mean(h reshape   2   keep dims true)   var   tf reduce mean(tf square(h reshape   mean)   2   keep dims true)   epsilon   tf constant(epsilon)   rstd   tf rsqrt(var   epsilon)   h reshape   (h reshape   mean)   rstd     reshape back original   h   tf reshape(h reshape   batch size  base   num units )   tf variable scope(scope)      reuse        tf get variable scope() reuse variables()     gamma   tf get variable(          ln gamma    4   num units           initializer tf constant initializer(gamma start))     use bias        beta   tf get variable(            ln beta    4   num units   initializer tf constant initializer(0 0))   use bias      return gamma   h   beta   return gamma   h   def layer norm(x                 num units                 scope  layer norm                  reuse false                 gamma start 1 0                 epsilon 1e 3                 use bias true)       calculate layer norm       axes    1    mean   tf reduce mean(x  axes  keep dims true)   x shifted   x   mean   var   tf reduce mean(tf square(x shifted)  axes  keep dims true)   inv std   tf rsqrt(var   epsilon)   tf variable scope(scope)      reuse true        tf get variable scope() reuse variables()     gamma   tf get variable(          ln gamma    num units           initializer tf constant initializer(gamma start))     use bias        beta   tf get variable(            ln beta    num units   initializer tf constant initializer(0 0))   output   gamma   (x shifted)   inv std   use bias      output    beta   return output   def raw layer norm(x  epsilon 1e 3)    axes    1    mean   tf reduce mean(x  axes  keep dims true)   std   tf sqrt(       tf reduce mean(tf square(x   mean)  axes  keep dims true)   epsilon)   output   (x   mean)   (std)   return output   def super linear(x                   output size                   scope none                   reuse false                   init w  ortho                    weight start 0 0                   use bias true                   bias start 0 0                   input size none)       performs linear operation  uses ortho init defined earlier       shape   x get shape() list()   tf variable scope(scope  linear )      reuse true        tf get variable scope() reuse variables()      w init   none    uniform     input size none        x size   shape 1      else        x size   input size     init w     zeros         w init   tf constant initializer(0 0)     elif init w     constant         w init   tf constant initializer(weight start)     elif init w     gaussian         w init   tf random normal initializer(stddev weight start)     elif init w     ortho         w init   lstm ortho initializer(1 0)      w   tf get variable(          super linear w    x size  output size   tf float32  initializer w init)     use bias        b   tf get variable(            super linear b    output size             tf float32            initializer tf constant initializer(bias start))       return tf matmul(x  w)   b     return tf matmul(x  w)   class layernormlstmcell(tf contrib rnn rnncell)       layer norm  ortho init  recurrent dropout without memory loss     https   arxiv org abs 1607 06450   layer norm   https   arxiv org abs 1603 05118   recurrent dropout without memory loss          def   init  (self                 num units                 forget bias 1 0                 use recurrent dropout false                 dropout keep prob 0 90)         initialize layer norm lstm cell       args        num units  int  the number units lstm cell        forget bias  float  the bias added forget gates (default 1 0)        use recurrent dropout  whether use recurrent dropout (default false)       dropout keep prob  float  dropout keep probability (default 0 90)             self num units   num units     self forget bias   forget bias     self use recurrent dropout   use recurrent dropout     self dropout keep prob   dropout keep prob     property   def input size(self)      return self num units     property   def output size(self)      return self num units     property   def state size(self)      return 2   self num units    def get output(self  state)      h  unused c   tf split(state  2  1)     return h    def   call  (self  x  state  timestep 0  scope none)      tf variable scope(scope type(self)   name  )        h  c   tf split(state  2  1)        h size   self num units       x size   x get shape() list() 1        batch size   x get shape() list() 0         w init   none    uniform        h init   lstm ortho initializer(1 0)        w xh   tf get variable(            w xh    x size  4   self num units   initializer w init)       w hh   tf get variable(            w hh    self num units  4   self num units   initializer h init)        concat   tf concat( x  h   1)    concat speed        w full   tf concat( w xh  w hh   0)       concat   tf matmul(concat  w full)     bias   live life without garbage             input gate  j   new input  f   forget gate    output gate       concat   layer norm all(concat  batch size  4  h size   ln )        j  f    tf split(concat  4  1)        self use recurrent dropout          g   tf nn dropout(tf tanh(j)  self dropout keep prob)       else          g   tf tanh(j)        new c   c   tf sigmoid(f   self forget bias)   tf sigmoid(i)   g       new h   tf tanh(layer norm(new c  h size   ln c ))   tf sigmoid(o)      return new h  tf concat( new h  new c   1)   class hyperlstmcell(tf contrib rnn rnncell)       hyperlstm ortho init  layer norm  recurrent dropout  memory loss     https   arxiv org abs 1609 09106   http   blog otoro net 2016 09 28 hyper networks           def   init  (self                 num units                 forget bias 1 0                 use recurrent dropout false                 dropout keep prob 0 90                 use layer norm true                 hyper num units 256                 hyper embedding size 32                 hyper use recurrent dropout false)         initialize layer norm hyperlstm cell       args        num units  int  the number units lstm cell        forget bias  float  the bias added forget gates (default 1 0)        use recurrent dropout  whether use recurrent dropout (default false)       dropout keep prob  float  dropout keep probability (default 0 90)       use layer norm  boolean  (default true)         controls whether use layernorm layers main lstm   hyperlstm cell        hyper num units  int  number units hyperlstm cell          (default 128  recommend experimenting 256 larger tasks)       hyper embedding size  int  size signals emitted hyperlstm cell          (default 16  recommend trying larger values large datasets)       hyper use recurrent dropout  boolean  (default false)         controls whether hyperlstm cell also uses recurrent dropout          recommend turning hyper num units becomes large (   512)            copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     internal imports    pylint  disable unused argument    pylint  disable unused argument    assumes lstm     uniform    keep w xh w hh separate well use different init methods     fuk tuples     performs layer norm multiple base (ie   g  j  lstm)    reshapes h perform layer norm parallel    reshape back original    uniform    uniform    concat speed      bias   live life without garbage       input gate  j   new input  f   forget gate    output gate    recurrent batch norm init trick (https   arxiv org abs 1603 09025)     cooijmans  da man     uniform    concatenate input hidden states hyperlstm input    split wxh contributions    split whh contributions    split bias    bias broadcasted       input gate  j   new input  f   forget gate    output gate ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"SketchRNN RNN definition.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# internal imports\nimport numpy as np\nimport tensorflow as tf\n\n\ndef orthogonal(shape):\n  \"\"\"Orthogonal initilaizer.\"\"\"\n  flat_shape = (shape[0], np.prod(shape[1:]))\n  a = np.random.normal(0.0, 1.0, flat_shape)\n  u, _, v = np.linalg.svd(a, full_matrices=False)\n  q = u if u.shape == flat_shape else v\n  return q.reshape(shape)\n\n\ndef orthogonal_initializer(scale=1.0):\n  \"\"\"Orthogonal initializer.\"\"\"\n  def _initializer(shape, dtype=tf.float32,\n                   partition_info=None):  # pylint: disable=unused-argument\n    return tf.constant(orthogonal(shape) * scale, dtype)\n\n  return _initializer\n\n\ndef lstm_ortho_initializer(scale=1.0):\n  \"\"\"LSTM orthogonal initializer.\"\"\"\n  def _initializer(shape, dtype=tf.float32,\n                   partition_info=None):  # pylint: disable=unused-argument\n    size_x = shape[0]\n    size_h = shape[1] // 4  # assumes lstm.\n    t = np.zeros(shape)\n    t[:, :size_h] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h:size_h * 2] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h * 2:size_h * 3] = orthogonal([size_x, size_h]) * scale\n    t[:, size_h * 3:] = orthogonal([size_x, size_h]) * scale\n    return tf.constant(t, dtype)\n\n  return _initializer\n\n\nclass LSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"Vanilla LSTM cell.\n\n  Uses ortho initializer, and also recurrent dropout without memory loss\n  (https://arxiv.org/abs/1603.05118)\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.9):\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n\n  @property\n  def state_size(self):\n    return 2 * self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def get_output(self, state):\n    unused_c, h = tf.split(state, 2, 1)\n    return h\n\n  def __call__(self, x, state, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      c, h = tf.split(state, 2, 1)\n\n      x_size = x.get_shape().as_list()[1]\n\n      w_init = None  # uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n      # Keep W_xh and W_hh separate here as well to use different init methods.\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n      bias = tf.get_variable(\n          'bias', [4 * self.num_units],\n          initializer=tf.constant_initializer(0.0))\n\n      concat = tf.concat([x, h], 1)\n      w_full = tf.concat([w_xh, w_hh], 0)\n      hidden = tf.matmul(concat, w_full) + bias\n\n      i, j, f, o = tf.split(hidden, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(new_c) * tf.sigmoid(o)\n\n      return new_h, tf.concat([new_c, new_h], 1)  # fuk tuples.\n\n\ndef layer_norm_all(h,\n                   batch_size,\n                   base,\n                   num_units,\n                   scope='layer_norm',\n                   reuse=False,\n                   gamma_start=1.0,\n                   epsilon=1e-3,\n                   use_bias=True):\n  \"\"\"Layer Norm (faster version, but not using defun).\"\"\"\n  # Performs layer norm on multiple base at once (ie, i, g, j, o for lstm)\n  # Reshapes h in to perform layer norm in parallel\n  h_reshape = tf.reshape(h, [batch_size, base, num_units])\n  mean = tf.reduce_mean(h_reshape, [2], keep_dims=True)\n  var = tf.reduce_mean(tf.square(h_reshape - mean), [2], keep_dims=True)\n  epsilon = tf.constant(epsilon)\n  rstd = tf.rsqrt(var + epsilon)\n  h_reshape = (h_reshape - mean) * rstd\n  # reshape back to original\n  h = tf.reshape(h_reshape, [batch_size, base * num_units])\n  with tf.variable_scope(scope):\n    if reuse:\n      tf.get_variable_scope().reuse_variables()\n    gamma = tf.get_variable(\n        'ln_gamma', [4 * num_units],\n        initializer=tf.constant_initializer(gamma_start))\n    if use_bias:\n      beta = tf.get_variable(\n          'ln_beta', [4 * num_units], initializer=tf.constant_initializer(0.0))\n  if use_bias:\n    return gamma * h + beta\n  return gamma * h\n\n\ndef layer_norm(x,\n               num_units,\n               scope='layer_norm',\n               reuse=False,\n               gamma_start=1.0,\n               epsilon=1e-3,\n               use_bias=True):\n  \"\"\"Calculate layer norm.\"\"\"\n  axes = [1]\n  mean = tf.reduce_mean(x, axes, keep_dims=True)\n  x_shifted = x - mean\n  var = tf.reduce_mean(tf.square(x_shifted), axes, keep_dims=True)\n  inv_std = tf.rsqrt(var + epsilon)\n  with tf.variable_scope(scope):\n    if reuse is True:\n      tf.get_variable_scope().reuse_variables()\n    gamma = tf.get_variable(\n        'ln_gamma', [num_units],\n        initializer=tf.constant_initializer(gamma_start))\n    if use_bias:\n      beta = tf.get_variable(\n          'ln_beta', [num_units], initializer=tf.constant_initializer(0.0))\n  output = gamma * (x_shifted) * inv_std\n  if use_bias:\n    output += beta\n  return output\n\n\ndef raw_layer_norm(x, epsilon=1e-3):\n  axes = [1]\n  mean = tf.reduce_mean(x, axes, keep_dims=True)\n  std = tf.sqrt(\n      tf.reduce_mean(tf.square(x - mean), axes, keep_dims=True) + epsilon)\n  output = (x - mean) / (std)\n  return output\n\n\ndef super_linear(x,\n                 output_size,\n                 scope=None,\n                 reuse=False,\n                 init_w='ortho',\n                 weight_start=0.0,\n                 use_bias=True,\n                 bias_start=0.0,\n                 input_size=None):\n  \"\"\"Performs linear operation. Uses ortho init defined earlier.\"\"\"\n  shape = x.get_shape().as_list()\n  with tf.variable_scope(scope or 'linear'):\n    if reuse is True:\n      tf.get_variable_scope().reuse_variables()\n\n    w_init = None  # uniform\n    if input_size is None:\n      x_size = shape[1]\n    else:\n      x_size = input_size\n    if init_w == 'zeros':\n      w_init = tf.constant_initializer(0.0)\n    elif init_w == 'constant':\n      w_init = tf.constant_initializer(weight_start)\n    elif init_w == 'gaussian':\n      w_init = tf.random_normal_initializer(stddev=weight_start)\n    elif init_w == 'ortho':\n      w_init = lstm_ortho_initializer(1.0)\n\n    w = tf.get_variable(\n        'super_linear_w', [x_size, output_size], tf.float32, initializer=w_init)\n    if use_bias:\n      b = tf.get_variable(\n          'super_linear_b', [output_size],\n          tf.float32,\n          initializer=tf.constant_initializer(bias_start))\n      return tf.matmul(x, w) + b\n    return tf.matmul(x, w)\n\n\nclass LayerNormLSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"Layer-Norm, with Ortho Init. and Recurrent Dropout without Memory Loss.\n\n  https://arxiv.org/abs/1607.06450 - Layer Norm\n  https://arxiv.org/abs/1603.05118 - Recurrent Dropout without Memory Loss\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.90):\n    \"\"\"Initialize the Layer Norm LSTM cell.\n\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (default 1.0).\n      use_recurrent_dropout: Whether to use Recurrent Dropout (default False)\n      dropout_keep_prob: float, dropout keep probability (default 0.90)\n    \"\"\"\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n\n  @property\n  def input_size(self):\n    return self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  @property\n  def state_size(self):\n    return 2 * self.num_units\n\n  def get_output(self, state):\n    h, unused_c = tf.split(state, 2, 1)\n    return h\n\n  def __call__(self, x, state, timestep=0, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      h, c = tf.split(state, 2, 1)\n\n      h_size = self.num_units\n      x_size = x.get_shape().as_list()[1]\n      batch_size = x.get_shape().as_list()[0]\n\n      w_init = None  # uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n\n      concat = tf.concat([x, h], 1)  # concat for speed.\n      w_full = tf.concat([w_xh, w_hh], 0)\n      concat = tf.matmul(concat, w_full)  #+ bias # live life without garbage.\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      concat = layer_norm_all(concat, batch_size, 4, h_size, 'ln_all')\n      i, j, f, o = tf.split(concat, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(layer_norm(new_c, h_size, 'ln_c')) * tf.sigmoid(o)\n\n    return new_h, tf.concat([new_h, new_c], 1)\n\n\nclass HyperLSTMCell(tf.contrib.rnn.RNNCell):\n  \"\"\"HyperLSTM with Ortho Init, Layer Norm, Recurrent Dropout, no Memory Loss.\n\n  https://arxiv.org/abs/1609.09106\n  http://blog.otoro.net/2016/09/28/hyper-networks/\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               use_recurrent_dropout=False,\n               dropout_keep_prob=0.90,\n               use_layer_norm=True,\n               hyper_num_units=256,\n               hyper_embedding_size=32,\n               hyper_use_recurrent_dropout=False):\n    \"\"\"Initialize the Layer Norm HyperLSTM cell.\n\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (default 1.0).\n      use_recurrent_dropout: Whether to use Recurrent Dropout (default False)\n      dropout_keep_prob: float, dropout keep probability (default 0.90)\n      use_layer_norm: boolean. (default True)\n        Controls whether we use LayerNorm layers in main LSTM & HyperLSTM cell.\n      hyper_num_units: int, number of units in HyperLSTM cell.\n        (default is 128, recommend experimenting with 256 for larger tasks)\n      hyper_embedding_size: int, size of signals emitted from HyperLSTM cell.\n        (default is 16, recommend trying larger values for large datasets)\n      hyper_use_recurrent_dropout: boolean. (default False)\n        Controls whether HyperLSTM cell also uses recurrent dropout.\n        Recommend turning this on only if hyper_num_units becomes large (>= 512)\n    \"\"\"\n    self.num_units = num_units\n    self.forget_bias = forget_bias\n    self.use_recurrent_dropout = use_recurrent_dropout\n    self.dropout_keep_prob = dropout_keep_prob\n    self.use_layer_norm = use_layer_norm\n    self.hyper_num_units = hyper_num_units\n    self.hyper_embedding_size = hyper_embedding_size\n    self.hyper_use_recurrent_dropout = hyper_use_recurrent_dropout\n\n    self.total_num_units = self.num_units + self.hyper_num_units\n\n    if self.use_layer_norm:\n      cell_fn = LayerNormLSTMCell\n    else:\n      cell_fn = LSTMCell\n    self.hyper_cell = cell_fn(\n        hyper_num_units,\n        use_recurrent_dropout=hyper_use_recurrent_dropout,\n        dropout_keep_prob=dropout_keep_prob)\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  @property\n  def state_size(self):\n    return 2 * self.total_num_units\n\n  def get_output(self, state):\n    total_h, unused_total_c = tf.split(state, 2, 1)\n    h = total_h[:, 0:self.num_units]\n    return h\n\n  def hyper_norm(self, layer, scope='hyper', use_bias=True):\n    num_units = self.num_units\n    embedding_size = self.hyper_embedding_size\n    # recurrent batch norm init trick (https://arxiv.org/abs/1603.09025).\n    init_gamma = 0.10  # cooijmans' da man.\n    with tf.variable_scope(scope):\n      zw = super_linear(\n          self.hyper_output,\n          embedding_size,\n          init_w='constant',\n          weight_start=0.00,\n          use_bias=True,\n          bias_start=1.0,\n          scope='zw')\n      alpha = super_linear(\n          zw,\n          num_units,\n          init_w='constant',\n          weight_start=init_gamma / embedding_size,\n          use_bias=False,\n          scope='alpha')\n      result = tf.multiply(alpha, layer)\n      if use_bias:\n        zb = super_linear(\n            self.hyper_output,\n            embedding_size,\n            init_w='gaussian',\n            weight_start=0.01,\n            use_bias=False,\n            bias_start=0.0,\n            scope='zb')\n        beta = super_linear(\n            zb,\n            num_units,\n            init_w='constant',\n            weight_start=0.00,\n            use_bias=False,\n            scope='beta')\n        result += beta\n    return result\n\n  def __call__(self, x, state, timestep=0, scope=None):\n    with tf.variable_scope(scope or type(self).__name__):\n      total_h, total_c = tf.split(state, 2, 1)\n      h = total_h[:, 0:self.num_units]\n      c = total_c[:, 0:self.num_units]\n      self.hyper_state = tf.concat(\n          [total_h[:, self.num_units:], total_c[:, self.num_units:]], 1)\n\n      batch_size = x.get_shape().as_list()[0]\n      x_size = x.get_shape().as_list()[1]\n      self._input_size = x_size\n\n      w_init = None  # uniform\n\n      h_init = lstm_ortho_initializer(1.0)\n\n      w_xh = tf.get_variable(\n          'W_xh', [x_size, 4 * self.num_units], initializer=w_init)\n      w_hh = tf.get_variable(\n          'W_hh', [self.num_units, 4 * self.num_units], initializer=h_init)\n      bias = tf.get_variable(\n          'bias', [4 * self.num_units],\n          initializer=tf.constant_initializer(0.0))\n\n      # concatenate the input and hidden states for hyperlstm input\n      hyper_input = tf.concat([x, h], 1)\n      hyper_output, hyper_new_state = self.hyper_cell(hyper_input,\n                                                      self.hyper_state)\n      self.hyper_output = hyper_output\n      self.hyper_state = hyper_new_state\n\n      xh = tf.matmul(x, w_xh)\n      hh = tf.matmul(h, w_hh)\n\n      # split Wxh contributions\n      ix, jx, fx, ox = tf.split(xh, 4, 1)\n      ix = self.hyper_norm(ix, 'hyper_ix', use_bias=False)\n      jx = self.hyper_norm(jx, 'hyper_jx', use_bias=False)\n      fx = self.hyper_norm(fx, 'hyper_fx', use_bias=False)\n      ox = self.hyper_norm(ox, 'hyper_ox', use_bias=False)\n\n      # split Whh contributions\n      ih, jh, fh, oh = tf.split(hh, 4, 1)\n      ih = self.hyper_norm(ih, 'hyper_ih', use_bias=True)\n      jh = self.hyper_norm(jh, 'hyper_jh', use_bias=True)\n      fh = self.hyper_norm(fh, 'hyper_fh', use_bias=True)\n      oh = self.hyper_norm(oh, 'hyper_oh', use_bias=True)\n\n      # split bias\n      ib, jb, fb, ob = tf.split(bias, 4, 0)  # bias is to be broadcasted.\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i = ix + ih + ib\n      j = jx + jh + jb\n      f = fx + fh + fb\n      o = ox + oh + ob\n\n      if self.use_layer_norm:\n        concat = tf.concat([i, j, f, o], 1)\n        concat = layer_norm_all(concat, batch_size, 4, self.num_units, 'ln_all')\n        i, j, f, o = tf.split(concat, 4, 1)\n\n      if self.use_recurrent_dropout:\n        g = tf.nn.dropout(tf.tanh(j), self.dropout_keep_prob)\n      else:\n        g = tf.tanh(j)\n\n      new_c = c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * g\n      new_h = tf.tanh(layer_norm(new_c, self.num_units, 'ln_c')) * tf.sigmoid(o)\n\n      hyper_h, hyper_c = tf.split(hyper_new_state, 2, 1)\n      new_total_h = tf.concat([new_h, hyper_h], 1)\n      new_total_c = tf.concat([new_c, hyper_c], 1)\n      new_total_state = tf.concat([new_total_h, new_total_c], 1)\n    return new_h, new_total_state\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "rnn.py", "id": "bc11134fc3e7acf18c519021d28a0a09", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-magenta/tensorflow-magenta-c3eda3d/magenta/models/sketch_rnn/rnn.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:52:33Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}