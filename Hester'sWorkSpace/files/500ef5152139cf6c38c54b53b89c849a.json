{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\nimport numpy as np\nimport scipy.optimize\n\n\ndef var_size(v):\n  return int(np.prod([int(d) for d in v.shape]))\n\n\ndef gradients(loss, var_list):\n  grads = tf.gradients(loss, var_list)\n  return [g if g is not None else tf.zeros(v.shape)\n          for g, v in zip(grads, var_list)]\n\ndef flatgrad(loss, var_list):\n  grads = gradients(loss, var_list)\n  return tf.concat([tf.reshape(grad, [-1])\n                    for (v, grad) in zip(var_list, grads)\n                    if grad is not None], 0)\n\n\ndef get_flat(var_list):\n  return tf.concat([tf.reshape(v, [-1]) for v in var_list], 0)\n\n\ndef set_from_flat(var_list, flat_theta):\n  assigns = []\n  shapes = [v.shape for v in var_list]\n  sizes = [var_size(v) for v in var_list]\n\n  start = 0\n  assigns = []\n  for (shape, size, v) in zip(shapes, sizes, var_list):\n    assigns.append(v.assign(\n        tf.reshape(flat_theta[start:start + size], shape)))\n    start += size\n  assert start == sum(sizes)\n\n  return tf.group(*assigns)\n\n\nclass LbfgsOptimization(object):\n\n  def __init__(self, max_iter=25, mix_frac=1.0):\n    self.max_iter = max_iter\n    self.mix_frac = mix_frac\n\n  def setup_placeholders(self):\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n    self.intended_values = tf.placeholder(tf.float32, [None], 'intended_values')\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.setup_placeholders()\n    self.values = values\n    self.targets = targets\n\n    self.raw_loss = (tf.reduce_sum((1 - pads) * tf.square(values - self.intended_values))\n                     / tf.reduce_sum(1 - pads))\n    self.loss_flat_gradient = flatgrad(self.raw_loss, var_list)\n\n    self.flat_vars = get_flat(var_list)\n    self.set_vars = set_from_flat(var_list, self.flat_theta)\n\n  def optimize(self, sess, feed_dict):\n    old_theta = sess.run(self.flat_vars)\n\n    old_values, targets = sess.run([self.values, self.targets], feed_dict=feed_dict)\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n    feed_dict = dict(feed_dict)\n    feed_dict[self.intended_values] = intended_values\n\n    def calc_loss_and_grad(theta):\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n      loss, grad = sess.run([self.raw_loss, self.loss_flat_gradient],\n                            feed_dict=feed_dict)\n      grad = grad.astype('float64')\n      return loss, grad\n\n    theta, _, _ = scipy.optimize.fmin_l_bfgs_b(\n        calc_loss_and_grad, old_theta, maxiter=self.max_iter)\n    sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n\n\nclass GradOptimization(object):\n\n  def __init__(self, learning_rate=0.001, max_iter=25, mix_frac=1.0):\n    self.learning_rate = learning_rate\n    self.max_iter = max_iter\n    self.mix_frac = mix_frac\n\n  def get_optimizer(self):\n    return tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n                                  epsilon=2e-4)\n\n  def setup_placeholders(self):\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n    self.intended_values = tf.placeholder(tf.float32, [None], 'intended_values')\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.setup_placeholders()\n    self.values = values\n    self.targets = targets\n\n    self.raw_loss = (tf.reduce_sum((1 - pads) * tf.square(values - self.intended_values))\n                     / tf.reduce_sum(1 - pads))\n\n    opt = self.get_optimizer()\n    params = var_list\n    grads = tf.gradients(self.raw_loss, params)\n    self.gradient_ops = opt.apply_gradients(zip(grads, params))\n\n  def optimize(self, sess, feed_dict):\n    old_values, targets = sess.run([self.values, self.targets], feed_dict=feed_dict)\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n\n    feed_dict = dict(feed_dict)\n    feed_dict[self.intended_values] = intended_values\n\n    for _ in xrange(self.max_iter):\n      sess.run(self.gradient_ops, feed_dict=feed_dict)\n\n\nclass BestFitOptimization(object):\n\n  def __init__(self, mix_frac=1.0):\n    self.mix_frac = mix_frac\n\n  def setup_placeholders(self):\n    self.new_regression_weight = tf.placeholder(\n        tf.float32, self.regression_weight.shape)\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.values = values\n    self.targets = targets\n\n    self.inputs = inputs\n    self.regression_weight = regression_weight\n\n    self.setup_placeholders()\n\n    self.update_regression_weight = tf.assign(\n        self.regression_weight, self.new_regression_weight)\n\n  def optimize(self, sess, feed_dict):\n    reg_input, reg_weight, old_values, targets = sess.run(\n        [self.inputs, self.regression_weight, self.values, self.targets],\n        feed_dict=feed_dict)\n\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n\n     taken from rllab\n    reg_coeff = 1e-5\n    for _ in range(5):\n      best_fit_weight = np.linalg.lstsq(\n          reg_input.T.dot(reg_input) +\n          reg_coeff * np.identity(reg_input.shape[1]),\n          reg_input.T.dot(intended_values))[0]\n      if not np.any(np.isnan(best_fit_weight)):\n        break\n      reg_coeff *= 10\n\n    if len(best_fit_weight.shape) == 1:\n      best_fit_weight = np.expand_dims(best_fit_weight, -1)\n\n    sess.run(self.update_regression_weight,\n             feed_dict={self.new_regression_weight: best_fit_weight})\n", "comments": "   optimizers mostly value estimate   gradient descent optimizer lbfgs optimizer best fit optimizer         copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       taken rllab ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Optimizers mostly for value estimate.\n\nGradient Descent optimizer\nLBFGS optimizer\nBest Fit optimizer\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\nimport numpy as np\nimport scipy.optimize\n\n\ndef var_size(v):\n  return int(np.prod([int(d) for d in v.shape]))\n\n\ndef gradients(loss, var_list):\n  grads = tf.gradients(loss, var_list)\n  return [g if g is not None else tf.zeros(v.shape)\n          for g, v in zip(grads, var_list)]\n\ndef flatgrad(loss, var_list):\n  grads = gradients(loss, var_list)\n  return tf.concat([tf.reshape(grad, [-1])\n                    for (v, grad) in zip(var_list, grads)\n                    if grad is not None], 0)\n\n\ndef get_flat(var_list):\n  return tf.concat([tf.reshape(v, [-1]) for v in var_list], 0)\n\n\ndef set_from_flat(var_list, flat_theta):\n  assigns = []\n  shapes = [v.shape for v in var_list]\n  sizes = [var_size(v) for v in var_list]\n\n  start = 0\n  assigns = []\n  for (shape, size, v) in zip(shapes, sizes, var_list):\n    assigns.append(v.assign(\n        tf.reshape(flat_theta[start:start + size], shape)))\n    start += size\n  assert start == sum(sizes)\n\n  return tf.group(*assigns)\n\n\nclass LbfgsOptimization(object):\n\n  def __init__(self, max_iter=25, mix_frac=1.0):\n    self.max_iter = max_iter\n    self.mix_frac = mix_frac\n\n  def setup_placeholders(self):\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n    self.intended_values = tf.placeholder(tf.float32, [None], 'intended_values')\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.setup_placeholders()\n    self.values = values\n    self.targets = targets\n\n    self.raw_loss = (tf.reduce_sum((1 - pads) * tf.square(values - self.intended_values))\n                     / tf.reduce_sum(1 - pads))\n    self.loss_flat_gradient = flatgrad(self.raw_loss, var_list)\n\n    self.flat_vars = get_flat(var_list)\n    self.set_vars = set_from_flat(var_list, self.flat_theta)\n\n  def optimize(self, sess, feed_dict):\n    old_theta = sess.run(self.flat_vars)\n\n    old_values, targets = sess.run([self.values, self.targets], feed_dict=feed_dict)\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n    feed_dict = dict(feed_dict)\n    feed_dict[self.intended_values] = intended_values\n\n    def calc_loss_and_grad(theta):\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n      loss, grad = sess.run([self.raw_loss, self.loss_flat_gradient],\n                            feed_dict=feed_dict)\n      grad = grad.astype('float64')\n      return loss, grad\n\n    theta, _, _ = scipy.optimize.fmin_l_bfgs_b(\n        calc_loss_and_grad, old_theta, maxiter=self.max_iter)\n    sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n\n\nclass GradOptimization(object):\n\n  def __init__(self, learning_rate=0.001, max_iter=25, mix_frac=1.0):\n    self.learning_rate = learning_rate\n    self.max_iter = max_iter\n    self.mix_frac = mix_frac\n\n  def get_optimizer(self):\n    return tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n                                  epsilon=2e-4)\n\n  def setup_placeholders(self):\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n    self.intended_values = tf.placeholder(tf.float32, [None], 'intended_values')\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.setup_placeholders()\n    self.values = values\n    self.targets = targets\n\n    self.raw_loss = (tf.reduce_sum((1 - pads) * tf.square(values - self.intended_values))\n                     / tf.reduce_sum(1 - pads))\n\n    opt = self.get_optimizer()\n    params = var_list\n    grads = tf.gradients(self.raw_loss, params)\n    self.gradient_ops = opt.apply_gradients(zip(grads, params))\n\n  def optimize(self, sess, feed_dict):\n    old_values, targets = sess.run([self.values, self.targets], feed_dict=feed_dict)\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n\n    feed_dict = dict(feed_dict)\n    feed_dict[self.intended_values] = intended_values\n\n    for _ in xrange(self.max_iter):\n      sess.run(self.gradient_ops, feed_dict=feed_dict)\n\n\nclass BestFitOptimization(object):\n\n  def __init__(self, mix_frac=1.0):\n    self.mix_frac = mix_frac\n\n  def setup_placeholders(self):\n    self.new_regression_weight = tf.placeholder(\n        tf.float32, self.regression_weight.shape)\n\n  def setup(self, var_list, values, targets, pads,\n            inputs, regression_weight):\n    self.values = values\n    self.targets = targets\n\n    self.inputs = inputs\n    self.regression_weight = regression_weight\n\n    self.setup_placeholders()\n\n    self.update_regression_weight = tf.assign(\n        self.regression_weight, self.new_regression_weight)\n\n  def optimize(self, sess, feed_dict):\n    reg_input, reg_weight, old_values, targets = sess.run(\n        [self.inputs, self.regression_weight, self.values, self.targets],\n        feed_dict=feed_dict)\n\n    intended_values = targets * self.mix_frac + old_values * (1 - self.mix_frac)\n\n    # taken from rllab\n    reg_coeff = 1e-5\n    for _ in range(5):\n      best_fit_weight = np.linalg.lstsq(\n          reg_input.T.dot(reg_input) +\n          reg_coeff * np.identity(reg_input.shape[1]),\n          reg_input.T.dot(intended_values))[0]\n      if not np.any(np.isnan(best_fit_weight)):\n        break\n      reg_coeff *= 10\n\n    if len(best_fit_weight.shape) == 1:\n      best_fit_weight = np.expand_dims(best_fit_weight, -1)\n\n    sess.run(self.update_regression_weight,\n             feed_dict={self.new_regression_weight: best_fit_weight})\n", "description": "Models and examples built with TensorFlow", "file_name": "optimizers.py", "id": "500ef5152139cf6c38c54b53b89c849a", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/pcl_rl/optimizers.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}