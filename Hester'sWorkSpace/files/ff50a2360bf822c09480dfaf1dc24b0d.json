{"author": "aws", "code": "\n\n\n Licensed under the Apache License, Version 2.0 (the \"License\"). You\n may not use this file except in compliance with the License. A copy of\n the License is located at\n\n     http://aws.amazon.com/apache2.0/\n\n or in the \"license\" file accompanying this file. This file is\n distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n ANY KIND, either express or implied. See the License for the specific\n language governing permissions and limitations under the License.\nimport mock\n\nfrom awscli.testutils import BaseAWSCommandParamsTest\nfrom awscli.testutils import capture_input, set_invalid_utime\nfrom awscli.compat import six\nfrom tests.functional.s3 import BaseS3TransferCommandTest\n\n\nclass BufferedBytesIO(six.BytesIO):\n    @property\n    def buffer(self):\n        return self\n\n\nclass BaseCPCommandTest(BaseS3TransferCommandTest):\n    prefix = 's3 cp '\n\n\nclass TestCPCommand(BaseCPCommandTest):\n    def test_operations_used_in_upload(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n\n    def test_key_name_added_when_only_bucket_provided(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'foo.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n\n    def test_trailing_slash_appended(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n         Here we're saying s3://bucket instead of s3://bucket/\n         This should still work the same as if we added the trailing slash.\n        cmdline = '%s %s s3://bucket' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'foo.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n\n    def test_upload_grants(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = ('%s %s s3://bucket/key.txt --grants read=id=foo '\n                   'full=id=bar readacl=id=biz writeacl=id=baz' %\n                   (self.prefix, full_path))\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': u'key.txt', 'Bucket': u'bucket', 'GrantRead': u'id=foo',\n             'GrantFullControl': u'id=bar', 'GrantReadACP': u'id=biz',\n             'GrantWriteACP': u'id=baz', 'ContentType': u'text/plain',\n             'Body': mock.ANY}\n        )\n\n    def test_upload_expires(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = ('%s %s s3://bucket/key.txt --expires 90' %\n                   (self.prefix, full_path))\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'key.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n        self.assertEqual(self.operations_called[0][1]['Expires'], '90')\n\n    def test_operations_used_in_download_file(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/key.txt %s' % (self.prefix,\n                                                 self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n         The only operations we should have called are HeadObject/GetObject.\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_operations_used_in_recursive_download(self):\n        self.parsed_responses = [\n            {'ETag': '\"foo-1\"', 'Contents': [], 'CommonPrefixes': []},\n        ]\n        cmdline = '%s s3://bucket/key.txt %s --recursive' % (\n            self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n         We called ListObjects but had no objects to download, so\n         we only have a single ListObjects operation being called.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n    def test_website_redirect_ignore_paramfile(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt --website-redirect %s' % \\\n            (self.prefix, full_path, 'http://someserver')\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n         Make sure that the specified web address is used as opposed to the\n         contents of the web address.\n        self.assertEqual(\n            self.operations_called[0][1]['WebsiteRedirectLocation'],\n            'http://someserver'\n        )\n\n    def test_metadata_copy(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertEqual(self.operations_called[1][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_copy_with_put_object(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s %s s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % (self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_copy_with_multipart_upload(self):\n        full_path = self.files.create_file('foo.txt', 'a' * 10 * (1024 ** 2))\n        self.parsed_responses = [\n            {'UploadId': 'foo'},\n            {'ETag': '\"foo-1\"'},\n            {'ETag': '\"foo-2\"'},\n            {}\n        ]\n        cmdline = ('%s %s s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % (self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 4,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name,\n                         'CreateMultipartUpload')\n        self.assertEqual(self.operations_called[0][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_directive_copy(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt s3://bucket/key2.txt'\n                   ' --metadata-directive REPLACE' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertEqual(self.operations_called[1][1]['MetadataDirective'],\n                         'REPLACE')\n\n    def test_no_metadata_directive_for_non_copy(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket --metadata-directive REPLACE' % \\\n            (self.prefix, full_path)\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertNotIn('MetadataDirective', self.operations_called[0][1])\n\n    def test_cp_succeeds_with_mimetype_errors(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt' % (self.prefix, full_path)\n        self.parsed_responses = [\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        with mock.patch('mimetypes.guess_type') as mock_guess_type:\n             This should throw a UnicodeDecodeError.\n            mock_guess_type.side_effect = lambda x: b'\\xe2'.decode('ascii')\n            self.run_cmd(cmdline, expected_rc=0)\n         Because of the decoding error the command should have succeeded\n         just that there was no content type added.\n        self.assertNotIn('ContentType', self.last_kwargs)\n\n    def test_cp_fails_with_utime_errors_but_continues(self):\n        full_path = self.files.create_file('foo.txt', '')\n        cmdline = '%s s3://bucket/key.txt %s' % (self.prefix, full_path)\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')}\n        ]\n        with mock.patch('os.utime') as mock_utime:\n            mock_utime.side_effect = OSError(1, '')\n            _, err, _ = self.run_cmd(cmdline, expected_rc=2)\n            self.assertIn('attempting to modify the utime', err)\n\n    def test_recursive_glacier_download_with_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/foo %s --recursive --force-glacier-transfer'\\\n                  % (self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_recursive_glacier_download_without_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            }\n        ]\n        cmdline = '%s s3://bucket/foo %s --recursive' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertIn('GLACIER', stderr)\n\n    def test_warns_on_glacier_incompatible_operation(self):\n        self.parsed_responses = [\n            {'ContentLength': '100', 'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt .' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n         There should not have been a download attempted because the\n         operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertIn('GLACIER', stderr)\n\n    def test_warns_on_glacier_incompatible_operation_for_multipart_file(self):\n        self.parsed_responses = [\n            {'ContentLength': str(20 * (1024 ** 2)),\n             'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt .' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n         There should not have been a download attempted because the\n         operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertIn('GLACIER', stderr)\n\n    def test_turn_off_glacier_warnings(self):\n        self.parsed_responses = [\n            {'ContentLength': str(20 * (1024 ** 2)),\n             'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = (\n            '%s s3://bucket/key.txt . --ignore-glacier-warnings' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=0)\n         There should not have been a download attempted because the\n         operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual('', stderr)\n\n    def test_cp_with_sse_flag(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'ServerSideEncryption': 'AES256'}\n        )\n\n    def test_cp_with_sse_c_flag(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse-c --sse-c-key foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'SSECustomerAlgorithm': 'AES256', 'SSECustomerKey': 'Zm9v',\n             'SSECustomerKeyMD5': 'rL0Y20zC+Fzt72VPzMSk2A=='}\n        )\n\n     Note ideally the kms sse with a key id would be integration tests\n     However, you cannot delete kms keys so there would be no way to clean\n     up the tests\n    def test_cp_upload_with_sse_kms_and_key_id(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse aws:kms --sse-kms-key-id foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_upload_large_file_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'UploadId': 'foo'},   CreateMultipartUpload\n            {'ETag': '\"foo\"'},   UploadPart\n            {'ETag': '\"foo\"'},   UploadPart\n            {}   CompleteMultipartUpload\n        ]\n        full_path = self.files.create_file('foo.txt', 'a' * 10 * (1024 ** 2))\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse aws:kms --sse-kms-key-id foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 4)\n\n         We are only really concerned that the CreateMultipartUpload\n         used the KMS key id.\n        self.assertEqual(\n            self.operations_called[0][0].name, 'CreateMultipartUpload')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_copy_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'ContentLength': 5, 'LastModified': '00:00:00Z'},   HeadObject\n            {}   CopyObject\n        ]\n        cmdline = (\n            '%s s3://bucket/key1.txt s3://bucket/key2.txt '\n            '--sse aws:kms --sse-kms-key-id foo' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2)\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertDictEqual(\n            self.operations_called[1][1],\n            {'Key': 'key2.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'CopySource': 'bucket/key1.txt',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_copy_large_file_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'ContentLength': 10 * (1024 ** 2),\n             'LastModified': '00:00:00Z'},   HeadObject\n            {'UploadId': 'foo'},   CreateMultipartUpload\n            {'CopyPartResult': {'ETag': '\"foo\"'}},   UploadPartCopy\n            {'CopyPartResult': {'ETag': '\"foo\"'}},   UploadPartCopy\n            {}   CompleteMultipartUpload\n        ]\n        cmdline = (\n            '%s s3://bucket/key1.txt s3://bucket/key2.txt '\n            '--sse aws:kms --sse-kms-key-id foo' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 5)\n\n         We are only really concerned that the CreateMultipartUpload\n         used the KMS key id.\n        self.assertEqual(\n            self.operations_called[1][0].name, 'CreateMultipartUpload')\n        self.assertDictEqual(\n            self.operations_called[1][1],\n            {'Key': 'key2.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cannot_use_recursive_with_stream(self):\n        cmdline = '%s - s3://bucket/key.txt --recursive' % self.prefix\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=255)\n        self.assertIn(\n            'Streaming currently is only compatible with non-recursive cp '\n            'commands', stderr)\n\n    def test_upload_unicode_path(self):\n        self.parsed_responses = [\n            {'ContentLength': 10,\n             'LastModified': '00:00:00Z'},   HeadObject\n            {'ETag': '\"foo\"'}   PutObject\n        ]\n        command = u's3 cp s3://bucket/\\u2603 s3://bucket/\\u2713'\n        stdout, stderr, rc = self.run_cmd(command, expected_rc=0)\n\n        success_message = (\n            u'copy: s3://bucket/\\u2603 to s3://bucket/\\u2713'\n        )\n        self.assertIn(success_message, stdout)\n\n        progress_message = 'Completed 10 Bytes'\n        self.assertIn(progress_message, stdout)\n\n    def test_cp_with_error_and_warning(self):\n        command = \"s3 cp %s s3://bucket/foo.txt\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        full_path = self.files.create_file('foo.txt', 'bar')\n        set_invalid_utime(full_path)\n        _, stderr, rc = self.run_cmd(command % full_path, expected_rc=1)\n        self.assertIn('upload failed', stderr)\n        self.assertIn('warning: File has an invalid timestamp.', stderr)\n\n\nclass TestStreamingCPCommand(BaseAWSCommandParamsTest):\n    def test_streaming_upload(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            self.run_cmd(command)\n\n        self.assertEqual(len(self.operations_called), 1)\n        model, args = self.operations_called[0]\n        expected_args = {\n            'Bucket': 'bucket',\n            'Key': 'streaming.txt',\n            'Body': mock.ANY\n        }\n\n        self.assertEqual(model.name, 'PutObject')\n        self.assertEqual(args, expected_args)\n\n    def test_streaming_upload_with_expected_size(self):\n        command = \"s3 cp - s3://bucket/streaming.txt --expected-size 4\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            self.run_cmd(command)\n\n        self.assertEqual(len(self.operations_called), 1)\n        model, args = self.operations_called[0]\n        expected_args = {\n            'Bucket': 'bucket',\n            'Key': 'streaming.txt',\n            'Body': mock.ANY\n        }\n\n        self.assertEqual(model.name, 'PutObject')\n        self.assertEqual(args, expected_args)\n\n    def test_streaming_upload_error(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            _, stderr, _ = self.run_cmd(command, expected_rc=1)\n\n        error_message = (\n            'An error occurred (NoSuchBucket) when calling '\n            'the PutObject operation: The specified bucket does not exist'\n        )\n        self.assertIn(error_message, stderr)\n\n    def test_streaming_upload_when_stdin_unavailable(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        with mock.patch('sys.stdin', None):\n            _, stderr, _ = self.run_cmd(command, expected_rc=1)\n\n        expected_message = (\n            'stdin is required for this operation, but is not available'\n        )\n        self.assertIn(expected_message, stderr)\n\n    def test_streaming_download(self):\n        command = \"s3 cp s3://bucket/streaming.txt -\"\n        self.parsed_responses = [\n            {\n                \"AcceptRanges\": \"bytes\",\n                \"LastModified\": \"Tue, 12 Jul 2016 21:26:07 GMT\",\n                \"ContentLength\": 4,\n                \"ETag\": '\"d3b07384d113edec49eaa6238ad5ff00\"',\n                \"Metadata\": {},\n                \"ContentType\": \"binary/octet-stream\"\n            },\n            {\n                \"AcceptRanges\": \"bytes\",\n                \"Metadata\": {},\n                \"ContentType\": \"binary/octet-stream\",\n                \"ContentLength\": 4,\n                \"ETag\": '\"d3b07384d113edec49eaa6238ad5ff00\"',\n                \"LastModified\": \"Tue, 12 Jul 2016 21:26:07 GMT\",\n                \"Body\": six.BytesIO(b'foo\\n')\n            }\n        ]\n\n        stdout, stderr, rc = self.run_cmd(command)\n        self.assertEqual(stdout, 'foo\\n')\n\n         Ensures no extra operations were called\n        self.assertEqual(len(self.operations_called), 2)\n        ops = [op[0].name for op in self.operations_called]\n        expected_ops = ['HeadObject', 'GetObject']\n        self.assertEqual(ops, expected_ops)\n\n    def test_streaming_download_error(self):\n        command = \"s3 cp s3://bucket/streaming.txt -\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        _, stderr, _ = self.run_cmd(command, expected_rc=1)\n        error_message = (\n            'An error occurred (NoSuchBucket) when calling '\n            'the HeadObject operation: The specified bucket does not exist'\n        )\n        self.assertIn(error_message, stderr)\n\n\nclass TestCpCommandWithRequesterPayer(BaseCPCommandTest):\n    def test_single_upload(self):\n        full_path = self.files.create_file('myfile', 'mycontent')\n        cmdline = (\n            '%s %s s3://mybucket/mykey --request-payer' % (\n                self.prefix, full_path\n            )\n        )\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('PutObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Body': mock.ANY,\n                })\n            ]\n        )\n\n    def test_multipart_upload(self):\n        full_path = self.files.create_file('myfile', 'a' * 10 * (1024 ** 2))\n        cmdline = (\n            '%s %s s3://mybucket/mykey --request-payer' % (\n                self.prefix, full_path))\n\n        self.parsed_responses = [\n            {'UploadId': 'myid'},       CreateMultipartUpload\n            {'ETag': '\"myetag\"'},       UploadPart\n            {'ETag': '\"myetag\"'},       UploadPart\n            {}                          CompleteMultipartUpload\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('CreateMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('UploadPart', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'PartNumber': mock.ANY,\n                    'Body': mock.ANY,\n                }),\n                ('UploadPart', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'PartNumber': mock.ANY,\n                    'Body': mock.ANY,\n\n                }),\n                ('CompleteMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'MultipartUpload': {'Parts': [\n                        {'ETag': '\"myetag\"', 'PartNumber': 1},\n                        {'ETag': '\"myetag\"', 'PartNumber': 2}]\n                    }\n                })\n            ]\n        )\n\n    def test_recursive_upload(self):\n        self.files.create_file('myfile', 'mycontent')\n        cmdline = (\n            '%s %s s3://mybucket/ --request-payer --recursive' % (\n                self.prefix, self.files.rootdir\n            )\n        )\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('PutObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'myfile',\n                    'RequestPayer': 'requester',\n                    'Body': mock.ANY,\n                })\n            ]\n        )\n\n    def test_single_download(self):\n        cmdline = '%s s3://mybucket/mykey %s --request-payer' % (\n            self.prefix, self.files.rootdir)\n\n        self.parsed_responses = [\n            {\"ContentLength\": 100, \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_ranged_download(self):\n        cmdline = '%s s3://mybucket/mykey %s --request-payer' % (\n            self.prefix, self.files.rootdir)\n\n        self.parsed_responses = [\n            {\"ContentLength\": 10 * (1024 ** 2), \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')}\n        ]\n\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Range': mock.ANY,\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Range': mock.ANY,\n                })\n            ]\n        )\n\n    def test_recursive_download(self):\n        cmdline = '%s s3://mybucket/ %s --request-payer --recursive' % (\n            self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_single_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/sourcekey s3://mybucket/mykey'\n        cmdline += ' --request-payer'\n        self.parsed_responses = [\n            {'ContentLength': 5, 'LastModified': '00:00:00Z'},\n            {}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'sourcebucket',\n                    'Key': 'sourcekey',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_multipart_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/sourcekey s3://mybucket/mykey'\n        cmdline += ' --request-payer'\n        self.parsed_responses = [\n            {'ContentLength': 10 * (1024 ** 2),\n             'LastModified': '00:00:00Z'},            HeadObject\n            {'UploadId': 'myid'},                     CreateMultipartUpload\n            {'CopyPartResult': {'ETag': '\"etag\"'}},   UploadPartCopy\n            {'CopyPartResult': {'ETag': '\"etag\"'}},   UploadPartCopy\n            {}                                        CompleteMultipartUpload\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'sourcebucket',\n                    'Key': 'sourcekey',\n                    'RequestPayer': 'requester',\n                }),\n                ('CreateMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester'\n                }),\n                ('UploadPartCopy', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'PartNumber': mock.ANY,\n                    'CopySourceRange': mock.ANY,\n\n                }),\n                ('UploadPartCopy', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'PartNumber': mock.ANY,\n                    'CopySourceRange': mock.ANY,\n                }),\n                ('CompleteMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'MultipartUpload': {'Parts': [\n                        {'ETag': '\"etag\"', 'PartNumber': 1},\n                        {'ETag': '\"etag\"', 'PartNumber': 2}]\n                    }\n                })\n            ]\n        )\n\n    def test_recursive_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/ s3://mybucket/'\n        cmdline += ' --request-payer'\n        cmdline += ' --recursive'\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n", "comments": "   usr bin env python    copyright 2013 amazon com  inc  affiliates  all rights reserved        licensed apache license  version 2 0 (the  license )  you    may use file except compliance license  a copy    license located           http   aws amazon com apache2 0         license  file accompanying file  this file    distributed  as is  basis  without warranties or conditions of    any kind  either express implied  see license specific    language governing permissions limitations license     the operation called putobject     the operation called putobject     here saying s3   bucket instead s3   bucket     this still work added trailing slash     the operation called putobject     the operation called putobject     the operation called putobject     the operations called headobject getobject     we called listobjects objects download     single listobjects operation called     make sure specified web address used opposed    contents web address     this throw unicodedecodeerror     because decoding error command succeeded    content type added     there download attempted    operation skipped glacier incompatible     there download attempted    operation skipped glacier incompatible     there download attempted    operation skipped glacier incompatible     note ideally kms sse key id would integration tests    however  cannot delete kms keys would way clean    tests    createmultipartupload    uploadpart    uploadpart    completemultipartupload    we really concerned createmultipartupload    used kms key id     headobject    copyobject    headobject    createmultipartupload    uploadpartcopy    uploadpartcopy    completemultipartupload    we really concerned createmultipartupload    used kms key id     headobject    putobject    ensures extra operations called    createmultipartupload    uploadpart    uploadpart    completemultipartupload    headobject    createmultipartupload    uploadpartcopy    uploadpartcopy    completemultipartupload ", "content": "#!/usr/bin/env python\n# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport mock\n\nfrom awscli.testutils import BaseAWSCommandParamsTest\nfrom awscli.testutils import capture_input, set_invalid_utime\nfrom awscli.compat import six\nfrom tests.functional.s3 import BaseS3TransferCommandTest\n\n\nclass BufferedBytesIO(six.BytesIO):\n    @property\n    def buffer(self):\n        return self\n\n\nclass BaseCPCommandTest(BaseS3TransferCommandTest):\n    prefix = 's3 cp '\n\n\nclass TestCPCommand(BaseCPCommandTest):\n    def test_operations_used_in_upload(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n\n    def test_key_name_added_when_only_bucket_provided(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'foo.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n\n    def test_trailing_slash_appended(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        # Here we're saying s3://bucket instead of s3://bucket/\n        # This should still work the same as if we added the trailing slash.\n        cmdline = '%s %s s3://bucket' % (self.prefix, full_path)\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'foo.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n\n    def test_upload_grants(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = ('%s %s s3://bucket/key.txt --grants read=id=foo '\n                   'full=id=bar readacl=id=biz writeacl=id=baz' %\n                   (self.prefix, full_path))\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': u'key.txt', 'Bucket': u'bucket', 'GrantRead': u'id=foo',\n             'GrantFullControl': u'id=bar', 'GrantReadACP': u'id=biz',\n             'GrantWriteACP': u'id=baz', 'ContentType': u'text/plain',\n             'Body': mock.ANY}\n        )\n\n    def test_upload_expires(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = ('%s %s s3://bucket/key.txt --expires 90' %\n                   (self.prefix, full_path))\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operation we should have called is PutObject.\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Key'], 'key.txt')\n        self.assertEqual(self.operations_called[0][1]['Bucket'], 'bucket')\n        self.assertEqual(self.operations_called[0][1]['Expires'], '90')\n\n    def test_operations_used_in_download_file(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/key.txt %s' % (self.prefix,\n                                                 self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        # The only operations we should have called are HeadObject/GetObject.\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_operations_used_in_recursive_download(self):\n        self.parsed_responses = [\n            {'ETag': '\"foo-1\"', 'Contents': [], 'CommonPrefixes': []},\n        ]\n        cmdline = '%s s3://bucket/key.txt %s --recursive' % (\n            self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        # We called ListObjects but had no objects to download, so\n        # we only have a single ListObjects operation being called.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n    def test_website_redirect_ignore_paramfile(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt --website-redirect %s' % \\\n            (self.prefix, full_path, 'http://someserver')\n        self.parsed_responses = [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        # Make sure that the specified web address is used as opposed to the\n        # contents of the web address.\n        self.assertEqual(\n            self.operations_called[0][1]['WebsiteRedirectLocation'],\n            'http://someserver'\n        )\n\n    def test_metadata_copy(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertEqual(self.operations_called[1][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_copy_with_put_object(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s %s s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % (self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertEqual(self.operations_called[0][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_copy_with_multipart_upload(self):\n        full_path = self.files.create_file('foo.txt', 'a' * 10 * (1024 ** 2))\n        self.parsed_responses = [\n            {'UploadId': 'foo'},\n            {'ETag': '\"foo-1\"'},\n            {'ETag': '\"foo-2\"'},\n            {}\n        ]\n        cmdline = ('%s %s s3://bucket/key2.txt'\n                   ' --metadata KeyName=Value' % (self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 4,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name,\n                         'CreateMultipartUpload')\n        self.assertEqual(self.operations_called[0][1]['Metadata'],\n                         {'KeyName': 'Value'})\n\n    def test_metadata_directive_copy(self):\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt s3://bucket/key2.txt'\n                   ' --metadata-directive REPLACE' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertEqual(self.operations_called[1][1]['MetadataDirective'],\n                         'REPLACE')\n\n    def test_no_metadata_directive_for_non_copy(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket --metadata-directive REPLACE' % \\\n            (self.prefix, full_path)\n        self.parsed_responses = \\\n            [{'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1,\n                         self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertNotIn('MetadataDirective', self.operations_called[0][1])\n\n    def test_cp_succeeds_with_mimetype_errors(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt' % (self.prefix, full_path)\n        self.parsed_responses = [\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}]\n        with mock.patch('mimetypes.guess_type') as mock_guess_type:\n            # This should throw a UnicodeDecodeError.\n            mock_guess_type.side_effect = lambda x: b'\\xe2'.decode('ascii')\n            self.run_cmd(cmdline, expected_rc=0)\n        # Because of the decoding error the command should have succeeded\n        # just that there was no content type added.\n        self.assertNotIn('ContentType', self.last_kwargs)\n\n    def test_cp_fails_with_utime_errors_but_continues(self):\n        full_path = self.files.create_file('foo.txt', '')\n        cmdline = '%s s3://bucket/key.txt %s' % (self.prefix, full_path)\n        self.parsed_responses = [\n            {\"ContentLength\": \"100\", \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')}\n        ]\n        with mock.patch('os.utime') as mock_utime:\n            mock_utime.side_effect = OSError(1, '')\n            _, err, _ = self.run_cmd(cmdline, expected_rc=2)\n            self.assertIn('attempting to modify the utime', err)\n\n    def test_recursive_glacier_download_with_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/foo %s --recursive --force-glacier-transfer'\\\n                  % (self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_recursive_glacier_download_without_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            }\n        ]\n        cmdline = '%s s3://bucket/foo %s --recursive' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertIn('GLACIER', stderr)\n\n    def test_warns_on_glacier_incompatible_operation(self):\n        self.parsed_responses = [\n            {'ContentLength': '100', 'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt .' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n        # There should not have been a download attempted because the\n        # operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertIn('GLACIER', stderr)\n\n    def test_warns_on_glacier_incompatible_operation_for_multipart_file(self):\n        self.parsed_responses = [\n            {'ContentLength': str(20 * (1024 ** 2)),\n             'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = ('%s s3://bucket/key.txt .' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n        # There should not have been a download attempted because the\n        # operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertIn('GLACIER', stderr)\n\n    def test_turn_off_glacier_warnings(self):\n        self.parsed_responses = [\n            {'ContentLength': str(20 * (1024 ** 2)),\n             'LastModified': '00:00:00Z',\n             'StorageClass': 'GLACIER'},\n        ]\n        cmdline = (\n            '%s s3://bucket/key.txt . --ignore-glacier-warnings' % self.prefix)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=0)\n        # There should not have been a download attempted because the\n        # operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')\n        self.assertEqual('', stderr)\n\n    def test_cp_with_sse_flag(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'ServerSideEncryption': 'AES256'}\n        )\n\n    def test_cp_with_sse_c_flag(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse-c --sse-c-key foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'SSECustomerAlgorithm': 'AES256', 'SSECustomerKey': 'Zm9v',\n             'SSECustomerKeyMD5': 'rL0Y20zC+Fzt72VPzMSk2A=='}\n        )\n\n    # Note ideally the kms sse with a key id would be integration tests\n    # However, you cannot delete kms keys so there would be no way to clean\n    # up the tests\n    def test_cp_upload_with_sse_kms_and_key_id(self):\n        full_path = self.files.create_file('foo.txt', 'contents')\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse aws:kms --sse-kms-key-id foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'PutObject')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'Body': mock.ANY,\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_upload_large_file_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'UploadId': 'foo'},  # CreateMultipartUpload\n            {'ETag': '\"foo\"'},  # UploadPart\n            {'ETag': '\"foo\"'},  # UploadPart\n            {}  # CompleteMultipartUpload\n        ]\n        full_path = self.files.create_file('foo.txt', 'a' * 10 * (1024 ** 2))\n        cmdline = (\n            '%s %s s3://bucket/key.txt --sse aws:kms --sse-kms-key-id foo' % (\n                self.prefix, full_path))\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 4)\n\n        # We are only really concerned that the CreateMultipartUpload\n        # used the KMS key id.\n        self.assertEqual(\n            self.operations_called[0][0].name, 'CreateMultipartUpload')\n        self.assertDictEqual(\n            self.operations_called[0][1],\n            {'Key': 'key.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_copy_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'ContentLength': 5, 'LastModified': '00:00:00Z'},  # HeadObject\n            {}  # CopyObject\n        ]\n        cmdline = (\n            '%s s3://bucket/key1.txt s3://bucket/key2.txt '\n            '--sse aws:kms --sse-kms-key-id foo' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2)\n        self.assertEqual(self.operations_called[1][0].name, 'CopyObject')\n        self.assertDictEqual(\n            self.operations_called[1][1],\n            {'Key': 'key2.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain', 'CopySource': 'bucket/key1.txt',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cp_copy_large_file_with_sse_kms_and_key_id(self):\n        self.parsed_responses = [\n            {'ContentLength': 10 * (1024 ** 2),\n             'LastModified': '00:00:00Z'},  # HeadObject\n            {'UploadId': 'foo'},  # CreateMultipartUpload\n            {'CopyPartResult': {'ETag': '\"foo\"'}},  # UploadPartCopy\n            {'CopyPartResult': {'ETag': '\"foo\"'}},  # UploadPartCopy\n            {}  # CompleteMultipartUpload\n        ]\n        cmdline = (\n            '%s s3://bucket/key1.txt s3://bucket/key2.txt '\n            '--sse aws:kms --sse-kms-key-id foo' % self.prefix)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 5)\n\n        # We are only really concerned that the CreateMultipartUpload\n        # used the KMS key id.\n        self.assertEqual(\n            self.operations_called[1][0].name, 'CreateMultipartUpload')\n        self.assertDictEqual(\n            self.operations_called[1][1],\n            {'Key': 'key2.txt', 'Bucket': 'bucket',\n             'ContentType': 'text/plain',\n             'SSEKMSKeyId': 'foo', 'ServerSideEncryption': 'aws:kms'}\n        )\n\n    def test_cannot_use_recursive_with_stream(self):\n        cmdline = '%s - s3://bucket/key.txt --recursive' % self.prefix\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=255)\n        self.assertIn(\n            'Streaming currently is only compatible with non-recursive cp '\n            'commands', stderr)\n\n    def test_upload_unicode_path(self):\n        self.parsed_responses = [\n            {'ContentLength': 10,\n             'LastModified': '00:00:00Z'},  # HeadObject\n            {'ETag': '\"foo\"'}  # PutObject\n        ]\n        command = u's3 cp s3://bucket/\\u2603 s3://bucket/\\u2713'\n        stdout, stderr, rc = self.run_cmd(command, expected_rc=0)\n\n        success_message = (\n            u'copy: s3://bucket/\\u2603 to s3://bucket/\\u2713'\n        )\n        self.assertIn(success_message, stdout)\n\n        progress_message = 'Completed 10 Bytes'\n        self.assertIn(progress_message, stdout)\n\n    def test_cp_with_error_and_warning(self):\n        command = \"s3 cp %s s3://bucket/foo.txt\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        full_path = self.files.create_file('foo.txt', 'bar')\n        set_invalid_utime(full_path)\n        _, stderr, rc = self.run_cmd(command % full_path, expected_rc=1)\n        self.assertIn('upload failed', stderr)\n        self.assertIn('warning: File has an invalid timestamp.', stderr)\n\n\nclass TestStreamingCPCommand(BaseAWSCommandParamsTest):\n    def test_streaming_upload(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            self.run_cmd(command)\n\n        self.assertEqual(len(self.operations_called), 1)\n        model, args = self.operations_called[0]\n        expected_args = {\n            'Bucket': 'bucket',\n            'Key': 'streaming.txt',\n            'Body': mock.ANY\n        }\n\n        self.assertEqual(model.name, 'PutObject')\n        self.assertEqual(args, expected_args)\n\n    def test_streaming_upload_with_expected_size(self):\n        command = \"s3 cp - s3://bucket/streaming.txt --expected-size 4\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            self.run_cmd(command)\n\n        self.assertEqual(len(self.operations_called), 1)\n        model, args = self.operations_called[0]\n        expected_args = {\n            'Bucket': 'bucket',\n            'Key': 'streaming.txt',\n            'Body': mock.ANY\n        }\n\n        self.assertEqual(model.name, 'PutObject')\n        self.assertEqual(args, expected_args)\n\n    def test_streaming_upload_error(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        binary_stdin = BufferedBytesIO(b'foo\\n')\n        with mock.patch('sys.stdin', binary_stdin):\n            _, stderr, _ = self.run_cmd(command, expected_rc=1)\n\n        error_message = (\n            'An error occurred (NoSuchBucket) when calling '\n            'the PutObject operation: The specified bucket does not exist'\n        )\n        self.assertIn(error_message, stderr)\n\n    def test_streaming_upload_when_stdin_unavailable(self):\n        command = \"s3 cp - s3://bucket/streaming.txt\"\n        self.parsed_responses = [{\n            'ETag': '\"c8afdb36c52cf4727836669019e69222\"'\n        }]\n\n        with mock.patch('sys.stdin', None):\n            _, stderr, _ = self.run_cmd(command, expected_rc=1)\n\n        expected_message = (\n            'stdin is required for this operation, but is not available'\n        )\n        self.assertIn(expected_message, stderr)\n\n    def test_streaming_download(self):\n        command = \"s3 cp s3://bucket/streaming.txt -\"\n        self.parsed_responses = [\n            {\n                \"AcceptRanges\": \"bytes\",\n                \"LastModified\": \"Tue, 12 Jul 2016 21:26:07 GMT\",\n                \"ContentLength\": 4,\n                \"ETag\": '\"d3b07384d113edec49eaa6238ad5ff00\"',\n                \"Metadata\": {},\n                \"ContentType\": \"binary/octet-stream\"\n            },\n            {\n                \"AcceptRanges\": \"bytes\",\n                \"Metadata\": {},\n                \"ContentType\": \"binary/octet-stream\",\n                \"ContentLength\": 4,\n                \"ETag\": '\"d3b07384d113edec49eaa6238ad5ff00\"',\n                \"LastModified\": \"Tue, 12 Jul 2016 21:26:07 GMT\",\n                \"Body\": six.BytesIO(b'foo\\n')\n            }\n        ]\n\n        stdout, stderr, rc = self.run_cmd(command)\n        self.assertEqual(stdout, 'foo\\n')\n\n        # Ensures no extra operations were called\n        self.assertEqual(len(self.operations_called), 2)\n        ops = [op[0].name for op in self.operations_called]\n        expected_ops = ['HeadObject', 'GetObject']\n        self.assertEqual(ops, expected_ops)\n\n    def test_streaming_download_error(self):\n        command = \"s3 cp s3://bucket/streaming.txt -\"\n        self.parsed_responses = [{\n            'Error': {\n                'Code': 'NoSuchBucket',\n                'Message': 'The specified bucket does not exist',\n                'BucketName': 'bucket'\n            }\n        }]\n        self.http_response.status_code = 404\n\n        _, stderr, _ = self.run_cmd(command, expected_rc=1)\n        error_message = (\n            'An error occurred (NoSuchBucket) when calling '\n            'the HeadObject operation: The specified bucket does not exist'\n        )\n        self.assertIn(error_message, stderr)\n\n\nclass TestCpCommandWithRequesterPayer(BaseCPCommandTest):\n    def test_single_upload(self):\n        full_path = self.files.create_file('myfile', 'mycontent')\n        cmdline = (\n            '%s %s s3://mybucket/mykey --request-payer' % (\n                self.prefix, full_path\n            )\n        )\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('PutObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Body': mock.ANY,\n                })\n            ]\n        )\n\n    def test_multipart_upload(self):\n        full_path = self.files.create_file('myfile', 'a' * 10 * (1024 ** 2))\n        cmdline = (\n            '%s %s s3://mybucket/mykey --request-payer' % (\n                self.prefix, full_path))\n\n        self.parsed_responses = [\n            {'UploadId': 'myid'},      # CreateMultipartUpload\n            {'ETag': '\"myetag\"'},      # UploadPart\n            {'ETag': '\"myetag\"'},      # UploadPart\n            {}                         # CompleteMultipartUpload\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('CreateMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('UploadPart', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'PartNumber': mock.ANY,\n                    'Body': mock.ANY,\n                }),\n                ('UploadPart', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'PartNumber': mock.ANY,\n                    'Body': mock.ANY,\n\n                }),\n                ('CompleteMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'UploadId': 'myid',\n                    'MultipartUpload': {'Parts': [\n                        {'ETag': '\"myetag\"', 'PartNumber': 1},\n                        {'ETag': '\"myetag\"', 'PartNumber': 2}]\n                    }\n                })\n            ]\n        )\n\n    def test_recursive_upload(self):\n        self.files.create_file('myfile', 'mycontent')\n        cmdline = (\n            '%s %s s3://mybucket/ --request-payer --recursive' % (\n                self.prefix, self.files.rootdir\n            )\n        )\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('PutObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'myfile',\n                    'RequestPayer': 'requester',\n                    'Body': mock.ANY,\n                })\n            ]\n        )\n\n    def test_single_download(self):\n        cmdline = '%s s3://mybucket/mykey %s --request-payer' % (\n            self.prefix, self.files.rootdir)\n\n        self.parsed_responses = [\n            {\"ContentLength\": 100, \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_ranged_download(self):\n        cmdline = '%s s3://mybucket/mykey %s --request-payer' % (\n            self.prefix, self.files.rootdir)\n\n        self.parsed_responses = [\n            {\"ContentLength\": 10 * (1024 ** 2), \"LastModified\": \"00:00:00Z\"},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')}\n        ]\n\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Range': mock.ANY,\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                    'Range': mock.ANY,\n                })\n            ]\n        )\n\n    def test_recursive_download(self):\n        cmdline = '%s s3://mybucket/ %s --request-payer --recursive' % (\n            self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('GetObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_single_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/sourcekey s3://mybucket/mykey'\n        cmdline += ' --request-payer'\n        self.parsed_responses = [\n            {'ContentLength': 5, 'LastModified': '00:00:00Z'},\n            {}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'sourcebucket',\n                    'Key': 'sourcekey',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_multipart_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/sourcekey s3://mybucket/mykey'\n        cmdline += ' --request-payer'\n        self.parsed_responses = [\n            {'ContentLength': 10 * (1024 ** 2),\n             'LastModified': '00:00:00Z'},           # HeadObject\n            {'UploadId': 'myid'},                    # CreateMultipartUpload\n            {'CopyPartResult': {'ETag': '\"etag\"'}},  # UploadPartCopy\n            {'CopyPartResult': {'ETag': '\"etag\"'}},  # UploadPartCopy\n            {}                                       # CompleteMultipartUpload\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('HeadObject', {\n                    'Bucket': 'sourcebucket',\n                    'Key': 'sourcekey',\n                    'RequestPayer': 'requester',\n                }),\n                ('CreateMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'RequestPayer': 'requester'\n                }),\n                ('UploadPartCopy', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'PartNumber': mock.ANY,\n                    'CopySourceRange': mock.ANY,\n\n                }),\n                ('UploadPartCopy', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/sourcekey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'PartNumber': mock.ANY,\n                    'CopySourceRange': mock.ANY,\n                }),\n                ('CompleteMultipartUpload', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'UploadId': 'myid',\n                    'RequestPayer': 'requester',\n                    'MultipartUpload': {'Parts': [\n                        {'ETag': '\"etag\"', 'PartNumber': 1},\n                        {'ETag': '\"etag\"', 'PartNumber': 2}]\n                    }\n                })\n            ]\n        )\n\n    def test_recursive_copy(self):\n        cmdline = self.prefix\n        cmdline += ' s3://sourcebucket/ s3://mybucket/'\n        cmdline += ' --request-payer'\n        cmdline += ' --recursive'\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n", "description": "Universal Command Line Interface for Amazon Web Services", "file_name": "test_cp_command.py", "id": "ff50a2360bf822c09480dfaf1dc24b0d", "language": "Python", "project_name": "aws-cli", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/aws-aws-cli/aws-aws-cli-d705c60/tests/functional/s3/test_cp_command.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:33:26Z", "url": "https://github.com/aws/aws-cli", "wiki": false}