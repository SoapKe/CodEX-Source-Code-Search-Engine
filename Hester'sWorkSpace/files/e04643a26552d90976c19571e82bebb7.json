{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\n\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python import spec_builder\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('spec_file', 'parser_spec.textproto',\n                    'Filename to save the spec to.')\n\n\ndef main(unused_argv):\n   Left-to-right, character-based LSTM.\n  char2word = spec_builder.ComponentSpecBuilder('char_lstm')\n  char2word.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  char2word.set_transition_system(name='char-shift-only', left_to_right='true')\n  char2word.add_fixed_feature(name='chars', fml='char-input.text-char',\n                              embedding_dim=16)\n\n   Lookahead LSTM reads right-to-left to represent the rightmost context of the\n   words. It gets word embeddings from the char model.\n  lookahead = spec_builder.ComponentSpecBuilder('lookahead')\n  lookahead.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  lookahead.set_transition_system(name='shift-only', left_to_right='false')\n  lookahead.add_link(source=char2word, fml='input.last-char-focus',\n                     embedding_dim=64)\n\n   Construct the tagger. This is a simple left-to-right LSTM sequence tagger.\n  tagger = spec_builder.ComponentSpecBuilder('tagger')\n  tagger.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  tagger.set_transition_system(name='tagger')\n  tagger.add_token_link(source=lookahead, fml='input.focus', embedding_dim=64)\n\n   Construct the parser.\n  parser = spec_builder.ComponentSpecBuilder('parser')\n  parser.set_network_unit(name='FeedForwardNetwork', hidden_layer_sizes='256',\n                          layer_norm_hidden='true')\n  parser.set_transition_system(name='arc-standard')\n  parser.add_token_link(source=lookahead, fml='input.focus', embedding_dim=64)\n  parser.add_token_link(\n      source=tagger, fml='input.focus stack.focus stack(1).focus',\n      embedding_dim=64)\n\n   Add discrete features of the predicted parse tree so far, like in Parsey\n   McParseface.\n  parser.add_fixed_feature(name='labels', embedding_dim=16,\n                           fml=' '.join([\n                               'stack.child(1).label',\n                               'stack.child(1).sibling(-1).label',\n                               'stack.child(-1).label',\n                               'stack.child(-1).sibling(1).label',\n                               'stack(1).child(1).label',\n                               'stack(1).child(1).sibling(-1).label',\n                               'stack(1).child(-1).label',\n                               'stack(1).child(-1).sibling(1).label',\n                               'stack.child(2).label',\n                               'stack.child(-2).label',\n                               'stack(1).child(2).label',\n                               'stack(1).child(-2).label']))\n\n   Recurrent connection for the arc-standard parser. For both tokens on the\n   stack, we connect to the last time step to either SHIFT or REDUCE that\n   token. This allows the parser to build up compositional representations of\n   phrases.\n  parser.add_link(\n      source=parser,   recurrent connection\n      name='rnn-stack',   unique identifier\n      fml='stack.focus stack(1).focus',   look for both stack tokens\n      source_translator='shift-reduce-step',   maps token indices -> step\n      embedding_dim=64)   project down to 64 dims\n\n  master_spec = spec_pb2.MasterSpec()\n  master_spec.component.extend(\n      [char2word.spec, lookahead.spec, tagger.spec, parser.spec])\n\n  with gfile.FastGFile(FLAGS.spec_file, 'w') as f:\n    f.write(str(master_spec).encode('utf-8'))\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   construct spec conll2017 parser baseline        copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       left right  character based lstm     lookahead lstm reads right left represent rightmost context    words  it gets word embeddings char model     construct tagger  this simple left right lstm sequence tagger     construct parser     add discrete features predicted parse tree far  like parsey    mcparseface     recurrent connection arc standard parser  for tokens    stack  connect last time step either shift reduce    token  this allows parser build compositional representations    phrases     recurrent connection    unique identifier    look stack tokens    maps token indices    step    project 64 dims ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Construct the spec for the CONLL2017 Parser baseline.\"\"\"\n\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\n\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python import spec_builder\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('spec_file', 'parser_spec.textproto',\n                    'Filename to save the spec to.')\n\n\ndef main(unused_argv):\n  # Left-to-right, character-based LSTM.\n  char2word = spec_builder.ComponentSpecBuilder('char_lstm')\n  char2word.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  char2word.set_transition_system(name='char-shift-only', left_to_right='true')\n  char2word.add_fixed_feature(name='chars', fml='char-input.text-char',\n                              embedding_dim=16)\n\n  # Lookahead LSTM reads right-to-left to represent the rightmost context of the\n  # words. It gets word embeddings from the char model.\n  lookahead = spec_builder.ComponentSpecBuilder('lookahead')\n  lookahead.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  lookahead.set_transition_system(name='shift-only', left_to_right='false')\n  lookahead.add_link(source=char2word, fml='input.last-char-focus',\n                     embedding_dim=64)\n\n  # Construct the tagger. This is a simple left-to-right LSTM sequence tagger.\n  tagger = spec_builder.ComponentSpecBuilder('tagger')\n  tagger.set_network_unit(\n      name='wrapped_units.LayerNormBasicLSTMNetwork',\n      hidden_layer_sizes='256')\n  tagger.set_transition_system(name='tagger')\n  tagger.add_token_link(source=lookahead, fml='input.focus', embedding_dim=64)\n\n  # Construct the parser.\n  parser = spec_builder.ComponentSpecBuilder('parser')\n  parser.set_network_unit(name='FeedForwardNetwork', hidden_layer_sizes='256',\n                          layer_norm_hidden='true')\n  parser.set_transition_system(name='arc-standard')\n  parser.add_token_link(source=lookahead, fml='input.focus', embedding_dim=64)\n  parser.add_token_link(\n      source=tagger, fml='input.focus stack.focus stack(1).focus',\n      embedding_dim=64)\n\n  # Add discrete features of the predicted parse tree so far, like in Parsey\n  # McParseface.\n  parser.add_fixed_feature(name='labels', embedding_dim=16,\n                           fml=' '.join([\n                               'stack.child(1).label',\n                               'stack.child(1).sibling(-1).label',\n                               'stack.child(-1).label',\n                               'stack.child(-1).sibling(1).label',\n                               'stack(1).child(1).label',\n                               'stack(1).child(1).sibling(-1).label',\n                               'stack(1).child(-1).label',\n                               'stack(1).child(-1).sibling(1).label',\n                               'stack.child(2).label',\n                               'stack.child(-2).label',\n                               'stack(1).child(2).label',\n                               'stack(1).child(-2).label']))\n\n  # Recurrent connection for the arc-standard parser. For both tokens on the\n  # stack, we connect to the last time step to either SHIFT or REDUCE that\n  # token. This allows the parser to build up compositional representations of\n  # phrases.\n  parser.add_link(\n      source=parser,  # recurrent connection\n      name='rnn-stack',  # unique identifier\n      fml='stack.focus stack(1).focus',  # look for both stack tokens\n      source_translator='shift-reduce-step',  # maps token indices -> step\n      embedding_dim=64)  # project down to 64 dims\n\n  master_spec = spec_pb2.MasterSpec()\n  master_spec.component.extend(\n      [char2word.spec, lookahead.spec, tagger.spec, parser.spec])\n\n  with gfile.FastGFile(FLAGS.spec_file, 'w') as f:\n    f.write(str(master_spec).encode('utf-8'))\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "make_parser_spec.py", "id": "e04643a26552d90976c19571e82bebb7", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/syntaxnet/dragnn/conll2017/make_parser_spec.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}