{"author": "chiphuyen", "code": "\"\"\" starter code for word2vec skip-gram model with NCE loss\nEager execution\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu) & Akshay Agrawal (akshayka@cs.stanford.edu)\nLecture 04\n\"\"\"\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\nimport utils\nimport word2vec_utils\n\n\n\n\n\n\n\nVOCAB_SIZE = 50000\nBATCH_SIZE = 128\nEMBED_SIZE = 128            \nSKIP_WINDOW = 1             \nNUM_SAMPLED = 64            \nLEARNING_RATE = 1.0\nNUM_TRAIN_STEPS = 100000\nVISUAL_FLD = 'visualization'\nSKIP_STEP = 5000\n\n\nDOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\nEXPECTED_BYTES = 31344016\n\nclass Word2Vec(object):\n  def __init__(self, vocab_size, embed_size, num_sampled=NUM_SAMPLED):\n    self.vocab_size = vocab_size\n    self.num_sampled = num_sampled\n    \n    \n    \n    \n    self.embed_matrix = None\n    self.nce_weight = None\n    self.nce_bias = None\n\n  def compute_loss(self, center_words, target_words):\n    \"\"\"Computes the forward pass of word2vec with the NCE loss.\"\"\" \n    \n    \n    \n    \n    embed = None\n\n    \n    \n    \n    \n    loss = None\n    return loss\n\n\ndef gen():\n  yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES,\n                                      VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW,\n                                      VISUAL_FLD)\n\ndef main():\n  dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32),\n                              (tf.TensorShape([BATCH_SIZE]),\n                              tf.TensorShape([BATCH_SIZE, 1])))\n  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n  \n  \n  \n  \n  model = None\n\n  \n  \n  \n  \n  grad_fn = None\n\n  total_loss = 0.0  \n  num_train_steps = 0\n  while num_train_steps < NUM_TRAIN_STEPS:\n    for center_words, target_words in tfe.Iterator(dataset):\n      if num_train_steps >= NUM_TRAIN_STEPS:\n        break\n\n      \n      \n      \n      \n      \n      if (num_train_steps + 1) % SKIP_STEP == 0:\n        print('Average loss at step {}: {:5.1f}'.format(\n                num_train_steps, total_loss / SKIP_STEP))\n        total_loss = 0.0\n      num_train_steps += 1\n\n\nif __name__ == '__main__':\n    main()\n", "comments": "    starter code word2vec skip gram model nce loss eager execution cs 20   tensorflow deep learning research  cs20 stanford edu chip huyen (chiphuyen cs stanford edu)   akshay agrawal (akshayka cs stanford edu) lecture 04      import os os environ  tf cpp min log level    2   import numpy np import tensorflow tf import tensorflow contrib eager tfe  import utils import word2vec utils    enable eager execution                                           to do                                               model hyperparameters vocab size   50000 batch size   128 embed size   128              dimension word embedding vectors skip window   1               context window num sampled   64              number negative examples sample learning rate   1 0 num train steps   100000 visual fld    visualization  skip step   5000    parameters downloading data download url    http   mattmahoney net dc text8 zip  expected bytes   31344016  class word2vec(object)    def   init  (self  vocab size  embed size  num sampled num sampled)      self vocab size   vocab size     self num sampled   num sampled       create variables  embedding matrix  nce weight  nce bias                                                  to do                                                    self embed matrix   none     self nce weight   none     self nce bias   none    def compute loss(self  center words  target words)         computes forward pass word2vec nce loss        enable eager execution                                             to do                                                model hyperparameters    dimension word embedding vectors    context window    number negative examples sample    parameters downloading data    create variables  embedding matrix  nce weight  nce bias                                            to do                                                look embeddings center words                                            to do                                                compute loss  using tf reduce mean tf nn nce loss                                            to do                                                create model                                            to do                                                create gradients function  using  tfe implicit value gradients                                             to do                                                average loss last skip step steps    compute loss gradients  take optimization step                                             to do                                             ", "content": "\"\"\" starter code for word2vec skip-gram model with NCE loss\nEager execution\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu) & Akshay Agrawal (akshayka@cs.stanford.edu)\nLecture 04\n\"\"\"\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\nimport utils\nimport word2vec_utils\n\n# Enable eager execution!\n#############################\n########## TO DO ############\n#############################\n\n# Model hyperparameters\nVOCAB_SIZE = 50000\nBATCH_SIZE = 128\nEMBED_SIZE = 128            # dimension of the word embedding vectors\nSKIP_WINDOW = 1             # the context window\nNUM_SAMPLED = 64            # number of negative examples to sample\nLEARNING_RATE = 1.0\nNUM_TRAIN_STEPS = 100000\nVISUAL_FLD = 'visualization'\nSKIP_STEP = 5000\n\n# Parameters for downloading data\nDOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\nEXPECTED_BYTES = 31344016\n\nclass Word2Vec(object):\n  def __init__(self, vocab_size, embed_size, num_sampled=NUM_SAMPLED):\n    self.vocab_size = vocab_size\n    self.num_sampled = num_sampled\n    # Create the variables: an embedding matrix, nce_weight, and nce_bias\n    #############################\n    ########## TO DO ############\n    #############################\n    self.embed_matrix = None\n    self.nce_weight = None\n    self.nce_bias = None\n\n  def compute_loss(self, center_words, target_words):\n    \"\"\"Computes the forward pass of word2vec with the NCE loss.\"\"\" \n    # Look up the embeddings for the center words\n    #############################\n    ########## TO DO ############\n    #############################\n    embed = None\n\n    # Compute the loss, using tf.reduce_mean and tf.nn.nce_loss\n    #############################\n    ########## TO DO ############\n    #############################\n    loss = None\n    return loss\n\n\ndef gen():\n  yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES,\n                                      VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW,\n                                      VISUAL_FLD)\n\ndef main():\n  dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32),\n                              (tf.TensorShape([BATCH_SIZE]),\n                              tf.TensorShape([BATCH_SIZE, 1])))\n  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n  # Create the model\n  #############################\n  ########## TO DO ############\n  #############################\n  model = None\n\n  # Create the gradients function, using `tfe.implicit_value_and_gradients`\n  #############################\n  ########## TO DO ############\n  #############################\n  grad_fn = None\n\n  total_loss = 0.0  # for average loss in the last SKIP_STEP steps\n  num_train_steps = 0\n  while num_train_steps < NUM_TRAIN_STEPS:\n    for center_words, target_words in tfe.Iterator(dataset):\n      if num_train_steps >= NUM_TRAIN_STEPS:\n        break\n\n      # Compute the loss and gradients, and take an optimization step.\n      #############################\n      ########## TO DO ############\n      #############################\n      \n      if (num_train_steps + 1) % SKIP_STEP == 0:\n        print('Average loss at step {}: {:5.1f}'.format(\n                num_train_steps, total_loss / SKIP_STEP))\n        total_loss = 0.0\n      num_train_steps += 1\n\n\nif __name__ == '__main__':\n    main()\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "04_word2vec_eager_starter.py", "id": "edba2fd8675df730df3a5a0e058e7543", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/04_word2vec_eager_starter.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}