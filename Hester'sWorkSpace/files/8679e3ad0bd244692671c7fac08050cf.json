{"author": "NVIDIA", "code": "\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\n\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n\nfrom models import VGGEncoder, VGGDecoder\n\n\nclass PhotoWCT(nn.Module):\n    def __init__(self):\n        super(PhotoWCT, self).__init__()\n        \n        self.e1 = VGGEncoder(1)\n        self.d1 = VGGDecoder(1)\n        self.e2 = VGGEncoder(2)\n        self.d2 = VGGDecoder(2)\n        self.e3 = VGGEncoder(3)\n        self.d3 = VGGDecoder(3)\n        self.e4 = VGGEncoder(4)\n        self.d4 = VGGDecoder(4)\n    \n    def transform(self, cont_img, styl_img, cont_seg, styl_seg):\n        self.__compute_label_info(cont_seg, styl_seg)\n        \n        sF4, sF3, sF2, sF1 = self.e4.forward_multiple(styl_img)\n        \n        cF4, cpool_idx, cpool1, cpool_idx2, cpool2, cpool_idx3, cpool3 = self.e4(cont_img)\n        sF4 = sF4.data.squeeze(0)\n        cF4 = cF4.data.squeeze(0)\n        csF4 = self.__feature_wct(cF4, sF4, cont_seg, styl_seg)\n        Im4 = self.d4(csF4, cpool_idx, cpool1, cpool_idx2, cpool2, cpool_idx3, cpool3)\n        \n        cF3, cpool_idx, cpool1, cpool_idx2, cpool2 = self.e3(Im4)\n        sF3 = sF3.data.squeeze(0)\n        cF3 = cF3.data.squeeze(0)\n        csF3 = self.__feature_wct(cF3, sF3, cont_seg, styl_seg)\n        Im3 = self.d3(csF3, cpool_idx, cpool1, cpool_idx2, cpool2)\n        \n        cF2, cpool_idx, cpool = self.e2(Im3)\n        sF2 = sF2.data.squeeze(0)\n        cF2 = cF2.data.squeeze(0)\n        csF2 = self.__feature_wct(cF2, sF2, cont_seg, styl_seg)\n        Im2 = self.d2(csF2, cpool_idx, cpool)\n        \n        cF1 = self.e1(Im2)\n        sF1 = sF1.data.squeeze(0)\n        cF1 = cF1.data.squeeze(0)\n        csF1 = self.__feature_wct(cF1, sF1, cont_seg, styl_seg)\n        Im1 = self.d1(csF1)\n        return Im1\n    \n    def __compute_label_info(self, cont_seg, styl_seg):\n        if cont_seg.size == False or styl_seg.size == False:\n            return\n        max_label = np.max(cont_seg) + 1\n        self.label_set = np.unique(cont_seg)\n        self.label_indicator = np.zeros(max_label)\n        for l in self.label_set:\n            \n            \n            is_valid = lambda a, b: a > 10 and b > 10 and a / b < 100 and b / a < 100\n            o_cont_mask = np.where(cont_seg.reshape(cont_seg.shape[0] * cont_seg.shape[1]) == l)\n            o_styl_mask = np.where(styl_seg.reshape(styl_seg.shape[0] * styl_seg.shape[1]) == l)\n            self.label_indicator[l] = is_valid(o_cont_mask[0].size, o_styl_mask[0].size)\n    \n    def __feature_wct(self, cont_feat, styl_feat, cont_seg, styl_seg):\n        cont_c, cont_h, cont_w = cont_feat.size(0), cont_feat.size(1), cont_feat.size(2)\n        styl_c, styl_h, styl_w = styl_feat.size(0), styl_feat.size(1), styl_feat.size(2)\n        cont_feat_view = cont_feat.view(cont_c, -1).clone()\n        styl_feat_view = styl_feat.view(styl_c, -1).clone()\n        \n        if cont_seg.size == False or styl_seg.size == False:\n            target_feature = self.__wct_core(cont_feat_view, styl_feat_view)\n        else:\n            target_feature = cont_feat.view(cont_c, -1).clone()\n    \n            t_cont_seg = np.asarray(Image.fromarray(cont_seg, mode='RGB').resize((cont_w, cont_h), Image.NEAREST))\n            t_styl_seg = np.asarray(Image.fromarray(styl_seg, mode='RGB').resize((styl_w, styl_h), Image.NEAREST))\n            \n            for l in self.label_set:\n                if self.label_indicator[l] == 0:\n                    continue\n                cont_mask = np.where(t_cont_seg.reshape(t_cont_seg.shape[0] * t_cont_seg.shape[1]) == l)\n                styl_mask = np.where(t_styl_seg.reshape(t_styl_seg.shape[0] * t_styl_seg.shape[1]) == l)\n                if cont_mask[0].size <= 0 or styl_mask[0].size <= 0:\n                    continue\n                \n                cont_indi = torch.LongTensor(cont_mask[0])\n                styl_indi = torch.LongTensor(styl_mask[0])\n                if self.is_cuda:\n                    cont_indi = cont_indi.cuda(0)\n                    styl_indi = styl_indi.cuda(0)\n                \n                cFFG = torch.index_select(cont_feat_view, 1, cont_indi)\n                sFFG = torch.index_select(styl_feat_view, 1, styl_indi)\n                tmp_target_feature = self.__wct_core(cFFG, sFFG)\n                target_feature.index_copy_(1, cont_indi, tmp_target_feature)\n        \n        target_feature = target_feature.view_as(cont_feat)\n        ccsF = target_feature.float().unsqueeze(0)\n        return ccsF\n    \n    def __wct_core(self, cont_feat, styl_feat):\n        cFSize = cont_feat.size()\n        c_mean = torch.mean(cont_feat, 1)  # c x (h x w)\n        c_mean = c_mean.unsqueeze(1).expand_as(cont_feat)\n        cont_feat = cont_feat - c_mean\n        \n        iden = torch.eye(cFSize[0])  ()\n        if self.is_cuda:\n            iden = iden.cuda()\n        \n        contentConv = torch.mm(cont_feat, cont_feat.t()).div(cFSize[1] - 1) + iden\n        \n        c_u, c_e, c_v = torch.svd(contentConv, some=False)\n        # c_e2, c_v = torch.eig(contentConv, True)\n        # c_e = c_e2[:,0]\n        \n        k_c = cFSize[0]\n        for i in range(cFSize[0] - 1, -1, -1):\n            if c_e[i] >= 0.00001:\n                k_c = i + 1\n                break\n        \n        sFSize = styl_feat.size()\n        s_mean = torch.mean(styl_feat, 1)\n        styl_feat = styl_feat - s_mean.unsqueeze(1).expand_as(styl_feat)\n        styleConv = torch.mm(styl_feat, styl_feat.t()).div(sFSize[1] - 1)\n        s_u, s_e, s_v = torch.svd(styleConv, some=False)\n        \n        k_s = sFSize[0]\n        for i in range(sFSize[0] - 1, -1, -1):\n            if s_e[i] >= 0.00001:\n                k_s = i + 1\n                break\n        \n        c_d = (c_e[0:k_c]).pow(-0.5)\n        step1 = torch.mm(c_v[:, 0:k_c], torch.diag(c_d))\n        step2 = torch.mm(step1, (c_v[:, 0:k_c].t()))\n        whiten_cF = torch.mm(step2, cont_feat)\n        \n        s_d = (s_e[0:k_s]).pow(0.5)\n        targetFeature = torch.mm(torch.mm(torch.mm(s_v[:, 0:k_s], torch.diag(s_d)), (s_v[:, 0:k_s].t())), whiten_cF)\n        targetFeature = targetFeature + s_mean.unsqueeze(1).expand_as(targetFeature)\n        return targetFeature\n    \n    @property\n    def is_cuda(self):\n        return next(self.parameters()).is_cuda\n", "comments": "    copyright (c) 2018 nvidia corporation   all rights reserved  licensed cc by nc sa 4 0 license (https   creativecommons org licenses nc sa 4 0 legalcode)         l  0       continue    c x (h x w)     double()    del iden    c e2  c v   torch eig(contentconv  true)    c e   c e2   0  ", "content": "\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\n\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n\nfrom models import VGGEncoder, VGGDecoder\n\n\nclass PhotoWCT(nn.Module):\n    def __init__(self):\n        super(PhotoWCT, self).__init__()\n        \n        self.e1 = VGGEncoder(1)\n        self.d1 = VGGDecoder(1)\n        self.e2 = VGGEncoder(2)\n        self.d2 = VGGDecoder(2)\n        self.e3 = VGGEncoder(3)\n        self.d3 = VGGDecoder(3)\n        self.e4 = VGGEncoder(4)\n        self.d4 = VGGDecoder(4)\n    \n    def transform(self, cont_img, styl_img, cont_seg, styl_seg):\n        self.__compute_label_info(cont_seg, styl_seg)\n        \n        sF4, sF3, sF2, sF1 = self.e4.forward_multiple(styl_img)\n        \n        cF4, cpool_idx, cpool1, cpool_idx2, cpool2, cpool_idx3, cpool3 = self.e4(cont_img)\n        sF4 = sF4.data.squeeze(0)\n        cF4 = cF4.data.squeeze(0)\n        csF4 = self.__feature_wct(cF4, sF4, cont_seg, styl_seg)\n        Im4 = self.d4(csF4, cpool_idx, cpool1, cpool_idx2, cpool2, cpool_idx3, cpool3)\n        \n        cF3, cpool_idx, cpool1, cpool_idx2, cpool2 = self.e3(Im4)\n        sF3 = sF3.data.squeeze(0)\n        cF3 = cF3.data.squeeze(0)\n        csF3 = self.__feature_wct(cF3, sF3, cont_seg, styl_seg)\n        Im3 = self.d3(csF3, cpool_idx, cpool1, cpool_idx2, cpool2)\n        \n        cF2, cpool_idx, cpool = self.e2(Im3)\n        sF2 = sF2.data.squeeze(0)\n        cF2 = cF2.data.squeeze(0)\n        csF2 = self.__feature_wct(cF2, sF2, cont_seg, styl_seg)\n        Im2 = self.d2(csF2, cpool_idx, cpool)\n        \n        cF1 = self.e1(Im2)\n        sF1 = sF1.data.squeeze(0)\n        cF1 = cF1.data.squeeze(0)\n        csF1 = self.__feature_wct(cF1, sF1, cont_seg, styl_seg)\n        Im1 = self.d1(csF1)\n        return Im1\n    \n    def __compute_label_info(self, cont_seg, styl_seg):\n        if cont_seg.size == False or styl_seg.size == False:\n            return\n        max_label = np.max(cont_seg) + 1\n        self.label_set = np.unique(cont_seg)\n        self.label_indicator = np.zeros(max_label)\n        for l in self.label_set:\n            # if l==0:\n            #   continue\n            is_valid = lambda a, b: a > 10 and b > 10 and a / b < 100 and b / a < 100\n            o_cont_mask = np.where(cont_seg.reshape(cont_seg.shape[0] * cont_seg.shape[1]) == l)\n            o_styl_mask = np.where(styl_seg.reshape(styl_seg.shape[0] * styl_seg.shape[1]) == l)\n            self.label_indicator[l] = is_valid(o_cont_mask[0].size, o_styl_mask[0].size)\n    \n    def __feature_wct(self, cont_feat, styl_feat, cont_seg, styl_seg):\n        cont_c, cont_h, cont_w = cont_feat.size(0), cont_feat.size(1), cont_feat.size(2)\n        styl_c, styl_h, styl_w = styl_feat.size(0), styl_feat.size(1), styl_feat.size(2)\n        cont_feat_view = cont_feat.view(cont_c, -1).clone()\n        styl_feat_view = styl_feat.view(styl_c, -1).clone()\n        \n        if cont_seg.size == False or styl_seg.size == False:\n            target_feature = self.__wct_core(cont_feat_view, styl_feat_view)\n        else:\n            target_feature = cont_feat.view(cont_c, -1).clone()\n    \n            t_cont_seg = np.asarray(Image.fromarray(cont_seg, mode='RGB').resize((cont_w, cont_h), Image.NEAREST))\n            t_styl_seg = np.asarray(Image.fromarray(styl_seg, mode='RGB').resize((styl_w, styl_h), Image.NEAREST))\n            \n            for l in self.label_set:\n                if self.label_indicator[l] == 0:\n                    continue\n                cont_mask = np.where(t_cont_seg.reshape(t_cont_seg.shape[0] * t_cont_seg.shape[1]) == l)\n                styl_mask = np.where(t_styl_seg.reshape(t_styl_seg.shape[0] * t_styl_seg.shape[1]) == l)\n                if cont_mask[0].size <= 0 or styl_mask[0].size <= 0:\n                    continue\n                \n                cont_indi = torch.LongTensor(cont_mask[0])\n                styl_indi = torch.LongTensor(styl_mask[0])\n                if self.is_cuda:\n                    cont_indi = cont_indi.cuda(0)\n                    styl_indi = styl_indi.cuda(0)\n                \n                cFFG = torch.index_select(cont_feat_view, 1, cont_indi)\n                sFFG = torch.index_select(styl_feat_view, 1, styl_indi)\n                tmp_target_feature = self.__wct_core(cFFG, sFFG)\n                target_feature.index_copy_(1, cont_indi, tmp_target_feature)\n        \n        target_feature = target_feature.view_as(cont_feat)\n        ccsF = target_feature.float().unsqueeze(0)\n        return ccsF\n    \n    def __wct_core(self, cont_feat, styl_feat):\n        cFSize = cont_feat.size()\n        c_mean = torch.mean(cont_feat, 1)  # c x (h x w)\n        c_mean = c_mean.unsqueeze(1).expand_as(cont_feat)\n        cont_feat = cont_feat - c_mean\n        \n        iden = torch.eye(cFSize[0])  # .double()\n        if self.is_cuda:\n            iden = iden.cuda()\n        \n        contentConv = torch.mm(cont_feat, cont_feat.t()).div(cFSize[1] - 1) + iden\n        # del iden\n        c_u, c_e, c_v = torch.svd(contentConv, some=False)\n        # c_e2, c_v = torch.eig(contentConv, True)\n        # c_e = c_e2[:,0]\n        \n        k_c = cFSize[0]\n        for i in range(cFSize[0] - 1, -1, -1):\n            if c_e[i] >= 0.00001:\n                k_c = i + 1\n                break\n        \n        sFSize = styl_feat.size()\n        s_mean = torch.mean(styl_feat, 1)\n        styl_feat = styl_feat - s_mean.unsqueeze(1).expand_as(styl_feat)\n        styleConv = torch.mm(styl_feat, styl_feat.t()).div(sFSize[1] - 1)\n        s_u, s_e, s_v = torch.svd(styleConv, some=False)\n        \n        k_s = sFSize[0]\n        for i in range(sFSize[0] - 1, -1, -1):\n            if s_e[i] >= 0.00001:\n                k_s = i + 1\n                break\n        \n        c_d = (c_e[0:k_c]).pow(-0.5)\n        step1 = torch.mm(c_v[:, 0:k_c], torch.diag(c_d))\n        step2 = torch.mm(step1, (c_v[:, 0:k_c].t()))\n        whiten_cF = torch.mm(step2, cont_feat)\n        \n        s_d = (s_e[0:k_s]).pow(0.5)\n        targetFeature = torch.mm(torch.mm(torch.mm(s_v[:, 0:k_s], torch.diag(s_d)), (s_v[:, 0:k_s].t())), whiten_cF)\n        targetFeature = targetFeature + s_mean.unsqueeze(1).expand_as(targetFeature)\n        return targetFeature\n    \n    @property\n    def is_cuda(self):\n        return next(self.parameters()).is_cuda\n", "description": "Style transfer, deep learning, feature transform", "file_name": "photo_wct.py", "id": "8679e3ad0bd244692671c7fac08050cf", "language": "Python", "project_name": "FastPhotoStyle", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/NVIDIA-FastPhotoStyle/NVIDIA-FastPhotoStyle-208d4f6/photo_wct.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:35:44Z", "url": "https://github.com/NVIDIA/FastPhotoStyle", "wiki": true}