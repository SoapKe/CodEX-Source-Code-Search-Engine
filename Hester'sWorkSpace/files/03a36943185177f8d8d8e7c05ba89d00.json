{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Utility functions for training.\"\"\"\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,\n                                                  labels,\n                                                  num_classes,\n                                                  ignore_label,\n                                                  loss_weight=1.0,\n                                                  upsample_logits=True,\n                                                  scope=None):\n  \"\"\"Adds softmax cross entropy loss for logits of each scale.\n\n  Args:\n    scales_to_logits: A map from logits names for different scales to logits.\n      The logits have shape [batch, logits_height, logits_width, num_classes].\n    labels: Groundtruth labels with shape [batch, image_height, image_width, 1].\n    num_classes: Integer, number of target classes.\n    ignore_label: Integer, label to ignore.\n    loss_weight: Float, loss weight.\n    upsample_logits: Boolean, upsample logits or not.\n    scope: String, the scope for the loss.\n\n  Raises:\n    ValueError: Label or logits is None.\n  \"\"\"\n  if labels is None:\n    raise ValueError('No label for softmax cross entropy loss.')\n\n  for scale, logits in scales_to_logits.iteritems():\n    loss_scope = None\n    if scope:\n      loss_scope = '%s_%s' % (scope, scale)\n\n    if upsample_logits:\n       Label is not downsampled, and instead we upsample logits.\n      logits = tf.image.resize_bilinear(\n          logits, tf.shape(labels)[1:3], align_corners=True)\n      scaled_labels = labels\n    else:\n       Label is downsampled to the same size as logits.\n      scaled_labels = tf.image.resize_nearest_neighbor(\n          labels, tf.shape(logits)[1:3], align_corners=True)\n\n    scaled_labels = tf.reshape(scaled_labels, shape=[-1])\n    not_ignore_mask = tf.to_float(tf.not_equal(scaled_labels,\n                                               ignore_label)) * loss_weight\n    one_hot_labels = slim.one_hot_encoding(\n        scaled_labels, num_classes, on_value=1.0, off_value=0.0)\n    tf.losses.softmax_cross_entropy(\n        one_hot_labels,\n        tf.reshape(logits, shape=[-1, num_classes]),\n        weights=not_ignore_mask,\n        scope=loss_scope)\n\n\ndef get_model_init_fn(train_logdir,\n                      tf_initial_checkpoint,\n                      initialize_last_layer,\n                      last_layers,\n                      ignore_missing_vars=False):\n  \"\"\"Gets the function initializing model variables from a checkpoint.\n\n  Args:\n    train_logdir: Log directory for training.\n    tf_initial_checkpoint: TensorFlow checkpoint for initialization.\n    initialize_last_layer: Initialize last layer or not.\n    last_layers: Last layers of the model.\n    ignore_missing_vars: Ignore missing variables in the checkpoint.\n\n  Returns:\n    Initialization function.\n  \"\"\"\n  if tf_initial_checkpoint is None:\n    tf.logging.info('Not initializing the model from a checkpoint.')\n    return None\n\n  if tf.train.latest_checkpoint(train_logdir):\n    tf.logging.info('Ignoring initialization; other checkpoint exists')\n    return None\n\n  tf.logging.info('Initializing model from path: %s', tf_initial_checkpoint)\n\n   Variables that will not be restored.\n  exclude_list = ['global_step']\n  if not initialize_last_layer:\n    exclude_list.extend(last_layers)\n\n  variables_to_restore = slim.get_variables_to_restore(exclude=exclude_list)\n\n  return slim.assign_from_checkpoint_fn(\n      tf_initial_checkpoint,\n      variables_to_restore,\n      ignore_missing_vars=ignore_missing_vars)\n\n\ndef get_model_gradient_multipliers(last_layers, last_layer_gradient_multiplier):\n  \"\"\"Gets the gradient multipliers.\n\n  The gradient multipliers will adjust the learning rates for model\n  variables. For the task of semantic segmentation, the models are\n  usually fine-tuned from the models trained on the task of image\n  classification. To fine-tune the models, we usually set larger (e.g.,\n  10 times larger) learning rate for the parameters of last layer.\n\n  Args:\n    last_layers: Scopes of last layers.\n    last_layer_gradient_multiplier: The gradient multiplier for last layers.\n\n  Returns:\n    The gradient multiplier map with variables as key, and multipliers as value.\n  \"\"\"\n  gradient_multipliers = {}\n\n  for var in slim.get_model_variables():\n     Double the learning rate for biases.\n    if 'biases' in var.op.name:\n      gradient_multipliers[var.op.name] = 2.\n\n     Use larger learning rate for last layer variables.\n    for layer in last_layers:\n      if layer in var.op.name and 'biases' in var.op.name:\n        gradient_multipliers[var.op.name] = 2 * last_layer_gradient_multiplier\n        break\n      elif layer in var.op.name:\n        gradient_multipliers[var.op.name] = last_layer_gradient_multiplier\n        break\n\n  return gradient_multipliers\n\n\ndef get_model_learning_rate(\n    learning_policy, base_learning_rate, learning_rate_decay_step,\n    learning_rate_decay_factor, training_number_of_steps, learning_power,\n    slow_start_step, slow_start_learning_rate):\n  \"\"\"Gets model's learning rate.\n\n  Computes the model's learning rate for different learning policy.\n  Right now, only \"step\" and \"poly\" are supported.\n  (1) The learning policy for \"step\" is computed as follows:\n    current_learning_rate = base_learning_rate *\n      learning_rate_decay_factor ^ (global_step / learning_rate_decay_step)\n  See tf.train.exponential_decay for details.\n  (2) The learning policy for \"poly\" is computed as follows:\n    current_learning_rate = base_learning_rate *\n      (1 - global_step / training_number_of_steps) ^ learning_power\n\n  Args:\n    learning_policy: Learning rate policy for training.\n    base_learning_rate: The base learning rate for model training.\n    learning_rate_decay_step: Decay the base learning rate at a fixed step.\n    learning_rate_decay_factor: The rate to decay the base learning rate.\n    training_number_of_steps: Number of steps for training.\n    learning_power: Power used for 'poly' learning policy.\n    slow_start_step: Training model with small learning rate for the first\n      few steps.\n    slow_start_learning_rate: The learning rate employed during slow start.\n\n  Returns:\n    Learning rate for the specified learning policy.\n\n  Raises:\n    ValueError: If learning policy is not recognized.\n  \"\"\"\n  global_step = tf.train.get_or_create_global_step()\n  if learning_policy == 'step':\n    learning_rate = tf.train.exponential_decay(\n        base_learning_rate,\n        global_step,\n        learning_rate_decay_step,\n        learning_rate_decay_factor,\n        staircase=True)\n  elif learning_policy == 'poly':\n    learning_rate = tf.train.polynomial_decay(\n        base_learning_rate,\n        global_step,\n        training_number_of_steps,\n        end_learning_rate=0,\n        power=learning_power)\n  else:\n    raise ValueError('Unknown learning policy.')\n\n   Employ small learning rate at the first few steps for warm start.\n  return tf.where(global_step < slow_start_step, slow_start_learning_rate,\n                  learning_rate)\n", "comments": "   utility functions training      import tensorflow tf  slim   tf contrib slim   def add softmax cross entropy loss scale(scales logits                                                    labels                                                    num classes                                                    ignore label                                                    loss weight 1 0                                                    upsample logits true                                                    scope none)       adds softmax cross entropy loss logits scale     args      scales logits  a map logits names different scales logits        the logits shape  batch  logits height  logits width  num classes       labels  groundtruth labels shape  batch  image height  image width  1       num classes  integer  number target classes      ignore label  integer  label ignore      loss weight  float  loss weight      upsample logits  boolean  upsample logits      scope  string  scope loss     raises      valueerror  label logits none          labels none      raise valueerror( no label softmax cross entropy loss  )    scale  logits scales logits iteritems()      loss scope   none     scope        loss scope         (scope  scale)      upsample logits          label downsampled  instead upsample logits        logits   tf image resize bilinear(           logits  tf shape(labels) 1 3   align corners true)       scaled labels   labels     else          label downsampled size logits        scaled labels   tf image resize nearest neighbor(           labels  tf shape(logits) 1 3   align corners true)      scaled labels   tf reshape(scaled labels  shape   1 )     ignore mask   tf float(tf equal(scaled labels                                                 ignore label))   loss weight     one hot labels   slim one hot encoding(         scaled labels  num classes  value 1 0  value 0 0)     tf losses softmax cross entropy(         one hot labels          tf reshape(logits  shape   1  num classes )          weights ignore mask          scope loss scope)   def get model init fn(train logdir                        tf initial checkpoint                        initialize last layer                        last layers                        ignore missing vars false)       gets function initializing model variables checkpoint     args      train logdir  log directory training      tf initial checkpoint  tensorflow checkpoint initialization      initialize last layer  initialize last layer      last layers  last layers model      ignore missing vars  ignore missing variables checkpoint     returns      initialization function          tf initial checkpoint none      tf logging info( not initializing model checkpoint  )     return none    tf train latest checkpoint(train logdir)      tf logging info( ignoring initialization  checkpoint exists )     return none    tf logging info( initializing model path     tf initial checkpoint)      variables restored    exclude list     global step     initialize last layer      exclude list extend(last layers)    variables restore   slim get variables restore(exclude exclude list)    return slim assign checkpoint fn(       tf initial checkpoint        variables restore        ignore missing vars ignore missing vars)   def get model gradient multipliers(last layers  last layer gradient multiplier)       gets gradient multipliers     the gradient multipliers adjust learning rates model   variables  for task semantic segmentation  models   usually fine tuned models trained task image   classification  to fine tune models  usually set larger (e g     10 times larger) learning rate parameters last layer     args      last layers  scopes last layers      last layer gradient multiplier  the gradient multiplier last layers     returns      the gradient multiplier map variables key  multipliers value          gradient multipliers         var slim get model variables()        double learning rate biases       biases  var op name        gradient multipliers var op name    2         use larger learning rate last layer variables      layer last layers        layer var op name  biases  var op name          gradient multipliers var op name    2   last layer gradient multiplier         break       elif layer var op name          gradient multipliers var op name    last layer gradient multiplier         break    return gradient multipliers   def get model learning rate(     learning policy  base learning rate  learning rate decay step      learning rate decay factor  training number steps  learning power      slow start step  slow start learning rate)       gets model learning rate     computes model learning rate different learning policy    right   step   poly  supported    (1) the learning policy  step  computed follows      current learning rate   base learning rate         learning rate decay factor   (global step   learning rate decay step)   see tf train exponential decay details    (2) the learning policy  poly  computed follows      current learning rate   base learning rate         (1   global step   training number steps)   learning power    args      learning policy  learning rate policy training      base learning rate  the base learning rate model training      learning rate decay step  decay base learning rate fixed step      learning rate decay factor  the rate decay base learning rate      training number steps  number steps training      learning power  power used  poly  learning policy      slow start step  training model small learning rate first       steps      slow start learning rate  the learning rate employed slow start     returns      learning rate specified learning policy     raises      valueerror  if learning policy recognized           copyright 2018 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       label downsampled  instead upsample logits     label downsampled size logits     variables restored     double learning rate biases     use larger learning rate last layer variables     employ small learning rate first steps warm start  ", "content": "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utility functions for training.\"\"\"\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,\n                                                  labels,\n                                                  num_classes,\n                                                  ignore_label,\n                                                  loss_weight=1.0,\n                                                  upsample_logits=True,\n                                                  scope=None):\n  \"\"\"Adds softmax cross entropy loss for logits of each scale.\n\n  Args:\n    scales_to_logits: A map from logits names for different scales to logits.\n      The logits have shape [batch, logits_height, logits_width, num_classes].\n    labels: Groundtruth labels with shape [batch, image_height, image_width, 1].\n    num_classes: Integer, number of target classes.\n    ignore_label: Integer, label to ignore.\n    loss_weight: Float, loss weight.\n    upsample_logits: Boolean, upsample logits or not.\n    scope: String, the scope for the loss.\n\n  Raises:\n    ValueError: Label or logits is None.\n  \"\"\"\n  if labels is None:\n    raise ValueError('No label for softmax cross entropy loss.')\n\n  for scale, logits in scales_to_logits.iteritems():\n    loss_scope = None\n    if scope:\n      loss_scope = '%s_%s' % (scope, scale)\n\n    if upsample_logits:\n      # Label is not downsampled, and instead we upsample logits.\n      logits = tf.image.resize_bilinear(\n          logits, tf.shape(labels)[1:3], align_corners=True)\n      scaled_labels = labels\n    else:\n      # Label is downsampled to the same size as logits.\n      scaled_labels = tf.image.resize_nearest_neighbor(\n          labels, tf.shape(logits)[1:3], align_corners=True)\n\n    scaled_labels = tf.reshape(scaled_labels, shape=[-1])\n    not_ignore_mask = tf.to_float(tf.not_equal(scaled_labels,\n                                               ignore_label)) * loss_weight\n    one_hot_labels = slim.one_hot_encoding(\n        scaled_labels, num_classes, on_value=1.0, off_value=0.0)\n    tf.losses.softmax_cross_entropy(\n        one_hot_labels,\n        tf.reshape(logits, shape=[-1, num_classes]),\n        weights=not_ignore_mask,\n        scope=loss_scope)\n\n\ndef get_model_init_fn(train_logdir,\n                      tf_initial_checkpoint,\n                      initialize_last_layer,\n                      last_layers,\n                      ignore_missing_vars=False):\n  \"\"\"Gets the function initializing model variables from a checkpoint.\n\n  Args:\n    train_logdir: Log directory for training.\n    tf_initial_checkpoint: TensorFlow checkpoint for initialization.\n    initialize_last_layer: Initialize last layer or not.\n    last_layers: Last layers of the model.\n    ignore_missing_vars: Ignore missing variables in the checkpoint.\n\n  Returns:\n    Initialization function.\n  \"\"\"\n  if tf_initial_checkpoint is None:\n    tf.logging.info('Not initializing the model from a checkpoint.')\n    return None\n\n  if tf.train.latest_checkpoint(train_logdir):\n    tf.logging.info('Ignoring initialization; other checkpoint exists')\n    return None\n\n  tf.logging.info('Initializing model from path: %s', tf_initial_checkpoint)\n\n  # Variables that will not be restored.\n  exclude_list = ['global_step']\n  if not initialize_last_layer:\n    exclude_list.extend(last_layers)\n\n  variables_to_restore = slim.get_variables_to_restore(exclude=exclude_list)\n\n  return slim.assign_from_checkpoint_fn(\n      tf_initial_checkpoint,\n      variables_to_restore,\n      ignore_missing_vars=ignore_missing_vars)\n\n\ndef get_model_gradient_multipliers(last_layers, last_layer_gradient_multiplier):\n  \"\"\"Gets the gradient multipliers.\n\n  The gradient multipliers will adjust the learning rates for model\n  variables. For the task of semantic segmentation, the models are\n  usually fine-tuned from the models trained on the task of image\n  classification. To fine-tune the models, we usually set larger (e.g.,\n  10 times larger) learning rate for the parameters of last layer.\n\n  Args:\n    last_layers: Scopes of last layers.\n    last_layer_gradient_multiplier: The gradient multiplier for last layers.\n\n  Returns:\n    The gradient multiplier map with variables as key, and multipliers as value.\n  \"\"\"\n  gradient_multipliers = {}\n\n  for var in slim.get_model_variables():\n    # Double the learning rate for biases.\n    if 'biases' in var.op.name:\n      gradient_multipliers[var.op.name] = 2.\n\n    # Use larger learning rate for last layer variables.\n    for layer in last_layers:\n      if layer in var.op.name and 'biases' in var.op.name:\n        gradient_multipliers[var.op.name] = 2 * last_layer_gradient_multiplier\n        break\n      elif layer in var.op.name:\n        gradient_multipliers[var.op.name] = last_layer_gradient_multiplier\n        break\n\n  return gradient_multipliers\n\n\ndef get_model_learning_rate(\n    learning_policy, base_learning_rate, learning_rate_decay_step,\n    learning_rate_decay_factor, training_number_of_steps, learning_power,\n    slow_start_step, slow_start_learning_rate):\n  \"\"\"Gets model's learning rate.\n\n  Computes the model's learning rate for different learning policy.\n  Right now, only \"step\" and \"poly\" are supported.\n  (1) The learning policy for \"step\" is computed as follows:\n    current_learning_rate = base_learning_rate *\n      learning_rate_decay_factor ^ (global_step / learning_rate_decay_step)\n  See tf.train.exponential_decay for details.\n  (2) The learning policy for \"poly\" is computed as follows:\n    current_learning_rate = base_learning_rate *\n      (1 - global_step / training_number_of_steps) ^ learning_power\n\n  Args:\n    learning_policy: Learning rate policy for training.\n    base_learning_rate: The base learning rate for model training.\n    learning_rate_decay_step: Decay the base learning rate at a fixed step.\n    learning_rate_decay_factor: The rate to decay the base learning rate.\n    training_number_of_steps: Number of steps for training.\n    learning_power: Power used for 'poly' learning policy.\n    slow_start_step: Training model with small learning rate for the first\n      few steps.\n    slow_start_learning_rate: The learning rate employed during slow start.\n\n  Returns:\n    Learning rate for the specified learning policy.\n\n  Raises:\n    ValueError: If learning policy is not recognized.\n  \"\"\"\n  global_step = tf.train.get_or_create_global_step()\n  if learning_policy == 'step':\n    learning_rate = tf.train.exponential_decay(\n        base_learning_rate,\n        global_step,\n        learning_rate_decay_step,\n        learning_rate_decay_factor,\n        staircase=True)\n  elif learning_policy == 'poly':\n    learning_rate = tf.train.polynomial_decay(\n        base_learning_rate,\n        global_step,\n        training_number_of_steps,\n        end_learning_rate=0,\n        power=learning_power)\n  else:\n    raise ValueError('Unknown learning policy.')\n\n  # Employ small learning rate at the first few steps for warm start.\n  return tf.where(global_step < slow_start_step, slow_start_learning_rate,\n                  learning_rate)\n", "description": "Models and examples built with TensorFlow", "file_name": "train_utils.py", "id": "03a36943185177f8d8d8e7c05ba89d00", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/deeplab/utils/train_utils.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}