{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\nr\"\"\"A simple demonstration of running VGGish in inference mode.\n\nThis is intended as a toy example that demonstrates how the various building\nblocks (feature extraction, model definition and loading, postprocessing) work\ntogether in an inference context.\n\nA WAV file (assumed to contain signed 16-bit PCM samples) is read in, converted\ninto log mel spectrogram examples, fed into VGGish, the raw embedding output is\nwhitened and quantized, and the postprocessed embeddings are optionally written\nin a SequenceExample to a TFRecord file (using the same format as the embedding\nfeatures released in AudioSet).\n\nUsage:\n   Run a WAV file through the model and print the embeddings. The model\n   checkpoint is loaded from vggish_model.ckpt and the PCA parameters are\n   loaded from vggish_pca_params.npz in the current directory.\n  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file\n\n   Run a WAV file through the model and also write the embeddings to\n   a TFRecord file. The model checkpoint and PCA parameters are explicitly\n   passed in as well.\n  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file \\\n                                    --tfrecord_file /path/to/tfrecord/file \\\n                                    --checkpoint /path/to/model/checkpoint \\\n                                    --pca_params /path/to/pca/params\n\n   Run a built-in input (a sine wav) through the model and print the\n   embeddings. Associated model files are read from the current directory.\n  $ python vggish_inference_demo.py\n\"\"\"\n\nfrom __future__ import print_function\n\nimport numpy as np\nfrom scipy.io import wavfile\nimport six\nimport tensorflow as tf\n\nimport vggish_input\nimport vggish_params\nimport vggish_postprocess\nimport vggish_slim\n\nflags = tf.app.flags\n\nflags.DEFINE_string(\n    'wav_file', None,\n    'Path to a wav file. Should contain signed 16-bit PCM samples. '\n    'If none is provided, a synthetic sound is used.')\n\nflags.DEFINE_string(\n    'checkpoint', 'vggish_model.ckpt',\n    'Path to the VGGish checkpoint file.')\n\nflags.DEFINE_string(\n    'pca_params', 'vggish_pca_params.npz',\n    'Path to the VGGish PCA parameters file.')\n\nflags.DEFINE_string(\n    'tfrecord_file', None,\n    'Path to a TFRecord file where embeddings will be written.')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n   In this simple example, we run the examples from a single audio file through\n   the model. If none is provided, we generate a synthetic input.\n  if FLAGS.wav_file:\n    wav_file = FLAGS.wav_file\n  else:\n     Write a WAV of a sine wav into an in-memory file object.\n    num_secs = 5\n    freq = 1000\n    sr = 44100\n    t = np.linspace(0, num_secs, int(num_secs * sr))\n    x = np.sin(2 * np.pi * freq * t)\n     Convert to signed 16-bit samples.\n    samples = np.clip(x * 32768, -32768, 32767).astype(np.int16)\n    wav_file = six.BytesIO()\n    wavfile.write(wav_file, sr, samples)\n    wav_file.seek(0)\n  examples_batch = vggish_input.wavfile_to_examples(wav_file)\n  print(examples_batch)\n\n   Prepare a postprocessor to munge the model embeddings.\n  pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n\n   If needed, prepare a record writer to store the postprocessed embeddings.\n  writer = tf.python_io.TFRecordWriter(\n      FLAGS.tfrecord_file) if FLAGS.tfrecord_file else None\n\n  with tf.Graph().as_default(), tf.Session() as sess:\n     Define the model in inference mode, load the checkpoint, and\n     locate input and output tensors.\n    vggish_slim.define_vggish_slim(training=False)\n    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n    features_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.INPUT_TENSOR_NAME)\n    embedding_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.OUTPUT_TENSOR_NAME)\n\n     Run inference and postprocessing.\n    [embedding_batch] = sess.run([embedding_tensor],\n                                 feed_dict={features_tensor: examples_batch})\n    print(embedding_batch)\n    postprocessed_batch = pproc.postprocess(embedding_batch)\n    print(postprocessed_batch)\n\n     Write the postprocessed embeddings as a SequenceExample, in a similar\n     format as the features released in AudioSet. Each row of the batch of\n     embeddings corresponds to roughly a second of audio (96 10ms frames), and\n     the rows are written as a sequence of bytes-valued features, where each\n     feature value contains the 128 bytes of the whitened quantized embedding.\n    seq_example = tf.train.SequenceExample(\n        feature_lists=tf.train.FeatureLists(\n            feature_list={\n                vggish_params.AUDIO_EMBEDDING_FEATURE_NAME:\n                    tf.train.FeatureList(\n                        feature=[\n                            tf.train.Feature(\n                                bytes_list=tf.train.BytesList(\n                                    value=[embedding.tobytes()]))\n                            for embedding in postprocessed_batch\n                        ]\n                    )\n            }\n        )\n    )\n    print(seq_example)\n    if writer:\n      writer.write(seq_example.SerializeToString())\n\n  if writer:\n    writer.close()\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   a simple demonstration running vggish inference mode   this intended toy example demonstrates various building blocks (feature extraction  model definition loading  postprocessing) work together inference context   a wav file (assumed contain signed 16 bit pcm samples) read  converted log mel spectrogram examples  fed vggish  raw embedding output whitened quantized  postprocessed embeddings optionally written sequenceexample tfrecord file (using format embedding features released audioset)   usage      run wav file model print embeddings  the model     checkpoint loaded vggish model ckpt pca parameters     loaded vggish pca params npz current directory      python vggish inference demo py   wav file  path wav file      run wav file model also write embeddings     tfrecord file  the model checkpoint pca parameters explicitly     passed well      python vggish inference demo py   wav file  path wav file                                         tfrecord file  path tfrecord file                                         checkpoint  path model checkpoint                                         pca params  path pca params      run built input (a sine wav) model print     embeddings  associated model files read current directory      python vggish inference demo py        copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       run wav file model print embeddings  the model    checkpoint loaded vggish model ckpt pca parameters    loaded vggish pca params npz current directory     run wav file model also write embeddings    tfrecord file  the model checkpoint pca parameters explicitly    passed well     run built input (a sine wav) model print    embeddings  associated model files read current directory     in simple example  run examples single audio file    model  if none provided  generate synthetic input     write wav sine wav memory file object     convert signed 16 bit samples     prepare postprocessor munge model embeddings     if needed  prepare record writer store postprocessed embeddings     define model inference mode  load checkpoint     locate input output tensors     run inference postprocessing     write postprocessed embeddings sequenceexample  similar    format features released audioset  each row batch    embeddings corresponds roughly second audio (96 10ms frames)     rows written sequence bytes valued features     feature value contains 128 bytes whitened quantized embedding  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr\"\"\"A simple demonstration of running VGGish in inference mode.\n\nThis is intended as a toy example that demonstrates how the various building\nblocks (feature extraction, model definition and loading, postprocessing) work\ntogether in an inference context.\n\nA WAV file (assumed to contain signed 16-bit PCM samples) is read in, converted\ninto log mel spectrogram examples, fed into VGGish, the raw embedding output is\nwhitened and quantized, and the postprocessed embeddings are optionally written\nin a SequenceExample to a TFRecord file (using the same format as the embedding\nfeatures released in AudioSet).\n\nUsage:\n  # Run a WAV file through the model and print the embeddings. The model\n  # checkpoint is loaded from vggish_model.ckpt and the PCA parameters are\n  # loaded from vggish_pca_params.npz in the current directory.\n  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file\n\n  # Run a WAV file through the model and also write the embeddings to\n  # a TFRecord file. The model checkpoint and PCA parameters are explicitly\n  # passed in as well.\n  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file \\\n                                    --tfrecord_file /path/to/tfrecord/file \\\n                                    --checkpoint /path/to/model/checkpoint \\\n                                    --pca_params /path/to/pca/params\n\n  # Run a built-in input (a sine wav) through the model and print the\n  # embeddings. Associated model files are read from the current directory.\n  $ python vggish_inference_demo.py\n\"\"\"\n\nfrom __future__ import print_function\n\nimport numpy as np\nfrom scipy.io import wavfile\nimport six\nimport tensorflow as tf\n\nimport vggish_input\nimport vggish_params\nimport vggish_postprocess\nimport vggish_slim\n\nflags = tf.app.flags\n\nflags.DEFINE_string(\n    'wav_file', None,\n    'Path to a wav file. Should contain signed 16-bit PCM samples. '\n    'If none is provided, a synthetic sound is used.')\n\nflags.DEFINE_string(\n    'checkpoint', 'vggish_model.ckpt',\n    'Path to the VGGish checkpoint file.')\n\nflags.DEFINE_string(\n    'pca_params', 'vggish_pca_params.npz',\n    'Path to the VGGish PCA parameters file.')\n\nflags.DEFINE_string(\n    'tfrecord_file', None,\n    'Path to a TFRecord file where embeddings will be written.')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  # In this simple example, we run the examples from a single audio file through\n  # the model. If none is provided, we generate a synthetic input.\n  if FLAGS.wav_file:\n    wav_file = FLAGS.wav_file\n  else:\n    # Write a WAV of a sine wav into an in-memory file object.\n    num_secs = 5\n    freq = 1000\n    sr = 44100\n    t = np.linspace(0, num_secs, int(num_secs * sr))\n    x = np.sin(2 * np.pi * freq * t)\n    # Convert to signed 16-bit samples.\n    samples = np.clip(x * 32768, -32768, 32767).astype(np.int16)\n    wav_file = six.BytesIO()\n    wavfile.write(wav_file, sr, samples)\n    wav_file.seek(0)\n  examples_batch = vggish_input.wavfile_to_examples(wav_file)\n  print(examples_batch)\n\n  # Prepare a postprocessor to munge the model embeddings.\n  pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n\n  # If needed, prepare a record writer to store the postprocessed embeddings.\n  writer = tf.python_io.TFRecordWriter(\n      FLAGS.tfrecord_file) if FLAGS.tfrecord_file else None\n\n  with tf.Graph().as_default(), tf.Session() as sess:\n    # Define the model in inference mode, load the checkpoint, and\n    # locate input and output tensors.\n    vggish_slim.define_vggish_slim(training=False)\n    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n    features_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.INPUT_TENSOR_NAME)\n    embedding_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.OUTPUT_TENSOR_NAME)\n\n    # Run inference and postprocessing.\n    [embedding_batch] = sess.run([embedding_tensor],\n                                 feed_dict={features_tensor: examples_batch})\n    print(embedding_batch)\n    postprocessed_batch = pproc.postprocess(embedding_batch)\n    print(postprocessed_batch)\n\n    # Write the postprocessed embeddings as a SequenceExample, in a similar\n    # format as the features released in AudioSet. Each row of the batch of\n    # embeddings corresponds to roughly a second of audio (96 10ms frames), and\n    # the rows are written as a sequence of bytes-valued features, where each\n    # feature value contains the 128 bytes of the whitened quantized embedding.\n    seq_example = tf.train.SequenceExample(\n        feature_lists=tf.train.FeatureLists(\n            feature_list={\n                vggish_params.AUDIO_EMBEDDING_FEATURE_NAME:\n                    tf.train.FeatureList(\n                        feature=[\n                            tf.train.Feature(\n                                bytes_list=tf.train.BytesList(\n                                    value=[embedding.tobytes()]))\n                            for embedding in postprocessed_batch\n                        ]\n                    )\n            }\n        )\n    )\n    print(seq_example)\n    if writer:\n      writer.write(seq_example.SerializeToString())\n\n  if writer:\n    writer.close()\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "vggish_inference_demo.py", "id": "fa911fb4484ec5031e71e776befb993d", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/audioset/vggish_inference_demo.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}