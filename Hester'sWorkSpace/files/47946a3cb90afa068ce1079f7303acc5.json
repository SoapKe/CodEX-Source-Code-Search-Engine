{"author": "tflearn", "code": "'''\nPedagogical example realization of wide & deep networks, using TensorFlow and TFLearn.\n\nThis is a re-implementation of http://arxiv.org/abs/1606.07792, using the combination\nof a wide linear model, and a deep feed-forward neural network, for binary classification  \nThis example realization is based on Tensorflow's TF.Learn tutorial \n(https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html),\nbut implemented in TFLearn.  Note that despite the closeness of names, TFLearn is distinct\nfrom TF.Learn (previously known as scikit flow).\n\nThis implementation explicitly presents the construction of layers in the deep part of the\nnetwork, and allows direct access to changing the layer architecture, and customization\nof methods used for regression and optimization.\n\nIn contrast, the TF.Learn tutorial offers more sophistication, but hides the layer\narchitecture behind a black box function, tf.contrib.learn.DNNLinearCombinedClassifier.\n\nSee https://github.com/ichuang/tflearn_wide_and_deep for more about this example.\n'''\n\nfrom __future__ import division, print_function\n\nimport os\nimport sys\nimport argparse\nimport tflearn\nimport tempfile\nimport urllib\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n\n\nCOLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n           \"income_bracket\"]\nLABEL_COLUMN = \"label\"\nCATEGORICAL_COLUMNS = {\"workclass\": 10, \"education\": 17, \"marital_status\":8, \n                       \"occupation\": 16, \"relationship\": 7, \"race\": 6, \n                       \"gender\": 3, \"native_country\": 43, \"age_binned\": 14}\nCONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n                      \"hours_per_week\"]\n\n\n\nclass TFLearnWideAndDeep(object):\n    '''\n    Wide and deep model, implemented using TFLearn\n    '''\n    AVAILABLE_MODELS = [\"wide\", \"deep\", \"wide+deep\"]\n    def __init__(self, model_type=\"wide+deep\", verbose=None, name=None, tensorboard_verbose=3, \n                 wide_learning_rate=0.001, deep_learning_rate=0.001, checkpoints_dir=None):\n        '''\n        model_type = `str`: wide or deep or wide+deep\n        verbose = `bool`\n        name = `str` used for run_id (defaults to model_type)\n        tensorboard_verbose = `int`: logging level for tensorboard (0, 1, 2, or 3)\n        wide_learning_rate = `float`: defaults to 0.001\n        deep_learning_rate = `float`: defaults to 0.001\n        checkpoints_dir = `str`: where checkpoint files will be stored (defaults to \"CHECKPOINTS\")\n        '''\n        self.model_type = model_type or \"wide+deep\"\n        assert self.model_type in self.AVAILABLE_MODELS\n        self.verbose = verbose or 0\n        self.tensorboard_verbose = tensorboard_verbose\n        self.name = name or self.model_type\t\n        self.data_columns = COLUMNS\n        self.continuous_columns = CONTINUOUS_COLUMNS\n        self.categorical_columns = CATEGORICAL_COLUMNS\t\n        self.label_column = LABEL_COLUMN\n        self.checkpoints_dir = checkpoints_dir or \"CHECKPOINTS\"\n        if not os.path.exists(self.checkpoints_dir):\n            os.mkdir(self.checkpoints_dir)\n            print(\"Created checkpoints directory %s\" % self.checkpoints_dir)\n        self.build_model([wide_learning_rate, deep_learning_rate])\n\n    def load_data(self, train_dfn=\"adult.data\", test_dfn=\"adult.test\"):\n        '''\n        Load data (use files offered in the Tensorflow wide_n_deep_tutorial)\n        '''\n        if not os.path.exists(train_dfn):\n            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_dfn)\n            print(\"Training data is downloaded to %s\" % train_dfn)\n\n        if not os.path.exists(test_dfn):\n            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_dfn)\n            print(\"Test data is downloaded to %s\" % test_dfn)\n\n        self.train_data = pd.read_csv(train_dfn, names=COLUMNS, skipinitialspace=True)\n        self.test_data = pd.read_csv(test_dfn, names=COLUMNS, skipinitialspace=True, skiprows=1)\n\n        self.train_data[self.label_column] = (self.train_data[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n        self.test_data[self.label_column] = (self.test_data[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n\n\n    def build_model(self, learning_rate=[0.001, 0.01]):\n        '''\n        Model - wide and deep - built using tflearn\n        '''\n        n_cc = len(self.continuous_columns)\n        n_categories = 1\t\t\t\n        input_shape = [None, n_cc]\n        if self.verbose:\n            print (\"=\"*77 + \" Model %s (type=%s)\" % (self.name, self.model_type))\n            print (\"  Input placeholder shape=%s\" % str(input_shape))\n        wide_inputs = tflearn.input_data(shape=input_shape, name=\"wide_X\")\n        if not isinstance(learning_rate, list):\n            learning_rate = [learning_rate, learning_rate]\t\n        if self.verbose:\n            print (\"  Learning rates (wide, deep)=%s\" % learning_rate)\n\n        with tf.name_scope(\"Y\"):\t\t\t# placeholder for target variable (i.e. trainY input)\n            Y_in = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"Y\")\n\n        with tf.variable_scope(None, \"cb_unit\", [wide_inputs]) as scope:\n            central_bias = tflearn.variables.variable('central_bias', shape=[1],\n                                                      initializer=tf.constant_initializer(np.random.randn()),\n                                                      trainable=True, restore=True)\n            tf.add_to_collection(tf.GraphKeys.LAYER_VARIABLES + '/cb_unit', central_bias)\n\n        if 'wide' in self.model_type:\n            wide_network = self.wide_model(wide_inputs, n_cc)\n            network = wide_network\n            wide_network_with_bias = tf.add(wide_network, central_bias, name=\"wide_with_bias\")\n\n        if 'deep' in self.model_type:\n            deep_network = self.deep_model(wide_inputs, n_cc)\n            deep_network_with_bias = tf.add(deep_network, central_bias, name=\"deep_with_bias\")\n            if 'wide' in self.model_type:\n                network = tf.add(wide_network, deep_network)\n                if self.verbose:\n                    print (\"Wide + deep model network %s\" % network)\n            else:\n                network = deep_network\n\n        network = tf.add(network, central_bias, name=\"add_central_bias\")\n\n        \n        with tf.name_scope('Monitors'):\n            predictions = tf.cast(tf.greater(network, 0), tf.int64)\n            print (\"predictions=%s\" % predictions)\n            Ybool = tf.cast(Y_in, tf.bool)\n            print (\"Ybool=%s\" % Ybool)\n            pos = tf.boolean_mask(predictions, Ybool)\n            neg = tf.boolean_mask(predictions, ~Ybool)\n            psize = tf.cast(tf.shape(pos)[0], tf.int64)\n            nsize = tf.cast(tf.shape(neg)[0], tf.int64)\n            true_positive = tf.reduce_sum(pos, name=\"true_positive\")\n            false_negative = tf.subtract(psize, true_positive, name=\"false_negative\")\n            false_positive = tf.reduce_sum(neg, name=\"false_positive\")\n            true_negative = tf.subtract(nsize, false_positive, name=\"true_negative\")\n            overall_accuracy = tf.truediv(tf.add(true_positive, true_negative), tf.add(nsize, psize), name=\"overall_accuracy\")\n        vmset = [true_positive, true_negative, false_positive, false_negative, overall_accuracy]\n\n        trainable_vars = tf.trainable_variables()\n        tv_deep = [v for v in trainable_vars if v.name.startswith('deep_')]\n        tv_wide = [v for v in trainable_vars if v.name.startswith('wide_')]\n\n        if self.verbose:\n            print (\"DEEP trainable_vars\")\n            for v in tv_deep:\n                print (\"  Variable %s: %s\" % (v.name, v))\n            print (\"WIDE trainable_vars\")\n            for v in tv_wide:\n                print (\"  Variable %s: %s\" % (v.name, v))\n\n        if 'wide' in self.model_type:\n            if not 'deep' in self.model_type:\n                tv_wide.append(central_bias)\n            tflearn.regression(wide_network_with_bias, \n                               placeholder=Y_in,\n                               optimizer='sgd', \n                               \n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[0],\n                               validation_monitors=vmset,\n                               trainable_vars=tv_wide,\n                               op_name=\"wide_regression\",\n                               name=\"Y\")\n\n        if 'deep' in self.model_type:\n            if not 'wide' in self.model_type:\n                tv_wide.append(central_bias)\n            tflearn.regression(deep_network_with_bias, \n                               placeholder=Y_in,\n                               optimizer='adam', \n                               \n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[1],\n                               validation_monitors=vmset if not 'wide' in self.model_type else None,\n                               trainable_vars=tv_deep,\n                               op_name=\"deep_regression\",\n                               name=\"Y\")\n\n        if self.model_type=='wide+deep':\t# learn central bias separately for wide+deep\n            tflearn.regression(network, \n                               placeholder=Y_in,\n                               optimizer='adam', \n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[0],\t\n                               trainable_vars=[central_bias],\n                               op_name=\"central_bias_regression\",\n                               name=\"Y\")\n\n        self.model = tflearn.DNN(network,\n                                 tensorboard_verbose=self.tensorboard_verbose,\n                                 max_checkpoints=5,\n                                 checkpoint_path=\"%s/%s.tfl\" % (self.checkpoints_dir, self.name),\n        )\n\n        if self.verbose:\n            print (\"Target variables:\")\n            for v in tf.get_collection(tf.GraphKeys.TARGETS):\n                print (\"  variable %s: %s\" % (v.name, v))\n\n            print (\"=\"*77)\n\n\n    def deep_model(self, wide_inputs, n_inputs, n_nodes=[100, 50], use_dropout=False):\n        '''\n        Model - deep, i.e. two-layer fully connected network model\n        '''\n        cc_input_var = {}\n        cc_embed_var = {}\n        flat_vars = []\n        if self.verbose:\n            print (\"--> deep model: %s categories, %d continuous\" % (len(self.categorical_columns), n_inputs))\n        for cc, cc_size in self.categorical_columns.items():\n            cc_input_var[cc] = tflearn.input_data(shape=[None, 1], name=\"%s_in\" % cc,  dtype=tf.int32)\n            \n            cc_embed_var[cc] = tflearn.layers.embedding_ops.embedding(cc_input_var[cc],    cc_size,  8, name=\"deep_%s_embed\" % cc)\n            if self.verbose:\n                print (\"    %s_embed = %s\" % (cc, cc_embed_var[cc]))\n            flat_vars.append(tf.squeeze(cc_embed_var[cc], squeeze_dims=[1], name=\"%s_squeeze\" % cc))\n\n        network = tf.concat([wide_inputs] + flat_vars, 1, name=\"deep_concat\")\n        for k in range(len(n_nodes)):\n            network = tflearn.fully_connected(network, n_nodes[k], activation=\"relu\", name=\"deep_fc%d\" % (k+1))\n            if use_dropout:\n                network = tflearn.dropout(network, 0.5, name=\"deep_dropout%d\" % (k+1))\n        if self.verbose:\n            print (\"Deep model network before output %s\" % network)\n        network = tflearn.fully_connected(network, 1, activation=\"linear\", name=\"deep_fc_output\", bias=False)\n        network = tf.reshape(network, [-1, 1])\t\n        if self.verbose:\n            print (\"Deep model network %s\" % network)\n        return network\n\n    def wide_model(self, inputs, n_inputs):\n        '''\n        Model - wide, i.e. normal linear model (for logistic regression)\n        '''\n        network = inputs\n        # use fully_connected (instad of single_unit) because fc works properly with batches, whereas single_unit is 1D only\n        network = tflearn.fully_connected(network, n_inputs, activation=\"linear\", name=\"wide_linear\", bias=False)\t# x*W (no bias)\n        network = tf.reduce_sum(network, 1, name=\"reduce_sum\")\t\n        network = tf.reshape(network, [-1, 1])\t\n        if self.verbose:\n            print (\"Wide model network %s\" % network)\n        return network\n\n    def prepare_input_data(self, input_data, name=\"\", category_map=None):\n        '''\n        Prepare input data dicts\n        '''\n        print (\"-\"*40 + \" Preparing %s\" % name)\n        X = input_data[self.continuous_columns].values.astype(np.float32)\n        Y = input_data[self.label_column].values.astype(np.float32)\n        Y = Y.reshape([-1, 1])\n        if self.verbose:\n            print (\"  Y shape=%s, X shape=%s\" % (Y.shape, X.shape))\n\n        X_dict = {\"wide_X\": X}\n\n        if 'deep' in self.model_type:\n            \n            td = input_data\n            if category_map is None:\n                category_map = {}\n                for cc in self.categorical_columns:\n                    if not cc in td.columns:\n                        continue\n                    cc_values = sorted(td[cc].unique())\n                    cc_max = 1+len(cc_values)\n                    cc_map = dict(zip(cc_values, range(1, cc_max)))\t# start from 1 to avoid 0:0 mapping (save 0 for missing)\n                    if self.verbose:\n                        print (\"  category %s max=%s,  map=%s\" % (cc, cc_max, cc_map))\n                    category_map[cc] = cc_map\n                \n            td = td.replace(category_map)\n    \n            # bin ages (cuts off extreme values)\n            age_bins = [ 0, 12, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 80, 65535 ]\n            td['age_binned'] = pd.cut(td['age'], age_bins, labels=False)\n            td = td.replace({'age_binned': {np.nan: 0}})\n            print (\"  %d age bins: age bins = %s\" % (len(age_bins), age_bins))\n\n            X_dict.update({ (\"%s_in\" % cc): td[cc].values.astype(np.int32).reshape([-1, 1]) for cc in self.categorical_columns})\n\n        Y_dict = {\"Y\": Y}\n        if self.verbose:\n            print (\"-\"*40)\n        return X_dict, Y_dict, category_map\n\n\n    def train(self, n_epoch=1000, snapshot_step=10, batch_size=None):\n\n        self.X_dict, self.Y_dict, category_map = self.prepare_input_data(self.train_data, \"train data\")\n        self.testX_dict, self.testY_dict, _ = self.prepare_input_data(self.test_data, \"test data\", category_map)\n        validation_batch_size = batch_size or self.testY_dict['Y'].shape[0]\n        batch_size = batch_size or self.Y_dict['Y'].shape[0]\n\n        print (\"Input data shape = %s; output data shape=%s, batch_size=%s\" % (str(self.X_dict['wide_X'].shape), \n                                                                               str(self.Y_dict['Y'].shape), \n                                                                               batch_size))\n        print (\"Test data shape = %s; output data shape=%s, validation_batch_size=%s\" % (str(self.testX_dict['wide_X'].shape), \n                                                                                         str(self.testY_dict['Y'].shape), \n                                                                                         validation_batch_size))\n        print (\"=\"*60 + \"  Training\")\n        self.model.fit(self.X_dict, \n                       self.Y_dict,\n                       n_epoch=n_epoch,\n                       validation_set=(self.testX_dict, self.testY_dict),\n                       snapshot_step=snapshot_step,\n                       batch_size=batch_size,\n                       validation_batch_size=validation_batch_size,\n                       show_metric=True, \n                       snapshot_epoch=False,\n                       shuffle=True,\n                       run_id=self.name,\n        )\n        \n    def evaluate(self):\n        logits = np.array(self.model.predict(self.testX_dict)).reshape([-1])\n        print (\"=\"*60 + \"  Evaluation\")\n        print (\"  logits: %s, min=%s, max=%s\" % (logits.shape, logits.min(), logits.max()))\n        probs =  1.0 / (1.0 + np.exp(-logits))\n        y_pred = pd.Series((probs > 0.5).astype(np.int32))\n        Y = pd.Series(self.testY_dict['Y'].astype(np.int32).reshape([-1]))\n        self.confusion_matrix = self.output_confusion_matrix(Y, y_pred)\n        print (\"=\"*60)\n\n    def output_confusion_matrix(self, y, y_pred):\n        assert y.size == y_pred.size\n        print(\"Actual IDV\")\n        print(y.value_counts())\n        print(\"Predicted IDV\")\n        print(y_pred.value_counts())\n        print()\n        print(\"Confusion matrix:\")\n        cmat = pd.crosstab(y_pred, y, rownames=['predictions'], colnames=['actual'])\n        print(cmat)\n        sys.stdout.flush()\n        return cmat\n    \n\n\ndef CommandLine(args=None):\n    '''\n    Main command line.  Accepts args, to allow for simple unit testing.\n    '''\n    flags = tf.app.flags\n    FLAGS = flags.FLAGS\n    if args:\n        FLAGS.__init__()\n        FLAGS.__dict__.update(args)\n\n    try:\n        flags.DEFINE_string(\"model_type\", \"wide+deep\",\"Valid model types: {'wide', 'deep', 'wide+deep'}.\")\n        flags.DEFINE_string(\"run_name\", None, \"name for this run (defaults to model type)\")\n        flags.DEFINE_string(\"load_weights\", None, \"filename with initial weights to load\")\n        flags.DEFINE_string(\"checkpoints_dir\", None, \"name of directory where checkpoints should be saved\")\n        flags.DEFINE_integer(\"n_epoch\", 200, \"Number of training epoch steps\")\n        flags.DEFINE_integer(\"snapshot_step\", 100, \"Step number when snapshot (and validation testing) is done\")\n        flags.DEFINE_float(\"wide_learning_rate\", 0.001, \"learning rate for the wide part of the model\")\n        flags.DEFINE_float(\"deep_learning_rate\", 0.001, \"learning rate for the deep part of the model\")\n        flags.DEFINE_boolean(\"verbose\", False, \"Verbose output\")\n    except argparse.ArgumentError:\n        pass\t\n\n    twad = TFLearnWideAndDeep(model_type=FLAGS.model_type, verbose=FLAGS.verbose, \n                              name=FLAGS.run_name, wide_learning_rate=FLAGS.wide_learning_rate,\n                              deep_learning_rate=FLAGS.deep_learning_rate,\n                              checkpoints_dir=FLAGS.checkpoints_dir)\n    twad.load_data()\n    if FLAGS.load_weights:\n        print (\"Loading initial weights from %s\" % FLAGS.load_weights)\n        twad.model.load(FLAGS.load_weights)\n    twad.train(n_epoch=FLAGS.n_epoch, snapshot_step=FLAGS.snapshot_step)\n    twad.evaluate()\n    return twad\n\n\n\n\ndef test_wide_and_deep():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"wide+deep\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\ndef test_deep():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"deep\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\ndef test_wide():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"wide\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\n\n\nif __name__==\"__main__\":\n    CommandLine()\n    None\n", "comments": "    pedagogical example realization wide   deep networks  using tensorflow tflearn   this implementation http   arxiv org abs 1606 07792  using combination wide linear model  deep feed forward neural network  binary classification   this example realization based tensorflow tf learn tutorial  (https   www tensorflow org versions r0 10 tutorials wide deep index html)  implemented tflearn   note despite closeness names  tflearn distinct tf learn (previously known scikit flow)   this implementation explicitly presents construction layers deep part network  allows direct access changing layer architecture  customization methods used regression optimization   in contrast  tf learn tutorial offers sophistication  hides layer architecture behind black box function  tf contrib learn dnnlinearcombinedclassifier   see https   github com ichuang tflearn wide deep example         future   import division  print function  import os import sys import argparse import tflearn import tempfile import urllib  import numpy np import pandas pd import tensorflow tf                                                                                  columns     age    workclass    fnlwgt    education    education num               marital status    occupation    relationship    race    gender               capital gain    capital loss    hours per week    native country               income bracket   label column    label  categorical columns     workclass   10   education   17   marital status  8                           occupation   16   relationship   7   race   6                           gender   3   native country   43   age binned   14  continuous columns     age    education num    capital gain    capital loss                          hours per week                                                                                    class tflearnwideanddeep(object)              wide deep model  implemented using tflearn             available models     wide    deep    wide deep       def   init  (self  model type  wide deep   verbose none  name none  tensorboard verbose 3                    wide learning rate 0 001  deep learning rate 0 001  checkpoints dir none)                      model type    str   wide deep wide deep         verbose    bool          name    str  used run id (defaults model type)         tensorboard verbose    int   logging level tensorboard (0  1  2  3)         wide learning rate    float   defaults 0 001         deep learning rate    float   defaults 0 001         checkpoints dir    str   checkpoint files stored (defaults  checkpoints )                     self model type   model type  wide deep          assert self model type self available models         self verbose   verbose 0         self tensorboard verbose   tensorboard verbose         self name   name self model type   name used run id         self data columns   columns         self continuous columns   continuous columns         self categorical columns   categorical columns   dict category name  category size         self label column   label column         self checkpoints dir   checkpoints dir  checkpoints          os path exists(self checkpoints dir)              os mkdir(self checkpoints dir)             print( created checkpoints directory     self checkpoints dir)         self build model( wide learning rate  deep learning rate )      def load data(self  train dfn  adult data   test dfn  adult test )                      load data (use files offered tensorflow wide n deep tutorial)                     os path exists(train dfn)              urllib urlretrieve( https   archive ics uci edu ml machine learning databases adult adult data   train dfn)             print( training data downloaded     train dfn)          os path exists(test dfn)              urllib urlretrieve( https   archive ics uci edu ml machine learning databases adult adult test   test dfn)             print( test data downloaded     test dfn)          self train data   pd read csv(train dfn  names columns  skipinitialspace true)         self test data   pd read csv(test dfn  names columns  skipinitialspace true  skiprows 1)          self train data self label column    (self train data  income bracket   apply(lambda x    50k  x)) astype(int)         self test data self label column    (self test data  income bracket   apply(lambda x    50k  x)) astype(int)       def build model(self  learning rate  0 001  0 01 )                      model   wide deep   built using tflearn                     n cc   len(self continuous columns)         n categories   1     two categories  idv idv         input shape    none  n cc          self verbose              print (    77     model  (type  s)    (self name  self model type))             print (   input placeholder shape     str(input shape))         wide inputs   tflearn input data(shape input shape  name  wide x )         isinstance(learning rate  list)              learning rate    learning rate  learning rate    wide  deep         self verbose              print (   learning rates (wide  deep)     learning rate)          tf name scope( y )      placeholder target variable (i e  trainy input)             y   tf placeholder(shape  none  1   dtype tf float32  name  y )          tf variable scope(none   cb unit    wide inputs ) scope              central bias   tflearn variables variable( central bias   shape  1                                                         initializer tf constant initializer(np random randn())                                                        trainable true  restore true)             tf add collection(tf graphkeys layer variables     cb unit   central bias)           wide  self model type              wide network   self wide model(wide inputs  n cc)             network   wide network             wide network bias   tf add(wide network  central bias  name  wide bias )           deep  self model type              deep network   self deep model(wide inputs  n cc)             deep network bias   tf add(deep network  central bias  name  deep bias )              wide  self model type                  network   tf add(wide network  deep network)                 self verbose                      print ( wide   deep model network     network)             else                  network   deep network          network   tf add(network  central bias  name  add central bias )            add validation monitor summaries giving confusion matrix entries         tf name scope( monitors )              predictions   tf cast(tf greater(network  0)  tf int64)             print ( predictions     predictions)             ybool   tf cast(y  tf bool)             print ( ybool     ybool)             pos   tf boolean mask(predictions  ybool)             neg   tf boolean mask(predictions   ybool)             psize   tf cast(tf shape(pos) 0   tf int64)             nsize   tf cast(tf shape(neg) 0   tf int64)             true positive   tf reduce sum(pos  name  true positive )             false negative   tf subtract(psize  true positive  name  false negative )             false positive   tf reduce sum(neg  name  false positive )             true negative   tf subtract(nsize  false positive  name  true negative )             overall accuracy   tf truediv(tf add(true positive  true negative)  tf add(nsize  psize)  name  overall accuracy )         vmset    true positive  true negative  false positive  false negative  overall accuracy           trainable vars   tf trainable variables()         tv deep    v v trainable vars v name startswith( deep  )          tv wide    v v trainable vars v name startswith( wide  )           self verbose              print ( deep trainable vars )             v tv deep                  print (   variable       (v name  v))             print ( wide trainable vars )             v tv wide                  print (   variable       (v name  v))           wide  self model type               deep  self model type                  tv wide append(central bias)             tflearn regression(wide network bias                                  placeholder y                                 optimizer  sgd                                    loss  roc auc score                                  loss  binary crossentropy                                  metric  accuracy                                  learning rate learning rate 0                                  validation monitors vmset                                 trainable vars tv wide                                 op name  wide regression                                  name  y )           deep  self model type               wide  self model type                  tv wide append(central bias)             tflearn regression(deep network bias                                  placeholder y                                 optimizer  adam                                    loss  roc auc score                                  loss  binary crossentropy                                  metric  accuracy                                  learning rate learning rate 1                                  validation monitors vmset  wide  self model type else none                                 trainable vars tv deep                                 op name  deep regression                                  name  y )          self model type   wide deep     learn central bias separately wide deep             tflearn regression(network                                  placeholder y                                 optimizer  adam                                   loss  binary crossentropy                                  metric  accuracy                                  learning rate learning rate 0     use wide learning rate                                trainable vars  central bias                                  op name  central bias regression                                  name  y )          self model   tflearn dnn(network                                   tensorboard verbose self tensorboard verbose                                   max checkpoints 5                                   checkpoint path    tfl    (self checkpoints dir  self name)          )          self verbose              print ( target variables  )             v tf get collection(tf graphkeys targets)                  print (   variable       (v name  v))              print (    77)       def deep model(self  wide inputs  n inputs  n nodes  100  50   use dropout false)                      model   deep  e  two layer fully connected network model                     cc input var              cc embed var              flat vars              self verbose              print (     deep model   categories   continuous    (len(self categorical columns)  n inputs))         cc  cc size self categorical columns items()              cc input var cc    tflearn input data(shape  none  1   name      cc   dtype tf int32)               embedding layers work cpu   no gpu implementation tensorflow  yet              cc embed var cc    tflearn layers embedding ops embedding(cc input var cc      cc size   8  name  deep  embed    cc)             self verbose                  print (      embed       (cc  cc embed var cc ))             flat vars append(tf squeeze(cc embed var cc   squeeze dims  1   name   squeeze    cc))          network   tf concat( wide inputs    flat vars  1  name  deep concat )         k range(len(n nodes))              network   tflearn fully connected(network  n nodes k   activation  relu   name  deep fc    (k 1))             use dropout                  network   tflearn dropout(network  0 5  name  deep dropout    (k 1))         self verbose              print ( deep model network output     network)         network   tflearn fully connected(network  1  activation  linear   name  deep fc output   bias false)         network   tf reshape(network    1  1 )   accuracy binary accuracy         self verbose              print ( deep model network     network)         return network      def wide model(self  inputs  n inputs)                      model   wide  e  normal linear model (for logistic regression)                     network   inputs           use fully connected (instad single unit) fc works properly batches  whereas single unit 1d         network   tflearn fully connected(network  n inputs  activation  linear   name  wide linear   bias false)   x w (no bias)         network   tf reduce sum(network  1  name  reduce sum )   batched sum  produce logits         network   tf reshape(network    1  1 )   accuracy binary accuracy         self verbose              print ( wide model network     network)         return network      def prepare input data(self  input data  name     category map none)                      prepare input data dicts                     print (    40     preparing     name)         x   input data self continuous columns  values astype(np float32)         y   input data self label column  values astype(np float32)         y   y reshape(  1  1 )         self verbose              print (   y shape   x shape     (y shape  x shape))          x dict     wide x   x            deep  self model type                map categorical value strings integers             td   input data             category map none                  category map                      cc self categorical columns                      cc td columns                          continue                     cc values   sorted(td cc  unique())                     cc max   1 len(cc values)                     cc map   dict(zip(cc values  range(1  cc max)))   start 1 avoid 0 0 mapping (save 0 missing)                     self verbose                          print (   category  max    map     (cc  cc max  cc map))                     category map cc    cc map                              td   td replace(category map)                    bin ages (cuts extreme values)             age bins     0  12  18  25  30  35  40  45  50  55  60  65  80  65535               td  age binned     pd cut(td  age    age bins  labels false)             td   td replace(  age binned    np nan  0  )             print (    age bins  age bins       (len(age bins)  age bins))              x dict update(  (     cc)  td cc  values astype(np int32) reshape(  1  1 ) cc self categorical columns )          y dict     y   y          self verbose              print (    40)         return x dict  y dict  category map       def train(self  n epoch 1000  snapshot step 10  batch size none)           self x dict  self y dict  category map   self prepare input data(self train data   train data )         self testx dict  self testy dict      self prepare input data(self test data   test data   category map)         validation batch size   batch size self testy dict  y   shape 0          batch size   batch size self y dict  y   shape 0           print ( input data shape     output data shape   batch size     (str(self x dict  wide x   shape)                                                                                  str(self y dict  y   shape)                                                                                  batch size))         print ( test data shape     output data shape   validation batch size     (str(self testx dict  wide x   shape)                                                                                            str(self testy dict  y   shape)                                                                                            validation batch size))         print (    60      training )         self model fit(self x dict                          self y dict                         n epoch n epoch                         validation set (self testx dict  self testy dict)                         snapshot step snapshot step                         batch size batch size                         validation batch size validation batch size                         show metric true                          snapshot epoch false                         shuffle true                         run id self name          )              def evaluate(self)          logits   np array(self model predict(self testx dict)) reshape(  1 )         print (    60      evaluation )         print (   logits    min   max     (logits shape  logits min()  logits max()))         probs    1 0   (1 0   np exp( logits))         pred   pd series((probs   0 5) astype(np int32))         y   pd series(self testy dict  y   astype(np int32) reshape(  1 ))         self confusion matrix   self output confusion matrix(y  pred)         print (    60)      def output confusion matrix(self   pred)          assert size    pred size         print( actual idv )         print(y value counts())         print( predicted idv )         print(y pred value counts())         print()         print( confusion matrix  )         cmat   pd crosstab(y pred   rownames   predictions    colnames   actual  )         print(cmat)         sys stdout flush()         return cmat                                                                                      def commandline(args none)              main command line   accepts args  allow simple unit testing                                                                                                                                                                             name used run id    dict category name  category size    two categories  idv idv    wide  deep    placeholder target variable (i e  trainy input)    add validation monitor summaries giving confusion matrix entries   loss  roc auc score     loss  roc auc score      learn central bias separately wide deep    use wide learning rate    embedding layers work cpu   no gpu implementation tensorflow  yet     accuracy binary accuracy    use fully connected (instad single unit) fc works properly batches  whereas single unit 1d    x w (no bias)    batched sum  produce logits    accuracy binary accuracy    map categorical value strings integers    start 1 avoid 0 0 mapping (save 0 missing)    bin ages (cuts extreme values)                                                                                    commandline run  testing                                                                                    unit tests                                                                                 ", "content": "'''\nPedagogical example realization of wide & deep networks, using TensorFlow and TFLearn.\n\nThis is a re-implementation of http://arxiv.org/abs/1606.07792, using the combination\nof a wide linear model, and a deep feed-forward neural network, for binary classification  \nThis example realization is based on Tensorflow's TF.Learn tutorial \n(https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html),\nbut implemented in TFLearn.  Note that despite the closeness of names, TFLearn is distinct\nfrom TF.Learn (previously known as scikit flow).\n\nThis implementation explicitly presents the construction of layers in the deep part of the\nnetwork, and allows direct access to changing the layer architecture, and customization\nof methods used for regression and optimization.\n\nIn contrast, the TF.Learn tutorial offers more sophistication, but hides the layer\narchitecture behind a black box function, tf.contrib.learn.DNNLinearCombinedClassifier.\n\nSee https://github.com/ichuang/tflearn_wide_and_deep for more about this example.\n'''\n\nfrom __future__ import division, print_function\n\nimport os\nimport sys\nimport argparse\nimport tflearn\nimport tempfile\nimport urllib\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n#-----------------------------------------------------------------------------\n\nCOLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n           \"income_bracket\"]\nLABEL_COLUMN = \"label\"\nCATEGORICAL_COLUMNS = {\"workclass\": 10, \"education\": 17, \"marital_status\":8, \n                       \"occupation\": 16, \"relationship\": 7, \"race\": 6, \n                       \"gender\": 3, \"native_country\": 43, \"age_binned\": 14}\nCONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n                      \"hours_per_week\"]\n\n#-----------------------------------------------------------------------------\n\nclass TFLearnWideAndDeep(object):\n    '''\n    Wide and deep model, implemented using TFLearn\n    '''\n    AVAILABLE_MODELS = [\"wide\", \"deep\", \"wide+deep\"]\n    def __init__(self, model_type=\"wide+deep\", verbose=None, name=None, tensorboard_verbose=3, \n                 wide_learning_rate=0.001, deep_learning_rate=0.001, checkpoints_dir=None):\n        '''\n        model_type = `str`: wide or deep or wide+deep\n        verbose = `bool`\n        name = `str` used for run_id (defaults to model_type)\n        tensorboard_verbose = `int`: logging level for tensorboard (0, 1, 2, or 3)\n        wide_learning_rate = `float`: defaults to 0.001\n        deep_learning_rate = `float`: defaults to 0.001\n        checkpoints_dir = `str`: where checkpoint files will be stored (defaults to \"CHECKPOINTS\")\n        '''\n        self.model_type = model_type or \"wide+deep\"\n        assert self.model_type in self.AVAILABLE_MODELS\n        self.verbose = verbose or 0\n        self.tensorboard_verbose = tensorboard_verbose\n        self.name = name or self.model_type\t# name is used for the run_id\n        self.data_columns = COLUMNS\n        self.continuous_columns = CONTINUOUS_COLUMNS\n        self.categorical_columns = CATEGORICAL_COLUMNS\t# dict with category_name: category_size\n        self.label_column = LABEL_COLUMN\n        self.checkpoints_dir = checkpoints_dir or \"CHECKPOINTS\"\n        if not os.path.exists(self.checkpoints_dir):\n            os.mkdir(self.checkpoints_dir)\n            print(\"Created checkpoints directory %s\" % self.checkpoints_dir)\n        self.build_model([wide_learning_rate, deep_learning_rate])\n\n    def load_data(self, train_dfn=\"adult.data\", test_dfn=\"adult.test\"):\n        '''\n        Load data (use files offered in the Tensorflow wide_n_deep_tutorial)\n        '''\n        if not os.path.exists(train_dfn):\n            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_dfn)\n            print(\"Training data is downloaded to %s\" % train_dfn)\n\n        if not os.path.exists(test_dfn):\n            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_dfn)\n            print(\"Test data is downloaded to %s\" % test_dfn)\n\n        self.train_data = pd.read_csv(train_dfn, names=COLUMNS, skipinitialspace=True)\n        self.test_data = pd.read_csv(test_dfn, names=COLUMNS, skipinitialspace=True, skiprows=1)\n\n        self.train_data[self.label_column] = (self.train_data[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n        self.test_data[self.label_column] = (self.test_data[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n\n\n    def build_model(self, learning_rate=[0.001, 0.01]):\n        '''\n        Model - wide and deep - built using tflearn\n        '''\n        n_cc = len(self.continuous_columns)\n        n_categories = 1\t\t\t# two categories: is_idv and is_not_idv\n        input_shape = [None, n_cc]\n        if self.verbose:\n            print (\"=\"*77 + \" Model %s (type=%s)\" % (self.name, self.model_type))\n            print (\"  Input placeholder shape=%s\" % str(input_shape))\n        wide_inputs = tflearn.input_data(shape=input_shape, name=\"wide_X\")\n        if not isinstance(learning_rate, list):\n            learning_rate = [learning_rate, learning_rate]\t# wide, deep\n        if self.verbose:\n            print (\"  Learning rates (wide, deep)=%s\" % learning_rate)\n\n        with tf.name_scope(\"Y\"):\t\t\t# placeholder for target variable (i.e. trainY input)\n            Y_in = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"Y\")\n\n        with tf.variable_scope(None, \"cb_unit\", [wide_inputs]) as scope:\n            central_bias = tflearn.variables.variable('central_bias', shape=[1],\n                                                      initializer=tf.constant_initializer(np.random.randn()),\n                                                      trainable=True, restore=True)\n            tf.add_to_collection(tf.GraphKeys.LAYER_VARIABLES + '/cb_unit', central_bias)\n\n        if 'wide' in self.model_type:\n            wide_network = self.wide_model(wide_inputs, n_cc)\n            network = wide_network\n            wide_network_with_bias = tf.add(wide_network, central_bias, name=\"wide_with_bias\")\n\n        if 'deep' in self.model_type:\n            deep_network = self.deep_model(wide_inputs, n_cc)\n            deep_network_with_bias = tf.add(deep_network, central_bias, name=\"deep_with_bias\")\n            if 'wide' in self.model_type:\n                network = tf.add(wide_network, deep_network)\n                if self.verbose:\n                    print (\"Wide + deep model network %s\" % network)\n            else:\n                network = deep_network\n\n        network = tf.add(network, central_bias, name=\"add_central_bias\")\n\n        # add validation monitor summaries giving confusion matrix entries\n        with tf.name_scope('Monitors'):\n            predictions = tf.cast(tf.greater(network, 0), tf.int64)\n            print (\"predictions=%s\" % predictions)\n            Ybool = tf.cast(Y_in, tf.bool)\n            print (\"Ybool=%s\" % Ybool)\n            pos = tf.boolean_mask(predictions, Ybool)\n            neg = tf.boolean_mask(predictions, ~Ybool)\n            psize = tf.cast(tf.shape(pos)[0], tf.int64)\n            nsize = tf.cast(tf.shape(neg)[0], tf.int64)\n            true_positive = tf.reduce_sum(pos, name=\"true_positive\")\n            false_negative = tf.subtract(psize, true_positive, name=\"false_negative\")\n            false_positive = tf.reduce_sum(neg, name=\"false_positive\")\n            true_negative = tf.subtract(nsize, false_positive, name=\"true_negative\")\n            overall_accuracy = tf.truediv(tf.add(true_positive, true_negative), tf.add(nsize, psize), name=\"overall_accuracy\")\n        vmset = [true_positive, true_negative, false_positive, false_negative, overall_accuracy]\n\n        trainable_vars = tf.trainable_variables()\n        tv_deep = [v for v in trainable_vars if v.name.startswith('deep_')]\n        tv_wide = [v for v in trainable_vars if v.name.startswith('wide_')]\n\n        if self.verbose:\n            print (\"DEEP trainable_vars\")\n            for v in tv_deep:\n                print (\"  Variable %s: %s\" % (v.name, v))\n            print (\"WIDE trainable_vars\")\n            for v in tv_wide:\n                print (\"  Variable %s: %s\" % (v.name, v))\n\n        if 'wide' in self.model_type:\n            if not 'deep' in self.model_type:\n                tv_wide.append(central_bias)\n            tflearn.regression(wide_network_with_bias, \n                               placeholder=Y_in,\n                               optimizer='sgd', \n                               #loss='roc_auc_score',\n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[0],\n                               validation_monitors=vmset,\n                               trainable_vars=tv_wide,\n                               op_name=\"wide_regression\",\n                               name=\"Y\")\n\n        if 'deep' in self.model_type:\n            if not 'wide' in self.model_type:\n                tv_wide.append(central_bias)\n            tflearn.regression(deep_network_with_bias, \n                               placeholder=Y_in,\n                               optimizer='adam', \n                               #loss='roc_auc_score',\n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[1],\n                               validation_monitors=vmset if not 'wide' in self.model_type else None,\n                               trainable_vars=tv_deep,\n                               op_name=\"deep_regression\",\n                               name=\"Y\")\n\n        if self.model_type=='wide+deep':\t# learn central bias separately for wide+deep\n            tflearn.regression(network, \n                               placeholder=Y_in,\n                               optimizer='adam', \n                               loss='binary_crossentropy',\n                               metric=\"accuracy\",\n                               learning_rate=learning_rate[0],\t# use wide learning rate\n                               trainable_vars=[central_bias],\n                               op_name=\"central_bias_regression\",\n                               name=\"Y\")\n\n        self.model = tflearn.DNN(network,\n                                 tensorboard_verbose=self.tensorboard_verbose,\n                                 max_checkpoints=5,\n                                 checkpoint_path=\"%s/%s.tfl\" % (self.checkpoints_dir, self.name),\n        )\n\n        if self.verbose:\n            print (\"Target variables:\")\n            for v in tf.get_collection(tf.GraphKeys.TARGETS):\n                print (\"  variable %s: %s\" % (v.name, v))\n\n            print (\"=\"*77)\n\n\n    def deep_model(self, wide_inputs, n_inputs, n_nodes=[100, 50], use_dropout=False):\n        '''\n        Model - deep, i.e. two-layer fully connected network model\n        '''\n        cc_input_var = {}\n        cc_embed_var = {}\n        flat_vars = []\n        if self.verbose:\n            print (\"--> deep model: %s categories, %d continuous\" % (len(self.categorical_columns), n_inputs))\n        for cc, cc_size in self.categorical_columns.items():\n            cc_input_var[cc] = tflearn.input_data(shape=[None, 1], name=\"%s_in\" % cc,  dtype=tf.int32)\n            # embedding layers only work on CPU!  No GPU implementation in tensorflow, yet!\n            cc_embed_var[cc] = tflearn.layers.embedding_ops.embedding(cc_input_var[cc],    cc_size,  8, name=\"deep_%s_embed\" % cc)\n            if self.verbose:\n                print (\"    %s_embed = %s\" % (cc, cc_embed_var[cc]))\n            flat_vars.append(tf.squeeze(cc_embed_var[cc], squeeze_dims=[1], name=\"%s_squeeze\" % cc))\n\n        network = tf.concat([wide_inputs] + flat_vars, 1, name=\"deep_concat\")\n        for k in range(len(n_nodes)):\n            network = tflearn.fully_connected(network, n_nodes[k], activation=\"relu\", name=\"deep_fc%d\" % (k+1))\n            if use_dropout:\n                network = tflearn.dropout(network, 0.5, name=\"deep_dropout%d\" % (k+1))\n        if self.verbose:\n            print (\"Deep model network before output %s\" % network)\n        network = tflearn.fully_connected(network, 1, activation=\"linear\", name=\"deep_fc_output\", bias=False)\n        network = tf.reshape(network, [-1, 1])\t# so that accuracy is binary_accuracy\n        if self.verbose:\n            print (\"Deep model network %s\" % network)\n        return network\n\n    def wide_model(self, inputs, n_inputs):\n        '''\n        Model - wide, i.e. normal linear model (for logistic regression)\n        '''\n        network = inputs\n        # use fully_connected (instad of single_unit) because fc works properly with batches, whereas single_unit is 1D only\n        network = tflearn.fully_connected(network, n_inputs, activation=\"linear\", name=\"wide_linear\", bias=False)\t# x*W (no bias)\n        network = tf.reduce_sum(network, 1, name=\"reduce_sum\")\t# batched sum, to produce logits\n        network = tf.reshape(network, [-1, 1])\t# so that accuracy is binary_accuracy\n        if self.verbose:\n            print (\"Wide model network %s\" % network)\n        return network\n\n    def prepare_input_data(self, input_data, name=\"\", category_map=None):\n        '''\n        Prepare input data dicts\n        '''\n        print (\"-\"*40 + \" Preparing %s\" % name)\n        X = input_data[self.continuous_columns].values.astype(np.float32)\n        Y = input_data[self.label_column].values.astype(np.float32)\n        Y = Y.reshape([-1, 1])\n        if self.verbose:\n            print (\"  Y shape=%s, X shape=%s\" % (Y.shape, X.shape))\n\n        X_dict = {\"wide_X\": X}\n\n        if 'deep' in self.model_type:\n            # map categorical value strings to integers\n            td = input_data\n            if category_map is None:\n                category_map = {}\n                for cc in self.categorical_columns:\n                    if not cc in td.columns:\n                        continue\n                    cc_values = sorted(td[cc].unique())\n                    cc_max = 1+len(cc_values)\n                    cc_map = dict(zip(cc_values, range(1, cc_max)))\t# start from 1 to avoid 0:0 mapping (save 0 for missing)\n                    if self.verbose:\n                        print (\"  category %s max=%s,  map=%s\" % (cc, cc_max, cc_map))\n                    category_map[cc] = cc_map\n                \n            td = td.replace(category_map)\n    \n            # bin ages (cuts off extreme values)\n            age_bins = [ 0, 12, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 80, 65535 ]\n            td['age_binned'] = pd.cut(td['age'], age_bins, labels=False)\n            td = td.replace({'age_binned': {np.nan: 0}})\n            print (\"  %d age bins: age bins = %s\" % (len(age_bins), age_bins))\n\n            X_dict.update({ (\"%s_in\" % cc): td[cc].values.astype(np.int32).reshape([-1, 1]) for cc in self.categorical_columns})\n\n        Y_dict = {\"Y\": Y}\n        if self.verbose:\n            print (\"-\"*40)\n        return X_dict, Y_dict, category_map\n\n\n    def train(self, n_epoch=1000, snapshot_step=10, batch_size=None):\n\n        self.X_dict, self.Y_dict, category_map = self.prepare_input_data(self.train_data, \"train data\")\n        self.testX_dict, self.testY_dict, _ = self.prepare_input_data(self.test_data, \"test data\", category_map)\n        validation_batch_size = batch_size or self.testY_dict['Y'].shape[0]\n        batch_size = batch_size or self.Y_dict['Y'].shape[0]\n\n        print (\"Input data shape = %s; output data shape=%s, batch_size=%s\" % (str(self.X_dict['wide_X'].shape), \n                                                                               str(self.Y_dict['Y'].shape), \n                                                                               batch_size))\n        print (\"Test data shape = %s; output data shape=%s, validation_batch_size=%s\" % (str(self.testX_dict['wide_X'].shape), \n                                                                                         str(self.testY_dict['Y'].shape), \n                                                                                         validation_batch_size))\n        print (\"=\"*60 + \"  Training\")\n        self.model.fit(self.X_dict, \n                       self.Y_dict,\n                       n_epoch=n_epoch,\n                       validation_set=(self.testX_dict, self.testY_dict),\n                       snapshot_step=snapshot_step,\n                       batch_size=batch_size,\n                       validation_batch_size=validation_batch_size,\n                       show_metric=True, \n                       snapshot_epoch=False,\n                       shuffle=True,\n                       run_id=self.name,\n        )\n        \n    def evaluate(self):\n        logits = np.array(self.model.predict(self.testX_dict)).reshape([-1])\n        print (\"=\"*60 + \"  Evaluation\")\n        print (\"  logits: %s, min=%s, max=%s\" % (logits.shape, logits.min(), logits.max()))\n        probs =  1.0 / (1.0 + np.exp(-logits))\n        y_pred = pd.Series((probs > 0.5).astype(np.int32))\n        Y = pd.Series(self.testY_dict['Y'].astype(np.int32).reshape([-1]))\n        self.confusion_matrix = self.output_confusion_matrix(Y, y_pred)\n        print (\"=\"*60)\n\n    def output_confusion_matrix(self, y, y_pred):\n        assert y.size == y_pred.size\n        print(\"Actual IDV\")\n        print(y.value_counts())\n        print(\"Predicted IDV\")\n        print(y_pred.value_counts())\n        print()\n        print(\"Confusion matrix:\")\n        cmat = pd.crosstab(y_pred, y, rownames=['predictions'], colnames=['actual'])\n        print(cmat)\n        sys.stdout.flush()\n        return cmat\n    \n#-----------------------------------------------------------------------------\n\ndef CommandLine(args=None):\n    '''\n    Main command line.  Accepts args, to allow for simple unit testing.\n    '''\n    flags = tf.app.flags\n    FLAGS = flags.FLAGS\n    if args:\n        FLAGS.__init__()\n        FLAGS.__dict__.update(args)\n\n    try:\n        flags.DEFINE_string(\"model_type\", \"wide+deep\",\"Valid model types: {'wide', 'deep', 'wide+deep'}.\")\n        flags.DEFINE_string(\"run_name\", None, \"name for this run (defaults to model type)\")\n        flags.DEFINE_string(\"load_weights\", None, \"filename with initial weights to load\")\n        flags.DEFINE_string(\"checkpoints_dir\", None, \"name of directory where checkpoints should be saved\")\n        flags.DEFINE_integer(\"n_epoch\", 200, \"Number of training epoch steps\")\n        flags.DEFINE_integer(\"snapshot_step\", 100, \"Step number when snapshot (and validation testing) is done\")\n        flags.DEFINE_float(\"wide_learning_rate\", 0.001, \"learning rate for the wide part of the model\")\n        flags.DEFINE_float(\"deep_learning_rate\", 0.001, \"learning rate for the deep part of the model\")\n        flags.DEFINE_boolean(\"verbose\", False, \"Verbose output\")\n    except argparse.ArgumentError:\n        pass\t# so that CommandLine can be run more than once, for testing\n\n    twad = TFLearnWideAndDeep(model_type=FLAGS.model_type, verbose=FLAGS.verbose, \n                              name=FLAGS.run_name, wide_learning_rate=FLAGS.wide_learning_rate,\n                              deep_learning_rate=FLAGS.deep_learning_rate,\n                              checkpoints_dir=FLAGS.checkpoints_dir)\n    twad.load_data()\n    if FLAGS.load_weights:\n        print (\"Loading initial weights from %s\" % FLAGS.load_weights)\n        twad.model.load(FLAGS.load_weights)\n    twad.train(n_epoch=FLAGS.n_epoch, snapshot_step=FLAGS.snapshot_step)\n    twad.evaluate()\n    return twad\n\n#-----------------------------------------------------------------------------\n# unit tests\n\ndef test_wide_and_deep():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"wide+deep\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\ndef test_deep():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"deep\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\ndef test_wide():\n    import glob\n    tf.reset_default_graph()\n    cdir = \"test_checkpoints\"\n    if os.path.exists(cdir):\n        os.system(\"rm -rf %s\" % cdir)\n    twad = CommandLine(args=dict(verbose=True, n_epoch=5, model_type=\"wide\", snapshot_step=5, \n                                 wide_learning_rate=0.0001, checkpoints_dir=cdir))\n    cfiles = glob.glob(\"%s/*.tfl-*\" % cdir)\n    print (\"cfiles=%s\" % cfiles)\n    assert(len(cfiles))\n    cm = twad.confusion_matrix.values.astype(np.float32)\n    assert(cm[1][1])\n\n#-----------------------------------------------------------------------------\n\nif __name__==\"__main__\":\n    CommandLine()\n    None\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "recommender_wide_and_deep.py", "id": "47946a3cb90afa068ce1079f7303acc5", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/others/recommender_wide_and_deep.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}