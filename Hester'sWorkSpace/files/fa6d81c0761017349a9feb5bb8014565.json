{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Adversarial losses for text models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n Dependency imports\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n Adversarial and virtual adversarial training parameters.\nflags.DEFINE_float('perturb_norm_length', 5.0,\n                   'Norm length of adversarial perturbation to be '\n                   'optimized with validation. '\n                   '5.0 is optimal on IMDB with virtual adversarial training. ')\n\n Virtual adversarial training parameters\nflags.DEFINE_integer('num_power_iteration', 1, 'The number of power iteration')\nflags.DEFINE_float('small_constant_for_finite_diff', 1e-1,\n                   'Small constant for finite difference method')\n\n Parameters for building the graph\nflags.DEFINE_string('adv_training_method', None,\n                    'The flag which specifies training method. '\n                    '\"\"    : non-adversarial training (e.g. for running the '\n                    '        semi-supervised sequence learning model) '\n                    '\"rp\"  : random perturbation training '\n                    '\"at\"  : adversarial training '\n                    '\"vat\" : virtual adversarial training '\n                    '\"atvat\" : at + vat ')\nflags.DEFINE_float('adv_reg_coeff', 1.0,\n                   'Regularization coefficient of adversarial loss.')\n\n\ndef random_perturbation_loss(embedded, length, loss_fn):\n  \"\"\"Adds noise to embeddings and recomputes classification loss.\"\"\"\n  noise = tf.random_normal(shape=tf.shape(embedded))\n  perturb = _scale_l2(_mask_by_length(noise, length), FLAGS.perturb_norm_length)\n  return loss_fn(embedded + perturb)\n\n\ndef adversarial_loss(embedded, loss, loss_fn):\n  \"\"\"Adds gradient to embedding and recomputes classification loss.\"\"\"\n  grad, = tf.gradients(\n      loss,\n      embedded,\n      aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n  grad = tf.stop_gradient(grad)\n  perturb = _scale_l2(grad, FLAGS.perturb_norm_length)\n  return loss_fn(embedded + perturb)\n\n\ndef virtual_adversarial_loss(logits, embedded, inputs,\n                             logits_from_embedding_fn):\n  \"\"\"Virtual adversarial loss.\n\n  Computes virtual adversarial perturbation by finite difference method and\n  power iteration, adds it to the embedding, and computes the KL divergence\n  between the new logits and the original logits.\n\n  Args:\n    logits: 3-D float Tensor, [batch_size, num_timesteps, m], where m=1 if\n      num_classes=2, otherwise m=num_classes.\n    embedded: 3-D float Tensor, [batch_size, num_timesteps, embedding_dim].\n    inputs: VatxtInput.\n    logits_from_embedding_fn: callable that takes embeddings and returns\n      classifier logits.\n\n  Returns:\n    kl: float scalar.\n  \"\"\"\n   Stop gradient of logits. See https://arxiv.org/abs/1507.00677 for details.\n  logits = tf.stop_gradient(logits)\n\n   Only care about the KL divergence on the final timestep.\n  weights = inputs.eos_weights\n  assert weights is not None\n  if FLAGS.single_label:\n    indices = tf.stack([tf.range(FLAGS.batch_size), inputs.length - 1], 1)\n    weights = tf.expand_dims(tf.gather_nd(inputs.eos_weights, indices), 1)\n\n   Initialize perturbation with random noise.\n   shape(embedded) = (batch_size, num_timesteps, embedding_dim)\n  d = tf.random_normal(shape=tf.shape(embedded))\n\n   Perform finite difference method and power iteration.\n   See Eq.(8) in the paper http://arxiv.org/pdf/1507.00677.pdf,\n   Adding small noise to input and taking gradient with respect to the noise\n   corresponds to 1 power iteration.\n  for _ in xrange(FLAGS.num_power_iteration):\n    d = _scale_l2(\n        _mask_by_length(d, inputs.length), FLAGS.small_constant_for_finite_diff)\n\n    d_logits = logits_from_embedding_fn(embedded + d)\n    kl = _kl_divergence_with_logits(logits, d_logits, weights)\n    d, = tf.gradients(\n        kl,\n        d,\n        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n    d = tf.stop_gradient(d)\n\n  perturb = _scale_l2(d, FLAGS.perturb_norm_length)\n  vadv_logits = logits_from_embedding_fn(embedded + perturb)\n  return _kl_divergence_with_logits(logits, vadv_logits, weights)\n\n\ndef random_perturbation_loss_bidir(embedded, length, loss_fn):\n  \"\"\"Adds noise to embeddings and recomputes classification loss.\"\"\"\n  noise = [tf.random_normal(shape=tf.shape(emb)) for emb in embedded]\n  masked = [_mask_by_length(n, length) for n in noise]\n  scaled = [_scale_l2(m, FLAGS.perturb_norm_length) for m in masked]\n  return loss_fn([e + s for (e, s) in zip(embedded, scaled)])\n\n\ndef adversarial_loss_bidir(embedded, loss, loss_fn):\n  \"\"\"Adds gradient to embeddings and recomputes classification loss.\"\"\"\n  grads = tf.gradients(\n      loss,\n      embedded,\n      aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n  adv_exs = [\n      emb + _scale_l2(tf.stop_gradient(g), FLAGS.perturb_norm_length)\n      for emb, g in zip(embedded, grads)\n  ]\n  return loss_fn(adv_exs)\n\n\ndef virtual_adversarial_loss_bidir(logits, embedded, inputs,\n                                   logits_from_embedding_fn):\n  \"\"\"Virtual adversarial loss for bidirectional models.\"\"\"\n  logits = tf.stop_gradient(logits)\n  f_inputs, _ = inputs\n  weights = f_inputs.eos_weights\n  if FLAGS.single_label:\n    indices = tf.stack([tf.range(FLAGS.batch_size), f_inputs.length - 1], 1)\n    weights = tf.expand_dims(tf.gather_nd(f_inputs.eos_weights, indices), 1)\n  assert weights is not None\n\n  perturbs = [\n      _mask_by_length(tf.random_normal(shape=tf.shape(emb)), f_inputs.length)\n      for emb in embedded\n  ]\n  for _ in xrange(FLAGS.num_power_iteration):\n    perturbs = [\n        _scale_l2(d, FLAGS.small_constant_for_finite_diff) for d in perturbs\n    ]\n    d_logits = logits_from_embedding_fn(\n        [emb + d for (emb, d) in zip(embedded, perturbs)])\n    kl = _kl_divergence_with_logits(logits, d_logits, weights)\n    perturbs = tf.gradients(\n        kl,\n        perturbs,\n        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n    perturbs = [tf.stop_gradient(d) for d in perturbs]\n\n  perturbs = [_scale_l2(d, FLAGS.perturb_norm_length) for d in perturbs]\n  vadv_logits = logits_from_embedding_fn(\n      [emb + d for (emb, d) in zip(embedded, perturbs)])\n  return _kl_divergence_with_logits(logits, vadv_logits, weights)\n\n\ndef _mask_by_length(t, length):\n  \"\"\"Mask t, 3-D [batch, time, dim], by length, 1-D [batch,].\"\"\"\n  maxlen = t.get_shape().as_list()[1]\n\n   Subtract 1 from length to prevent the perturbation from going on 'eos'\n  mask = tf.sequence_mask(length - 1, maxlen=maxlen)\n  mask = tf.expand_dims(tf.cast(mask, tf.float32), -1)\n   shape(mask) = (batch, num_timesteps, 1)\n  return t * mask\n\n\ndef _scale_l2(x, norm_length):\n   shape(x) = (batch, num_timesteps, d)\n   Divide x by max(abs(x)) for a numerically stable L2 norm.\n   2norm(x) = a * 2norm(x/a)\n   Scale over the full sequence, dims (1, 2)\n  alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n  l2_norm = alpha * tf.sqrt(\n      tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n  x_unit = x / l2_norm\n  return norm_length * x_unit\n\n\ndef _kl_divergence_with_logits(q_logits, p_logits, weights):\n  \"\"\"Returns weighted KL divergence between distributions q and p.\n\n  Args:\n    q_logits: logits for 1st argument of KL divergence shape\n              [batch_size, num_timesteps, num_classes] if num_classes > 2, and\n              [batch_size, num_timesteps] if num_classes == 2.\n    p_logits: logits for 2nd argument of KL divergence with same shape q_logits.\n    weights: 1-D float tensor with shape [batch_size, num_timesteps].\n             Elements should be 1.0 only on end of sequences\n\n  Returns:\n    KL: float scalar.\n  \"\"\"\n   For logistic regression\n  if FLAGS.num_classes == 2:\n    q = tf.nn.sigmoid(q_logits)\n    kl = (-tf.nn.sigmoid_cross_entropy_with_logits(logits=q_logits, labels=q) +\n          tf.nn.sigmoid_cross_entropy_with_logits(logits=p_logits, labels=q))\n    kl = tf.squeeze(kl, 2)\n\n   For softmax regression\n  else:\n    q = tf.nn.softmax(q_logits)\n    kl = tf.reduce_sum(\n        q * (tf.nn.log_softmax(q_logits) - tf.nn.log_softmax(p_logits)), -1)\n\n  num_labels = tf.reduce_sum(weights)\n  num_labels = tf.where(tf.equal(num_labels, 0.), 1., num_labels)\n\n  kl.get_shape().assert_has_rank(2)\n  weights.get_shape().assert_has_rank(2)\n\n  loss = tf.identity(tf.reduce_sum(weights * kl) / num_labels, name='kl')\n  return loss\n", "comments": "   adversarial losses text models       future   import absolute import   future   import division   future   import print function    dependency imports  six moves import xrange import tensorflow tf  flags   tf app flags flags   flags flags    adversarial virtual adversarial training parameters  flags define float( perturb norm length   5 0                      norm length adversarial perturbation                       optimized validation                        5 0 optimal imdb virtual adversarial training   )    virtual adversarial training parameters flags define integer( num power iteration   1   the number power iteration ) flags define float( small constant finite diff   1e 1                      small constant finite difference method )    parameters building graph flags define string( adv training method   none                       the flag specifies training method                                 non adversarial training (e g  running                                semi supervised sequence learning model)                         rp     random perturbation training                             adversarial training                         vat    virtual adversarial training                         atvat      vat  ) flags define float( adv reg coeff   1 0                      regularization coefficient adversarial loss  )   def random perturbation loss(embedded  length  loss fn)       adds noise embeddings recomputes classification loss       noise   tf random normal(shape tf shape(embedded))   perturb    scale l2( mask length(noise  length)  flags perturb norm length)   return loss fn(embedded   perturb)   def adversarial loss(embedded  loss  loss fn)       adds gradient embedding recomputes classification loss       grad    tf gradients(       loss        embedded        aggregation method tf aggregationmethod experimental accumulate n)   grad   tf stop gradient(grad)   perturb    scale l2(grad  flags perturb norm length)   return loss fn(embedded   perturb)   def virtual adversarial loss(logits  embedded  inputs                               logits embedding fn)       virtual adversarial loss     computes virtual adversarial perturbation finite difference method   power iteration  adds embedding  computes kl divergence   new logits original logits     args      logits  3 d float tensor   batch size  num timesteps    1       num classes 2  otherwise num classes      embedded  3 d float tensor   batch size  num timesteps  embedding dim       inputs  vatxtinput      logits embedding fn  callable takes embeddings returns       classifier logits     returns      kl  float scalar            stop gradient logits  see https   arxiv org abs 1507 00677 details    logits   tf stop gradient(logits)      only care kl divergence final timestep    weights   inputs eos weights   assert weights none   flags single label      indices   tf stack( tf range(flags batch size)  inputs length   1   1)     weights   tf expand dims(tf gather nd(inputs eos weights  indices)  1)      initialize perturbation random noise      shape(embedded)   (batch size  num timesteps  embedding dim)     tf random normal(shape tf shape(embedded))      perform finite difference method power iteration      see eq (8) paper http   arxiv org pdf 1507 00677 pdf      adding small noise input taking gradient respect noise     corresponds 1 power iteration      xrange(flags num power iteration)         scale l2(          mask length(d  inputs length)  flags small constant finite diff)      logits   logits embedding fn(embedded   d)     kl    kl divergence logits(logits  logits  weights)        tf gradients(         kl                   aggregation method tf aggregationmethod experimental accumulate n)       tf stop gradient(d)    perturb    scale l2(d  flags perturb norm length)   vadv logits   logits embedding fn(embedded   perturb)   return  kl divergence logits(logits  vadv logits  weights)   def random perturbation loss bidir(embedded  length  loss fn)       adds noise embeddings recomputes classification loss       noise    tf random normal(shape tf shape(emb)) emb embedded    masked     mask length(n  length) n noise    scaled     scale l2(m  flags perturb norm length) masked    return loss fn( e   (e  s) zip(embedded  scaled) )   def adversarial loss bidir(embedded  loss  loss fn)       adds gradient embeddings recomputes classification loss       grads   tf gradients(       loss        embedded        aggregation method tf aggregationmethod experimental accumulate n)   adv exs           emb    scale l2(tf stop gradient(g)  flags perturb norm length)       emb  g zip(embedded  grads)       return loss fn(adv exs)   def virtual adversarial loss bidir(logits  embedded  inputs                                     logits embedding fn)       virtual adversarial loss bidirectional models       logits   tf stop gradient(logits)   f inputs      inputs   weights   f inputs eos weights   flags single label      indices   tf stack( tf range(flags batch size)  f inputs length   1   1)     weights   tf expand dims(tf gather nd(f inputs eos weights  indices)  1)   assert weights none    perturbs            mask length(tf random normal(shape tf shape(emb))  f inputs length)       emb embedded         xrange(flags num power iteration)      perturbs              scale l2(d  flags small constant finite diff) perturbs           logits   logits embedding fn(          emb   (emb  d) zip(embedded  perturbs) )     kl    kl divergence logits(logits  logits  weights)     perturbs   tf gradients(         kl          perturbs          aggregation method tf aggregationmethod experimental accumulate n)     perturbs    tf stop gradient(d) perturbs     perturbs     scale l2(d  flags perturb norm length) perturbs    vadv logits   logits embedding fn(        emb   (emb  d) zip(embedded  perturbs) )   return  kl divergence logits(logits  vadv logits  weights)   def  mask length(t  length)       mask  3 d  batch  time  dim   length  1 d  batch         maxlen   get shape() list() 1       subtract 1 length prevent perturbation going  eos    mask   tf sequence mask(length   1  maxlen maxlen)   mask   tf expand dims(tf cast(mask  tf float32)   1)     shape(mask)   (batch  num timesteps  1)   return   mask   def  scale l2(x  norm length)      shape(x)   (batch  num timesteps  d)     divide x max(abs(x)) numerically stable l2 norm      2norm(x)     2norm(x a)     scale full sequence  dims (1  2)   alpha   tf reduce max(tf abs(x)  (1  2)  keep dims true)   1e 12   l2 norm   alpha   tf sqrt(       tf reduce sum(tf pow(x   alpha  2)  (1  2)  keep dims true)   1e 6)   x unit   x   l2 norm   return norm length   x unit   def  kl divergence logits(q logits  p logits  weights)       returns weighted kl divergence distributions q p     args      q logits  logits 1st argument kl divergence shape                batch size  num timesteps  num classes  num classes   2                 batch size  num timesteps  num classes    2      p logits  logits 2nd argument kl divergence shape q logits      weights  1 d float tensor shape  batch size  num timesteps                elements 1 0 end sequences    returns      kl  float scalar           copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       dependency imports    adversarial virtual adversarial training parameters     virtual adversarial training parameters    parameters building graph    stop gradient logits  see https   arxiv org abs 1507 00677 details     only care kl divergence final timestep     initialize perturbation random noise     shape(embedded)   (batch size  num timesteps  embedding dim)    perform finite difference method power iteration     see eq (8) paper http   arxiv org pdf 1507 00677 pdf     adding small noise input taking gradient respect noise    corresponds 1 power iteration     subtract 1 length prevent perturbation going  eos     shape(mask)   (batch  num timesteps  1)    shape(x)   (batch  num timesteps  d)    divide x max(abs(x)) numerically stable l2 norm     2norm(x)     2norm(x a)    scale full sequence  dims (1  2)    for logistic regression    for softmax regression ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Adversarial losses for text models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Dependency imports\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Adversarial and virtual adversarial training parameters.\nflags.DEFINE_float('perturb_norm_length', 5.0,\n                   'Norm length of adversarial perturbation to be '\n                   'optimized with validation. '\n                   '5.0 is optimal on IMDB with virtual adversarial training. ')\n\n# Virtual adversarial training parameters\nflags.DEFINE_integer('num_power_iteration', 1, 'The number of power iteration')\nflags.DEFINE_float('small_constant_for_finite_diff', 1e-1,\n                   'Small constant for finite difference method')\n\n# Parameters for building the graph\nflags.DEFINE_string('adv_training_method', None,\n                    'The flag which specifies training method. '\n                    '\"\"    : non-adversarial training (e.g. for running the '\n                    '        semi-supervised sequence learning model) '\n                    '\"rp\"  : random perturbation training '\n                    '\"at\"  : adversarial training '\n                    '\"vat\" : virtual adversarial training '\n                    '\"atvat\" : at + vat ')\nflags.DEFINE_float('adv_reg_coeff', 1.0,\n                   'Regularization coefficient of adversarial loss.')\n\n\ndef random_perturbation_loss(embedded, length, loss_fn):\n  \"\"\"Adds noise to embeddings and recomputes classification loss.\"\"\"\n  noise = tf.random_normal(shape=tf.shape(embedded))\n  perturb = _scale_l2(_mask_by_length(noise, length), FLAGS.perturb_norm_length)\n  return loss_fn(embedded + perturb)\n\n\ndef adversarial_loss(embedded, loss, loss_fn):\n  \"\"\"Adds gradient to embedding and recomputes classification loss.\"\"\"\n  grad, = tf.gradients(\n      loss,\n      embedded,\n      aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n  grad = tf.stop_gradient(grad)\n  perturb = _scale_l2(grad, FLAGS.perturb_norm_length)\n  return loss_fn(embedded + perturb)\n\n\ndef virtual_adversarial_loss(logits, embedded, inputs,\n                             logits_from_embedding_fn):\n  \"\"\"Virtual adversarial loss.\n\n  Computes virtual adversarial perturbation by finite difference method and\n  power iteration, adds it to the embedding, and computes the KL divergence\n  between the new logits and the original logits.\n\n  Args:\n    logits: 3-D float Tensor, [batch_size, num_timesteps, m], where m=1 if\n      num_classes=2, otherwise m=num_classes.\n    embedded: 3-D float Tensor, [batch_size, num_timesteps, embedding_dim].\n    inputs: VatxtInput.\n    logits_from_embedding_fn: callable that takes embeddings and returns\n      classifier logits.\n\n  Returns:\n    kl: float scalar.\n  \"\"\"\n  # Stop gradient of logits. See https://arxiv.org/abs/1507.00677 for details.\n  logits = tf.stop_gradient(logits)\n\n  # Only care about the KL divergence on the final timestep.\n  weights = inputs.eos_weights\n  assert weights is not None\n  if FLAGS.single_label:\n    indices = tf.stack([tf.range(FLAGS.batch_size), inputs.length - 1], 1)\n    weights = tf.expand_dims(tf.gather_nd(inputs.eos_weights, indices), 1)\n\n  # Initialize perturbation with random noise.\n  # shape(embedded) = (batch_size, num_timesteps, embedding_dim)\n  d = tf.random_normal(shape=tf.shape(embedded))\n\n  # Perform finite difference method and power iteration.\n  # See Eq.(8) in the paper http://arxiv.org/pdf/1507.00677.pdf,\n  # Adding small noise to input and taking gradient with respect to the noise\n  # corresponds to 1 power iteration.\n  for _ in xrange(FLAGS.num_power_iteration):\n    d = _scale_l2(\n        _mask_by_length(d, inputs.length), FLAGS.small_constant_for_finite_diff)\n\n    d_logits = logits_from_embedding_fn(embedded + d)\n    kl = _kl_divergence_with_logits(logits, d_logits, weights)\n    d, = tf.gradients(\n        kl,\n        d,\n        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n    d = tf.stop_gradient(d)\n\n  perturb = _scale_l2(d, FLAGS.perturb_norm_length)\n  vadv_logits = logits_from_embedding_fn(embedded + perturb)\n  return _kl_divergence_with_logits(logits, vadv_logits, weights)\n\n\ndef random_perturbation_loss_bidir(embedded, length, loss_fn):\n  \"\"\"Adds noise to embeddings and recomputes classification loss.\"\"\"\n  noise = [tf.random_normal(shape=tf.shape(emb)) for emb in embedded]\n  masked = [_mask_by_length(n, length) for n in noise]\n  scaled = [_scale_l2(m, FLAGS.perturb_norm_length) for m in masked]\n  return loss_fn([e + s for (e, s) in zip(embedded, scaled)])\n\n\ndef adversarial_loss_bidir(embedded, loss, loss_fn):\n  \"\"\"Adds gradient to embeddings and recomputes classification loss.\"\"\"\n  grads = tf.gradients(\n      loss,\n      embedded,\n      aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n  adv_exs = [\n      emb + _scale_l2(tf.stop_gradient(g), FLAGS.perturb_norm_length)\n      for emb, g in zip(embedded, grads)\n  ]\n  return loss_fn(adv_exs)\n\n\ndef virtual_adversarial_loss_bidir(logits, embedded, inputs,\n                                   logits_from_embedding_fn):\n  \"\"\"Virtual adversarial loss for bidirectional models.\"\"\"\n  logits = tf.stop_gradient(logits)\n  f_inputs, _ = inputs\n  weights = f_inputs.eos_weights\n  if FLAGS.single_label:\n    indices = tf.stack([tf.range(FLAGS.batch_size), f_inputs.length - 1], 1)\n    weights = tf.expand_dims(tf.gather_nd(f_inputs.eos_weights, indices), 1)\n  assert weights is not None\n\n  perturbs = [\n      _mask_by_length(tf.random_normal(shape=tf.shape(emb)), f_inputs.length)\n      for emb in embedded\n  ]\n  for _ in xrange(FLAGS.num_power_iteration):\n    perturbs = [\n        _scale_l2(d, FLAGS.small_constant_for_finite_diff) for d in perturbs\n    ]\n    d_logits = logits_from_embedding_fn(\n        [emb + d for (emb, d) in zip(embedded, perturbs)])\n    kl = _kl_divergence_with_logits(logits, d_logits, weights)\n    perturbs = tf.gradients(\n        kl,\n        perturbs,\n        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n    perturbs = [tf.stop_gradient(d) for d in perturbs]\n\n  perturbs = [_scale_l2(d, FLAGS.perturb_norm_length) for d in perturbs]\n  vadv_logits = logits_from_embedding_fn(\n      [emb + d for (emb, d) in zip(embedded, perturbs)])\n  return _kl_divergence_with_logits(logits, vadv_logits, weights)\n\n\ndef _mask_by_length(t, length):\n  \"\"\"Mask t, 3-D [batch, time, dim], by length, 1-D [batch,].\"\"\"\n  maxlen = t.get_shape().as_list()[1]\n\n  # Subtract 1 from length to prevent the perturbation from going on 'eos'\n  mask = tf.sequence_mask(length - 1, maxlen=maxlen)\n  mask = tf.expand_dims(tf.cast(mask, tf.float32), -1)\n  # shape(mask) = (batch, num_timesteps, 1)\n  return t * mask\n\n\ndef _scale_l2(x, norm_length):\n  # shape(x) = (batch, num_timesteps, d)\n  # Divide x by max(abs(x)) for a numerically stable L2 norm.\n  # 2norm(x) = a * 2norm(x/a)\n  # Scale over the full sequence, dims (1, 2)\n  alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n  l2_norm = alpha * tf.sqrt(\n      tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n  x_unit = x / l2_norm\n  return norm_length * x_unit\n\n\ndef _kl_divergence_with_logits(q_logits, p_logits, weights):\n  \"\"\"Returns weighted KL divergence between distributions q and p.\n\n  Args:\n    q_logits: logits for 1st argument of KL divergence shape\n              [batch_size, num_timesteps, num_classes] if num_classes > 2, and\n              [batch_size, num_timesteps] if num_classes == 2.\n    p_logits: logits for 2nd argument of KL divergence with same shape q_logits.\n    weights: 1-D float tensor with shape [batch_size, num_timesteps].\n             Elements should be 1.0 only on end of sequences\n\n  Returns:\n    KL: float scalar.\n  \"\"\"\n  # For logistic regression\n  if FLAGS.num_classes == 2:\n    q = tf.nn.sigmoid(q_logits)\n    kl = (-tf.nn.sigmoid_cross_entropy_with_logits(logits=q_logits, labels=q) +\n          tf.nn.sigmoid_cross_entropy_with_logits(logits=p_logits, labels=q))\n    kl = tf.squeeze(kl, 2)\n\n  # For softmax regression\n  else:\n    q = tf.nn.softmax(q_logits)\n    kl = tf.reduce_sum(\n        q * (tf.nn.log_softmax(q_logits) - tf.nn.log_softmax(p_logits)), -1)\n\n  num_labels = tf.reduce_sum(weights)\n  num_labels = tf.where(tf.equal(num_labels, 0.), 1., num_labels)\n\n  kl.get_shape().assert_has_rank(2)\n  weights.get_shape().assert_has_rank(2)\n\n  loss = tf.identity(tf.reduce_sum(weights * kl) / num_labels, name='kl')\n  return loss\n", "description": "Models and examples built with TensorFlow", "file_name": "adversarial_losses.py", "id": "fa6d81c0761017349a9feb5bb8014565", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/adversarial_text/adversarial_losses.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}