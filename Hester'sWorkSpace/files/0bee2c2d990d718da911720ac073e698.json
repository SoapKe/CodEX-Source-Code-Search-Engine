{"author": "songrotek", "code": "from __future__ import print_function\nimport os\nimport re\nfrom six.moves.urllib.request import urlopen\nfrom six.moves.urllib.error import HTTPError\nimport urllib2\nimport shutil\nimport argparse\nimport mistune\nimport bs4 as BeautifulSoup\nimport socket\nimport time\nimport requests\n\n\nimport sys  \n\nreload(sys)  \nsys.setdefaultencoding('utf8')\n\ndef download_pdf(link, location, name):\n    try:\n        response = requests.get(link)\n        with open(os.path.join(location, name), 'wb') as f:\n        \tf.write(response.content)\n        \tf.close()\n    except HTTPError:\n        print('>>> Error 404: cannot be downloaded!\\n') \n        raise   \n    except socket.timeout:\n        print(\" \".join((\"can't download\", link, \"due to connection timeout!\")) )\n        raise\n\ndef clean_pdf_link(link):\n    if 'arxiv' in link:\n        link = link.replace('abs', 'pdf')   \n        if not(link.endswith('.pdf')):\n            link = '.'.join((link, 'pdf'))\n\n    print(link)\n    return link\n\ndef clean_text(text, replacements = {':': '_', ' ': '_', '/': '_', '.': '', '\"': ''}):\n    for key, rep in replacements.items():\n        text = text.replace(key, rep)\n    return text    \n\ndef print_title(title, pattern = \"-\"):\n    print('\\n'.join((\"\", title, pattern * len(title)))) \n\ndef get_extension(link):\n    extension = os.path.splitext(link)[1][1:]\n    if extension in ['pdf', 'html']:\n        return extension\n    if 'pdf' in extension:\n        return 'pdf'    \n    return 'pdf'    \n\ndef shorten_title(title):\n    m1 = re.search('[[0-9]*]', title)\n    m2 = re.search('\".*\"', title)\n    if m1:\n        title = m1.group(0)\n    if m2:\n        title = ' '.join((title, m2.group(0)))   \n    return title[:50] + ' [...]'    \n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description = 'Download all the PDF/HTML links into README.md')\n    parser.add_argument('-d', action=\"store\", dest=\"directory\")\n    parser.add_argument('--no-html', action=\"store_true\", dest=\"nohtml\", default = False)\n    parser.add_argument('--overwrite', action=\"store_true\", default = False)    \n    results = parser.parse_args()\n\n    output_directory = 'pdfs' if results.directory is None else results.directory\n\n    forbidden_extensions = ['html', 'htm'] if results.nohtml else []\n\n    if results.overwrite and os.path.exists(output_directory):\n        shutil.rmtree(output_directory)\n\n    with open('README.md') as readme:\n        readme_html = mistune.markdown(readme.read())\n        readme_soup = BeautifulSoup.BeautifulSoup(readme_html, \"html.parser\")\n\n    point = readme_soup.find_all('h1')[1]\n\n    failures = []\n    while point is not None:\n        if point.name:\n            if re.search('h[1-2]', point.name):\n                if point.name == 'h1':\n                    h1_directory = os.path.join(output_directory, clean_text(point.text))\n                    current_directory = h1_directory\n                elif point.name == 'h2':\n                    current_directory = os.path.join(h1_directory, clean_text(point.text))  \n                if not os.path.exists(current_directory):\n                    os.makedirs(current_directory)\n                print_title(point.text)\n\n            if point.name == 'p':\n                link = point.find('a')\n                if link is not None:\n                    link = clean_pdf_link(link.attrs['href'])\n                    ext = get_extension(link)\n                    print(ext)\n                    if not ext in forbidden_extensions:\n                        print(shorten_title(point.text) + ' (' + link + ')')\n                        try:\n                            name = clean_text(point.text.split('[' + ext + ']')[0])\n                            fullname = '.'.join((name, ext))\n                            if not os.path.exists('/'.join((current_directory, fullname)) ):\n                                download_pdf(link, current_directory, '.'.join((name, ext)))\n                        except KeyboardInterrupt:\n                            try:\n                                print(\"Press Ctrl-C in 1 second to quit\")\n                                time.sleep(1)\n                            except KeyboardInterrupt:\n                                print(\"Cancelling..\")\n                                break\n                        except:\n                            failures.append(point.text)\n                        \n        point = point.next_sibling          \n\n    print('Done!')\n    if failures:\n        print('Some downloads have failed:')\n        for fail in failures:\n            print('> ' + fail)\n", "comments": "  encoding utf8   ", "content": "from __future__ import print_function\nimport os\nimport re\nfrom six.moves.urllib.request import urlopen\nfrom six.moves.urllib.error import HTTPError\nimport urllib2\nimport shutil\nimport argparse\nimport mistune\nimport bs4 as BeautifulSoup\nimport socket\nimport time\nimport requests\n\n# encoding=utf8  \nimport sys  \n\nreload(sys)  \nsys.setdefaultencoding('utf8')\n\ndef download_pdf(link, location, name):\n    try:\n        response = requests.get(link)\n        with open(os.path.join(location, name), 'wb') as f:\n        \tf.write(response.content)\n        \tf.close()\n    except HTTPError:\n        print('>>> Error 404: cannot be downloaded!\\n') \n        raise   \n    except socket.timeout:\n        print(\" \".join((\"can't download\", link, \"due to connection timeout!\")) )\n        raise\n\ndef clean_pdf_link(link):\n    if 'arxiv' in link:\n        link = link.replace('abs', 'pdf')   \n        if not(link.endswith('.pdf')):\n            link = '.'.join((link, 'pdf'))\n\n    print(link)\n    return link\n\ndef clean_text(text, replacements = {':': '_', ' ': '_', '/': '_', '.': '', '\"': ''}):\n    for key, rep in replacements.items():\n        text = text.replace(key, rep)\n    return text    \n\ndef print_title(title, pattern = \"-\"):\n    print('\\n'.join((\"\", title, pattern * len(title)))) \n\ndef get_extension(link):\n    extension = os.path.splitext(link)[1][1:]\n    if extension in ['pdf', 'html']:\n        return extension\n    if 'pdf' in extension:\n        return 'pdf'    \n    return 'pdf'    \n\ndef shorten_title(title):\n    m1 = re.search('[[0-9]*]', title)\n    m2 = re.search('\".*\"', title)\n    if m1:\n        title = m1.group(0)\n    if m2:\n        title = ' '.join((title, m2.group(0)))   \n    return title[:50] + ' [...]'    \n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description = 'Download all the PDF/HTML links into README.md')\n    parser.add_argument('-d', action=\"store\", dest=\"directory\")\n    parser.add_argument('--no-html', action=\"store_true\", dest=\"nohtml\", default = False)\n    parser.add_argument('--overwrite', action=\"store_true\", default = False)    \n    results = parser.parse_args()\n\n    output_directory = 'pdfs' if results.directory is None else results.directory\n\n    forbidden_extensions = ['html', 'htm'] if results.nohtml else []\n\n    if results.overwrite and os.path.exists(output_directory):\n        shutil.rmtree(output_directory)\n\n    with open('README.md') as readme:\n        readme_html = mistune.markdown(readme.read())\n        readme_soup = BeautifulSoup.BeautifulSoup(readme_html, \"html.parser\")\n\n    point = readme_soup.find_all('h1')[1]\n\n    failures = []\n    while point is not None:\n        if point.name:\n            if re.search('h[1-2]', point.name):\n                if point.name == 'h1':\n                    h1_directory = os.path.join(output_directory, clean_text(point.text))\n                    current_directory = h1_directory\n                elif point.name == 'h2':\n                    current_directory = os.path.join(h1_directory, clean_text(point.text))  \n                if not os.path.exists(current_directory):\n                    os.makedirs(current_directory)\n                print_title(point.text)\n\n            if point.name == 'p':\n                link = point.find('a')\n                if link is not None:\n                    link = clean_pdf_link(link.attrs['href'])\n                    ext = get_extension(link)\n                    print(ext)\n                    if not ext in forbidden_extensions:\n                        print(shorten_title(point.text) + ' (' + link + ')')\n                        try:\n                            name = clean_text(point.text.split('[' + ext + ']')[0])\n                            fullname = '.'.join((name, ext))\n                            if not os.path.exists('/'.join((current_directory, fullname)) ):\n                                download_pdf(link, current_directory, '.'.join((name, ext)))\n                        except KeyboardInterrupt:\n                            try:\n                                print(\"Press Ctrl-C in 1 second to quit\")\n                                time.sleep(1)\n                            except KeyboardInterrupt:\n                                print(\"Cancelling..\")\n                                break\n                        except:\n                            failures.append(point.text)\n                        \n        point = point.next_sibling          \n\n    print('Done!')\n    if failures:\n        print('Some downloads have failed:')\n        for fail in failures:\n            print('> ' + fail)\n", "description": "Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!", "file_name": "download.py", "id": "0bee2c2d990d718da911720ac073e698", "language": "Python", "project_name": "Deep-Learning-Papers-Reading-Roadmap", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/songrotek-Deep-Learning-Papers-Reading-Roadmap/songrotek-Deep-Learning-Papers-Reading-Roadmap-be372fc/download.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:56:58Z", "url": "https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap", "wiki": true}