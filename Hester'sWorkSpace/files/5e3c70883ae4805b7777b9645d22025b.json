{"author": "chiphuyen", "code": "\"\"\" Solution for simple logistic regression model for MNIST\nwith tf.data module\nMNIST dataset: yann.lecun.com/exdb/mnist/\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nimport utils\n\n\nlearning_rate = 0.01\nbatch_size = 128\nn_epochs = 30\nn_train = 60000\nn_test = 10000\n\n\nmnist_folder = 'data/mnist'\nutils.download_mnist(mnist_folder)\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\n\n\ntrain_data = tf.data.Dataset.from_tensor_slices(train)\ntrain_data = train_data.shuffle(10000) \ntrain_data = train_data.batch(batch_size)\n\ntest_data = tf.data.Dataset.from_tensor_slices(test)\ntest_data = test_data.batch(batch_size)\n\niterator = tf.data.Iterator.from_structure(train_data.output_types, \n                                           train_data.output_shapes)\nimg, label = iterator.get_next()\n\ntrain_init = iterator.make_initializer(train_data)\t\ntest_init = iterator.make_initializer(test_data)\t\n\n\n\n\n# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n\nw = tf.get_variable(name='weights', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\nb = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())\n\n\n\n\nlogits = tf.matmul(img, w) + b \n\n\n\nentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label, name='entropy')\nloss = tf.reduce_mean(entropy, name='loss') \n\n\n\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n\npreds = tf.nn.softmax(logits)\ncorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\nwriter = tf.summary.FileWriter('./graphs/logreg', tf.get_default_graph())\nwith tf.Session() as sess:\n   \n    start_time = time.time()\n    sess.run(tf.global_variables_initializer())\n\n    \n    for i in range(n_epochs): \t\n        sess.run(train_init)\t\n        total_loss = 0\n        n_batches = 0\n        try:\n            while True:\n                _, l = sess.run([optimizer, loss])\n                total_loss += l\n                n_batches += 1\n        except tf.errors.OutOfRangeError:\n            pass\n        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n    print('Total time: {0} seconds'.format(time.time() - start_time))\n\n    \n    sess.run(test_init)\t\t\t\n    total_correct_preds = 0\n    try:\n        while True:\n            accuracy_batch = sess.run(accuracy)\n            total_correct_preds += accuracy_batch\n    except tf.errors.OutOfRangeError:\n        pass\n\n    print('Accuracy {0}'.format(total_correct_preds/n_test))\nwriter.close()\n", "comments": "    solution simple logistic regression model mnist tf data module mnist dataset  yann lecun com exdb mnist  created chip huyen (chiphuyen cs stanford edu) cs20   tensorflow deep learning research  cs20 stanford edu lecture 03        define paramaters model    step 1  read data    step 2  create datasets iterator    want shuffle data    initializer train data    initializer train data    step 3  create weights bias    w initialized random variables mean 0  stddev 0 01    b initialized 0    shape w depends dimension x y y   tf matmul(x  w)    shape b depends y    step 4  build model    model returns logits     logits later passed softmax layer    step 5  define loss function    use cross entropy softmax logits loss function    computes mean examples batch    step 6  define training op    using gradient descent learning rate 0 01 minimize loss    step 7  calculate accuracy test set    train model n epochs times    drawing samples train data    test model    drawing samples test data ", "content": "\"\"\" Solution for simple logistic regression model for MNIST\nwith tf.data module\nMNIST dataset: yann.lecun.com/exdb/mnist/\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nimport utils\n\n# Define paramaters for the model\nlearning_rate = 0.01\nbatch_size = 128\nn_epochs = 30\nn_train = 60000\nn_test = 10000\n\n# Step 1: Read in data\nmnist_folder = 'data/mnist'\nutils.download_mnist(mnist_folder)\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\n\n# Step 2: Create datasets and iterator\ntrain_data = tf.data.Dataset.from_tensor_slices(train)\ntrain_data = train_data.shuffle(10000) # if you want to shuffle your data\ntrain_data = train_data.batch(batch_size)\n\ntest_data = tf.data.Dataset.from_tensor_slices(test)\ntest_data = test_data.batch(batch_size)\n\niterator = tf.data.Iterator.from_structure(train_data.output_types, \n                                           train_data.output_shapes)\nimg, label = iterator.get_next()\n\ntrain_init = iterator.make_initializer(train_data)\t# initializer for train_data\ntest_init = iterator.make_initializer(test_data)\t# initializer for train_data\n\n# Step 3: create weights and bias\n# w is initialized to random variables with mean of 0, stddev of 0.01\n# b is initialized to 0\n# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n# shape of b depends on Y\nw = tf.get_variable(name='weights', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\nb = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())\n\n# Step 4: build model\n# the model that returns the logits.\n# this logits will be later passed through softmax layer\nlogits = tf.matmul(img, w) + b \n\n# Step 5: define loss function\n# use cross entropy of softmax of logits as the loss function\nentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label, name='entropy')\nloss = tf.reduce_mean(entropy, name='loss') # computes the mean over all the examples in the batch\n\n# Step 6: define training op\n# using gradient descent with learning rate of 0.01 to minimize loss\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n# Step 7: calculate accuracy with test set\npreds = tf.nn.softmax(logits)\ncorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\nwriter = tf.summary.FileWriter('./graphs/logreg', tf.get_default_graph())\nwith tf.Session() as sess:\n   \n    start_time = time.time()\n    sess.run(tf.global_variables_initializer())\n\n    # train the model n_epochs times\n    for i in range(n_epochs): \t\n        sess.run(train_init)\t# drawing samples from train_data\n        total_loss = 0\n        n_batches = 0\n        try:\n            while True:\n                _, l = sess.run([optimizer, loss])\n                total_loss += l\n                n_batches += 1\n        except tf.errors.OutOfRangeError:\n            pass\n        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n    print('Total time: {0} seconds'.format(time.time() - start_time))\n\n    # test the model\n    sess.run(test_init)\t\t\t# drawing samples from test_data\n    total_correct_preds = 0\n    try:\n        while True:\n            accuracy_batch = sess.run(accuracy)\n            total_correct_preds += accuracy_batch\n    except tf.errors.OutOfRangeError:\n        pass\n\n    print('Accuracy {0}'.format(total_correct_preds/n_test))\nwriter.close()\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "03_logreg.py", "id": "5e3c70883ae4805b7777b9645d22025b", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/03_logreg.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}