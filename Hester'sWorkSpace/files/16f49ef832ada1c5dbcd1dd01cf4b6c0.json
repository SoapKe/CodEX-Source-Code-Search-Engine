{"author": "spotify", "code": " -*- coding: utf-8 -*-\n\n Copyright 2012-2015 Spotify AB\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\nimport luigi\nfrom luigi.contrib.s3 import S3Target\nfrom luigi.contrib.spark import SparkSubmitTask, PySparkTask\n\n\nclass InlinePySparkWordCount(PySparkTask):\n    \"\"\"\n    This task runs a :py:class:`luigi.contrib.spark.PySparkTask` task\n    over the target data in :py:meth:`wordcount.input` (a file in S3) and\n    writes the result into its :py:meth:`wordcount.output` target (a file in S3).\n\n    This class uses :py:meth:`luigi.contrib.spark.PySparkTask.main`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: spark://spark.example.org:7077\n         py-packages: numpy, pandas\n\n    \"\"\"\n    driver_memory = '2g'\n    executor_memory = '3g'\n\n    def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")\n\n    def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')\n\n    def main(self, sc, *args):\n        sc.textFile(self.input().path) \\\n          .flatMap(lambda line: line.split()) \\\n          .map(lambda word: (word, 1)) \\\n          .reduceByKey(lambda a, b: a + b) \\\n          .saveAsTextFile(self.output().path)\n\n\nclass PySparkWordCount(SparkSubmitTask):\n    \"\"\"\n    This task is the same as :py:class:`InlinePySparkWordCount` above but uses\n    an external python driver file specified in :py:meth:`app`\n\n    It runs a :py:class:`luigi.contrib.spark.SparkSubmitTask` task\n    over the target data in :py:meth:`wordcount.input` (a file in S3) and\n    writes the result into its :py:meth:`wordcount.output` target (a file in S3).\n\n    This class uses :py:meth:`luigi.contrib.spark.SparkSubmitTask.run`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: spark://spark.example.org:7077\n        deploy-mode: client\n\n    \"\"\"\n    driver_memory = '2g'\n    executor_memory = '3g'\n    total_executor_cores = luigi.IntParameter(default=100, significant=False)\n\n    name = \"PySpark Word Count\"\n    app = 'wordcount.py'\n\n    def app_options(self):\n         These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]\n\n    def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")\n\n    def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')\n\n\n'''\n// Corresponding example Spark Job, running Word count with Spark's Python API\n// This file would have to be saved into wordcount.py\n\nimport sys\nfrom pyspark import SparkContext\n\nif __name__ == \"__main__\":\n\n    sc = SparkContext()\n    sc.textFile(sys.argv[1]) \\\n      .flatMap(lambda line: line.split()) \\\n      .map(lambda word: (word, 1)) \\\n      .reduceByKey(lambda a, b: a + b) \\\n      .saveAsTextFile(sys.argv[2])\n'''\n", "comments": "        this task runs  py class  luigi contrib spark pysparktask  task     target data  py meth  wordcount input  (a file s3)     writes result  py meth  wordcount output  target (a file s3)       this class uses  py meth  luigi contrib spark pysparktask main        example luigi configuration             spark          spark submit   usr local spark bin spark submit         master  spark   spark example org 7077           py packages  numpy  pandas              driver memory    2g      executor memory    3g       def input(self)          return s3target( s3n   bucket example org wordcount input )      def output(self)          return s3target( s3n   bucket example org wordcount output )      def main(self  sc   args)          sc textfile(self input() path)              flatmap(lambda line  line split())              map(lambda word  (word  1))              reducebykey(lambda  b    b)              saveastextfile(self output() path)   class pysparkwordcount(sparksubmittask)              this task  py class  inlinepysparkwordcount  uses     external python driver file specified  py meth  app       it runs  py class  luigi contrib spark sparksubmittask  task     target data  py meth  wordcount input  (a file s3)     writes result  py meth  wordcount output  target (a file s3)       this class uses  py meth  luigi contrib spark sparksubmittask run        example luigi configuration             spark          spark submit   usr local spark bin spark submit         master  spark   spark example org 7077         deploy mode  client                  corresponding example spark job  running word count spark python api    this file would saved wordcount py  import sys pyspark import sparkcontext    name         main          sc   sparkcontext()     sc textfile(sys argv 1 )          flatmap(lambda line  line split())          map(lambda word  (word  1))          reducebykey(lambda  b    b)          saveastextfile(sys argv 2 )            coding  utf 8           copyright 2012 2015 spotify ab       licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license       http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license        py packages  numpy  pandas    these passed spark main args defined order  ", "content": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport luigi\nfrom luigi.contrib.s3 import S3Target\nfrom luigi.contrib.spark import SparkSubmitTask, PySparkTask\n\n\nclass InlinePySparkWordCount(PySparkTask):\n    \"\"\"\n    This task runs a :py:class:`luigi.contrib.spark.PySparkTask` task\n    over the target data in :py:meth:`wordcount.input` (a file in S3) and\n    writes the result into its :py:meth:`wordcount.output` target (a file in S3).\n\n    This class uses :py:meth:`luigi.contrib.spark.PySparkTask.main`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: spark://spark.example.org:7077\n        # py-packages: numpy, pandas\n\n    \"\"\"\n    driver_memory = '2g'\n    executor_memory = '3g'\n\n    def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")\n\n    def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')\n\n    def main(self, sc, *args):\n        sc.textFile(self.input().path) \\\n          .flatMap(lambda line: line.split()) \\\n          .map(lambda word: (word, 1)) \\\n          .reduceByKey(lambda a, b: a + b) \\\n          .saveAsTextFile(self.output().path)\n\n\nclass PySparkWordCount(SparkSubmitTask):\n    \"\"\"\n    This task is the same as :py:class:`InlinePySparkWordCount` above but uses\n    an external python driver file specified in :py:meth:`app`\n\n    It runs a :py:class:`luigi.contrib.spark.SparkSubmitTask` task\n    over the target data in :py:meth:`wordcount.input` (a file in S3) and\n    writes the result into its :py:meth:`wordcount.output` target (a file in S3).\n\n    This class uses :py:meth:`luigi.contrib.spark.SparkSubmitTask.run`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: spark://spark.example.org:7077\n        deploy-mode: client\n\n    \"\"\"\n    driver_memory = '2g'\n    executor_memory = '3g'\n    total_executor_cores = luigi.IntParameter(default=100, significant=False)\n\n    name = \"PySpark Word Count\"\n    app = 'wordcount.py'\n\n    def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]\n\n    def input(self):\n        return S3Target(\"s3n://bucket.example.org/wordcount.input\")\n\n    def output(self):\n        return S3Target('s3n://bucket.example.org/wordcount.output')\n\n\n'''\n// Corresponding example Spark Job, running Word count with Spark's Python API\n// This file would have to be saved into wordcount.py\n\nimport sys\nfrom pyspark import SparkContext\n\nif __name__ == \"__main__\":\n\n    sc = SparkContext()\n    sc.textFile(sys.argv[1]) \\\n      .flatMap(lambda line: line.split()) \\\n      .map(lambda word: (word, 1)) \\\n      .reduceByKey(lambda a, b: a + b) \\\n      .saveAsTextFile(sys.argv[2])\n'''\n", "description": "Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. ", "file_name": "pyspark_wc.py", "id": "16f49ef832ada1c5dbcd1dd01cf4b6c0", "language": "Python", "project_name": "luigi", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/spotify-luigi/spotify-luigi-3cf763d/examples/pyspark_wc.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:09:25Z", "url": "https://github.com/spotify/luigi", "wiki": false}