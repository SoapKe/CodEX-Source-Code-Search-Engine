{"author": "mwaskom", "code": "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport nose.tools as nt\nimport numpy.testing as npt\ntry:\n    import pandas.testing as pdt\nexcept ImportError:\n    import pandas.util.testing as pdt\nfrom numpy.testing.decorators import skipif\nfrom nose import SkipTest\n\ntry:\n    import statsmodels.regression.linear_model as smlm\n    _no_statsmodels = False\nexcept ImportError:\n    _no_statsmodels = True\n\nfrom .. import regression as lm\nfrom ..palettes import color_palette\n\nrs = np.random.RandomState(0)\n\n\nclass TestLinearPlotter(object):\n\n    rs = np.random.RandomState(77)\n    df = pd.DataFrame(dict(x=rs.normal(size=60),\n                           d=rs.randint(-2, 3, 60),\n                           y=rs.gamma(4, size=60),\n                           s=np.tile(list(\"abcdefghij\"), 6)))\n    df[\"z\"] = df.y + rs.randn(60)\n    df[\"y_na\"] = df.y.copy()\n    df.loc[[10, 20, 30], 'y_na'] = np.nan\n\n    def test_establish_variables_from_frame(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y=\"y\")\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_establish_variables_from_series(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(None, x=self.df.x, y=self.df.y)\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        nt.assert_is(p.data, None)\n\n    def test_establish_variables_from_array(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(None,\n                              x=self.df.x.values,\n                              y=self.df.y.values)\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y)\n        nt.assert_is(p.data, None)\n\n    def test_establish_variables_from_mix(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y=self.df.y)\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_establish_variables_from_bad(self):\n\n        p = lm._LinearPlotter()\n        with nt.assert_raises(ValueError):\n            p.establish_variables(None, x=\"x\", y=self.df.y)\n\n    def test_dropna(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y_na=\"y_na\")\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y_na, self.df.y_na)\n\n        p.dropna(\"x\", \"y_na\")\n        mask = self.df.y_na.notnull()\n        pdt.assert_series_equal(p.x, self.df.x[mask])\n        pdt.assert_series_equal(p.y_na, self.df.y_na[mask])\n\n\nclass TestRegressionPlotter(object):\n\n    rs = np.random.RandomState(49)\n\n    grid = np.linspace(-3, 3, 30)\n    n_boot = 100\n    bins_numeric = 3\n    bins_given = [-1, 0, 1]\n\n    df = pd.DataFrame(dict(x=rs.normal(size=60),\n                           d=rs.randint(-2, 3, 60),\n                           y=rs.gamma(4, size=60),\n                           s=np.tile(list(range(6)), 10)))\n    df[\"z\"] = df.y + rs.randn(60)\n    df[\"y_na\"] = df.y.copy()\n\n    bw_err = rs.randn(6)[df.s.values] * 2\n    df.y += bw_err\n\n    p = 1 / (1 + np.exp(-(df.x * 2 + rs.randn(60))))\n    df[\"c\"] = [rs.binomial(1, p_i) for p_i in p]\n    df.loc[[10, 20, 30], 'y_na'] = np.nan\n\n    def test_variables_from_frame(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, units=\"s\")\n\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_series_equal(p.units, self.df.s)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_variables_from_series(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y, units=self.df.s)\n\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y)\n        npt.assert_array_equal(p.units, self.df.s)\n        nt.assert_is(p.data, None)\n\n    def test_variables_from_mix(self):\n\n        p = lm._RegressionPlotter(\"x\", self.df.y + 1, data=self.df)\n\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y + 1)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_dropna(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df)\n        nt.assert_equal(len(p.x), pd.notnull(self.df.y_na).sum())\n\n        p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n        nt.assert_equal(len(p.x), len(self.df.y_na))\n\n    def test_ci(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95)\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, 95)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95, x_ci=68)\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, 68)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95, x_ci=\"sd\")\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, \"sd\")\n\n    @skipif(_no_statsmodels)\n    def test_fast_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        \n        yhat_fast, _ = p.fit_fast(self.grid)\n\n        \n        yhat_smod, _ = p.fit_statsmodels(self.grid, smlm.OLS)\n\n        \n        npt.assert_array_almost_equal(yhat_fast, yhat_smod)\n\n    @skipif(_no_statsmodels)\n    def test_regress_poly(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        \n        yhat_poly, _ = p.fit_poly(self.grid, 1)\n\n        \n        yhat_smod, _ = p.fit_statsmodels(self.grid, smlm.OLS)\n\n        \n        npt.assert_array_almost_equal(yhat_poly, yhat_smod)\n\n    def test_regress_logx(self):\n\n        x = np.arange(1, 10)\n        y = np.arange(1, 10)\n        grid = np.linspace(1, 10, 100)\n        p = lm._RegressionPlotter(x, y, n_boot=self.n_boot)\n\n        yhat_lin, _ = p.fit_fast(grid)\n        yhat_log, _ = p.fit_logx(grid)\n\n        nt.assert_greater(yhat_lin[0], yhat_log[0])\n        nt.assert_greater(yhat_log[20], yhat_lin[20])\n        nt.assert_greater(yhat_lin[90], yhat_log[90])\n\n    @skipif(_no_statsmodels)\n    def test_regress_n_boot(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        # Fast (linear algebra) version\n        _, boots_fast = p.fit_fast(self.grid)\n        npt.assert_equal(boots_fast.shape, (self.n_boot, self.grid.size))\n\n        # Slower (np.polyfit) version\n        _, boots_poly = p.fit_poly(self.grid, 1)\n        npt.assert_equal(boots_poly.shape, (self.n_boot, self.grid.size))\n\n        # Slowest (statsmodels) version\n        _, boots_smod = p.fit_statsmodels(self.grid, smlm.OLS)\n        npt.assert_equal(boots_smod.shape, (self.n_boot, self.grid.size))\n\n    @skipif(_no_statsmodels)\n    def test_regress_without_bootstrap(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  n_boot=self.n_boot, ci=None)\n\n        # Fast (linear algebra) version\n        _, boots_fast = p.fit_fast(self.grid)\n        nt.assert_is(boots_fast, None)\n\n        # Slower (np.polyfit) version\n        _, boots_poly = p.fit_poly(self.grid, 1)\n        nt.assert_is(boots_poly, None)\n\n        # Slowest (statsmodels) version\n        _, boots_smod = p.fit_statsmodels(self.grid, smlm.OLS)\n        nt.assert_is(boots_smod, None)\n\n    def test_numeric_bins(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_numeric)\n        npt.assert_equal(len(bins), self.bins_numeric)\n        npt.assert_array_equal(np.unique(x_binned), bins)\n\n    def test_provided_bins(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_given)\n        npt.assert_array_equal(np.unique(x_binned), self.bins_given)\n\n    def test_bin_results(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_given)\n        nt.assert_greater(self.df.x[x_binned == 0].min(),\n                          self.df.x[x_binned == -1].max())\n        nt.assert_greater(self.df.x[x_binned == 1].min(),\n                          self.df.x[x_binned == 0].max())\n\n    def test_scatter_data(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.d)\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, x_jitter=.1)\n        x, y = p.scatter_data\n        nt.assert_true((x != self.df.d).any())\n        npt.assert_array_less(np.abs(self.df.d - x), np.repeat(.1, len(x)))\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, y_jitter=.05)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.d)\n        npt.assert_array_less(np.abs(self.df.y - y), np.repeat(.1, len(y)))\n\n    def test_estimate_data(self):\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, x_estimator=np.mean)\n\n        x, y, ci = p.estimate_data\n\n        npt.assert_array_equal(x, np.sort(np.unique(self.df.d)))\n        npt.assert_array_almost_equal(y, self.df.groupby(\"d\").y.mean())\n        npt.assert_array_less(np.array(ci)[:, 0], y)\n        npt.assert_array_less(y, np.array(ci)[:, 1])\n\n    def test_estimate_cis(self):\n\n        \n        np.random.seed(123)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=95)\n        _, _, ci_big = p.estimate_data\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=50)\n        _, _, ci_wee = p.estimate_data\n        npt.assert_array_less(np.diff(ci_wee), np.diff(ci_big))\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=None)\n        _, _, ci_nil = p.estimate_data\n        npt.assert_array_equal(ci_nil, [None] * len(ci_nil))\n\n    def test_estimate_units(self):\n\n        \n        np.random.seed(345)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  units=\"s\", x_bins=3)\n        _, _, ci_big = p.estimate_data\n        ci_big = np.diff(ci_big, axis=1)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, x_bins=3)\n        _, _, ci_wee = p.estimate_data\n        ci_wee = np.diff(ci_wee, axis=1)\n\n        npt.assert_array_less(ci_wee, ci_big)\n\n    def test_partial(self):\n\n        x = self.rs.randn(100)\n        y = x + self.rs.randn(100)\n        z = x + self.rs.randn(100)\n\n        p = lm._RegressionPlotter(y, z)\n        _, r_orig = np.corrcoef(p.x, p.y)[0]\n\n        p = lm._RegressionPlotter(y, z, y_partial=x)\n        _, r_semipartial = np.corrcoef(p.x, p.y)[0]\n        nt.assert_less(r_semipartial, r_orig)\n\n        p = lm._RegressionPlotter(y, z, x_partial=x, y_partial=x)\n        _, r_partial = np.corrcoef(p.x, p.y)[0]\n        nt.assert_less(r_partial, r_orig)\n\n    @skipif(_no_statsmodels)\n    def test_logistic_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"c\", data=self.df,\n                                  logistic=True, n_boot=self.n_boot)\n        _, yhat, _ = p.fit_regression(x_range=(-3, 3))\n        npt.assert_array_less(yhat, 1)\n        npt.assert_array_less(0, yhat)\n\n    @skipif(_no_statsmodels)\n    def test_logistic_perfect_separation(self):\n\n        y = self.df.x > self.df.x.mean()\n        p = lm._RegressionPlotter(\"x\", y, data=self.df,\n                                  logistic=True, n_boot=10)\n        with np.errstate(all=\"ignore\"):\n            _, yhat, _ = p.fit_regression(x_range=(-3, 3))\n        nt.assert_true(np.isnan(yhat).all())\n\n    @skipif(_no_statsmodels)\n    def test_robust_regression(self):\n\n        p_ols = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                      n_boot=self.n_boot)\n        _, ols_yhat, _ = p_ols.fit_regression(x_range=(-3, 3))\n\n        p_robust = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                         robust=True, n_boot=self.n_boot)\n        _, robust_yhat, _ = p_robust.fit_regression(x_range=(-3, 3))\n\n        nt.assert_equal(len(ols_yhat), len(robust_yhat))\n\n    @skipif(_no_statsmodels)\n    def test_lowess_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, lowess=True)\n        grid, yhat, err_bands = p.fit_regression(x_range=(-3, 3))\n\n        nt.assert_equal(len(grid), len(yhat))\n        nt.assert_is(err_bands, None)\n\n    def test_regression_options(self):\n\n        with nt.assert_raises(ValueError):\n            lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  lowess=True, order=2)\n\n        with nt.assert_raises(ValueError):\n            lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  lowess=True, logistic=True)\n\n    def test_regression_limits(self):\n\n        f, ax = plt.subplots()\n        ax.scatter(self.df.x, self.df.y)\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df)\n        grid, _, _ = p.fit_regression(ax)\n        xlim = ax.get_xlim()\n        nt.assert_equal(grid.min(), xlim[0])\n        nt.assert_equal(grid.max(), xlim[1])\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, truncate=True)\n        grid, _, _ = p.fit_regression()\n        nt.assert_equal(grid.min(), self.df.x.min())\n        nt.assert_equal(grid.max(), self.df.x.max())\n\n\nclass TestRegressionPlots(object):\n\n    rs = np.random.RandomState(56)\n    df = pd.DataFrame(dict(x=rs.randn(90),\n                           y=rs.randn(90) + 5,\n                           z=rs.randint(0, 1, 90),\n                           g=np.repeat(list(\"abc\"), 30),\n                           h=np.tile(list(\"xy\"), 45),\n                           u=np.tile(np.arange(6), 15)))\n    bw_err = rs.randn(6)[df.u.values]\n    df.y += bw_err\n\n    def test_regplot_basic(self):\n\n        f, ax = plt.subplots()\n        lm.regplot(\"x\", \"y\", self.df)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 2)\n\n        x, y = ax.collections[0].get_offsets().T\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n    def test_regplot_selective(self):\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter=False, ax=ax)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, fit_reg=False)\n        nt.assert_equal(len(ax.lines), 0)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, ci=None)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n    def test_regplot_scatter_kws_alpha(self):\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_is(ax.collections[0]._alpha, None)\n        nt.assert_equal(ax.collections[0]._facecolors[0, 3], 0.5)\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_equal(ax.collections[0]._alpha, 0.8)\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color,\n                                                        'alpha': 0.4})\n        nt.assert_equal(ax.collections[0]._alpha, 0.4)\n\n        f, ax = plt.subplots()\n        color = 'r'\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_equal(ax.collections[0]._alpha, 0.8)\n\n    def test_regplot_binned(self):\n\n        ax = lm.regplot(\"x\", \"y\", self.df, x_bins=5)\n        nt.assert_equal(len(ax.lines), 6)\n        nt.assert_equal(len(ax.collections), 2)\n\n    def test_lmplot_basic(self):\n\n        g = lm.lmplot(\"x\", \"y\", self.df)\n        ax = g.axes[0, 0]\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 2)\n\n        x, y = ax.collections[0].get_offsets().T\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n    def test_lmplot_hue(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\")\n        ax = g.axes[0, 0]\n\n        nt.assert_equal(len(ax.lines), 2)\n        nt.assert_equal(len(ax.collections), 4)\n\n    def test_lmplot_markers(self):\n\n        g1 = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=\"s\")\n        nt.assert_equal(g1.hue_kws, {\"marker\": [\"s\", \"s\"]})\n\n        g2 = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=[\"o\", \"s\"])\n        nt.assert_equal(g2.hue_kws, {\"marker\": [\"o\", \"s\"]})\n\n        with nt.assert_raises(ValueError):\n            lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=[\"o\", \"s\", \"d\"])\n\n    def test_lmplot_marker_linewidths(self):\n\n        if mpl.__version__ == \"1.4.2\":\n            raise SkipTest\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\",\n                      fit_reg=False, markers=[\"o\", \"+\"])\n        c = g.axes[0, 0].collections\n        nt.assert_equal(c[0].get_linewidths()[0], 0)\n        rclw = mpl.rcParams[\"lines.linewidth\"]\n        nt.assert_equal(c[1].get_linewidths()[0], rclw)\n\n    def test_lmplot_facets(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, row=\"g\", col=\"h\")\n        nt.assert_equal(g.axes.shape, (3, 2))\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, col=\"u\", col_wrap=4)\n        nt.assert_equal(g.axes.shape, (6,))\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", col=\"u\")\n        nt.assert_equal(g.axes.shape, (1, 6))\n\n    def test_lmplot_hue_col_nolegend(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, col=\"h\", hue=\"h\")\n        nt.assert_is(g._legend, None)\n\n    def test_lmplot_scatter_kws(self):\n\n        g = lm.lmplot(\"x\", \"y\", hue=\"h\", data=self.df, ci=None)\n        red_scatter, blue_scatter = g.axes[0, 0].collections\n\n        red, blue = color_palette(n_colors=2)\n        npt.assert_array_equal(red, red_scatter.get_facecolors()[0, :3])\n        npt.assert_array_equal(blue, blue_scatter.get_facecolors()[0, :3])\n\n    def test_residplot(self):\n\n        x, y = self.df.x, self.df.y\n        ax = lm.residplot(x, y)\n\n        resid = y - np.polyval(np.polyfit(x, y, 1), x)\n        x_plot, y_plot = ax.collections[0].get_offsets().T\n\n        npt.assert_array_equal(x, x_plot)\n        npt.assert_array_almost_equal(resid, y_plot)\n\n    @skipif(_no_statsmodels)\n    def test_residplot_lowess(self):\n\n        ax = lm.residplot(\"x\", \"y\", self.df, lowess=True)\n        nt.assert_equal(len(ax.lines), 2)\n\n        x, y = ax.lines[1].get_xydata().T\n        npt.assert_array_equal(x, np.sort(self.df.x))\n\n    def test_three_point_colors(self):\n\n        x, y = np.random.randn(2, 3)\n        ax = lm.regplot(x, y, color=(1, 0, 0))\n        color = ax.collections[0].get_facecolors()\n        npt.assert_almost_equal(color[0, :3],\n                                (1, 0, 0))\n", "comments": "  fit  fast  function  linear algebra    fit using statsmodels function ols model    compare vector hat values    fit first order polynomial    fit using statsmodels function ols model    compare vector hat values    fast (linear algebra) version    slower (np polyfit) version    slowest (statsmodels) version    fast (linear algebra) version    slower (np polyfit) version    slowest (statsmodels) version    set known good seed avoid test stochastically failing    seed rng locally ", "content": "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport nose.tools as nt\nimport numpy.testing as npt\ntry:\n    import pandas.testing as pdt\nexcept ImportError:\n    import pandas.util.testing as pdt\nfrom numpy.testing.decorators import skipif\nfrom nose import SkipTest\n\ntry:\n    import statsmodels.regression.linear_model as smlm\n    _no_statsmodels = False\nexcept ImportError:\n    _no_statsmodels = True\n\nfrom .. import regression as lm\nfrom ..palettes import color_palette\n\nrs = np.random.RandomState(0)\n\n\nclass TestLinearPlotter(object):\n\n    rs = np.random.RandomState(77)\n    df = pd.DataFrame(dict(x=rs.normal(size=60),\n                           d=rs.randint(-2, 3, 60),\n                           y=rs.gamma(4, size=60),\n                           s=np.tile(list(\"abcdefghij\"), 6)))\n    df[\"z\"] = df.y + rs.randn(60)\n    df[\"y_na\"] = df.y.copy()\n    df.loc[[10, 20, 30], 'y_na'] = np.nan\n\n    def test_establish_variables_from_frame(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y=\"y\")\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_establish_variables_from_series(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(None, x=self.df.x, y=self.df.y)\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        nt.assert_is(p.data, None)\n\n    def test_establish_variables_from_array(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(None,\n                              x=self.df.x.values,\n                              y=self.df.y.values)\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y)\n        nt.assert_is(p.data, None)\n\n    def test_establish_variables_from_mix(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y=self.df.y)\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_establish_variables_from_bad(self):\n\n        p = lm._LinearPlotter()\n        with nt.assert_raises(ValueError):\n            p.establish_variables(None, x=\"x\", y=self.df.y)\n\n    def test_dropna(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(self.df, x=\"x\", y_na=\"y_na\")\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y_na, self.df.y_na)\n\n        p.dropna(\"x\", \"y_na\")\n        mask = self.df.y_na.notnull()\n        pdt.assert_series_equal(p.x, self.df.x[mask])\n        pdt.assert_series_equal(p.y_na, self.df.y_na[mask])\n\n\nclass TestRegressionPlotter(object):\n\n    rs = np.random.RandomState(49)\n\n    grid = np.linspace(-3, 3, 30)\n    n_boot = 100\n    bins_numeric = 3\n    bins_given = [-1, 0, 1]\n\n    df = pd.DataFrame(dict(x=rs.normal(size=60),\n                           d=rs.randint(-2, 3, 60),\n                           y=rs.gamma(4, size=60),\n                           s=np.tile(list(range(6)), 10)))\n    df[\"z\"] = df.y + rs.randn(60)\n    df[\"y_na\"] = df.y.copy()\n\n    bw_err = rs.randn(6)[df.s.values] * 2\n    df.y += bw_err\n\n    p = 1 / (1 + np.exp(-(df.x * 2 + rs.randn(60))))\n    df[\"c\"] = [rs.binomial(1, p_i) for p_i in p]\n    df.loc[[10, 20, 30], 'y_na'] = np.nan\n\n    def test_variables_from_frame(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, units=\"s\")\n\n        pdt.assert_series_equal(p.x, self.df.x)\n        pdt.assert_series_equal(p.y, self.df.y)\n        pdt.assert_series_equal(p.units, self.df.s)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_variables_from_series(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y, units=self.df.s)\n\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y)\n        npt.assert_array_equal(p.units, self.df.s)\n        nt.assert_is(p.data, None)\n\n    def test_variables_from_mix(self):\n\n        p = lm._RegressionPlotter(\"x\", self.df.y + 1, data=self.df)\n\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y + 1)\n        pdt.assert_frame_equal(p.data, self.df)\n\n    def test_dropna(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df)\n        nt.assert_equal(len(p.x), pd.notnull(self.df.y_na).sum())\n\n        p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n        nt.assert_equal(len(p.x), len(self.df.y_na))\n\n    def test_ci(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95)\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, 95)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95, x_ci=68)\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, 68)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, ci=95, x_ci=\"sd\")\n        nt.assert_equal(p.ci, 95)\n        nt.assert_equal(p.x_ci, \"sd\")\n\n    @skipif(_no_statsmodels)\n    def test_fast_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        # Fit with the \"fast\" function, which just does linear algebra\n        yhat_fast, _ = p.fit_fast(self.grid)\n\n        # Fit using the statsmodels function with an OLS model\n        yhat_smod, _ = p.fit_statsmodels(self.grid, smlm.OLS)\n\n        # Compare the vector of y_hat values\n        npt.assert_array_almost_equal(yhat_fast, yhat_smod)\n\n    @skipif(_no_statsmodels)\n    def test_regress_poly(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        # Fit an first-order polynomial\n        yhat_poly, _ = p.fit_poly(self.grid, 1)\n\n        # Fit using the statsmodels function with an OLS model\n        yhat_smod, _ = p.fit_statsmodels(self.grid, smlm.OLS)\n\n        # Compare the vector of y_hat values\n        npt.assert_array_almost_equal(yhat_poly, yhat_smod)\n\n    def test_regress_logx(self):\n\n        x = np.arange(1, 10)\n        y = np.arange(1, 10)\n        grid = np.linspace(1, 10, 100)\n        p = lm._RegressionPlotter(x, y, n_boot=self.n_boot)\n\n        yhat_lin, _ = p.fit_fast(grid)\n        yhat_log, _ = p.fit_logx(grid)\n\n        nt.assert_greater(yhat_lin[0], yhat_log[0])\n        nt.assert_greater(yhat_log[20], yhat_lin[20])\n        nt.assert_greater(yhat_lin[90], yhat_log[90])\n\n    @skipif(_no_statsmodels)\n    def test_regress_n_boot(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, n_boot=self.n_boot)\n\n        # Fast (linear algebra) version\n        _, boots_fast = p.fit_fast(self.grid)\n        npt.assert_equal(boots_fast.shape, (self.n_boot, self.grid.size))\n\n        # Slower (np.polyfit) version\n        _, boots_poly = p.fit_poly(self.grid, 1)\n        npt.assert_equal(boots_poly.shape, (self.n_boot, self.grid.size))\n\n        # Slowest (statsmodels) version\n        _, boots_smod = p.fit_statsmodels(self.grid, smlm.OLS)\n        npt.assert_equal(boots_smod.shape, (self.n_boot, self.grid.size))\n\n    @skipif(_no_statsmodels)\n    def test_regress_without_bootstrap(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  n_boot=self.n_boot, ci=None)\n\n        # Fast (linear algebra) version\n        _, boots_fast = p.fit_fast(self.grid)\n        nt.assert_is(boots_fast, None)\n\n        # Slower (np.polyfit) version\n        _, boots_poly = p.fit_poly(self.grid, 1)\n        nt.assert_is(boots_poly, None)\n\n        # Slowest (statsmodels) version\n        _, boots_smod = p.fit_statsmodels(self.grid, smlm.OLS)\n        nt.assert_is(boots_smod, None)\n\n    def test_numeric_bins(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_numeric)\n        npt.assert_equal(len(bins), self.bins_numeric)\n        npt.assert_array_equal(np.unique(x_binned), bins)\n\n    def test_provided_bins(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_given)\n        npt.assert_array_equal(np.unique(x_binned), self.bins_given)\n\n    def test_bin_results(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x_binned, bins = p.bin_predictor(self.bins_given)\n        nt.assert_greater(self.df.x[x_binned == 0].min(),\n                          self.df.x[x_binned == -1].max())\n        nt.assert_greater(self.df.x[x_binned == 1].min(),\n                          self.df.x[x_binned == 0].max())\n\n    def test_scatter_data(self):\n\n        p = lm._RegressionPlotter(self.df.x, self.df.y)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.d)\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, x_jitter=.1)\n        x, y = p.scatter_data\n        nt.assert_true((x != self.df.d).any())\n        npt.assert_array_less(np.abs(self.df.d - x), np.repeat(.1, len(x)))\n        npt.assert_array_equal(y, self.df.y)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, y_jitter=.05)\n        x, y = p.scatter_data\n        npt.assert_array_equal(x, self.df.d)\n        npt.assert_array_less(np.abs(self.df.y - y), np.repeat(.1, len(y)))\n\n    def test_estimate_data(self):\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y, x_estimator=np.mean)\n\n        x, y, ci = p.estimate_data\n\n        npt.assert_array_equal(x, np.sort(np.unique(self.df.d)))\n        npt.assert_array_almost_equal(y, self.df.groupby(\"d\").y.mean())\n        npt.assert_array_less(np.array(ci)[:, 0], y)\n        npt.assert_array_less(y, np.array(ci)[:, 1])\n\n    def test_estimate_cis(self):\n\n        # set known good seed to avoid the test stochastically failing\n        np.random.seed(123)\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=95)\n        _, _, ci_big = p.estimate_data\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=50)\n        _, _, ci_wee = p.estimate_data\n        npt.assert_array_less(np.diff(ci_wee), np.diff(ci_big))\n\n        p = lm._RegressionPlotter(self.df.d, self.df.y,\n                                  x_estimator=np.mean, ci=None)\n        _, _, ci_nil = p.estimate_data\n        npt.assert_array_equal(ci_nil, [None] * len(ci_nil))\n\n    def test_estimate_units(self):\n\n        # Seed the RNG locally\n        np.random.seed(345)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  units=\"s\", x_bins=3)\n        _, _, ci_big = p.estimate_data\n        ci_big = np.diff(ci_big, axis=1)\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, x_bins=3)\n        _, _, ci_wee = p.estimate_data\n        ci_wee = np.diff(ci_wee, axis=1)\n\n        npt.assert_array_less(ci_wee, ci_big)\n\n    def test_partial(self):\n\n        x = self.rs.randn(100)\n        y = x + self.rs.randn(100)\n        z = x + self.rs.randn(100)\n\n        p = lm._RegressionPlotter(y, z)\n        _, r_orig = np.corrcoef(p.x, p.y)[0]\n\n        p = lm._RegressionPlotter(y, z, y_partial=x)\n        _, r_semipartial = np.corrcoef(p.x, p.y)[0]\n        nt.assert_less(r_semipartial, r_orig)\n\n        p = lm._RegressionPlotter(y, z, x_partial=x, y_partial=x)\n        _, r_partial = np.corrcoef(p.x, p.y)[0]\n        nt.assert_less(r_partial, r_orig)\n\n    @skipif(_no_statsmodels)\n    def test_logistic_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"c\", data=self.df,\n                                  logistic=True, n_boot=self.n_boot)\n        _, yhat, _ = p.fit_regression(x_range=(-3, 3))\n        npt.assert_array_less(yhat, 1)\n        npt.assert_array_less(0, yhat)\n\n    @skipif(_no_statsmodels)\n    def test_logistic_perfect_separation(self):\n\n        y = self.df.x > self.df.x.mean()\n        p = lm._RegressionPlotter(\"x\", y, data=self.df,\n                                  logistic=True, n_boot=10)\n        with np.errstate(all=\"ignore\"):\n            _, yhat, _ = p.fit_regression(x_range=(-3, 3))\n        nt.assert_true(np.isnan(yhat).all())\n\n    @skipif(_no_statsmodels)\n    def test_robust_regression(self):\n\n        p_ols = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                      n_boot=self.n_boot)\n        _, ols_yhat, _ = p_ols.fit_regression(x_range=(-3, 3))\n\n        p_robust = lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                         robust=True, n_boot=self.n_boot)\n        _, robust_yhat, _ = p_robust.fit_regression(x_range=(-3, 3))\n\n        nt.assert_equal(len(ols_yhat), len(robust_yhat))\n\n    @skipif(_no_statsmodels)\n    def test_lowess_regression(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, lowess=True)\n        grid, yhat, err_bands = p.fit_regression(x_range=(-3, 3))\n\n        nt.assert_equal(len(grid), len(yhat))\n        nt.assert_is(err_bands, None)\n\n    def test_regression_options(self):\n\n        with nt.assert_raises(ValueError):\n            lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  lowess=True, order=2)\n\n        with nt.assert_raises(ValueError):\n            lm._RegressionPlotter(\"x\", \"y\", data=self.df,\n                                  lowess=True, logistic=True)\n\n    def test_regression_limits(self):\n\n        f, ax = plt.subplots()\n        ax.scatter(self.df.x, self.df.y)\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df)\n        grid, _, _ = p.fit_regression(ax)\n        xlim = ax.get_xlim()\n        nt.assert_equal(grid.min(), xlim[0])\n        nt.assert_equal(grid.max(), xlim[1])\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, truncate=True)\n        grid, _, _ = p.fit_regression()\n        nt.assert_equal(grid.min(), self.df.x.min())\n        nt.assert_equal(grid.max(), self.df.x.max())\n\n\nclass TestRegressionPlots(object):\n\n    rs = np.random.RandomState(56)\n    df = pd.DataFrame(dict(x=rs.randn(90),\n                           y=rs.randn(90) + 5,\n                           z=rs.randint(0, 1, 90),\n                           g=np.repeat(list(\"abc\"), 30),\n                           h=np.tile(list(\"xy\"), 45),\n                           u=np.tile(np.arange(6), 15)))\n    bw_err = rs.randn(6)[df.u.values]\n    df.y += bw_err\n\n    def test_regplot_basic(self):\n\n        f, ax = plt.subplots()\n        lm.regplot(\"x\", \"y\", self.df)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 2)\n\n        x, y = ax.collections[0].get_offsets().T\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n    def test_regplot_selective(self):\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter=False, ax=ax)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, fit_reg=False)\n        nt.assert_equal(len(ax.lines), 0)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n        f, ax = plt.subplots()\n        ax = lm.regplot(\"x\", \"y\", self.df, ci=None)\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 1)\n        ax.clear()\n\n    def test_regplot_scatter_kws_alpha(self):\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_is(ax.collections[0]._alpha, None)\n        nt.assert_equal(ax.collections[0]._facecolors[0, 3], 0.5)\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_equal(ax.collections[0]._alpha, 0.8)\n\n        f, ax = plt.subplots()\n        color = np.array([[0.3, 0.8, 0.5]])\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color,\n                                                        'alpha': 0.4})\n        nt.assert_equal(ax.collections[0]._alpha, 0.4)\n\n        f, ax = plt.subplots()\n        color = 'r'\n        ax = lm.regplot(\"x\", \"y\", self.df, scatter_kws={'color': color})\n        nt.assert_equal(ax.collections[0]._alpha, 0.8)\n\n    def test_regplot_binned(self):\n\n        ax = lm.regplot(\"x\", \"y\", self.df, x_bins=5)\n        nt.assert_equal(len(ax.lines), 6)\n        nt.assert_equal(len(ax.collections), 2)\n\n    def test_lmplot_basic(self):\n\n        g = lm.lmplot(\"x\", \"y\", self.df)\n        ax = g.axes[0, 0]\n        nt.assert_equal(len(ax.lines), 1)\n        nt.assert_equal(len(ax.collections), 2)\n\n        x, y = ax.collections[0].get_offsets().T\n        npt.assert_array_equal(x, self.df.x)\n        npt.assert_array_equal(y, self.df.y)\n\n    def test_lmplot_hue(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\")\n        ax = g.axes[0, 0]\n\n        nt.assert_equal(len(ax.lines), 2)\n        nt.assert_equal(len(ax.collections), 4)\n\n    def test_lmplot_markers(self):\n\n        g1 = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=\"s\")\n        nt.assert_equal(g1.hue_kws, {\"marker\": [\"s\", \"s\"]})\n\n        g2 = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=[\"o\", \"s\"])\n        nt.assert_equal(g2.hue_kws, {\"marker\": [\"o\", \"s\"]})\n\n        with nt.assert_raises(ValueError):\n            lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", markers=[\"o\", \"s\", \"d\"])\n\n    def test_lmplot_marker_linewidths(self):\n\n        if mpl.__version__ == \"1.4.2\":\n            raise SkipTest\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\",\n                      fit_reg=False, markers=[\"o\", \"+\"])\n        c = g.axes[0, 0].collections\n        nt.assert_equal(c[0].get_linewidths()[0], 0)\n        rclw = mpl.rcParams[\"lines.linewidth\"]\n        nt.assert_equal(c[1].get_linewidths()[0], rclw)\n\n    def test_lmplot_facets(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, row=\"g\", col=\"h\")\n        nt.assert_equal(g.axes.shape, (3, 2))\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, col=\"u\", col_wrap=4)\n        nt.assert_equal(g.axes.shape, (6,))\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, hue=\"h\", col=\"u\")\n        nt.assert_equal(g.axes.shape, (1, 6))\n\n    def test_lmplot_hue_col_nolegend(self):\n\n        g = lm.lmplot(\"x\", \"y\", data=self.df, col=\"h\", hue=\"h\")\n        nt.assert_is(g._legend, None)\n\n    def test_lmplot_scatter_kws(self):\n\n        g = lm.lmplot(\"x\", \"y\", hue=\"h\", data=self.df, ci=None)\n        red_scatter, blue_scatter = g.axes[0, 0].collections\n\n        red, blue = color_palette(n_colors=2)\n        npt.assert_array_equal(red, red_scatter.get_facecolors()[0, :3])\n        npt.assert_array_equal(blue, blue_scatter.get_facecolors()[0, :3])\n\n    def test_residplot(self):\n\n        x, y = self.df.x, self.df.y\n        ax = lm.residplot(x, y)\n\n        resid = y - np.polyval(np.polyfit(x, y, 1), x)\n        x_plot, y_plot = ax.collections[0].get_offsets().T\n\n        npt.assert_array_equal(x, x_plot)\n        npt.assert_array_almost_equal(resid, y_plot)\n\n    @skipif(_no_statsmodels)\n    def test_residplot_lowess(self):\n\n        ax = lm.residplot(\"x\", \"y\", self.df, lowess=True)\n        nt.assert_equal(len(ax.lines), 2)\n\n        x, y = ax.lines[1].get_xydata().T\n        npt.assert_array_equal(x, np.sort(self.df.x))\n\n    def test_three_point_colors(self):\n\n        x, y = np.random.randn(2, 3)\n        ax = lm.regplot(x, y, color=(1, 0, 0))\n        color = ax.collections[0].get_facecolors()\n        npt.assert_almost_equal(color[0, :3],\n                                (1, 0, 0))\n", "description": "Statistical data visualization using matplotlib", "file_name": "test_regression.py", "id": "6a78f4eb7e67705fbe21cf4ff0540a16", "language": "Python", "project_name": "seaborn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/mwaskom-seaborn/mwaskom-seaborn-71a807b/seaborn/tests/test_regression.py", "save_time": "", "source": "", "update_at": "2018-03-18T02:14:24Z", "url": "https://github.com/mwaskom/seaborn", "wiki": false}