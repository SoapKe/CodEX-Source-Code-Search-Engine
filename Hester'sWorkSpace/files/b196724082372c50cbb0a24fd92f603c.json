{"author": "openai", "code": "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom gym.spaces import prng\n\nfrom scipy.stats import genpareto\nimport numpy as np\nimport numpy.random\n\ndef flip(edge, np_random):\n    return np_random.uniform() < edge\n\nclass KellyCoinflipEnv(gym.Env):\n    \"\"\"The Kelly coinflip game is a simple gambling introduced by Haghani & Dewey 2016's 'Rational Decision-Making Under Uncertainty: Observed Betting Patterns on a Biased Coin' (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2856963), to test human decision-making in a setting like that of the stock market: positive expected value but highly stochastic; they found many subjects performed badly, often going broke, even though optimal play would reach the maximum with ~95% probability. In the coinflip game, the player starts with $25.00 to gamble over 300 rounds; each round, they can bet anywhere up to their net worth (in penny increments), and then a coin is flipped; with P=0.6, the player wins twice what they bet, otherwise, they lose it. $250 is the maximum players are allowed to have. At the end of the 300 rounds, they keep whatever they have. The human subjects earned an average of $91; a simple use of the Kelly criterion (https://en.wikipedia.org/wiki/Kelly_criterion), giving a strategy of betting 20% until the cap is hit, would earn $240; a decision tree analysis shows that optimal play earns $246 (https://www.gwern.net/Coin-flip). The game short-circuits when either wealth = $0 (since one can never recover) or wealth = cap (trivial optimal play: one simply bets nothing thereafter). In this implementation, we default to the paper settings of $25, 60% odds, wealth cap of $250, and 300 rounds. To specify the action space in advance, we multiply the wealth cap (in dollars) by 100 (to allow for all penny bets); should one attempt to bet more money than one has, it is rounded down to one's net worth. (Alternately, a mistaken bet could end the episode immediately; it's not clear to me which version would be better.) For a harder version which randomizes the 3 key parameters, see the Generalized Kelly coinflip game.\"\"\"\n    metadata = {'render.modes': ['human']}\n    def __init__(self, initialWealth=25.0, edge=0.6, maxWealth=250.0, maxRounds=300):\n\n        self.action_space = spaces.Discrete(int(maxWealth*100)) \n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, maxWealth, [1]), # (w,b)\n            spaces.Discrete(maxRounds+1)))\n        self.reward_range = (0, maxWealth)\n        self.edge = edge\n        self.wealth = initialWealth\n        self.initialWealth = initialWealth\n        self.maxRounds = maxRounds\n        self.maxWealth = maxWealth\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        action = action/100.0 \n        if action > self.wealth: \n          action = self.wealth\n        if self.wealth < 0.000001:\n            done = True\n            reward = 0.0\n        else:\n          if self.rounds == 0:\n            done = True\n            reward = self.wealth\n          else:\n            self.rounds = self.rounds - 1\n            done = False\n            reward = 0.0\n            coinflip = flip(self.edge, self.np_random)\n            if coinflip:\n              self.wealth = min(self.maxWealth, self.wealth + action)\n            else:\n              self.wealth = self.wealth - action\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (np.array([self.wealth]), self.rounds)\n\n    def reset(self):\n        self.rounds = self.maxRounds\n        self.wealth = self.initialWealth\n        return self._get_obs()\n\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds)\n\nclass KellyCoinflipGeneralizedEnv(gym.Env):\n    \"\"\"The Generalized Kelly coinflip game is an extension by ArthurB & Gwern Branwen which expands the Kelly coinflip game MDP into a POMDP, where the 3 key parameters (edge, maximum wealth, and number of rounds) are unknown random variables drawn from 3 distributions: a Beta(7,3) for the coinflip edge 0-1, a N(300,25) the total number of rounds, and a Pareto(5,200) for the wealth cap. These distributions are chosen to be conjugate & easily updatable, to allow for inference (other choices like the geometric for number of rounds wouldn't make observations informative), and to loosely reflect what a human might expect in the original Kelly coinflip game given that the number of rounds wasn't strictly fixed and they weren't told the wealth cap until they neared it. With these particular distributions, the entire history of the game can be summarized into a few sufficient statistics of rounds-elapsed/wins/losses/max-wealth-ever-reached, from which the Bayes-optimal decision can (in theory) be made; to avoid all agents having to tediously track those sufficient statistics manually in the same way, the observation space is augmented from wealth/rounds-left (rounds-left is deleted because it is a hidden variable) to current-wealth/rounds-elapsed/wins/losses/maximum-observed-wealth. The simple Kelly coinflip game can easily be solved by calculating decision trees, but the Generalized Kelly coinflip game may be intractable (although the analysis for the edge case alone suggests that the Bayes-optimal value may be very close to what one would calculate using a decision tree for any specific case), and represents a good challenge for RL agents.\"\"\"\n    metadata = {'render.modes': ['human']}\n    def __init__(self, initialWealth=25.0, edgePriorAlpha=7, edgePriorBeta=3, maxWealthAlpha=5.0, maxWealthM=200.0, maxRoundsMean=300.0, maxRoundsSD=25.0, reseed=True):\n        # store the hyperparameters for passing back into __init__() during resets so the same hyperparameters govern the next game's parameters, as the user expects: TODO: this is boilerplate, is there any more elegant way to do this?\n        self.initialWealth=float(initialWealth)\n        self.edgePriorAlpha=edgePriorAlpha\n        self.edgePriorBeta=edgePriorBeta\n        self.maxWealthAlpha=maxWealthAlpha\n        self.maxWealthM=maxWealthM\n        self.maxRoundsMean=maxRoundsMean\n        self.maxRoundsSD=maxRoundsSD\n\n        \n        edge = prng.np_random.beta(edgePriorAlpha, edgePriorBeta)\n        maxWealth = round(genpareto.rvs(maxWealthAlpha, maxWealthM, random_state=prng.np_random))\n        maxRounds = int(round(prng.np_random.normal(maxRoundsMean, maxRoundsSD)))\n\n        \n        \n        self.maxEverWealth = float(self.initialWealth)\n        \n        self.wins = 0\n        self.losses = 0\n        \n        self.roundsElapsed = 0\n\n        \n        self.action_space = spaces.Discrete(int(maxWealth*100))\n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, maxWealth, shape=[1]), \n            spaces.Discrete(maxRounds+1), \n            spaces.Discrete(maxRounds+1), \n            spaces.Discrete(maxRounds+1), \n            spaces.Box(0, maxWealth, [1]))) \n        self.reward_range = (0, maxWealth)\n        self.edge = edge\n        self.wealth = self.initialWealth\n        self.maxRounds = maxRounds\n        self.rounds = self.maxRounds\n        self.maxWealth = maxWealth\n        if reseed or not hasattr(self, 'np_random') : self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        action = action/100.0\n        if action > self.wealth:\n          action = self.wealth\n        if self.wealth < 0.000001:\n            done = True\n            reward = 0.0\n        else:\n          if self.rounds == 0:\n            done = True\n            reward = self.wealth\n          else:\n            self.rounds = self.rounds - 1\n            done = False\n            reward = 0.0\n            coinflip = flip(self.edge, self.np_random)\n            self.roundsElapsed = self.roundsElapsed+1\n            if coinflip:\n              self.wealth = min(self.maxWealth, self.wealth + action)\n              self.maxEverWealth = max(self.wealth, self.maxEverWealth)\n              self.wins = self.wins+1\n            else:\n              self.wealth = self.wealth - action\n              self.losses = self.losses+1\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (np.array([float(self.wealth)]), self.roundsElapsed, self.wins, self.losses, np.array([float(self.maxEverWealth)]))\n    def reset(self):\n        \n        self.__init__(initialWealth=self.initialWealth, edgePriorAlpha=self.edgePriorAlpha, edgePriorBeta=self.edgePriorBeta, maxWealthAlpha=self.maxWealthAlpha, maxWealthM=self.maxWealthM, maxRoundsMean=self.maxRoundsMean, maxRoundsSD=self.maxRoundsSD, reseed=False)\n        return self._get_obs()\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds, \"; True edge: \", self.edge,\n              \"; True max wealth: \", self.maxWealth, \"; True stopping time: \", self.maxRounds, \"; Rounds left: \",\n              self.maxRounds - self.roundsElapsed)\n", "comments": "   the kelly coinflip game simple gambling introduced haghani   dewey 2016  rational decision making under uncertainty  observed betting patterns biased coin  (https   papers ssrn com sol3 papers cfm abstract id 2856963)  test human decision making setting like stock market  positive expected value highly stochastic  found many subjects performed badly  often going broke  even though optimal play would reach maximum  95  probability  in coinflip game  player starts  25 00 gamble 300 rounds  round  bet anywhere net worth (in penny increments)  coin flipped  p 0 6  player wins twice bet  otherwise  lose   250 maximum players allowed  at end 300 rounds  keep whatever  the human subjects earned average  91  simple use kelly criterion (https   en wikipedia org wiki kelly criterion)  giving strategy betting 20  cap hit  would earn  240  decision tree analysis shows optimal play earns  246 (https   www gwern net coin flip)  the game short circuits either wealth    0 (since one never recover) wealth   cap (trivial optimal play  one simply bets nothing thereafter)  in implementation  default paper settings  25  60  odds  wealth cap  250  300 rounds  to specify action space advance  multiply wealth cap (in dollars) 100 (to allow penny bets)  one attempt bet money one  rounded one net worth  (alternately  mistaken bet could end episode immediately  clear version would better ) for harder version randomizes 3 key parameters  see generalized kelly coinflip game         metadata     render modes     human        def   init  (self  initialwealth 25 0  edge 0 6  maxwealth 250 0  maxrounds 300)           self action space   spaces discrete(int(maxwealth 100))   betting penny increments         self observation space   spaces tuple((             spaces box(0  maxwealth   1 )    (w b)             spaces discrete(maxrounds 1)))         self reward range   (0  maxwealth)         self edge   edge         self wealth   initialwealth         self initialwealth   initialwealth         self maxrounds   maxrounds         self maxwealth   maxwealth         self seed()         self reset()      def seed(self  seed none)          self np random  seed   seeding np random(seed)         return  seed       def step(self  action)          action   action 100 0   convert pennies dollars         action   self wealth    treat attempts bet possess    betting everything           action   self wealth         self wealth   0 000001              done   true             reward   0 0         else            self rounds    0              done   true             reward   self wealth           else              self rounds   self rounds   1             done   false             reward   0 0             coinflip   flip(self edge  self np random)             coinflip                self wealth   min(self maxwealth  self wealth   action)             else                self wealth   self wealth   action         return self  get obs()  reward  done          def  get obs(self)          return (np array( self wealth )  self rounds)      def reset(self)          self rounds   self maxrounds         self wealth   self initialwealth         return self  get obs()      def render(self  mode  human )          print( current wealth     self wealth     rounds left     self rounds)  class kellycoinflipgeneralizedenv(gym env)         the generalized kelly coinflip game extension arthurb   gwern branwen expands kelly coinflip game mdp pomdp  3 key parameters (edge  maximum wealth  number rounds) unknown random variables drawn 3 distributions  beta(7 3) coinflip edge 0 1  n(300 25) total number rounds  pareto(5 200) wealth cap  these distributions chosen conjugate   easily updatable  allow inference (other choices like geometric number rounds make observations informative)  loosely reflect human might expect original kelly coinflip game given number rounds strictly fixed told wealth cap neared  with particular distributions  entire history game summarized sufficient statistics rounds elapsed wins losses max wealth ever reached  bayes optimal decision (in theory) made  avoid agents tediously track sufficient statistics manually way  observation space augmented wealth rounds left (rounds left deleted hidden variable) current wealth rounds elapsed wins losses maximum observed wealth  the simple kelly coinflip game easily solved calculating decision trees  generalized kelly coinflip game may intractable (although analysis edge case alone suggests bayes optimal value may close one would calculate using decision tree specific case)  represents good challenge rl agents        generalized kelly coinflip game distributions     betting penny increments    (w b)    convert pennies dollars    treat attempts bet possess    betting everything    store hyperparameters passing back   init  () resets hyperparameters govern next game parameters  user expects  todo  boilerplate  elegant way     draw game set parameters     add additional global variable sufficient statistic pareto distribution wealth cap     alpha update  x  simply highest wealth count seen date     coinflip edge  total wins losses     number rounds  need remember many rounds played     rest proceeds     current wealth    rounds elapsed    wins    losses    maximum observed wealth    init everything draw new parameters etc  preserve rng reproducibility pass hyperparameters originally specified  ", "content": "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom gym.spaces import prng\n# for Generalized Kelly coinflip game distributions:\nfrom scipy.stats import genpareto\nimport numpy as np\nimport numpy.random\n\ndef flip(edge, np_random):\n    return np_random.uniform() < edge\n\nclass KellyCoinflipEnv(gym.Env):\n    \"\"\"The Kelly coinflip game is a simple gambling introduced by Haghani & Dewey 2016's 'Rational Decision-Making Under Uncertainty: Observed Betting Patterns on a Biased Coin' (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2856963), to test human decision-making in a setting like that of the stock market: positive expected value but highly stochastic; they found many subjects performed badly, often going broke, even though optimal play would reach the maximum with ~95% probability. In the coinflip game, the player starts with $25.00 to gamble over 300 rounds; each round, they can bet anywhere up to their net worth (in penny increments), and then a coin is flipped; with P=0.6, the player wins twice what they bet, otherwise, they lose it. $250 is the maximum players are allowed to have. At the end of the 300 rounds, they keep whatever they have. The human subjects earned an average of $91; a simple use of the Kelly criterion (https://en.wikipedia.org/wiki/Kelly_criterion), giving a strategy of betting 20% until the cap is hit, would earn $240; a decision tree analysis shows that optimal play earns $246 (https://www.gwern.net/Coin-flip). The game short-circuits when either wealth = $0 (since one can never recover) or wealth = cap (trivial optimal play: one simply bets nothing thereafter). In this implementation, we default to the paper settings of $25, 60% odds, wealth cap of $250, and 300 rounds. To specify the action space in advance, we multiply the wealth cap (in dollars) by 100 (to allow for all penny bets); should one attempt to bet more money than one has, it is rounded down to one's net worth. (Alternately, a mistaken bet could end the episode immediately; it's not clear to me which version would be better.) For a harder version which randomizes the 3 key parameters, see the Generalized Kelly coinflip game.\"\"\"\n    metadata = {'render.modes': ['human']}\n    def __init__(self, initialWealth=25.0, edge=0.6, maxWealth=250.0, maxRounds=300):\n\n        self.action_space = spaces.Discrete(int(maxWealth*100)) # betting in penny increments\n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, maxWealth, [1]), # (w,b)\n            spaces.Discrete(maxRounds+1)))\n        self.reward_range = (0, maxWealth)\n        self.edge = edge\n        self.wealth = initialWealth\n        self.initialWealth = initialWealth\n        self.maxRounds = maxRounds\n        self.maxWealth = maxWealth\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        action = action/100.0 # convert from pennies to dollars\n        if action > self.wealth: # treat attempts to bet more than possess as == betting everything\n          action = self.wealth\n        if self.wealth < 0.000001:\n            done = True\n            reward = 0.0\n        else:\n          if self.rounds == 0:\n            done = True\n            reward = self.wealth\n          else:\n            self.rounds = self.rounds - 1\n            done = False\n            reward = 0.0\n            coinflip = flip(self.edge, self.np_random)\n            if coinflip:\n              self.wealth = min(self.maxWealth, self.wealth + action)\n            else:\n              self.wealth = self.wealth - action\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (np.array([self.wealth]), self.rounds)\n\n    def reset(self):\n        self.rounds = self.maxRounds\n        self.wealth = self.initialWealth\n        return self._get_obs()\n\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds)\n\nclass KellyCoinflipGeneralizedEnv(gym.Env):\n    \"\"\"The Generalized Kelly coinflip game is an extension by ArthurB & Gwern Branwen which expands the Kelly coinflip game MDP into a POMDP, where the 3 key parameters (edge, maximum wealth, and number of rounds) are unknown random variables drawn from 3 distributions: a Beta(7,3) for the coinflip edge 0-1, a N(300,25) the total number of rounds, and a Pareto(5,200) for the wealth cap. These distributions are chosen to be conjugate & easily updatable, to allow for inference (other choices like the geometric for number of rounds wouldn't make observations informative), and to loosely reflect what a human might expect in the original Kelly coinflip game given that the number of rounds wasn't strictly fixed and they weren't told the wealth cap until they neared it. With these particular distributions, the entire history of the game can be summarized into a few sufficient statistics of rounds-elapsed/wins/losses/max-wealth-ever-reached, from which the Bayes-optimal decision can (in theory) be made; to avoid all agents having to tediously track those sufficient statistics manually in the same way, the observation space is augmented from wealth/rounds-left (rounds-left is deleted because it is a hidden variable) to current-wealth/rounds-elapsed/wins/losses/maximum-observed-wealth. The simple Kelly coinflip game can easily be solved by calculating decision trees, but the Generalized Kelly coinflip game may be intractable (although the analysis for the edge case alone suggests that the Bayes-optimal value may be very close to what one would calculate using a decision tree for any specific case), and represents a good challenge for RL agents.\"\"\"\n    metadata = {'render.modes': ['human']}\n    def __init__(self, initialWealth=25.0, edgePriorAlpha=7, edgePriorBeta=3, maxWealthAlpha=5.0, maxWealthM=200.0, maxRoundsMean=300.0, maxRoundsSD=25.0, reseed=True):\n        # store the hyperparameters for passing back into __init__() during resets so the same hyperparameters govern the next game's parameters, as the user expects: TODO: this is boilerplate, is there any more elegant way to do this?\n        self.initialWealth=float(initialWealth)\n        self.edgePriorAlpha=edgePriorAlpha\n        self.edgePriorBeta=edgePriorBeta\n        self.maxWealthAlpha=maxWealthAlpha\n        self.maxWealthM=maxWealthM\n        self.maxRoundsMean=maxRoundsMean\n        self.maxRoundsSD=maxRoundsSD\n\n        # draw this game's set of parameters:\n        edge = prng.np_random.beta(edgePriorAlpha, edgePriorBeta)\n        maxWealth = round(genpareto.rvs(maxWealthAlpha, maxWealthM, random_state=prng.np_random))\n        maxRounds = int(round(prng.np_random.normal(maxRoundsMean, maxRoundsSD)))\n\n        # add an additional global variable which is the sufficient statistic for the Pareto distribution on wealth cap;\n        # alpha doesn't update, but x_m does, and simply is the highest wealth count we've seen to date:\n        self.maxEverWealth = float(self.initialWealth)\n        # for the coinflip edge, it is total wins/losses:\n        self.wins = 0\n        self.losses = 0\n        # for the number of rounds, we need to remember how many rounds we've played:\n        self.roundsElapsed = 0\n\n        # the rest proceeds as before:\n        self.action_space = spaces.Discrete(int(maxWealth*100))\n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, maxWealth, shape=[1]), # current wealth\n            spaces.Discrete(maxRounds+1), # rounds elapsed\n            spaces.Discrete(maxRounds+1), # wins\n            spaces.Discrete(maxRounds+1), # losses\n            spaces.Box(0, maxWealth, [1]))) # maximum observed wealth\n        self.reward_range = (0, maxWealth)\n        self.edge = edge\n        self.wealth = self.initialWealth\n        self.maxRounds = maxRounds\n        self.rounds = self.maxRounds\n        self.maxWealth = maxWealth\n        if reseed or not hasattr(self, 'np_random') : self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        action = action/100.0\n        if action > self.wealth:\n          action = self.wealth\n        if self.wealth < 0.000001:\n            done = True\n            reward = 0.0\n        else:\n          if self.rounds == 0:\n            done = True\n            reward = self.wealth\n          else:\n            self.rounds = self.rounds - 1\n            done = False\n            reward = 0.0\n            coinflip = flip(self.edge, self.np_random)\n            self.roundsElapsed = self.roundsElapsed+1\n            if coinflip:\n              self.wealth = min(self.maxWealth, self.wealth + action)\n              self.maxEverWealth = max(self.wealth, self.maxEverWealth)\n              self.wins = self.wins+1\n            else:\n              self.wealth = self.wealth - action\n              self.losses = self.losses+1\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (np.array([float(self.wealth)]), self.roundsElapsed, self.wins, self.losses, np.array([float(self.maxEverWealth)]))\n    def reset(self):\n        # re-init everything to draw new parameters etc, but preserve the RNG for reproducibility and pass in the same hyperparameters as originally specified:\n        self.__init__(initialWealth=self.initialWealth, edgePriorAlpha=self.edgePriorAlpha, edgePriorBeta=self.edgePriorBeta, maxWealthAlpha=self.maxWealthAlpha, maxWealthM=self.maxWealthM, maxRoundsMean=self.maxRoundsMean, maxRoundsSD=self.maxRoundsSD, reseed=False)\n        return self._get_obs()\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds, \"; True edge: \", self.edge,\n              \"; True max wealth: \", self.maxWealth, \"; True stopping time: \", self.maxRounds, \"; Rounds left: \",\n              self.maxRounds - self.roundsElapsed)\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "kellycoinflip.py", "id": "b196724082372c50cbb0a24fd92f603c", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/toy_text/kellycoinflip.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}