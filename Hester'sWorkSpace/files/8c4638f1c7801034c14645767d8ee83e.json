{"author": "tflearn", "code": "\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tflearn\n\n\n\n\n\n\nimport tflearn.datasets.mnist as mnist\nmnist_data = mnist.read_data_sets(one_hot=True)\n\n\nwith tf.Graph().as_default():\n    \n    X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n    Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n\n    net = tf.reshape(X, [-1, 28, 28, 1])\n\n    \n    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n    net = tflearn.max_pool_2d(net, 2)\n    net = tflearn.local_response_normalization(net)\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n    net = tflearn.max_pool_2d(net, 2)\n    net = tflearn.local_response_normalization(net)\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 128, activation='tanh')\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 256, activation='tanh')\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 10, activation='linear')\n\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n\n    \n    init = tf.initialize_all_variables()\n\n    \n    with tf.Session() as sess:\n        sess.run(init)\n\n        batch_size = 128\n        for epoch in range(2): \n            avg_cost = 0.\n            total_batch = int(mnist_data.train.num_examples/batch_size)\n            for i in range(total_batch):\n                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n                cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n                avg_cost += cost/total_batch\n                if i % 20 == 0:\n                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n                          \"Loss:\", str(cost))\n", "comments": "    this tutorial introduce combine tflearn tensorflow  using tflearn trainer regular tensorflow graph                                                   high level api  using tflearn wrappers                                              using mnist dataset    user defined placeholders    placeholders data labels    using tflearn wrappers network building    defining ops using tensorflow    initializing variables    launch graph    2 epochs ", "content": "\"\"\"\nThis tutorial will introduce how to combine TFLearn and Tensorflow, using\nTFLearn trainer with regular Tensorflow graph.\n\"\"\"\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tflearn\n\n# --------------------------------------\n# High-Level API: Using TFLearn wrappers\n# --------------------------------------\n\n# Using MNIST Dataset\nimport tflearn.datasets.mnist as mnist\nmnist_data = mnist.read_data_sets(one_hot=True)\n\n# User defined placeholders\nwith tf.Graph().as_default():\n    # Placeholders for data and labels\n    X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n    Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n\n    net = tf.reshape(X, [-1, 28, 28, 1])\n\n    # Using TFLearn wrappers for network building\n    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n    net = tflearn.max_pool_2d(net, 2)\n    net = tflearn.local_response_normalization(net)\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n    net = tflearn.max_pool_2d(net, 2)\n    net = tflearn.local_response_normalization(net)\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 128, activation='tanh')\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 256, activation='tanh')\n    net = tflearn.dropout(net, 0.8)\n    net = tflearn.fully_connected(net, 10, activation='linear')\n\n    # Defining other ops using Tensorflow\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n\n    # Initializing the variables\n    init = tf.initialize_all_variables()\n\n    # Launch the graph\n    with tf.Session() as sess:\n        sess.run(init)\n\n        batch_size = 128\n        for epoch in range(2): # 2 epochs\n            avg_cost = 0.\n            total_batch = int(mnist_data.train.num_examples/batch_size)\n            for i in range(total_batch):\n                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n                cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n                avg_cost += cost/total_batch\n                if i % 20 == 0:\n                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n                          \"Loss:\", str(cost))\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "layers.py", "id": "8c4638f1c7801034c14645767d8ee83e", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/extending_tensorflow/layers.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}