{"author": "chiphuyen", "code": "\"\"\" Starter code for a simple regression example using eager execution.\nCreated by Akshay Agrawal (akshayka@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 04\n\"\"\"\nimport time\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nimport matplotlib.pyplot as plt\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n# In order to use eager execution, `tfe.enable_eager_execution()` must be\n\n\n\n\n\n\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\ndataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n\n\n\n\n\nw = None\nb = None\n\n\ndef prediction(x):\n  \n  \n  \n  pass\n\n# Define loss functions of the form: L(y, y_predicted)\ndef squared_loss(y, y_predicted):\n  \n  \n  \n  pass\n\ndef huber_loss(y, y_predicted):\n  \"\"\"Huber loss with `m` set to `1.0`.\"\"\"\n  \n  \n  \n  pass\n\ndef train(loss_fn):\n  \"\"\"Train a regression model evaluated using `loss_fn`.\"\"\"\n  print('Training; loss function: ' + loss_fn.__name__)\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\n  \n  \n  \n  \n  def loss_for_example(x, y):\n    pass\n\n  \n  \n  \n  \n  grad_fn = None\n\n  start = time.time()\n  for epoch in range(100):\n    total_loss = 0.0\n    for x_i, y_i in tfe.Iterator(dataset):\n      \n      \n      \n      \n      optimizer.apply_gradients(gradients)\n      total_loss += loss\n    if epoch % 10 == 0:\n      print('Epoch {0}: {1}'.format(epoch, total_loss / n_samples))\n  print('Took: %f seconds' % (time.time() - start))\n  print('Eager execution exhibits significant overhead per operation. '\n        'As you increase your batch size, the impact of the overhead will '\n        'become less noticeable. Eager execution is under active development: '\n        'expect performance to increase substantially in the near future!')\n\ntrain(huber_loss)\nplt.plot(data[:,0], data[:,1], 'bo')\n# The `.numpy()` method of a tensor retrieves the NumPy array backing it.\n# In future versions of eager, you won't need to call `.numpy()` and will\n\n\nplt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r',\n         label=\"huber regression\")\nplt.legend()\nplt.show()\n", "comments": "    starter code simple regression example using eager execution  created akshay agrawal (akshayka cs stanford edu) cs20   tensorflow deep learning research  cs20 stanford edu lecture 04     import time  import tensorflow tf import tensorflow contrib eager tfe import matplotlib pyplot plt  import utils  data file    data birth life 2010 txt     in order use eager execution   tfe enable eager execution()  must   called beginning tensorflow program                                           to do                                               read data dataset  data  n samples   utils read birth life data(data file) dataset   tf data dataset tensor slices((data   0   data   1 ))    create weight bias variables  initialized 0 0                                           to do                                            w   none b   none    define linear predictor  def prediction(x)                                               to do                                                pass    define loss functions form  l(y  predicted) def squared loss(y  predicted)                                               to do                                                pass  def huber loss(y  predicted)       huber loss   set  1 0                                                   to do                                                pass  def train(loss fn)       train regression model evaluated using  loss fn         in order use eager execution   tfe enable eager execution()  must    called beginning tensorflow program                                             to do                                                read data dataset     create weight bias variables  initialized 0 0                                             to do                                                define linear predictor                                             to do                                                define loss functions form  l(y  predicted)                                            to do                                                                                        to do                                                define function differentiate                                             to do                                                obtain gradients function using  tfe implicit value gradients                                              to do                                                compute loss gradient  take optimization step                                             to do                                                the   numpy()  method tensor retrieves numpy array backing     in future versions eager  need call   numpy()     instead able  cases  pass tensors wherever numpy arrays    expected  ", "content": "\"\"\" Starter code for a simple regression example using eager execution.\nCreated by Akshay Agrawal (akshayka@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 04\n\"\"\"\nimport time\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nimport matplotlib.pyplot as plt\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n# In order to use eager execution, `tfe.enable_eager_execution()` must be\n# called at the very beginning of a TensorFlow program.\n#############################\n########## TO DO ############\n#############################\n\n# Read the data into a dataset.\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\ndataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n\n# Create weight and bias variables, initialized to 0.0.\n#############################\n########## TO DO ############\n#############################\nw = None\nb = None\n\n# Define the linear predictor.\ndef prediction(x):\n  #############################\n  ########## TO DO ############\n  #############################\n  pass\n\n# Define loss functions of the form: L(y, y_predicted)\ndef squared_loss(y, y_predicted):\n  #############################\n  ########## TO DO ############\n  #############################\n  pass\n\ndef huber_loss(y, y_predicted):\n  \"\"\"Huber loss with `m` set to `1.0`.\"\"\"\n  #############################\n  ########## TO DO ############\n  #############################\n  pass\n\ndef train(loss_fn):\n  \"\"\"Train a regression model evaluated using `loss_fn`.\"\"\"\n  print('Training; loss function: ' + loss_fn.__name__)\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\n  # Define the function through which to differentiate.\n  #############################\n  ########## TO DO ############\n  #############################\n  def loss_for_example(x, y):\n    pass\n\n  # Obtain a gradients function using `tfe.implicit_value_and_gradients`.\n  #############################\n  ########## TO DO ############\n  #############################\n  grad_fn = None\n\n  start = time.time()\n  for epoch in range(100):\n    total_loss = 0.0\n    for x_i, y_i in tfe.Iterator(dataset):\n      # Compute the loss and gradient, and take an optimization step.\n      #############################\n      ########## TO DO ############\n      #############################\n      optimizer.apply_gradients(gradients)\n      total_loss += loss\n    if epoch % 10 == 0:\n      print('Epoch {0}: {1}'.format(epoch, total_loss / n_samples))\n  print('Took: %f seconds' % (time.time() - start))\n  print('Eager execution exhibits significant overhead per operation. '\n        'As you increase your batch size, the impact of the overhead will '\n        'become less noticeable. Eager execution is under active development: '\n        'expect performance to increase substantially in the near future!')\n\ntrain(huber_loss)\nplt.plot(data[:,0], data[:,1], 'bo')\n# The `.numpy()` method of a tensor retrieves the NumPy array backing it.\n# In future versions of eager, you won't need to call `.numpy()` and will\n# instead be able to, in most cases, pass Tensors wherever NumPy arrays are\n# expected.\nplt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r',\n         label=\"huber regression\")\nplt.legend()\nplt.show()\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "04_linreg_eager_starter.py", "id": "6b57aa05edf638ce041c4ff9e4ddd5f5", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/04_linreg_eager_starter.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}