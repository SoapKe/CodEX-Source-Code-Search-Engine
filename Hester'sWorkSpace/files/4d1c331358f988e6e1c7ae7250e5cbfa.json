{"author": "donnemartin", "code": "import cPickle as pkl\nimport time\n\nimport numpy\nimport theano\nfrom theano import config\nimport theano.tensor as T\nfrom theano.tensor.nnet import categorical_crossentropy\n\nfrom fuel.datasets import TextFile\nfrom fuel.streams import DataStream\nfrom fuel.schemes import ConstantScheme\nfrom fuel.transformers import Batch, Padding\n\n\n\n\n\n\nTRAIN_FILE = '/u/brakelp/temp/traindata.txt'\nVAL_FILE = '/u/brakelp/temp/valdata.txt'\nDICT_FILE = '/u/brakelp/temp/dictionary.pkl'\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\ndef gauss_weight(ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = numpy.random.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\nclass LogisticRegression(object):\n    \"\"\"Multi-class Logistic Regression Class\n\n    The logistic regression is fully described by a weight matrix :math:`W`\n    and bias vector :math:`b`. Classification is done by projecting data\n    points onto a set of hyperplanes, the distance to which is used to\n    determine a class membership probability.\n    \"\"\"\n\n    def __init__(self, input, n_in, n_out):\n        \"\"\" Initialize the parameters of the logistic regression\n\n        :type input: theano.tensor.TensorType\n        :param input: symbolic variable that describes the input of the\n                      architecture (one minibatch)\n\n        :type n_in: int\n        :param n_in: number of input units, the dimension of the space in\n                     which the datapoints lie\n\n        :type n_out: int\n        :param n_out: number of output units, the dimension of the space in\n                      which the labels lie\n\n        \"\"\"\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        self.W = theano.shared(value=numpy.zeros((n_in, n_out),\n                                                 dtype=theano.config.floatX),\n                               name='W', borrow=True)\n        \n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name='b', borrow=True)\n\n        \n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, 2)[:, :, None])\n        pmf = energy_exp / energy_exp.sum(2)[:, :, None]\n        self.p_y_given_x = pmf\n\n        \n        \n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n\n        \n        self.params = [self.W, self.b]\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        \n        self.W_i = theano.shared(gauss_weight(n_in, n_h), 'W_i', borrow=True)\n        self.W_f = theano.shared(gauss_weight(n_in, n_h), 'W_f', borrow=True)\n        self.W_c = theano.shared(gauss_weight(n_in, n_h), 'W_c', borrow=True)\n        self.W_o = theano.shared(gauss_weight(n_in, n_h), 'W_o', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(n_h), 'U_i', borrow=True)\n        self.U_f = theano.shared(gauss_weight(n_h), 'U_f', borrow=True)\n        self.U_c = theano.shared(gauss_weight(n_h), 'U_c', borrow=True)\n        self.U_o = theano.shared(gauss_weight(n_h), 'U_o', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_i', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_f', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_c', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_o', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef train_model(batch_size=100, n_h=50, n_epochs=40):\n\n    \n    dictionary = pkl.load(open(DICT_FILE, 'r'))\n    dictionary['~'] = len(dictionary)\n    reverse_mapping = dict((j, i) for i, j in dictionary.items())\n\n    print(\"Loading the data\")\n    train = TextFile(files=[TRAIN_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    train_stream = DataStream.default_stream(train)\n\n    \n    train_stream = Batch(train_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    train_stream = Padding(train_stream)\n\n    \n    val = TextFile(files=[VAL_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    val_stream = DataStream.default_stream(val)\n\n    \n    val_stream = Batch(val_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    val_stream = Padding(val_stream)\n\n    print('Building model')\n\n    \n    rng = numpy.random.RandomState(12345)\n\n    x = T.lmatrix('x')\n    mask = T.matrix('mask')\n\n    \n    recurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\n\n    logreg_layer = LogisticRegression(input=recurrent_layer.output[:-1],\n                                      n_in=n_h, n_out=111)\n\n    cost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                             x[1:],\n                                             mask[1:]) / batch_size\n\n    \n    params = logreg_layer.params + recurrent_layer.params\n\n    \n    grads = T.grad(cost, params)\n\n    \n    \n    \n    \n    # (params[i], grads[i]) pairs.\n    learning_rate = 0.1\n    updates = [\n        (param_i, param_i - learning_rate * grad_i)\n        for param_i, grad_i in zip(params, grads)\n    ]\n\n    update_model = theano.function([x, mask], cost, updates=updates)\n\n    evaluate_model = theano.function([x, mask], cost)\n\n    \n    x_t = T.iscalar()\n    h_p = T.vector()\n    c_p = T.vector()\n    h_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\n    energy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\n    energy_exp = T.exp(energy - T.max(energy, 1)[:, None])\n\n    output = energy_exp / energy_exp.sum(1)[:, None]\n    single_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n\n    start_time = time.clock()\n\n    iteration = 0\n\n    for epoch in range(n_epochs):\n        print 'epoch:', epoch\n\n        for x_, mask_ in train_stream.get_epoch_iterator():\n            iteration += 1\n\n            cross_entropy = update_model(x_.T, mask_.T)\n\n\n            \n            if iteration % 40 == 0:\n                try:\n                    prediction = numpy.ones(111, dtype=config.floatX) / 111.0\n                    h_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    c_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    initial = 'the meaning of life is '\n                    sentence = initial\n                    for char in initial:\n                        x_t = dictionary[char]\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                    sample = numpy.random.multinomial(1, prediction.flatten())\n                    for i in range(450):\n                        x_t = numpy.argmax(sample)\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                        sentence += reverse_mapping[x_t]\n                        sample = numpy.random.multinomial(1, prediction.flatten())\n                    print 'LSTM: \"' + sentence + '\"'\n                except ValueError:\n                    print 'Something went wrong during sentence generation.'\n\n            if iteration % 40 == 0:\n                print 'epoch:', epoch, '  minibatch:', iteration\n                val_scores = []\n                for x_val, mask_val in val_stream.get_epoch_iterator():\n                    val_scores.append(evaluate_model(x_val.T, mask_val.T))\n                print 'Average validation CE per sentence:', numpy.mean(val_scores)\n\n    end_time = time.clock()\n    print('Optimization complete.')\n    print('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n\n\nif __name__ == '__main__':\n    train_model()\n", "comments": "   multi class logistic regression class      the logistic regression fully described weight matrix  math  w      bias vector  math  b   classification done projecting data     points onto set hyperplanes  distance used     determine class membership probability               def   init  (self  input  n  n out)              initialize parameters logistic regression           type input  theano tensor tensortype          param input  symbolic variable describes input                       architecture (one minibatch)           type n  int          param n  number input units  dimension space                      datapoints lie           type n  int          param n  number output units  dimension space                       labels lie                 these files downloaded    http   www etud iro umontreal ca  brakelp train txt gz    http   www etud iro umontreal ca  brakelp dictionary pkl    forget change paths gunzip train txt gz    initialize 0 weights w matrix shape (n  n out)    initialize baises b vector n 0s    compute vector class membership probabilities symbolic form    compute prediction class whose probability maximal    symbolic form    parameters model    init params    self output format (batchsize  n h)    load datasets fuel    organize data batches pad shorter sequences zeros    idem dito validation text    organize data batches pad shorter sequences zeros    set random number generator  seeds consistency    construct lstm layer    create list model parameters fit gradient descent    create list gradients model parameters    update model function updates model parameters    sgd since model many parameters  would tedious    manually create update rule model parameter  we thus    create updates list automatically looping    (params   grads ) pairs     define compile function generating sequence step step     generate text 20 minibatches ", "content": "import cPickle as pkl\nimport time\n\nimport numpy\nimport theano\nfrom theano import config\nimport theano.tensor as T\nfrom theano.tensor.nnet import categorical_crossentropy\n\nfrom fuel.datasets import TextFile\nfrom fuel.streams import DataStream\nfrom fuel.schemes import ConstantScheme\nfrom fuel.transformers import Batch, Padding\n\n\n# These files can be downloaded from\n# http://www-etud.iro.umontreal.ca/~brakelp/train.txt.gz\n# http://www-etud.iro.umontreal.ca/~brakelp/dictionary.pkl\n# don't forget to change the paths and gunzip train.txt.gz\nTRAIN_FILE = '/u/brakelp/temp/traindata.txt'\nVAL_FILE = '/u/brakelp/temp/valdata.txt'\nDICT_FILE = '/u/brakelp/temp/dictionary.pkl'\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\ndef gauss_weight(ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = numpy.random.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\nclass LogisticRegression(object):\n    \"\"\"Multi-class Logistic Regression Class\n\n    The logistic regression is fully described by a weight matrix :math:`W`\n    and bias vector :math:`b`. Classification is done by projecting data\n    points onto a set of hyperplanes, the distance to which is used to\n    determine a class membership probability.\n    \"\"\"\n\n    def __init__(self, input, n_in, n_out):\n        \"\"\" Initialize the parameters of the logistic regression\n\n        :type input: theano.tensor.TensorType\n        :param input: symbolic variable that describes the input of the\n                      architecture (one minibatch)\n\n        :type n_in: int\n        :param n_in: number of input units, the dimension of the space in\n                     which the datapoints lie\n\n        :type n_out: int\n        :param n_out: number of output units, the dimension of the space in\n                      which the labels lie\n\n        \"\"\"\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        self.W = theano.shared(value=numpy.zeros((n_in, n_out),\n                                                 dtype=theano.config.floatX),\n                               name='W', borrow=True)\n        # initialize the baises b as a vector of n_out 0s\n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name='b', borrow=True)\n\n        # compute vector of class-membership probabilities in symbolic form\n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, 2)[:, :, None])\n        pmf = energy_exp / energy_exp.sum(2)[:, :, None]\n        self.p_y_given_x = pmf\n\n        # compute prediction as class whose probability is maximal in\n        # symbolic form\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        # Init params\n        self.W_i = theano.shared(gauss_weight(n_in, n_h), 'W_i', borrow=True)\n        self.W_f = theano.shared(gauss_weight(n_in, n_h), 'W_f', borrow=True)\n        self.W_c = theano.shared(gauss_weight(n_in, n_h), 'W_c', borrow=True)\n        self.W_o = theano.shared(gauss_weight(n_in, n_h), 'W_o', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(n_h), 'U_i', borrow=True)\n        self.U_f = theano.shared(gauss_weight(n_h), 'U_f', borrow=True)\n        self.U_c = theano.shared(gauss_weight(n_h), 'U_c', borrow=True)\n        self.U_o = theano.shared(gauss_weight(n_h), 'U_o', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_i', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_f', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_c', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_o', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef train_model(batch_size=100, n_h=50, n_epochs=40):\n\n    # Load the datasets with Fuel\n    dictionary = pkl.load(open(DICT_FILE, 'r'))\n    dictionary['~'] = len(dictionary)\n    reverse_mapping = dict((j, i) for i, j in dictionary.items())\n\n    print(\"Loading the data\")\n    train = TextFile(files=[TRAIN_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    train_stream = DataStream.default_stream(train)\n\n    # organize data in batches and pad shorter sequences with zeros\n    train_stream = Batch(train_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    train_stream = Padding(train_stream)\n\n    # idem dito for the validation text\n    val = TextFile(files=[VAL_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    val_stream = DataStream.default_stream(val)\n\n    # organize data in batches and pad shorter sequences with zeros\n    val_stream = Batch(val_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    val_stream = Padding(val_stream)\n\n    print('Building model')\n\n    # Set the random number generator' seeds for consistency\n    rng = numpy.random.RandomState(12345)\n\n    x = T.lmatrix('x')\n    mask = T.matrix('mask')\n\n    # Construct the LSTM layer\n    recurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\n\n    logreg_layer = LogisticRegression(input=recurrent_layer.output[:-1],\n                                      n_in=n_h, n_out=111)\n\n    cost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                             x[1:],\n                                             mask[1:]) / batch_size\n\n    # create a list of all model parameters to be fit by gradient descent\n    params = logreg_layer.params + recurrent_layer.params\n\n    # create a list of gradients for all model parameters\n    grads = T.grad(cost, params)\n\n    # update_model is a function that updates the model parameters by\n    # SGD Since this model has many parameters, it would be tedious to\n    # manually create an update rule for each model parameter. We thus\n    # create the updates list by automatically looping over all\n    # (params[i], grads[i]) pairs.\n    learning_rate = 0.1\n    updates = [\n        (param_i, param_i - learning_rate * grad_i)\n        for param_i, grad_i in zip(params, grads)\n    ]\n\n    update_model = theano.function([x, mask], cost, updates=updates)\n\n    evaluate_model = theano.function([x, mask], cost)\n\n    # Define and compile a function for generating a sequence step by step.\n    x_t = T.iscalar()\n    h_p = T.vector()\n    c_p = T.vector()\n    h_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\n    energy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\n    energy_exp = T.exp(energy - T.max(energy, 1)[:, None])\n\n    output = energy_exp / energy_exp.sum(1)[:, None]\n    single_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n\n    start_time = time.clock()\n\n    iteration = 0\n\n    for epoch in range(n_epochs):\n        print 'epoch:', epoch\n\n        for x_, mask_ in train_stream.get_epoch_iterator():\n            iteration += 1\n\n            cross_entropy = update_model(x_.T, mask_.T)\n\n\n            # Generate some text after each 20 minibatches\n            if iteration % 40 == 0:\n                try:\n                    prediction = numpy.ones(111, dtype=config.floatX) / 111.0\n                    h_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    c_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    initial = 'the meaning of life is '\n                    sentence = initial\n                    for char in initial:\n                        x_t = dictionary[char]\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                    sample = numpy.random.multinomial(1, prediction.flatten())\n                    for i in range(450):\n                        x_t = numpy.argmax(sample)\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                        sentence += reverse_mapping[x_t]\n                        sample = numpy.random.multinomial(1, prediction.flatten())\n                    print 'LSTM: \"' + sentence + '\"'\n                except ValueError:\n                    print 'Something went wrong during sentence generation.'\n\n            if iteration % 40 == 0:\n                print 'epoch:', epoch, '  minibatch:', iteration\n                val_scores = []\n                for x_val, mask_val in val_stream.get_epoch_iterator():\n                    val_scores.append(evaluate_model(x_val.T, mask_val.T))\n                print 'Average validation CE per sentence:', numpy.mean(val_scores)\n\n    end_time = time.clock()\n    print('Optimization complete.')\n    print('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n\n\nif __name__ == '__main__':\n    train_model()\n", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "lstm_text.py", "id": "4d1c331358f988e6e1c7ae7250e5cbfa", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/deep-learning/theano-tutorial/rnn_tutorial/lstm_text.py", "save_time": "", "source": "", "update_at": "2018-03-18T12:16:56Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}