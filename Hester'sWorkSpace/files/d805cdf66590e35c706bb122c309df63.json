{"author": "chiphuyen", "code": "\"\"\" Solution for simple logistic regression model for MNIST\nwith placeholder\nMNIST dataset: yann.lecun.com/exdb/mnist/\nCreated by Chip Huyen (huyenn@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport time\n\nimport utils\n\n\nlearning_rate = 0.01\nbatch_size = 128\nn_epochs = 30\n\n\n\nmnist = input_data.read_data_sets('data/mnist', one_hot=True)\nX_batch, Y_batch = mnist.train.next_batch(batch_size)\n\n\n# each image in the MNIST data is of shape 28*28 = 784\n\n\n\nX = tf.placeholder(tf.float32, [batch_size, 784], name='image') \nY = tf.placeholder(tf.int32, [batch_size, 10], name='label')\n\n\n\n\n# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n\nw = tf.get_variable(name='weights', shape=(784, 10), initializer=tf.random_normal_initializer())\nb = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())\n\n\n\n\nlogits = tf.matmul(X, w) + b \n\n\n\nentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\nloss = tf.reduce_mean(entropy) \n# loss = tf.reduce_mean(-tf.reduce_sum(tf.nn.softmax(logits) * tf.log(Y), reduction_indices=[1]))\n\n\n\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n\npreds = tf.nn.softmax(logits)\ncorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\nwriter = tf.summary.FileWriter('./graphs/logreg_placeholder', tf.get_default_graph())\nwith tf.Session() as sess:\n\tstart_time = time.time()\n\tsess.run(tf.global_variables_initializer())\t\n\tn_batches = int(mnist.train.num_examples/batch_size)\n\t\n\t\n\tfor i in range(n_epochs): \n\t\ttotal_loss = 0\n\n\t\tfor j in range(n_batches):\n\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n\t\t\t_, loss_batch = sess.run([optimizer, loss], {X: X_batch, Y:Y_batch}) \n\t\t\ttotal_loss += loss_batch\n\t\tprint('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n\tprint('Total time: {0} seconds'.format(time.time() - start_time))\n\n\t\n\tn_batches = int(mnist.test.num_examples/batch_size)\n\ttotal_correct_preds = 0\n\n\tfor i in range(n_batches):\n\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n\t\taccuracy_batch = sess.run(accuracy, {X: X_batch, Y:Y_batch})\n\t\ttotal_correct_preds += accuracy_batch\t\n\n\tprint('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n\nwriter.close()\n", "comments": "    solution simple logistic regression model mnist placeholder mnist dataset  yann lecun com exdb mnist  created chip huyen (huyenn cs stanford edu) cs20   tensorflow deep learning research  cs20 stanford edu lecture 03        define paramaters model    step 1  read data    using tf learn built function load mnist data folder data mnist    step 2  create placeholders features labels    image mnist data shape 28 28   784    therefore  image represented 1x784 tensor    10 classes image  corresponding digits 0   9      lable one hot vector     step 3  create weights bias    w initialized random variables mean 0  stddev 0 01    b initialized 0    shape w depends dimension x y y   tf matmul(x  w)    shape b depends y    step 4  build model    model returns logits     logits later passed softmax layer    step 5  define loss function    use cross entropy softmax logits loss function    computes mean examples batch    loss   tf reduce mean( tf reduce sum(tf nn softmax(logits)   tf log(y)  reduction indices  1 ))    step 6  define training op    using gradient descent learning rate 0 01 minimize loss    step 7  calculate accuracy test set    train model n epochs times    test model ", "content": "\"\"\" Solution for simple logistic regression model for MNIST\nwith placeholder\nMNIST dataset: yann.lecun.com/exdb/mnist/\nCreated by Chip Huyen (huyenn@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport time\n\nimport utils\n\n# Define paramaters for the model\nlearning_rate = 0.01\nbatch_size = 128\nn_epochs = 30\n\n# Step 1: Read in data\n# using TF Learn's built in function to load MNIST data to the folder data/mnist\nmnist = input_data.read_data_sets('data/mnist', one_hot=True)\nX_batch, Y_batch = mnist.train.next_batch(batch_size)\n\n# Step 2: create placeholders for features and labels\n# each image in the MNIST data is of shape 28*28 = 784\n# therefore, each image is represented with a 1x784 tensor\n# there are 10 classes for each image, corresponding to digits 0 - 9. \n# each lable is one hot vector.\nX = tf.placeholder(tf.float32, [batch_size, 784], name='image') \nY = tf.placeholder(tf.int32, [batch_size, 10], name='label')\n\n# Step 3: create weights and bias\n# w is initialized to random variables with mean of 0, stddev of 0.01\n# b is initialized to 0\n# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n# shape of b depends on Y\nw = tf.get_variable(name='weights', shape=(784, 10), initializer=tf.random_normal_initializer())\nb = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())\n\n# Step 4: build model\n# the model that returns the logits.\n# this logits will be later passed through softmax layer\nlogits = tf.matmul(X, w) + b \n\n# Step 5: define loss function\n# use cross entropy of softmax of logits as the loss function\nentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\nloss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n# loss = tf.reduce_mean(-tf.reduce_sum(tf.nn.softmax(logits) * tf.log(Y), reduction_indices=[1]))\n\n# Step 6: define training op\n# using gradient descent with learning rate of 0.01 to minimize loss\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n# Step 7: calculate accuracy with test set\npreds = tf.nn.softmax(logits)\ncorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n\nwriter = tf.summary.FileWriter('./graphs/logreg_placeholder', tf.get_default_graph())\nwith tf.Session() as sess:\n\tstart_time = time.time()\n\tsess.run(tf.global_variables_initializer())\t\n\tn_batches = int(mnist.train.num_examples/batch_size)\n\t\n\t# train the model n_epochs times\n\tfor i in range(n_epochs): \n\t\ttotal_loss = 0\n\n\t\tfor j in range(n_batches):\n\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n\t\t\t_, loss_batch = sess.run([optimizer, loss], {X: X_batch, Y:Y_batch}) \n\t\t\ttotal_loss += loss_batch\n\t\tprint('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n\tprint('Total time: {0} seconds'.format(time.time() - start_time))\n\n\t# test the model\n\tn_batches = int(mnist.test.num_examples/batch_size)\n\ttotal_correct_preds = 0\n\n\tfor i in range(n_batches):\n\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n\t\taccuracy_batch = sess.run(accuracy, {X: X_batch, Y:Y_batch})\n\t\ttotal_correct_preds += accuracy_batch\t\n\n\tprint('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n\nwriter.close()\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "03_logreg_placeholder.py", "id": "d805cdf66590e35c706bb122c309df63", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/03_logreg_placeholder.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}