{"author": "scrapy", "code": "\n\n\n\nimport re\n\n# Used for remembering the file (and its contents)\n\n_filename = None\n_contents = None\n\n\nline_re = re.compile(u'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n\n\ntry:\n    with open(\"build/linkcheck/output.txt\") as out:\n        output_lines = out.readlines()\nexcept IOError:\n    print(\"linkcheck output not found; please run linkcheck first.\")\n    exit(1)\n\n\nfor line in output_lines:\n    match = re.match(line_re, line)\n\n    if match:\n        newfilename = match.group(1)\n        errortype = match.group(2)\n\n        \n        \n        if errortype.lower() in [\"broken\", \"local\"]:\n            print(\"Not Fixed: \" + line)\n        else:\n            \n            if newfilename != _filename:\n\n                \n                if _filename:\n                    with open(_filename, \"w\") as _file:\n                        _file.write(_contents)\n\n                _filename = newfilename\n\n                \n                with open(_filename) as _file:\n                    _contents = _file.read()\n\n            _contents = _contents.replace(match.group(3), match.group(4))\n    else:\n        \n        print(\"Not Understood: \" + line)\n", "comments": "     linkfix   companion sphinx linkcheck builder   uses linkcheck output file fix links docs   originally created issue  https   github com scrapy scrapy issues 606  author  dufferzafar         usr bin python    used remembering file (and contents)    open file     a regex matches standard linkcheck output lines    read lines linkcheck output file    for every line  fix respective file    broken links fixed    i sure local ones     if new file    update previous file    read new file memory    we understand current line means  ", "content": "#!/usr/bin/python\n\n\"\"\"\n\nLinkfix - a companion to sphinx's linkcheck builder.\n\nUses the linkcheck's output file to fix links in docs.\n\nOriginally created for this issue:\nhttps://github.com/scrapy/scrapy/issues/606\n\nAuthor: dufferzafar\n\"\"\"\n\nimport re\n\n# Used for remembering the file (and its contents)\n# so we don't have to open the same file again.\n_filename = None\n_contents = None\n\n# A regex that matches standard linkcheck output lines\nline_re = re.compile(u'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n\n# Read lines from the linkcheck output file\ntry:\n    with open(\"build/linkcheck/output.txt\") as out:\n        output_lines = out.readlines()\nexcept IOError:\n    print(\"linkcheck output not found; please run linkcheck first.\")\n    exit(1)\n\n# For every line, fix the respective file\nfor line in output_lines:\n    match = re.match(line_re, line)\n\n    if match:\n        newfilename = match.group(1)\n        errortype = match.group(2)\n\n        # Broken links can't be fixed and\n        # I am not sure what do with the local ones.\n        if errortype.lower() in [\"broken\", \"local\"]:\n            print(\"Not Fixed: \" + line)\n        else:\n            # If this is a new file\n            if newfilename != _filename:\n\n                # Update the previous file\n                if _filename:\n                    with open(_filename, \"w\") as _file:\n                        _file.write(_contents)\n\n                _filename = newfilename\n\n                # Read the new file to memory\n                with open(_filename) as _file:\n                    _contents = _file.read()\n\n            _contents = _contents.replace(match.group(3), match.group(4))\n    else:\n        # We don't understand what the current line means!\n        print(\"Not Understood: \" + line)\n", "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.", "file_name": "linkfix.py", "id": "46ab101e5a1103d5294810a490f463dc", "language": "Python", "project_name": "scrapy", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/scrapy-scrapy/scrapy-scrapy-6a7cdf9/docs/utils/linkfix.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:39:41Z", "url": "https://github.com/scrapy/scrapy", "wiki": true}