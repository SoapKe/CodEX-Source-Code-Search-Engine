{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\"\"\"Module to load the Dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n internal imports\nimport numpy as np\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import utils\n\n\n FFT Specgram Shapes\nSPECGRAM_REGISTRY = {\n    (nfft, hop): shape for nfft, hop, shape in zip(\n        [256, 256, 512, 512, 1024, 1024],\n        [64, 128, 128, 256, 256, 512],\n        [[129, 1001, 2], [129, 501, 2], [257, 501, 2],\n         [257, 251, 2], [513, 251, 2], [513, 126, 2]])\n}\n\n\nclass NSynthDataset(object):\n  \"\"\"Dataset object to help manage the TFRecord loading.\"\"\"\n\n  def __init__(self, tfrecord_path, is_training=True):\n    self.is_training = is_training\n    self.record_path = tfrecord_path\n\n  def get_example(self, batch_size):\n    \"\"\"Get a single example from the tfrecord file.\n\n    Args:\n      batch_size: Int, minibatch size.\n\n    Returns:\n      tf.Example protobuf parsed from tfrecord.\n    \"\"\"\n    reader = tf.TFRecordReader()\n    num_epochs = None if self.is_training else 1\n    capacity = batch_size\n    path_queue = tf.train.input_producer(\n        [self.record_path],\n        num_epochs=num_epochs,\n        shuffle=self.is_training,\n        capacity=capacity)\n    unused_key, serialized_example = reader.read(path_queue)\n    features = {\n        \"note_str\": tf.FixedLenFeature([], dtype=tf.string),\n        \"pitch\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"velocity\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"audio\": tf.FixedLenFeature([64000], dtype=tf.float32),\n        \"qualities\": tf.FixedLenFeature([10], dtype=tf.int64),\n        \"instrument_source\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"instrument_family\": tf.FixedLenFeature([1], dtype=tf.int64),\n    }\n    example = tf.parse_single_example(serialized_example, features)\n    return example\n\n  def get_wavenet_batch(self, batch_size, length=64000):\n    \"\"\"Get the Tensor expressions from the reader.\n\n    Args:\n      batch_size: The integer batch size.\n      length: Number of timesteps of a cropped sample to produce.\n\n    Returns:\n      A dict of key:tensor pairs. This includes \"pitch\", \"wav\", and \"key\".\n    \"\"\"\n    example = self.get_example(batch_size)\n    wav = example[\"audio\"]\n    wav = tf.slice(wav, [0], [64000])\n    pitch = tf.squeeze(example[\"pitch\"])\n    key = tf.squeeze(example[\"note_str\"])\n\n    if self.is_training:\n       random crop\n      crop = tf.random_crop(wav, [length])\n      crop = tf.reshape(crop, [1, length])\n      key, crop, pitch = tf.train.shuffle_batch(\n          [key, crop, pitch],\n          batch_size,\n          num_threads=4,\n          capacity=500 * batch_size,\n          min_after_dequeue=200 * batch_size)\n    else:\n       fixed center crop\n      offset = (64000 - length) // 2   24320\n      crop = tf.slice(wav, [offset], [length])\n      crop = tf.reshape(crop, [1, length])\n      key, crop, pitch = tf.train.shuffle_batch(\n          [key, crop, pitch],\n          batch_size,\n          num_threads=4,\n          capacity=500 * batch_size,\n          min_after_dequeue=200 * batch_size)\n\n    crop = tf.reshape(tf.cast(crop, tf.float32), [batch_size, length])\n    pitch = tf.cast(pitch, tf.int32)\n    return {\"pitch\": pitch, \"wav\": crop, \"key\": key}\n\n  def get_baseline_batch(self, hparams):\n    \"\"\"Get the Tensor expressions from the reader.\n\n    Args:\n      hparams: Hyperparameters object with specgram parameters.\n\n    Returns:\n      A dict of key:tensor pairs. This includes \"pitch\", \"wav\", and \"key\".\n    \"\"\"\n    example = self.get_example(hparams.batch_size)\n    audio = tf.slice(example[\"audio\"], [0], [64000])\n    audio = tf.reshape(audio, [1, 64000])\n    pitch = tf.slice(example[\"pitch\"], [0], [1])\n    velocity = tf.slice(example[\"velocity\"], [0], [1])\n    instrument_source = tf.slice(example[\"instrument_source\"], [0], [1])\n    instrument_family = tf.slice(example[\"instrument_family\"], [0], [1])\n    qualities = tf.slice(example[\"qualities\"], [0], [10])\n    qualities = tf.reshape(qualities, [1, 10])\n\n     Get Specgrams\n    hop_length = hparams.hop_length\n    n_fft = hparams.n_fft\n    if hop_length and n_fft:\n      specgram = utils.tf_specgram(\n          audio,\n          n_fft=n_fft,\n          hop_length=hop_length,\n          mask=hparams.mask,\n          log_mag=hparams.log_mag,\n          re_im=hparams.re_im,\n          dphase=hparams.dphase,\n          mag_only=hparams.mag_only)\n      shape = [1] + SPECGRAM_REGISTRY[(n_fft, hop_length)]\n      if hparams.mag_only:\n        shape[-1] = 1\n      specgram = tf.reshape(specgram, shape)\n      tf.logging.info(\"SPECGRAM BEFORE PADDING\", specgram)\n\n      if hparams.pad:\n         Pad and crop specgram to 256x256\n        num_padding = 2**int(np.ceil(np.log(shape[2]) / np.log(2))) - shape[2]\n        tf.logging.info(\"num_pading: %d\" % num_padding)\n        specgram = tf.reshape(specgram, shape)\n        specgram = tf.pad(specgram, [[0, 0], [0, 0], [0, num_padding], [0, 0]])\n        specgram = tf.slice(specgram, [0, 0, 0, 0], [-1, shape[1] - 1, -1, -1])\n        tf.logging.info(\"SPECGRAM AFTER PADDING\", specgram)\n\n     Form a Batch\n    if self.is_training:\n      (audio, velocity, pitch, specgram,\n       instrument_source, instrument_family,\n       qualities) = tf.train.shuffle_batch(\n           [\n               audio, velocity, pitch, specgram,\n               instrument_source, instrument_family, qualities\n           ],\n           batch_size=hparams.batch_size,\n           capacity=20 * hparams.batch_size,\n           min_after_dequeue=10 * hparams.batch_size,\n           enqueue_many=True)\n    elif hparams.batch_size > 1:\n      (audio, velocity, pitch, specgram,\n       instrument_source, instrument_family, qualities) = tf.train.batch(\n           [\n               audio, velocity, pitch, specgram,\n               instrument_source, instrument_family, qualities\n           ],\n           batch_size=hparams.batch_size,\n           capacity=10 * hparams.batch_size,\n           enqueue_many=True)\n\n    audio.set_shape([hparams.batch_size, 64000])\n\n    batch = dict(\n        pitch=pitch,\n        velocity=velocity,\n        audio=audio,\n        instrument_source=instrument_source,\n        instrument_family=instrument_family,\n        qualities=qualities,\n        spectrogram=specgram)\n\n    return batch\n", "comments": "   module load dataset        future   import absolute import   future   import division   future   import print function    internal imports import numpy np import tensorflow tf  magenta models nsynth import utils     fft specgram shapes specgram registry         (nfft  hop)  shape nfft  hop  shape zip(          256  256  512  512  1024  1024            64  128  128  256  256  512             129  1001  2    129  501  2    257  501  2             257  251  2    513  251  2    513  126  2  )     class nsynthdataset(object)       dataset object help manage tfrecord loading        def   init  (self  tfrecord path  training true)      self training   training     self record path   tfrecord path    def get example(self  batch size)         get single example tfrecord file       args        batch size  int  minibatch size       returns        tf example protobuf parsed tfrecord              reader   tf tfrecordreader()     num epochs   none self training else 1     capacity   batch size     path queue   tf train input producer(          self record path           num epochs num epochs          shuffle self training          capacity capacity)     unused key  serialized example   reader read(path queue)     features              note str   tf fixedlenfeature(    dtype tf string)           pitch   tf fixedlenfeature( 1   dtype tf int64)           velocity   tf fixedlenfeature( 1   dtype tf int64)           audio   tf fixedlenfeature( 64000   dtype tf float32)           qualities   tf fixedlenfeature( 10   dtype tf int64)           instrument source   tf fixedlenfeature( 1   dtype tf int64)           instrument family   tf fixedlenfeature( 1   dtype tf int64)            example   tf parse single example(serialized example  features)     return example    def get wavenet batch(self  batch size  length 64000)         get tensor expressions reader       args        batch size  the integer batch size        length  number timesteps cropped sample produce       returns        a dict key tensor pairs  this includes  pitch    wav    key               example   self get example(batch size)     wav   example  audio       wav   tf slice(wav   0    64000 )     pitch   tf squeeze(example  pitch  )     key   tf squeeze(example  note str  )      self training          random crop       crop   tf random crop(wav   length )       crop   tf reshape(crop   1  length )       key  crop  pitch   tf train shuffle batch(            key  crop  pitch             batch size            num threads 4            capacity 500   batch size            min dequeue 200   batch size)     else          fixed center crop       offset   (64000   length)    2    24320       crop   tf slice(wav   offset    length )       crop   tf reshape(crop   1  length )       key  crop  pitch   tf train shuffle batch(            key  crop  pitch             batch size            num threads 4            capacity 500   batch size            min dequeue 200   batch size)      crop   tf reshape(tf cast(crop  tf float32)   batch size  length )     pitch   tf cast(pitch  tf int32)     return   pitch   pitch   wav   crop   key   key     def get baseline batch(self  hparams)         get tensor expressions reader       args        hparams  hyperparameters object specgram parameters       returns        a dict key tensor pairs  this includes  pitch    wav    key              copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     internal imports    fft specgram shapes    random crop    fixed center crop    24320    get specgrams    pad crop specgram 256x256    form batch ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Module to load the Dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# internal imports\nimport numpy as np\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import utils\n\n\n# FFT Specgram Shapes\nSPECGRAM_REGISTRY = {\n    (nfft, hop): shape for nfft, hop, shape in zip(\n        [256, 256, 512, 512, 1024, 1024],\n        [64, 128, 128, 256, 256, 512],\n        [[129, 1001, 2], [129, 501, 2], [257, 501, 2],\n         [257, 251, 2], [513, 251, 2], [513, 126, 2]])\n}\n\n\nclass NSynthDataset(object):\n  \"\"\"Dataset object to help manage the TFRecord loading.\"\"\"\n\n  def __init__(self, tfrecord_path, is_training=True):\n    self.is_training = is_training\n    self.record_path = tfrecord_path\n\n  def get_example(self, batch_size):\n    \"\"\"Get a single example from the tfrecord file.\n\n    Args:\n      batch_size: Int, minibatch size.\n\n    Returns:\n      tf.Example protobuf parsed from tfrecord.\n    \"\"\"\n    reader = tf.TFRecordReader()\n    num_epochs = None if self.is_training else 1\n    capacity = batch_size\n    path_queue = tf.train.input_producer(\n        [self.record_path],\n        num_epochs=num_epochs,\n        shuffle=self.is_training,\n        capacity=capacity)\n    unused_key, serialized_example = reader.read(path_queue)\n    features = {\n        \"note_str\": tf.FixedLenFeature([], dtype=tf.string),\n        \"pitch\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"velocity\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"audio\": tf.FixedLenFeature([64000], dtype=tf.float32),\n        \"qualities\": tf.FixedLenFeature([10], dtype=tf.int64),\n        \"instrument_source\": tf.FixedLenFeature([1], dtype=tf.int64),\n        \"instrument_family\": tf.FixedLenFeature([1], dtype=tf.int64),\n    }\n    example = tf.parse_single_example(serialized_example, features)\n    return example\n\n  def get_wavenet_batch(self, batch_size, length=64000):\n    \"\"\"Get the Tensor expressions from the reader.\n\n    Args:\n      batch_size: The integer batch size.\n      length: Number of timesteps of a cropped sample to produce.\n\n    Returns:\n      A dict of key:tensor pairs. This includes \"pitch\", \"wav\", and \"key\".\n    \"\"\"\n    example = self.get_example(batch_size)\n    wav = example[\"audio\"]\n    wav = tf.slice(wav, [0], [64000])\n    pitch = tf.squeeze(example[\"pitch\"])\n    key = tf.squeeze(example[\"note_str\"])\n\n    if self.is_training:\n      # random crop\n      crop = tf.random_crop(wav, [length])\n      crop = tf.reshape(crop, [1, length])\n      key, crop, pitch = tf.train.shuffle_batch(\n          [key, crop, pitch],\n          batch_size,\n          num_threads=4,\n          capacity=500 * batch_size,\n          min_after_dequeue=200 * batch_size)\n    else:\n      # fixed center crop\n      offset = (64000 - length) // 2  # 24320\n      crop = tf.slice(wav, [offset], [length])\n      crop = tf.reshape(crop, [1, length])\n      key, crop, pitch = tf.train.shuffle_batch(\n          [key, crop, pitch],\n          batch_size,\n          num_threads=4,\n          capacity=500 * batch_size,\n          min_after_dequeue=200 * batch_size)\n\n    crop = tf.reshape(tf.cast(crop, tf.float32), [batch_size, length])\n    pitch = tf.cast(pitch, tf.int32)\n    return {\"pitch\": pitch, \"wav\": crop, \"key\": key}\n\n  def get_baseline_batch(self, hparams):\n    \"\"\"Get the Tensor expressions from the reader.\n\n    Args:\n      hparams: Hyperparameters object with specgram parameters.\n\n    Returns:\n      A dict of key:tensor pairs. This includes \"pitch\", \"wav\", and \"key\".\n    \"\"\"\n    example = self.get_example(hparams.batch_size)\n    audio = tf.slice(example[\"audio\"], [0], [64000])\n    audio = tf.reshape(audio, [1, 64000])\n    pitch = tf.slice(example[\"pitch\"], [0], [1])\n    velocity = tf.slice(example[\"velocity\"], [0], [1])\n    instrument_source = tf.slice(example[\"instrument_source\"], [0], [1])\n    instrument_family = tf.slice(example[\"instrument_family\"], [0], [1])\n    qualities = tf.slice(example[\"qualities\"], [0], [10])\n    qualities = tf.reshape(qualities, [1, 10])\n\n    # Get Specgrams\n    hop_length = hparams.hop_length\n    n_fft = hparams.n_fft\n    if hop_length and n_fft:\n      specgram = utils.tf_specgram(\n          audio,\n          n_fft=n_fft,\n          hop_length=hop_length,\n          mask=hparams.mask,\n          log_mag=hparams.log_mag,\n          re_im=hparams.re_im,\n          dphase=hparams.dphase,\n          mag_only=hparams.mag_only)\n      shape = [1] + SPECGRAM_REGISTRY[(n_fft, hop_length)]\n      if hparams.mag_only:\n        shape[-1] = 1\n      specgram = tf.reshape(specgram, shape)\n      tf.logging.info(\"SPECGRAM BEFORE PADDING\", specgram)\n\n      if hparams.pad:\n        # Pad and crop specgram to 256x256\n        num_padding = 2**int(np.ceil(np.log(shape[2]) / np.log(2))) - shape[2]\n        tf.logging.info(\"num_pading: %d\" % num_padding)\n        specgram = tf.reshape(specgram, shape)\n        specgram = tf.pad(specgram, [[0, 0], [0, 0], [0, num_padding], [0, 0]])\n        specgram = tf.slice(specgram, [0, 0, 0, 0], [-1, shape[1] - 1, -1, -1])\n        tf.logging.info(\"SPECGRAM AFTER PADDING\", specgram)\n\n    # Form a Batch\n    if self.is_training:\n      (audio, velocity, pitch, specgram,\n       instrument_source, instrument_family,\n       qualities) = tf.train.shuffle_batch(\n           [\n               audio, velocity, pitch, specgram,\n               instrument_source, instrument_family, qualities\n           ],\n           batch_size=hparams.batch_size,\n           capacity=20 * hparams.batch_size,\n           min_after_dequeue=10 * hparams.batch_size,\n           enqueue_many=True)\n    elif hparams.batch_size > 1:\n      (audio, velocity, pitch, specgram,\n       instrument_source, instrument_family, qualities) = tf.train.batch(\n           [\n               audio, velocity, pitch, specgram,\n               instrument_source, instrument_family, qualities\n           ],\n           batch_size=hparams.batch_size,\n           capacity=10 * hparams.batch_size,\n           enqueue_many=True)\n\n    audio.set_shape([hparams.batch_size, 64000])\n\n    batch = dict(\n        pitch=pitch,\n        velocity=velocity,\n        audio=audio,\n        instrument_source=instrument_source,\n        instrument_family=instrument_family,\n        qualities=qualities,\n        spectrogram=specgram)\n\n    return batch\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "reader.py", "id": "454cbcb5477676337ee132d1aaac106e", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-magenta/tensorflow-magenta-c3eda3d/magenta/models/nsynth/reader.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:52:33Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}