{"author": "karpathy", "code": "import argparse\nimport json\nimport time\nimport datetime\nimport numpy as np\nimport code\nimport socket\nimport os\nimport sys\nimport cPickle as pickle\n\nfrom imagernn.data_provider import getDataProvider\nfrom imagernn.solver import Solver\nfrom imagernn.imagernn_utils import decodeGenerator, eval_split\n\ndef preProBuildWordVocab(sentence_iterator, word_count_threshold):\n  \n  \n  print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n  t0 = time.time()\n  word_counts = {}\n  nsents = 0\n  for sent in sentence_iterator:\n    nsents += 1\n    for w in sent['tokens']:\n      word_counts[w] = word_counts.get(w, 0) + 1\n  vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n  print 'filtered words from %d to %d in %.2fs' % (len(word_counts), len(vocab), time.time() - t0)\n\n  \n  # - there are K+1 possible inputs (START token and all the words)\n  # - there are K+1 possible outputs (END token and all the words)\n  \n  \n  ixtoword = {}\n  ixtoword[0] = '.'  \n  wordtoix = {}\n  wordtoix['#START\n  ix = 1\n  for w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\n  \n  # of the labels (words) and how often they occur. We will use this vector to initialize\n  \n  # very quickly (which is just the network learning this anyway, for the most part). This makes\n  \n  \n  word_counts['.'] = nsents\n  bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n  bias_init_vector /= np.sum(bias_init_vector) \n  bias_init_vector = np.log(bias_init_vector)\n  bias_init_vector -= np.max(bias_init_vector) \n  return wordtoix, ixtoword, bias_init_vector\n\ndef RNNGenCost(batch, model, params, misc):\n  \n  regc = params['regc'] \n  BatchGenerator = decodeGenerator(params)\n  wordtoix = misc['wordtoix']\n\n  \n  \n  \n  Ys, gen_caches = BatchGenerator.forward(batch, model, params, misc, predict_mode = False)\n\n  \n  loss_cost = 0.0\n  dYs = []\n  logppl = 0.0\n  logppln = 0\n  for i,pair in enumerate(batch):\n    img = pair['image']\n    \n    gtix = [ wordtoix[w] for w in pair['sentence']['tokens'] if w in wordtoix ]\n    gtix.append(0) \n    \n    Y = Ys[i]\n    maxes = np.amax(Y, axis=1, keepdims=True)\n    e = np.exp(Y - maxes) \n    P = e / np.sum(e, axis=1, keepdims=True)\n    loss_cost += - np.sum(np.log(1e-20 + P[range(len(gtix)),gtix])) \n    logppl += - np.sum(np.log2(1e-20 + P[range(len(gtix)),gtix])) \n    logppln += len(gtix)\n\n    \n    for iy,y in enumerate(gtix):\n      P[iy,y] -= 1 \n    dYs.append(P)\n\n  \n  grads = BatchGenerator.backward(dYs, gen_caches)\n\n  \n  reg_cost = 0.0\n  if regc > 0:    \n    for p in misc['regularize']:\n      mat = model[p]\n      reg_cost += 0.5 * regc * np.sum(mat * mat)\n      grads[p] += regc * mat\n\n  \n  batch_size = len(batch)\n  reg_cost /= batch_size\n  loss_cost /= batch_size\n  for k in grads: grads[k] /= batch_size\n\n  \n  out = {}\n  out['cost'] = {'reg_cost' : reg_cost, 'loss_cost' : loss_cost, 'total_cost' : loss_cost + reg_cost}\n  out['grad'] = grads\n  out['stats'] = { 'ppl2' : 2 ** (logppl / logppln)}\n  return out\n\ndef main(params):\n  batch_size = params['batch_size']\n  dataset = params['dataset']\n  word_count_threshold = params['word_count_threshold']\n  do_grad_check = params['do_grad_check']\n  max_epochs = params['max_epochs']\n  host = socket.gethostname() \n\n  \n  dp = getDataProvider(dataset)\n\n  misc = {} \n\n  \n  \n  misc['wordtoix'], misc['ixtoword'], bias_init_vector = preProBuildWordVocab(dp.iterSentences('train'), word_count_threshold)\n\n  \n  BatchGenerator = decodeGenerator(params)\n  init_struct = BatchGenerator.init(params, misc)\n  model, misc['update'], misc['regularize'] = (init_struct['model'], init_struct['update'], init_struct['regularize'])\n\n  \n  model['bd'] = bias_init_vector.reshape(1, bias_init_vector.size)\n\n  print 'model init done.'\n  print 'model has keys: ' + ', '.join(model.keys())\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['update'])\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['regularize'])\n  print 'number of learnable parameters total: %d' % (sum(model[k].shape[0] * model[k].shape[1] for k in misc['update']), )\n\n  if params.get('init_model_from', ''):\n    \n    checkpoint = pickle.load(open(params['init_model_from'], 'rb'))\n    model = checkpoint['model'] \n\n  \n  solver = Solver()\n  def costfun(batch, model):\n    \n    return RNNGenCost(batch, model, params, misc)\n\n  \n  num_sentences_total = dp.getSplitSize('train', ofwhat = 'sentences')\n  num_iters_one_epoch = num_sentences_total / batch_size\n  max_iters = max_epochs * num_iters_one_epoch\n  eval_period_in_epochs = params['eval_period']\n  eval_period_in_iters = max(1, int(num_iters_one_epoch * eval_period_in_epochs))\n  abort = False\n  top_val_ppl2 = -1\n  smooth_train_ppl2 = len(misc['ixtoword']) \n  val_ppl2 = len(misc['ixtoword'])\n  last_status_write_time = 0 \n  json_worker_status = {}\n  json_worker_status['params'] = params\n  json_worker_status['history'] = []\n  for it in xrange(max_iters):\n    if abort: break\n    t0 = time.time()\n    \n    batch = [dp.sampleImageSentencePair() for i in xrange(batch_size)]\n    \n    step_struct = solver.step(batch, model, costfun, **params)\n    cost = step_struct['cost']\n    dt = time.time() - t0\n\n    \n    train_ppl2 = step_struct['stats']['ppl2']\n    smooth_train_ppl2 = 0.99 * smooth_train_ppl2 + 0.01 * train_ppl2 \n    if it == 0: smooth_train_ppl2 = train_ppl2 \n    epoch = it * 1.0 / num_iters_one_epoch\n    print '%d/%d batch done in %.3fs. at epoch %.2f. loss cost = %f, reg cost = %f, ppl2 = %.2f (smooth %.2f)' \\\n          % (it, max_iters, dt, epoch, cost['loss_cost'], cost['reg_cost'], \\\n             train_ppl2, smooth_train_ppl2)\n\n    # perform gradient check if desired, with a bit of a burnin time (10 iterations)\n    if it == 10 and do_grad_check:\n      print 'disabling dropout for gradient check...'\n      params['drop_prob_encoder'] = 0\n      params['drop_prob_decoder'] = 0\n      solver.gradCheck(batch, model, costfun)\n      print 'done gradcheck, exitting.'\n      sys.exit() \n\n    \n    total_cost = cost['total_cost']\n    if it == 0:\n      total_cost0 = total_cost \n    if total_cost > total_cost0 * 2:\n      print 'Aboring, cost seems to be exploding. Run gradcheck? Lower the learning rate?'\n      abort = True \n\n    \n    tnow = time.time()\n    if tnow > last_status_write_time + 60*1: \n      last_status_write_time = tnow\n      jstatus = {}\n      jstatus['time'] = datetime.datetime.now().isoformat()\n      jstatus['iter'] = (it, max_iters)\n      jstatus['epoch'] = (epoch, max_epochs)\n      jstatus['time_per_batch'] = dt\n      jstatus['smooth_train_ppl2'] = smooth_train_ppl2\n      jstatus['val_ppl2'] = val_ppl2 \n      jstatus['train_ppl2'] = train_ppl2\n      json_worker_status['history'].append(jstatus)\n      status_file = os.path.join(params['worker_status_output_directory'], host + '_status.json')\n      try:\n        json.dump(json_worker_status, open(status_file, 'w'))\n      except Exception, e: \n        print 'tried to write worker status into %s but got error:' % (status_file, )\n        print e\n\n    \n    is_last_iter = (it+1) == max_iters\n    if (((it+1) % eval_period_in_iters) == 0 and it < max_iters - 5) or is_last_iter:\n      val_ppl2 = eval_split('val', dp, model, params, misc) \n      print 'validation perplexity = %f' % (val_ppl2, )\n      \n      \n      min_ppl_or_abort = params['min_ppl_or_abort']\n      if val_ppl2 > min_ppl_or_abort and min_ppl_or_abort > 0:\n        print 'aborting job because validation perplexity %f < %f' % (val_ppl2, min_ppl_or_abort)\n        abort = True \n\n      write_checkpoint_ppl_threshold = params['write_checkpoint_ppl_threshold']\n      if val_ppl2 < top_val_ppl2 or top_val_ppl2 < 0:\n        if val_ppl2 < write_checkpoint_ppl_threshold or write_checkpoint_ppl_threshold < 0:\n          \n          \n          top_val_ppl2 = val_ppl2\n          filename = 'model_checkpoint_%s_%s_%s_%.2f.p' % (dataset, host, params['fappend'], val_ppl2)\n          filepath = os.path.join(params['checkpoint_output_directory'], filename)\n          checkpoint = {}\n          checkpoint['it'] = it\n          checkpoint['epoch'] = epoch\n          checkpoint['model'] = model\n          checkpoint['params'] = params\n          checkpoint['perplexity'] = val_ppl2\n          checkpoint['wordtoix'] = misc['wordtoix']\n          checkpoint['ixtoword'] = misc['ixtoword']\n          try:\n            pickle.dump(checkpoint, open(filepath, \"wb\"))\n            print 'saved checkpoint in %s' % (filepath, )\n          except Exception, e: \n            print 'tried to write checkpoint into %s but got error: ' % (filepat, )\n            print e\n\n\nif __name__ == \"__main__\":\n\n  parser = argparse.ArgumentParser()\n\n  \n  parser.add_argument('-d', '--dataset', dest='dataset', default='flickr8k', help='dataset: flickr8k/flickr30k')\n  parser.add_argument('-a', '--do_grad_check', dest='do_grad_check', type=int, default=0, help='perform gradcheck? program will block for visual inspection and will need manual user input')\n  parser.add_argument('--fappend', dest='fappend', type=str, default='baseline', help='append this string to checkpoint filenames')\n  parser.add_argument('-o', '--checkpoint_output_directory', dest='checkpoint_output_directory', type=str, default='cv/', help='output directory to write checkpoints to')\n  parser.add_argument('--worker_status_output_directory', dest='worker_status_output_directory', type=str, default='status/', help='directory to write worker status JSON blobs to')\n  parser.add_argument('--write_checkpoint_ppl_threshold', dest='write_checkpoint_ppl_threshold', type=float, default=-1, help='ppl threshold above which we dont bother writing a checkpoint to save space')\n  parser.add_argument('--init_model_from', dest='init_model_from', type=str, default='', help='initialize the model parameters from some specific checkpoint?')\n  \n  \n  parser.add_argument('--generator', dest='generator', type=str, default='lstm', help='generator to use')\n  parser.add_argument('--image_encoding_size', dest='image_encoding_size', type=int, default=256, help='size of the image encoding')\n  parser.add_argument('--word_encoding_size', dest='word_encoding_size', type=int, default=256, help='size of word encoding')\n  parser.add_argument('--hidden_size', dest='hidden_size', type=int, default=256, help='size of hidden layer in generator RNNs')\n  \n  parser.add_argument('--tanhC_version', dest='tanhC_version', type=int, default=0, help='use tanh version of LSTM?')\n  \n  parser.add_argument('--rnn_relu_encoders', dest='rnn_relu_encoders', type=int, default=0, help='relu encoders before going to RNN?')\n  parser.add_argument('--rnn_feed_once', dest='rnn_feed_once', type=int, default=0, help='feed image to the rnn only single time?')\n\n  \n  parser.add_argument('-c', '--regc', dest='regc', type=float, default=1e-8, help='regularization strength')\n  parser.add_argument('-m', '--max_epochs', dest='max_epochs', type=int, default=50, help='number of epochs to train for')\n  parser.add_argument('--solver', dest='solver', type=str, default='rmsprop', help='solver type: vanilla/adagrad/adadelta/rmsprop')\n  parser.add_argument('--momentum', dest='momentum', type=float, default=0.0, help='momentum for vanilla sgd')\n  parser.add_argument('--decay_rate', dest='decay_rate', type=float, default=0.999, help='decay rate for adadelta/rmsprop')\n  parser.add_argument('--smooth_eps', dest='smooth_eps', type=float, default=1e-8, help='epsilon smoothing for rmsprop/adagrad/adadelta')\n  parser.add_argument('-l', '--learning_rate', dest='learning_rate', type=float, default=1e-3, help='solver learning rate')\n  parser.add_argument('-b', '--batch_size', dest='batch_size', type=int, default=100, help='batch size')\n  parser.add_argument('--grad_clip', dest='grad_clip', type=float, default=5, help='clip gradients (normalized by batch size)? elementwise. if positive, at what threshold?')\n  parser.add_argument('--drop_prob_encoder', dest='drop_prob_encoder', type=float, default=0.5, help='what dropout to apply right after the encoder to an RNN/LSTM')\n  parser.add_argument('--drop_prob_decoder', dest='drop_prob_decoder', type=float, default=0.5, help='what dropout to apply right before the decoder in an RNN/LSTM')\n\n  \n  parser.add_argument('--word_count_threshold', dest='word_count_threshold', type=int, default=5, help='if a word occurs less than this number of times in training data, it is discarded')\n\n  \n  parser.add_argument('-p', '--eval_period', dest='eval_period', type=float, default=1.0, help='in units of epochs, how often do we evaluate on val set?')\n  parser.add_argument('--eval_batch_size', dest='eval_batch_size', type=int, default=100, help='for faster validation performance evaluation, what batch size to use on val img/sentences?')\n  parser.add_argument('--eval_max_images', dest='eval_max_images', type=int, default=-1, help='for efficiency we can use a smaller number of images to get validation error')\n  parser.add_argument('--min_ppl_or_abort', dest='min_ppl_or_abort', type=float , default=-1, help='if validation perplexity is below this threshold the job will abort')\n\n  args = parser.parse_args()\n  params = vars(args) \n  print 'parsed parameters:'\n  print json.dumps(params, indent = 2)\n  main(params)", "comments": "    cost function  returns cost gradients model        count word counts threshold    shouldnt expensive operation    k distinct words       k 1 possible inputs (start token words)      k 1 possible outputs (end token words)    use ixtoword take predicted indeces map words output visualization    use wordtoix take raw words get index word vector matrix    period end sentence  make first dimension end token        0   make first vector start token    compute bias vector  related log probability distribution    labels (words) often occur  we use vector initialize    decoder weights  loss function doesnt show huge increase performance    quickly (which network learning anyway  part)  this makes    visualizations cost function nicer look like hockey stick     example flickr8k  brings initial perplexity  2500  170     normalize frequencies    shift nice numeric range    regularization cost    forward rnn image sentence pair    generator returns list matrices word probabilities    list cache objects needed backprop    compute softmax costs generated sentences  gradients top    ground truth indeces sentence expect see    forget end token must predicted end     fetch predicted probabilities  rows    numerical stability shift good numerical range    note  add smoothing get infs    also accumulate log2 perplexities    lets clever optimize speed derive gradient place quickly    softmax derivatives pretty simple    backprop rnn    add l2 regularization cost gradients    normalize cost gradient batch size    return output json    get computer hostname    fetch data provider    stores various misc items need passed around framework    go training sentences find vocabulary want use  e  words occur    least word count threshold number times    delegate initialization model generator class    force overwrite  this bit hack  happy    load checkpoint    overwrite model    initialize solver cost function    wrap cost function abstract things away solver    calculate many iterations need    initially size dictionary confusion    writing worker job status reports    fetch batch data    evaluate cost  gradient perform parameter update    print training statistics    smooth exponentially decaying moving average    start start    perform gradient check desired  bit burnin time (10 iterations)    hmmm  probably exit    detect loss exploding kill job    store initial cost    set abort flag  break    logging  write json files visual inspection training    every lets write report    write last available one    todo clever    perform perplexity evaluation validation set save model checkpoint good    perform evaluation val set    abort training perplexity good    abort job    beat previous record first time    and also beat user defined threshold doesnt exist    todo clever    global setup settings  checkpoints    model parameters    lstm specific params    rnn specific params    optimization parameters    data preprocessing parameters    evaluation parameters    convert ordinary dict ", "content": "import argparse\nimport json\nimport time\nimport datetime\nimport numpy as np\nimport code\nimport socket\nimport os\nimport sys\nimport cPickle as pickle\n\nfrom imagernn.data_provider import getDataProvider\nfrom imagernn.solver import Solver\nfrom imagernn.imagernn_utils import decodeGenerator, eval_split\n\ndef preProBuildWordVocab(sentence_iterator, word_count_threshold):\n  # count up all word counts so that we can threshold\n  # this shouldnt be too expensive of an operation\n  print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n  t0 = time.time()\n  word_counts = {}\n  nsents = 0\n  for sent in sentence_iterator:\n    nsents += 1\n    for w in sent['tokens']:\n      word_counts[w] = word_counts.get(w, 0) + 1\n  vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n  print 'filtered words from %d to %d in %.2fs' % (len(word_counts), len(vocab), time.time() - t0)\n\n  # with K distinct words:\n  # - there are K+1 possible inputs (START token and all the words)\n  # - there are K+1 possible outputs (END token and all the words)\n  # we use ixtoword to take predicted indeces and map them to words for output visualization\n  # we use wordtoix to take raw words and get their index in word vector matrix\n  ixtoword = {}\n  ixtoword[0] = '.'  # period at the end of the sentence. make first dimension be end token\n  wordtoix = {}\n  wordtoix['#START#'] = 0 # make first vector be the start token\n  ix = 1\n  for w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\n  # compute bias vector, which is related to the log probability of the distribution\n  # of the labels (words) and how often they occur. We will use this vector to initialize\n  # the decoder weights, so that the loss function doesnt show a huge increase in performance\n  # very quickly (which is just the network learning this anyway, for the most part). This makes\n  # the visualizations of the cost function nicer because it doesn't look like a hockey stick.\n  # for example on Flickr8K, doing this brings down initial perplexity from ~2500 to ~170.\n  word_counts['.'] = nsents\n  bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n  bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n  bias_init_vector = np.log(bias_init_vector)\n  bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n  return wordtoix, ixtoword, bias_init_vector\n\ndef RNNGenCost(batch, model, params, misc):\n  \"\"\" cost function, returns cost and gradients for model \"\"\"\n  regc = params['regc'] # regularization cost\n  BatchGenerator = decodeGenerator(params)\n  wordtoix = misc['wordtoix']\n\n  # forward the RNN on each image sentence pair\n  # the generator returns a list of matrices that have word probabilities\n  # and a list of cache objects that will be needed for backprop\n  Ys, gen_caches = BatchGenerator.forward(batch, model, params, misc, predict_mode = False)\n\n  # compute softmax costs for all generated sentences, and the gradients on top\n  loss_cost = 0.0\n  dYs = []\n  logppl = 0.0\n  logppln = 0\n  for i,pair in enumerate(batch):\n    img = pair['image']\n    # ground truth indeces for this sentence we expect to see\n    gtix = [ wordtoix[w] for w in pair['sentence']['tokens'] if w in wordtoix ]\n    gtix.append(0) # don't forget END token must be predicted in the end!\n    # fetch the predicted probabilities, as rows\n    Y = Ys[i]\n    maxes = np.amax(Y, axis=1, keepdims=True)\n    e = np.exp(Y - maxes) # for numerical stability shift into good numerical range\n    P = e / np.sum(e, axis=1, keepdims=True)\n    loss_cost += - np.sum(np.log(1e-20 + P[range(len(gtix)),gtix])) # note: add smoothing to not get infs\n    logppl += - np.sum(np.log2(1e-20 + P[range(len(gtix)),gtix])) # also accumulate log2 perplexities\n    logppln += len(gtix)\n\n    # lets be clever and optimize for speed here to derive the gradient in place quickly\n    for iy,y in enumerate(gtix):\n      P[iy,y] -= 1 # softmax derivatives are pretty simple\n    dYs.append(P)\n\n  # backprop the RNN\n  grads = BatchGenerator.backward(dYs, gen_caches)\n\n  # add L2 regularization cost and gradients\n  reg_cost = 0.0\n  if regc > 0:    \n    for p in misc['regularize']:\n      mat = model[p]\n      reg_cost += 0.5 * regc * np.sum(mat * mat)\n      grads[p] += regc * mat\n\n  # normalize the cost and gradient by the batch size\n  batch_size = len(batch)\n  reg_cost /= batch_size\n  loss_cost /= batch_size\n  for k in grads: grads[k] /= batch_size\n\n  # return output in json\n  out = {}\n  out['cost'] = {'reg_cost' : reg_cost, 'loss_cost' : loss_cost, 'total_cost' : loss_cost + reg_cost}\n  out['grad'] = grads\n  out['stats'] = { 'ppl2' : 2 ** (logppl / logppln)}\n  return out\n\ndef main(params):\n  batch_size = params['batch_size']\n  dataset = params['dataset']\n  word_count_threshold = params['word_count_threshold']\n  do_grad_check = params['do_grad_check']\n  max_epochs = params['max_epochs']\n  host = socket.gethostname() # get computer hostname\n\n  # fetch the data provider\n  dp = getDataProvider(dataset)\n\n  misc = {} # stores various misc items that need to be passed around the framework\n\n  # go over all training sentences and find the vocabulary we want to use, i.e. the words that occur\n  # at least word_count_threshold number of times\n  misc['wordtoix'], misc['ixtoword'], bias_init_vector = preProBuildWordVocab(dp.iterSentences('train'), word_count_threshold)\n\n  # delegate the initialization of the model to the Generator class\n  BatchGenerator = decodeGenerator(params)\n  init_struct = BatchGenerator.init(params, misc)\n  model, misc['update'], misc['regularize'] = (init_struct['model'], init_struct['update'], init_struct['regularize'])\n\n  # force overwrite here. This is a bit of a hack, not happy about it\n  model['bd'] = bias_init_vector.reshape(1, bias_init_vector.size)\n\n  print 'model init done.'\n  print 'model has keys: ' + ', '.join(model.keys())\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['update'])\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['regularize'])\n  print 'number of learnable parameters total: %d' % (sum(model[k].shape[0] * model[k].shape[1] for k in misc['update']), )\n\n  if params.get('init_model_from', ''):\n    # load checkpoint\n    checkpoint = pickle.load(open(params['init_model_from'], 'rb'))\n    model = checkpoint['model'] # overwrite the model\n\n  # initialize the Solver and the cost function\n  solver = Solver()\n  def costfun(batch, model):\n    # wrap the cost function to abstract some things away from the Solver\n    return RNNGenCost(batch, model, params, misc)\n\n  # calculate how many iterations we need\n  num_sentences_total = dp.getSplitSize('train', ofwhat = 'sentences')\n  num_iters_one_epoch = num_sentences_total / batch_size\n  max_iters = max_epochs * num_iters_one_epoch\n  eval_period_in_epochs = params['eval_period']\n  eval_period_in_iters = max(1, int(num_iters_one_epoch * eval_period_in_epochs))\n  abort = False\n  top_val_ppl2 = -1\n  smooth_train_ppl2 = len(misc['ixtoword']) # initially size of dictionary of confusion\n  val_ppl2 = len(misc['ixtoword'])\n  last_status_write_time = 0 # for writing worker job status reports\n  json_worker_status = {}\n  json_worker_status['params'] = params\n  json_worker_status['history'] = []\n  for it in xrange(max_iters):\n    if abort: break\n    t0 = time.time()\n    # fetch a batch of data\n    batch = [dp.sampleImageSentencePair() for i in xrange(batch_size)]\n    # evaluate cost, gradient and perform parameter update\n    step_struct = solver.step(batch, model, costfun, **params)\n    cost = step_struct['cost']\n    dt = time.time() - t0\n\n    # print training statistics\n    train_ppl2 = step_struct['stats']['ppl2']\n    smooth_train_ppl2 = 0.99 * smooth_train_ppl2 + 0.01 * train_ppl2 # smooth exponentially decaying moving average\n    if it == 0: smooth_train_ppl2 = train_ppl2 # start out where we start out\n    epoch = it * 1.0 / num_iters_one_epoch\n    print '%d/%d batch done in %.3fs. at epoch %.2f. loss cost = %f, reg cost = %f, ppl2 = %.2f (smooth %.2f)' \\\n          % (it, max_iters, dt, epoch, cost['loss_cost'], cost['reg_cost'], \\\n             train_ppl2, smooth_train_ppl2)\n\n    # perform gradient check if desired, with a bit of a burnin time (10 iterations)\n    if it == 10 and do_grad_check:\n      print 'disabling dropout for gradient check...'\n      params['drop_prob_encoder'] = 0\n      params['drop_prob_decoder'] = 0\n      solver.gradCheck(batch, model, costfun)\n      print 'done gradcheck, exitting.'\n      sys.exit() # hmmm. probably should exit here\n\n    # detect if loss is exploding and kill the job if so\n    total_cost = cost['total_cost']\n    if it == 0:\n      total_cost0 = total_cost # store this initial cost\n    if total_cost > total_cost0 * 2:\n      print 'Aboring, cost seems to be exploding. Run gradcheck? Lower the learning rate?'\n      abort = True # set the abort flag, we'll break out\n\n    # logging: write JSON files for visual inspection of the training\n    tnow = time.time()\n    if tnow > last_status_write_time + 60*1: # every now and then lets write a report\n      last_status_write_time = tnow\n      jstatus = {}\n      jstatus['time'] = datetime.datetime.now().isoformat()\n      jstatus['iter'] = (it, max_iters)\n      jstatus['epoch'] = (epoch, max_epochs)\n      jstatus['time_per_batch'] = dt\n      jstatus['smooth_train_ppl2'] = smooth_train_ppl2\n      jstatus['val_ppl2'] = val_ppl2 # just write the last available one\n      jstatus['train_ppl2'] = train_ppl2\n      json_worker_status['history'].append(jstatus)\n      status_file = os.path.join(params['worker_status_output_directory'], host + '_status.json')\n      try:\n        json.dump(json_worker_status, open(status_file, 'w'))\n      except Exception, e: # todo be more clever here\n        print 'tried to write worker status into %s but got error:' % (status_file, )\n        print e\n\n    # perform perplexity evaluation on the validation set and save a model checkpoint if it's good\n    is_last_iter = (it+1) == max_iters\n    if (((it+1) % eval_period_in_iters) == 0 and it < max_iters - 5) or is_last_iter:\n      val_ppl2 = eval_split('val', dp, model, params, misc) # perform the evaluation on VAL set\n      print 'validation perplexity = %f' % (val_ppl2, )\n      \n      # abort training if the perplexity is no good\n      min_ppl_or_abort = params['min_ppl_or_abort']\n      if val_ppl2 > min_ppl_or_abort and min_ppl_or_abort > 0:\n        print 'aborting job because validation perplexity %f < %f' % (val_ppl2, min_ppl_or_abort)\n        abort = True # abort the job\n\n      write_checkpoint_ppl_threshold = params['write_checkpoint_ppl_threshold']\n      if val_ppl2 < top_val_ppl2 or top_val_ppl2 < 0:\n        if val_ppl2 < write_checkpoint_ppl_threshold or write_checkpoint_ppl_threshold < 0:\n          # if we beat a previous record or if this is the first time\n          # AND we also beat the user-defined threshold or it doesnt exist\n          top_val_ppl2 = val_ppl2\n          filename = 'model_checkpoint_%s_%s_%s_%.2f.p' % (dataset, host, params['fappend'], val_ppl2)\n          filepath = os.path.join(params['checkpoint_output_directory'], filename)\n          checkpoint = {}\n          checkpoint['it'] = it\n          checkpoint['epoch'] = epoch\n          checkpoint['model'] = model\n          checkpoint['params'] = params\n          checkpoint['perplexity'] = val_ppl2\n          checkpoint['wordtoix'] = misc['wordtoix']\n          checkpoint['ixtoword'] = misc['ixtoword']\n          try:\n            pickle.dump(checkpoint, open(filepath, \"wb\"))\n            print 'saved checkpoint in %s' % (filepath, )\n          except Exception, e: # todo be more clever here\n            print 'tried to write checkpoint into %s but got error: ' % (filepat, )\n            print e\n\n\nif __name__ == \"__main__\":\n\n  parser = argparse.ArgumentParser()\n\n  # global setup settings, and checkpoints\n  parser.add_argument('-d', '--dataset', dest='dataset', default='flickr8k', help='dataset: flickr8k/flickr30k')\n  parser.add_argument('-a', '--do_grad_check', dest='do_grad_check', type=int, default=0, help='perform gradcheck? program will block for visual inspection and will need manual user input')\n  parser.add_argument('--fappend', dest='fappend', type=str, default='baseline', help='append this string to checkpoint filenames')\n  parser.add_argument('-o', '--checkpoint_output_directory', dest='checkpoint_output_directory', type=str, default='cv/', help='output directory to write checkpoints to')\n  parser.add_argument('--worker_status_output_directory', dest='worker_status_output_directory', type=str, default='status/', help='directory to write worker status JSON blobs to')\n  parser.add_argument('--write_checkpoint_ppl_threshold', dest='write_checkpoint_ppl_threshold', type=float, default=-1, help='ppl threshold above which we dont bother writing a checkpoint to save space')\n  parser.add_argument('--init_model_from', dest='init_model_from', type=str, default='', help='initialize the model parameters from some specific checkpoint?')\n  \n  # model parameters\n  parser.add_argument('--generator', dest='generator', type=str, default='lstm', help='generator to use')\n  parser.add_argument('--image_encoding_size', dest='image_encoding_size', type=int, default=256, help='size of the image encoding')\n  parser.add_argument('--word_encoding_size', dest='word_encoding_size', type=int, default=256, help='size of word encoding')\n  parser.add_argument('--hidden_size', dest='hidden_size', type=int, default=256, help='size of hidden layer in generator RNNs')\n  # lstm-specific params\n  parser.add_argument('--tanhC_version', dest='tanhC_version', type=int, default=0, help='use tanh version of LSTM?')\n  # rnn-specific params\n  parser.add_argument('--rnn_relu_encoders', dest='rnn_relu_encoders', type=int, default=0, help='relu encoders before going to RNN?')\n  parser.add_argument('--rnn_feed_once', dest='rnn_feed_once', type=int, default=0, help='feed image to the rnn only single time?')\n\n  # optimization parameters\n  parser.add_argument('-c', '--regc', dest='regc', type=float, default=1e-8, help='regularization strength')\n  parser.add_argument('-m', '--max_epochs', dest='max_epochs', type=int, default=50, help='number of epochs to train for')\n  parser.add_argument('--solver', dest='solver', type=str, default='rmsprop', help='solver type: vanilla/adagrad/adadelta/rmsprop')\n  parser.add_argument('--momentum', dest='momentum', type=float, default=0.0, help='momentum for vanilla sgd')\n  parser.add_argument('--decay_rate', dest='decay_rate', type=float, default=0.999, help='decay rate for adadelta/rmsprop')\n  parser.add_argument('--smooth_eps', dest='smooth_eps', type=float, default=1e-8, help='epsilon smoothing for rmsprop/adagrad/adadelta')\n  parser.add_argument('-l', '--learning_rate', dest='learning_rate', type=float, default=1e-3, help='solver learning rate')\n  parser.add_argument('-b', '--batch_size', dest='batch_size', type=int, default=100, help='batch size')\n  parser.add_argument('--grad_clip', dest='grad_clip', type=float, default=5, help='clip gradients (normalized by batch size)? elementwise. if positive, at what threshold?')\n  parser.add_argument('--drop_prob_encoder', dest='drop_prob_encoder', type=float, default=0.5, help='what dropout to apply right after the encoder to an RNN/LSTM')\n  parser.add_argument('--drop_prob_decoder', dest='drop_prob_decoder', type=float, default=0.5, help='what dropout to apply right before the decoder in an RNN/LSTM')\n\n  # data preprocessing parameters\n  parser.add_argument('--word_count_threshold', dest='word_count_threshold', type=int, default=5, help='if a word occurs less than this number of times in training data, it is discarded')\n\n  # evaluation parameters\n  parser.add_argument('-p', '--eval_period', dest='eval_period', type=float, default=1.0, help='in units of epochs, how often do we evaluate on val set?')\n  parser.add_argument('--eval_batch_size', dest='eval_batch_size', type=int, default=100, help='for faster validation performance evaluation, what batch size to use on val img/sentences?')\n  parser.add_argument('--eval_max_images', dest='eval_max_images', type=int, default=-1, help='for efficiency we can use a smaller number of images to get validation error')\n  parser.add_argument('--min_ppl_or_abort', dest='min_ppl_or_abort', type=float , default=-1, help='if validation perplexity is below this threshold the job will abort')\n\n  args = parser.parse_args()\n  params = vars(args) # convert to ordinary dict\n  print 'parsed parameters:'\n  print json.dumps(params, indent = 2)\n  main(params)", "description": "NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.", "file_name": "driver.py", "id": "1ead9bd0690f58d58b03316848c0806a", "language": "Python", "project_name": "neuraltalk", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/karpathy-neuraltalk/karpathy-neuraltalk-c36bd0a/driver.py", "save_time": "", "source": "", "update_at": "2018-03-17T13:13:54Z", "url": "https://github.com/karpathy/neuraltalk", "wiki": true}