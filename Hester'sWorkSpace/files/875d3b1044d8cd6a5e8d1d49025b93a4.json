{"author": "spotify", "code": " -*- coding: utf-8 -*-\n\n Copyright 2012-2015 Spotify AB\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n\nimport random\n\nimport luigi\nimport luigi.format\nimport luigi.contrib.hdfs\nfrom luigi.contrib.spark import SparkSubmitTask\n\n\nclass UserItemMatrix(luigi.Task):\n\n    : the size of the data being generated\n    data_size = luigi.IntParameter()\n\n    def run(self):\n        \"\"\"\n        Generates :py:attr:`~.UserItemMatrix.data_size` elements.\n        Writes this data in \\ separated value format into the target :py:func:`~/.UserItemMatrix.output`.\n\n        The data has the following elements:\n\n        * `user` is the default Elasticsearch id field,\n        * `track`: the text,\n        * `rating`: the day when the data was created.\n\n        \"\"\"\n        w = self.output().open('w')\n        for user in range(self.data_size):\n            track = int(random.random() * self.data_size)\n            w.write('%d\\%d\\%f' % (user, track, 1.0))\n        w.close()\n\n    def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('data-matrix', format=luigi.format.Gzip)\n\n\nclass SparkALS(SparkSubmitTask):\n    \"\"\"\n    This task runs a :py:class:`luigi.contrib.spark.SparkSubmitTask` task\n    over the target data returned by :py:meth:`~/.UserItemMatrix.output` and\n    writes the result into its :py:meth:`~.SparkALS.output` target (a file in HDFS).\n\n    This class uses :py:meth:`luigi.contrib.spark.SparkSubmitTask.run`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: yarn-client\n\n    \"\"\"\n    data_size = luigi.IntParameter(default=1000)\n\n    driver_memory = '2g'\n    executor_memory = '3g'\n    num_executors = luigi.IntParameter(default=100)\n\n    app = 'my-spark-assembly.jar'\n    entry_class = 'com.spotify.spark.ImplicitALS'\n\n    def app_options(self):\n         These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]\n\n    def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.UserItemMatrix`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return UserItemMatrix(self.data_size)\n\n    def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n         The corresponding Spark job outputs as GZip format.\n        return luigi.contrib.hdfs.HdfsTarget('als-output/', format=luigi.format.Gzip)\n\n\n'''\n// Corresponding example Spark Job, a wrapper around the MLLib ALS job.\n// This class would have to be jarred into my-spark-assembly.jar\n// using sbt assembly (or package) and made available to the Luigi job\n// above.\n\npackage com.spotify.spark\n\nimport org.apache.spark._\nimport org.apache.spark.mllib.recommendation.{Rating, ALS}\nimport org.apache.hadoop.io.compress.GzipCodec\n\nobject ImplicitALS {\n\n  def main(args: Array[String]) {\n    val sc = new SparkContext(args(0), \"ImplicitALS\")\n    val input = args(1)\n    val output = args(2)\n\n    val ratings = sc.textFile(input)\n      .map { l: String =>\n        val t = l.split('\\t')\n        Rating(t(0).toInt, t(1).toInt, t(2).toFloat)\n      }\n\n    val model = ALS.trainImplicit(ratings, 40, 20, 0.8, 150)\n    model\n      .productFeatures\n      .map { case (id, vec) =>\n        id + \"\\t\" + vec.map(d => \"%.6f\".format(d)).mkString(\" \")\n      }\n      .saveAsTextFile(output, classOf[GzipCodec])\n\n    sc.stop()\n  }\n}\n'''\n", "comments": "            generates  py attr    useritemmatrix data size  elements          writes data   separated value format target  py func     useritemmatrix output            the data following elements              user  default elasticsearch id field             track   text             rating   day data created                       w   self output() open( w )         user range(self data size)              track   int(random random()   self data size)             w write(    f    (user  track  1 0))         w close()      def output(self)                      returns target output task          in case  successful execution task create file hdfs            return  target output task           rtype  object ( py class   luigi target target )                     return luigi contrib hdfs hdfstarget( data matrix   format luigi format gzip)   class sparkals(sparksubmittask)              this task runs  py class  luigi contrib spark sparksubmittask  task     target data returned  py meth     useritemmatrix output      writes result  py meth    sparkals output  target (a file hdfs)       this class uses  py meth  luigi contrib spark sparksubmittask run        example luigi configuration             spark          spark submit   usr local spark bin spark submit         master  yarn client              data size   luigi intparameter(default 1000)      driver memory    2g      executor memory    3g      num executors   luigi intparameter(default 100)      app    spark assembly jar      entry class    com spotify spark implicitals       def app options(self)            these passed spark main args defined order          return  self input() path  self output() path       def requires(self)                      this task dependencies              py class    useritemmatrix            return  object ( py class  luigi task task )                     return useritemmatrix(self data size)      def output(self)                      returns target output task          in case  successful execution task create file hdfs            return  target output task           rtype  object ( py class   luigi target target )                     corresponding example spark job  wrapper around mllib als job     this class would jarred spark assembly jar    using sbt assembly (or package) made available luigi job      package com spotify spark  import org apache spark   import org apache spark mllib recommendation  rating  als  import org apache hadoop io compress gzipcodec  object implicitals      def main(args  array string )       val sc   new sparkcontext(args(0)   implicitals )     val input   args(1)     val output   args(2)      val ratings   sc textfile(input)        map   l  string            val   l split(  )         rating(t(0) toint  t(1) toint  t(2) tofloat)              val model   als trainimplicit(ratings  40  20  0 8  150)     model        productfeatures        map   case (id  vec)            id        vec map(d       6f  format(d)) mkstring(   )                saveastextfile(output  classof gzipcodec )      sc stop()                  coding  utf 8           copyright 2012 2015 spotify ab       licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license       http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license         size data generated    these passed spark main args defined order     the corresponding spark job outputs gzip format  ", "content": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\n\nimport luigi\nimport luigi.format\nimport luigi.contrib.hdfs\nfrom luigi.contrib.spark import SparkSubmitTask\n\n\nclass UserItemMatrix(luigi.Task):\n\n    #: the size of the data being generated\n    data_size = luigi.IntParameter()\n\n    def run(self):\n        \"\"\"\n        Generates :py:attr:`~.UserItemMatrix.data_size` elements.\n        Writes this data in \\ separated value format into the target :py:func:`~/.UserItemMatrix.output`.\n\n        The data has the following elements:\n\n        * `user` is the default Elasticsearch id field,\n        * `track`: the text,\n        * `rating`: the day when the data was created.\n\n        \"\"\"\n        w = self.output().open('w')\n        for user in range(self.data_size):\n            track = int(random.random() * self.data_size)\n            w.write('%d\\%d\\%f' % (user, track, 1.0))\n        w.close()\n\n    def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        return luigi.contrib.hdfs.HdfsTarget('data-matrix', format=luigi.format.Gzip)\n\n\nclass SparkALS(SparkSubmitTask):\n    \"\"\"\n    This task runs a :py:class:`luigi.contrib.spark.SparkSubmitTask` task\n    over the target data returned by :py:meth:`~/.UserItemMatrix.output` and\n    writes the result into its :py:meth:`~.SparkALS.output` target (a file in HDFS).\n\n    This class uses :py:meth:`luigi.contrib.spark.SparkSubmitTask.run`.\n\n    Example luigi configuration::\n\n        [spark]\n        spark-submit: /usr/local/spark/bin/spark-submit\n        master: yarn-client\n\n    \"\"\"\n    data_size = luigi.IntParameter(default=1000)\n\n    driver_memory = '2g'\n    executor_memory = '3g'\n    num_executors = luigi.IntParameter(default=100)\n\n    app = 'my-spark-assembly.jar'\n    entry_class = 'com.spotify.spark.ImplicitALS'\n\n    def app_options(self):\n        # These are passed to the Spark main args in the defined order.\n        return [self.input().path, self.output().path]\n\n    def requires(self):\n        \"\"\"\n        This task's dependencies:\n\n        * :py:class:`~.UserItemMatrix`\n\n        :return: object (:py:class:`luigi.task.Task`)\n        \"\"\"\n        return UserItemMatrix(self.data_size)\n\n    def output(self):\n        \"\"\"\n        Returns the target output for this task.\n        In this case, a successful execution of this task will create a file in HDFS.\n\n        :return: the target output for this task.\n        :rtype: object (:py:class:`~luigi.target.Target`)\n        \"\"\"\n        # The corresponding Spark job outputs as GZip format.\n        return luigi.contrib.hdfs.HdfsTarget('als-output/', format=luigi.format.Gzip)\n\n\n'''\n// Corresponding example Spark Job, a wrapper around the MLLib ALS job.\n// This class would have to be jarred into my-spark-assembly.jar\n// using sbt assembly (or package) and made available to the Luigi job\n// above.\n\npackage com.spotify.spark\n\nimport org.apache.spark._\nimport org.apache.spark.mllib.recommendation.{Rating, ALS}\nimport org.apache.hadoop.io.compress.GzipCodec\n\nobject ImplicitALS {\n\n  def main(args: Array[String]) {\n    val sc = new SparkContext(args(0), \"ImplicitALS\")\n    val input = args(1)\n    val output = args(2)\n\n    val ratings = sc.textFile(input)\n      .map { l: String =>\n        val t = l.split('\\t')\n        Rating(t(0).toInt, t(1).toInt, t(2).toFloat)\n      }\n\n    val model = ALS.trainImplicit(ratings, 40, 20, 0.8, 150)\n    model\n      .productFeatures\n      .map { case (id, vec) =>\n        id + \"\\t\" + vec.map(d => \"%.6f\".format(d)).mkString(\" \")\n      }\n      .saveAsTextFile(output, classOf[GzipCodec])\n\n    sc.stop()\n  }\n}\n'''\n", "description": "Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. ", "file_name": "spark_als.py", "id": "875d3b1044d8cd6a5e8d1d49025b93a4", "language": "Python", "project_name": "luigi", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/spotify-luigi/spotify-luigi-3cf763d/examples/spark_als.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:09:25Z", "url": "https://github.com/spotify/luigi", "wiki": false}