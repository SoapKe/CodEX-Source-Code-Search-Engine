{"author": "yunjey", "code": "import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.003\n\n\ntrain_dataset = dsets.MNIST(root='./data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# BiRNN Model (Many-to-One)\nclass BiRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BiRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                            batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size*2, num_classes)  \n    \n    def forward(self, x):\n        \n        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).cuda() \n        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).cuda()\n        \n        \n        out, _ = self.lstm(x, (h0, c0))\n        \n        \n        out = self.fc(out[:, -1, :])\n        return out\n\nrnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\nrnn.cuda()\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size)).cuda()\n        labels = Variable(labels).cuda()\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size)).cuda()\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted.cpu() == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n\ntorch.save(rnn.state_dict(), 'rnn.pkl')", "comments": "  hyper parameters    mnist dataset    data loader (input pipeline)    birnn model (many one)    2 bidirection     set initial states    2 bidirection     forward propagate rnn    decode hidden state last time step    loss optimizer    train model    forward   backward   optimize    test model    save model ", "content": "import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.003\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='./data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# BiRNN Model (Many-to-One)\nclass BiRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BiRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                            batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection \n    \n    def forward(self, x):\n        # Set initial states\n        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).cuda() # 2 for bidirection \n        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).cuda()\n        \n        # Forward propagate RNN\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Decode hidden state of last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\nrnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\nrnn.cuda()\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size)).cuda()\n        labels = Variable(labels).cuda()\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size)).cuda()\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted.cpu() == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'rnn.pkl')", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main-gpu.py", "id": "b02ccfdce93b99c33cf9aee34025477e", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main-gpu.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}