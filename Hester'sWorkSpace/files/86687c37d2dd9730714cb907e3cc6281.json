{"author": "yunjey", "code": "\n\nimport torch \nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\nfrom data_utils import Dictionary, Corpus\n\n\nembed_size = 128\nhidden_size = 1024\nnum_layers = 1\nnum_epochs = 5\nnum_samples = 1000   \nbatch_size = 20\nseq_length = 30\nlearning_rate = 0.002\n\n\ntrain_path = './data/train.txt'\nsample_path = './sample.txt'\ncorpus = Corpus()\nids = corpus.get_data(train_path, batch_size)\nvocab_size = len(corpus.dictionary)\nnum_batches = ids.size(1) // seq_length\n\n\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(RNNLM, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.init_weights()\n        \n    def init_weights(self):\n        self.embed.weight.data.uniform_(-0.1, 0.1)\n        self.linear.bias.data.fill_(0)\n        self.linear.weight.data.uniform_(-0.1, 0.1)\n        \n    def forward(self, x, h):\n        \n        x = self.embed(x) \n        \n        \n        out, h = self.lstm(x, h)\n        \n        # Reshape output to (batch_size*sequence_length, hidden_size)\n        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n        \n        \n        out = self.linear(out)  \n        return out, h\n    \nmodel = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\nmodel.cuda()\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\ndef detach(states):\n    return [state.detach() for state in states] \n\n\nfor epoch in range(num_epochs):\n    \n    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda(),\n              Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda())\n    \n    for i in range(0, ids.size(1) - seq_length, seq_length):\n        \n        inputs = Variable(ids[:, i:i+seq_length]).cuda()\n        targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n        \n        # Forward + Backward + Optimize\n        model.zero_grad()\n        states = detach(states)\n        outputs, states = model(inputs, states) \n        loss = criterion(outputs, targets.view(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n        optimizer.step()\n\n        step = (i+1) // seq_length\n        if step % 100 == 0:\n            print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n                   (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n\n\nwith open(sample_path, 'w') as f:\n    \n    state = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(),\n         Variable(torch.zeros(num_layers, 1, hidden_size)).cuda())\n\n    \n    prob = torch.ones(vocab_size)\n    input = Variable(torch.multinomial(prob, num_samples=1).unsqueeze(1),\n                     volatile=True).cuda()\n\n    for i in range(num_samples):\n        \n        output, state = model(input, state)\n        \n        \n        prob = output.squeeze().data.exp().cpu()\n        word_id = torch.multinomial(prob, 1)[0]\n        \n        \n        input.data.fill_(word_id)\n        \n        \n        word = corpus.dictionary.idx2word[word_id]\n        word = '\\n' if word == '<eos>' else word + ' '\n        f.write(word)\n\n        if (i+1) % 100 == 0:\n            print('Sampled [%d/%d] words and save to %s'%(i+1, num_samples, sample_path))\n\n\ntorch.save(model.state_dict(), 'model.pkl')\n", "comments": "  some part code referenced     https   github com pytorch examples tree master word language model     hyper parameters    number words sampled    load penn treebank dataset    rnn based language model    embed word ids vectors    forward propagate rnn      reshape output (batch size sequence length  hidden size)    decode hidden states time step    loss optimizer    truncated backpropagation     training    initial hidden memory states    get batch inputs targets    forward   backward   optimize    sampling    set intial hidden ane memory states    select one word id randomly    forward propagate rnn     sample word id    feed sampled word id next time step    file write    save trained model ", "content": "# Some part of the code was referenced from below.\n# https://github.com/pytorch/examples/tree/master/word_language_model \nimport torch \nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\nfrom data_utils import Dictionary, Corpus\n\n# Hyper Parameters\nembed_size = 128\nhidden_size = 1024\nnum_layers = 1\nnum_epochs = 5\nnum_samples = 1000   # number of words to be sampled\nbatch_size = 20\nseq_length = 30\nlearning_rate = 0.002\n\n# Load Penn Treebank Dataset\ntrain_path = './data/train.txt'\nsample_path = './sample.txt'\ncorpus = Corpus()\nids = corpus.get_data(train_path, batch_size)\nvocab_size = len(corpus.dictionary)\nnum_batches = ids.size(1) // seq_length\n\n# RNN Based Language Model\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(RNNLM, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.init_weights()\n        \n    def init_weights(self):\n        self.embed.weight.data.uniform_(-0.1, 0.1)\n        self.linear.bias.data.fill_(0)\n        self.linear.weight.data.uniform_(-0.1, 0.1)\n        \n    def forward(self, x, h):\n        # Embed word ids to vectors\n        x = self.embed(x) \n        \n        # Forward propagate RNN  \n        out, h = self.lstm(x, h)\n        \n        # Reshape output to (batch_size*sequence_length, hidden_size)\n        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n        \n        # Decode hidden states of all time step\n        out = self.linear(out)  \n        return out, h\n    \nmodel = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\nmodel.cuda()\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Truncated Backpropagation \ndef detach(states):\n    return [state.detach() for state in states] \n\n# Training\nfor epoch in range(num_epochs):\n    # Initial hidden and memory states\n    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda(),\n              Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda())\n    \n    for i in range(0, ids.size(1) - seq_length, seq_length):\n        # Get batch inputs and targets\n        inputs = Variable(ids[:, i:i+seq_length]).cuda()\n        targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n        \n        # Forward + Backward + Optimize\n        model.zero_grad()\n        states = detach(states)\n        outputs, states = model(inputs, states) \n        loss = criterion(outputs, targets.view(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n        optimizer.step()\n\n        step = (i+1) // seq_length\n        if step % 100 == 0:\n            print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n                   (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n\n# Sampling\nwith open(sample_path, 'w') as f:\n    # Set intial hidden ane memory states\n    state = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(),\n         Variable(torch.zeros(num_layers, 1, hidden_size)).cuda())\n\n    # Select one word id randomly\n    prob = torch.ones(vocab_size)\n    input = Variable(torch.multinomial(prob, num_samples=1).unsqueeze(1),\n                     volatile=True).cuda()\n\n    for i in range(num_samples):\n        # Forward propagate rnn \n        output, state = model(input, state)\n        \n        # Sample a word id\n        prob = output.squeeze().data.exp().cpu()\n        word_id = torch.multinomial(prob, 1)[0]\n        \n        # Feed sampled word id to next time step\n        input.data.fill_(word_id)\n        \n        # File write\n        word = corpus.dictionary.idx2word[word_id]\n        word = '\\n' if word == '<eos>' else word + ' '\n        f.write(word)\n\n        if (i+1) % 100 == 0:\n            print('Sampled [%d/%d] words and save to %s'%(i+1, num_samples, sample_path))\n\n# Save the Trained Model\ntorch.save(model.state_dict(), 'model.pkl')\n", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main-gpu.py", "id": "86687c37d2dd9730714cb907e3cc6281", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/02-intermediate/language_model/main-gpu.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}