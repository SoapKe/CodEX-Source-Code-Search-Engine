{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\nimport numpy as np\n\n\ndef var_size(v):\n  return int(np.prod([int(d) for d in v.shape]))\n\n\ndef gradients(loss, var_list):\n  grads = tf.gradients(loss, var_list)\n  return [g if g is not None else tf.zeros(v.shape)\n          for g, v in zip(grads, var_list)]\n\ndef flatgrad(loss, var_list):\n  grads = gradients(loss, var_list)\n  return tf.concat([tf.reshape(grad, [-1])\n                    for (v, grad) in zip(var_list, grads)\n                    if grad is not None], 0)\n\n\ndef get_flat(var_list):\n  return tf.concat([tf.reshape(v, [-1]) for v in var_list], 0)\n\n\ndef set_from_flat(var_list, flat_theta):\n  assigns = []\n  shapes = [v.shape for v in var_list]\n  sizes = [var_size(v) for v in var_list]\n\n  start = 0\n  assigns = []\n  for (shape, size, v) in zip(shapes, sizes, var_list):\n    assigns.append(v.assign(\n        tf.reshape(flat_theta[start:start + size], shape)))\n    start += size\n  assert start == sum(sizes)\n\n  return tf.group(*assigns)\n\n\nclass TrustRegionOptimization(object):\n\n  def __init__(self, max_divergence=0.1, cg_damping=0.1):\n    self.max_divergence = max_divergence\n    self.cg_damping = cg_damping\n\n  def setup_placeholders(self):\n    self.flat_tangent = tf.placeholder(tf.float32, [None], 'flat_tangent')\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n\n  def setup(self, var_list, raw_loss, self_divergence,\n            divergence=None):\n    self.setup_placeholders()\n\n    self.raw_loss = raw_loss\n    self.divergence = divergence\n    self.loss_flat_gradient = flatgrad(raw_loss, var_list)\n    self.divergence_gradient = gradients(self_divergence, var_list)\n\n    shapes = [var.shape for var in var_list]\n    sizes = [var_size(var) for var in var_list]\n\n    start = 0\n    tangents = []\n    for shape, size in zip(shapes, sizes):\n      param = tf.reshape(self.flat_tangent[start:start + size], shape)\n      tangents.append(param)\n      start += size\n    assert start == sum(sizes)\n\n    self.grad_vector_product = sum(\n        tf.reduce_sum(g * t) for (g, t) in zip(self.divergence_gradient, tangents))\n    self.fisher_vector_product = flatgrad(self.grad_vector_product, var_list)\n\n    self.flat_vars = get_flat(var_list)\n    self.set_vars = set_from_flat(var_list, self.flat_theta)\n\n  def optimize(self, sess, feed_dict):\n    old_theta = sess.run(self.flat_vars)\n    loss_flat_grad = sess.run(self.loss_flat_gradient,\n                              feed_dict=feed_dict)\n\n    def calc_fisher_vector_product(tangent):\n      feed_dict[self.flat_tangent] = tangent\n      fvp = sess.run(self.fisher_vector_product,\n                     feed_dict=feed_dict)\n      fvp += self.cg_damping * tangent\n      return fvp\n\n    step_dir = conjugate_gradient(calc_fisher_vector_product, -loss_flat_grad)\n\n    shs = 0.5 * step_dir.dot(calc_fisher_vector_product(step_dir))\n    lm = np.sqrt(shs / self.max_divergence)\n    fullstep = step_dir / lm\n    neggdotstepdir = -loss_flat_grad.dot(step_dir)\n\n    def calc_loss(theta):\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n      if self.divergence is None:\n        return sess.run(self.raw_loss, feed_dict=feed_dict), True\n      else:\n        raw_loss, divergence = sess.run(\n            [self.raw_loss, self.divergence], feed_dict=feed_dict)\n        return raw_loss, divergence < self.max_divergence\n\n     find optimal theta\n    theta = linesearch(calc_loss, old_theta, fullstep, neggdotstepdir / lm)\n    if self.divergence is not None:\n      final_divergence = sess.run(self.divergence, feed_dict=feed_dict)\n    else:\n      final_divergence = None\n\n     set vars accordingly\n    if final_divergence is None or final_divergence < self.max_divergence:\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n    else:\n      sess.run(self.set_vars, feed_dict={self.flat_theta: old_theta})\n\n\ndef conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n  p = b.copy()\n  r = b.copy()\n  x = np.zeros_like(b)\n  rdotr = r.dot(r)\n  for i in xrange(cg_iters):\n    z = f_Ax(p)\n    v = rdotr / p.dot(z)\n    x += v * p\n    r -= v * z\n    newrdotr = r.dot(r)\n    mu = newrdotr / rdotr\n    p = r + mu * p\n    rdotr = newrdotr\n    if rdotr < residual_tol:\n      break\n  return x\n\n\ndef linesearch(f, x, fullstep, expected_improve_rate):\n  accept_ratio = 0.1\n  max_backtracks = 10\n\n  fval, _ = f(x)\n  for (_n_backtracks, stepfrac) in enumerate(.5 ** np.arange(max_backtracks)):\n    xnew = x + stepfrac * fullstep\n    newfval, valid = f(xnew)\n    if not valid:\n      continue\n    actual_improve = fval - newfval\n    expected_improve = expected_improve_rate * stepfrac\n    ratio = actual_improve / expected_improve\n    if ratio > accept_ratio and actual_improve > 0:\n      return xnew\n\n  return x\n", "comments": "   trust region optimization   a lot adapted code  see schulman modular rl  wojzaremba trpo  etc          copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       find optimal theta    set vars accordingly ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Trust region optimization.\n\nA lot of this is adapted from other's code.\nSee Schulman's Modular RL, wojzaremba's TRPO, etc.\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\nimport numpy as np\n\n\ndef var_size(v):\n  return int(np.prod([int(d) for d in v.shape]))\n\n\ndef gradients(loss, var_list):\n  grads = tf.gradients(loss, var_list)\n  return [g if g is not None else tf.zeros(v.shape)\n          for g, v in zip(grads, var_list)]\n\ndef flatgrad(loss, var_list):\n  grads = gradients(loss, var_list)\n  return tf.concat([tf.reshape(grad, [-1])\n                    for (v, grad) in zip(var_list, grads)\n                    if grad is not None], 0)\n\n\ndef get_flat(var_list):\n  return tf.concat([tf.reshape(v, [-1]) for v in var_list], 0)\n\n\ndef set_from_flat(var_list, flat_theta):\n  assigns = []\n  shapes = [v.shape for v in var_list]\n  sizes = [var_size(v) for v in var_list]\n\n  start = 0\n  assigns = []\n  for (shape, size, v) in zip(shapes, sizes, var_list):\n    assigns.append(v.assign(\n        tf.reshape(flat_theta[start:start + size], shape)))\n    start += size\n  assert start == sum(sizes)\n\n  return tf.group(*assigns)\n\n\nclass TrustRegionOptimization(object):\n\n  def __init__(self, max_divergence=0.1, cg_damping=0.1):\n    self.max_divergence = max_divergence\n    self.cg_damping = cg_damping\n\n  def setup_placeholders(self):\n    self.flat_tangent = tf.placeholder(tf.float32, [None], 'flat_tangent')\n    self.flat_theta = tf.placeholder(tf.float32, [None], 'flat_theta')\n\n  def setup(self, var_list, raw_loss, self_divergence,\n            divergence=None):\n    self.setup_placeholders()\n\n    self.raw_loss = raw_loss\n    self.divergence = divergence\n    self.loss_flat_gradient = flatgrad(raw_loss, var_list)\n    self.divergence_gradient = gradients(self_divergence, var_list)\n\n    shapes = [var.shape for var in var_list]\n    sizes = [var_size(var) for var in var_list]\n\n    start = 0\n    tangents = []\n    for shape, size in zip(shapes, sizes):\n      param = tf.reshape(self.flat_tangent[start:start + size], shape)\n      tangents.append(param)\n      start += size\n    assert start == sum(sizes)\n\n    self.grad_vector_product = sum(\n        tf.reduce_sum(g * t) for (g, t) in zip(self.divergence_gradient, tangents))\n    self.fisher_vector_product = flatgrad(self.grad_vector_product, var_list)\n\n    self.flat_vars = get_flat(var_list)\n    self.set_vars = set_from_flat(var_list, self.flat_theta)\n\n  def optimize(self, sess, feed_dict):\n    old_theta = sess.run(self.flat_vars)\n    loss_flat_grad = sess.run(self.loss_flat_gradient,\n                              feed_dict=feed_dict)\n\n    def calc_fisher_vector_product(tangent):\n      feed_dict[self.flat_tangent] = tangent\n      fvp = sess.run(self.fisher_vector_product,\n                     feed_dict=feed_dict)\n      fvp += self.cg_damping * tangent\n      return fvp\n\n    step_dir = conjugate_gradient(calc_fisher_vector_product, -loss_flat_grad)\n\n    shs = 0.5 * step_dir.dot(calc_fisher_vector_product(step_dir))\n    lm = np.sqrt(shs / self.max_divergence)\n    fullstep = step_dir / lm\n    neggdotstepdir = -loss_flat_grad.dot(step_dir)\n\n    def calc_loss(theta):\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n      if self.divergence is None:\n        return sess.run(self.raw_loss, feed_dict=feed_dict), True\n      else:\n        raw_loss, divergence = sess.run(\n            [self.raw_loss, self.divergence], feed_dict=feed_dict)\n        return raw_loss, divergence < self.max_divergence\n\n    # find optimal theta\n    theta = linesearch(calc_loss, old_theta, fullstep, neggdotstepdir / lm)\n    if self.divergence is not None:\n      final_divergence = sess.run(self.divergence, feed_dict=feed_dict)\n    else:\n      final_divergence = None\n\n    # set vars accordingly\n    if final_divergence is None or final_divergence < self.max_divergence:\n      sess.run(self.set_vars, feed_dict={self.flat_theta: theta})\n    else:\n      sess.run(self.set_vars, feed_dict={self.flat_theta: old_theta})\n\n\ndef conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n  p = b.copy()\n  r = b.copy()\n  x = np.zeros_like(b)\n  rdotr = r.dot(r)\n  for i in xrange(cg_iters):\n    z = f_Ax(p)\n    v = rdotr / p.dot(z)\n    x += v * p\n    r -= v * z\n    newrdotr = r.dot(r)\n    mu = newrdotr / rdotr\n    p = r + mu * p\n    rdotr = newrdotr\n    if rdotr < residual_tol:\n      break\n  return x\n\n\ndef linesearch(f, x, fullstep, expected_improve_rate):\n  accept_ratio = 0.1\n  max_backtracks = 10\n\n  fval, _ = f(x)\n  for (_n_backtracks, stepfrac) in enumerate(.5 ** np.arange(max_backtracks)):\n    xnew = x + stepfrac * fullstep\n    newfval, valid = f(xnew)\n    if not valid:\n      continue\n    actual_improve = fval - newfval\n    expected_improve = expected_improve_rate * stepfrac\n    ratio = actual_improve / expected_improve\n    if ratio > accept_ratio and actual_improve > 0:\n      return xnew\n\n  return x\n", "description": "Models and examples built with TensorFlow", "file_name": "trust_region.py", "id": "a30c2085d8b506cf76e136c946e88fd4", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/pcl_rl/trust_region.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}