{"author": "yunjey", "code": "import torch \nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n\n\n# 1. Basic autograd example 1               (Line 21 to 36)\n# 2. Basic autograd example 2               (Line 39 to 77)\n# 3. Loading data from numpy                (Line 80 to 83)\n# 4. Implementing the input pipline         (Line 86 to 113)\n# 5. Input pipline for custom dataset       (Line 116 to 138)\n# 6. Using pretrained model                 (Line 141 to 155)\n# 7. Save and load model                    (Line 158 to 165) \n\n\n\n\nx = Variable(torch.Tensor([1]), requires_grad=True)\nw = Variable(torch.Tensor([2]), requires_grad=True)\nb = Variable(torch.Tensor([3]), requires_grad=True)\n\n\ny = w * x + b    # y = 2 * x + 3\n\n\ny.backward()\n\n\nprint(x.grad)    \nprint(w.grad)    \nprint(b.grad)    \n\n\n\n\nx = Variable(torch.randn(5, 3))\ny = Variable(torch.randn(5, 2))\n\n\nlinear = nn.Linear(3, 2)\nprint ('w: ', linear.weight)\nprint ('b: ', linear.bias)\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n\n\npred = linear(x)\n\n\nloss = criterion(pred, y)\nprint('loss: ', loss.data[0])\n\n\nloss.backward()\n\n\nprint ('dL/dw: ', linear.weight.grad) \nprint ('dL/db: ', linear.bias.grad)\n\n# 1-step Optimization (gradient descent).\noptimizer.step()\n\n\n# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n\n\npred = linear(x)\nloss = criterion(pred, y)\nprint('loss after 1 step optimization: ', loss.data[0])\n\n\n\na = np.array([[1,2], [3,4]])\nb = torch.from_numpy(a)      \nc = b.numpy()                \n\n\n\n\ntrain_dataset = dsets.CIFAR10(root='../data/',\n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\n# Select one data pair (read data from disk).\nimage, label = train_dataset[0]\nprint (image.size())\nprint (label)\n\n# Data Loader (this provides queue and thread in a very simple way).\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=100, \n                                           shuffle=True,\n                                           num_workers=2)\n\n\ndata_iter = iter(train_loader)\n\n\nimages, labels = data_iter.next()\n\n\nfor images, labels in train_loader:\n    \n    pass\n\n\n\n\nclass CustomDataset(data.Dataset):\n    def __init__(self):\n        \n        \n        pass\n    def __getitem__(self, index):\n        \n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        # 2. Preprocess the data (e.g. torchvision.Transform).\n        # 3. Return a data pair (e.g. image and label).\n        pass\n    def __len__(self):\n        \n        return 0 \n\n\ncustom_dataset = CustomDataset()\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n                                           batch_size=100, \n                                           shuffle=True,\n                                           num_workers=2)\n\n\n\n\nresnet = torchvision.models.resnet18(pretrained=True)\n\n\nfor param in resnet.parameters():\n    param.requires_grad = False\n    \n\nresnet.fc = nn.Linear(resnet.fc.in_features, 100)  \n\n\nimages = Variable(torch.randn(10, 3, 224, 224))\noutputs = resnet(images)\nprint (outputs.size())   # (10, 100)\n\n\n\n\ntorch.save(resnet, 'model.pkl')\nmodel = torch.load('model.pkl')\n\n# Save and load only the model parameters(recommended).\ntorch.save(resnet.state_dict(), 'params.pkl')\nresnet.load_state_dict(torch.load('params.pkl'))\n", "comments": "                            table contents                                1  basic autograd example 1               (line 21 36)    2  basic autograd example 2               (line 39 77)    3  loading data numpy                (line 80 83)    4  implementing input pipline         (line 86 113)    5  input pipline custom dataset       (line 116 138)    6  using pretrained model                 (line 141 155)    7  save load model                    (line 158 165)                            basic autograd example 1                             create tensors     build computational graph       2   x   3    compute gradients     print gradients     x grad   2     w grad   1     b grad   1                             basic autograd example 2                             create tensors     build linear layer     build loss optimizer     forward propagation     compute loss     backpropagation     print gradients     1 step optimization (gradient descent)     you also optimization low level shown     linear weight data sub (0 01   linear weight grad data)    linear bias data sub (0 01   linear bias grad data)    print loss optimization                             loading data numpy                              convert numpy array torch tensor    convert torch tensor numpy array                         implementing input pipline                           download construct dataset     select one data pair (read data disk)     data loader (this provides queue thread simple way)     when iteration starts  queue thread start load dataset files     mini batch images labels     actual usage data loader     your training code written                         input pipline custom dataset                           you build custom dataset     todo    1  initialize file path list file names      todo    1  read one data file (e g  using numpy fromfile  pil image open)     2  preprocess data (e g  torchvision transform)     3  return data pair (e g  image label)     you change 0 total size dataset     then  use prebuilt torch data loader                                using pretrained model                                download load pretrained resnet     if want finetune top layer model     replace top layer finetuning     100 example     for test     (10  100)                                save load model                                  save load entire model     save load model parameters(recommended)  ", "content": "import torch \nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n\n#========================== Table of Contents ==========================#\n# 1. Basic autograd example 1               (Line 21 to 36)\n# 2. Basic autograd example 2               (Line 39 to 77)\n# 3. Loading data from numpy                (Line 80 to 83)\n# 4. Implementing the input pipline         (Line 86 to 113)\n# 5. Input pipline for custom dataset       (Line 116 to 138)\n# 6. Using pretrained model                 (Line 141 to 155)\n# 7. Save and load model                    (Line 158 to 165) \n\n\n#======================= Basic autograd example 1 =======================#\n# Create tensors.\nx = Variable(torch.Tensor([1]), requires_grad=True)\nw = Variable(torch.Tensor([2]), requires_grad=True)\nb = Variable(torch.Tensor([3]), requires_grad=True)\n\n# Build a computational graph.\ny = w * x + b    # y = 2 * x + 3\n\n# Compute gradients.\ny.backward()\n\n# Print out the gradients.\nprint(x.grad)    # x.grad = 2 \nprint(w.grad)    # w.grad = 1 \nprint(b.grad)    # b.grad = 1 \n\n\n#======================== Basic autograd example 2 =======================#\n# Create tensors.\nx = Variable(torch.randn(5, 3))\ny = Variable(torch.randn(5, 2))\n\n# Build a linear layer.\nlinear = nn.Linear(3, 2)\nprint ('w: ', linear.weight)\nprint ('b: ', linear.bias)\n\n# Build Loss and Optimizer.\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n\n# Forward propagation.\npred = linear(x)\n\n# Compute loss.\nloss = criterion(pred, y)\nprint('loss: ', loss.data[0])\n\n# Backpropagation.\nloss.backward()\n\n# Print out the gradients.\nprint ('dL/dw: ', linear.weight.grad) \nprint ('dL/db: ', linear.bias.grad)\n\n# 1-step Optimization (gradient descent).\noptimizer.step()\n\n# You can also do optimization at the low level as shown below.\n# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n\n# Print out the loss after optimization.\npred = linear(x)\nloss = criterion(pred, y)\nprint('loss after 1 step optimization: ', loss.data[0])\n\n\n#======================== Loading data from numpy ========================#\na = np.array([[1,2], [3,4]])\nb = torch.from_numpy(a)      # convert numpy array to torch tensor\nc = b.numpy()                # convert torch tensor to numpy array\n\n\n#===================== Implementing the input pipline =====================#\n# Download and construct dataset.\ntrain_dataset = dsets.CIFAR10(root='../data/',\n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\n# Select one data pair (read data from disk).\nimage, label = train_dataset[0]\nprint (image.size())\nprint (label)\n\n# Data Loader (this provides queue and thread in a very simple way).\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=100, \n                                           shuffle=True,\n                                           num_workers=2)\n\n# When iteration starts, queue and thread start to load dataset from files.\ndata_iter = iter(train_loader)\n\n# Mini-batch images and labels.\nimages, labels = data_iter.next()\n\n# Actual usage of data loader is as below.\nfor images, labels in train_loader:\n    # Your training code will be written here\n    pass\n\n\n#===================== Input pipline for custom dataset =====================#\n# You should build custom dataset as below.\nclass CustomDataset(data.Dataset):\n    def __init__(self):\n        # TODO\n        # 1. Initialize file path or list of file names. \n        pass\n    def __getitem__(self, index):\n        # TODO\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        # 2. Preprocess the data (e.g. torchvision.Transform).\n        # 3. Return a data pair (e.g. image and label).\n        pass\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return 0 \n\n# Then, you can just use prebuilt torch's data loader. \ncustom_dataset = CustomDataset()\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n                                           batch_size=100, \n                                           shuffle=True,\n                                           num_workers=2)\n\n\n#========================== Using pretrained model ==========================#\n# Download and load pretrained resnet.\nresnet = torchvision.models.resnet18(pretrained=True)\n\n# If you want to finetune only top layer of the model.\nfor param in resnet.parameters():\n    param.requires_grad = False\n    \n# Replace top layer for finetuning.\nresnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is for example.\n\n# For test.\nimages = Variable(torch.randn(10, 3, 224, 224))\noutputs = resnet(images)\nprint (outputs.size())   # (10, 100)\n\n\n#============================ Save and load the model ============================#\n# Save and load the entire model.\ntorch.save(resnet, 'model.pkl')\nmodel = torch.load('model.pkl')\n\n# Save and load only the model parameters(recommended).\ntorch.save(resnet.state_dict(), 'params.pkl')\nresnet.load_state_dict(torch.load('params.pkl'))\n", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main.py", "id": "07951137ff3ab5d6d398c65573595714", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/01-basics/pytorch_basics/main.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}