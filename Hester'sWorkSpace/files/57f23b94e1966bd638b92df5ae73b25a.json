{"author": "scrapy", "code": "\n\nfrom scrapy.spiders import Spider\nfrom scrapy.http import Request\n\n\nclass QPSSpider(Spider):\n\n    name = 'qps'\n    benchurl = 'http://localhost:8880/'\n\n    \n    max_concurrent_requests = 8\n    \n    qps = None \n    download_delay = None\n    \n    latency = None\n    \n    slots = 1\n\n    def __init__(self, *a, **kw):\n        super(QPSSpider, self).__init__(*a, **kw)\n        if self.qps is not None:\n            self.qps = float(self.qps)\n            self.download_delay = 1 / self.qps\n        elif self.download_delay is not None:\n            self.download_delay = float(self.download_delay)\n\n    def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += '?latency={0}'.format(self.latency)\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1\n\n    def parse(self, response):\n        pass\n", "comments": "    a spider generate light requests meassure qps troughput  usage       scrapy runspider qpsclient py   loglevel info   set randomize download delay 0   set concurrent requests 50  qps 10  latency 0 3         max concurrency limited global concurrent requests setting    requests per second goal     1   download delay    time seconds delay server responses    number slots create ", "content": "\"\"\"\nA spider that generate light requests to meassure QPS troughput\n\nusage:\n\n    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n\n\"\"\"\n\nfrom scrapy.spiders import Spider\nfrom scrapy.http import Request\n\n\nclass QPSSpider(Spider):\n\n    name = 'qps'\n    benchurl = 'http://localhost:8880/'\n\n    # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n    max_concurrent_requests = 8\n    # Requests per second goal\n    qps = None # same as: 1 / download_delay\n    download_delay = None\n    # time in seconds to delay server responses\n    latency = None\n    # number of slots to create\n    slots = 1\n\n    def __init__(self, *a, **kw):\n        super(QPSSpider, self).__init__(*a, **kw)\n        if self.qps is not None:\n            self.qps = float(self.qps)\n            self.download_delay = 1 / self.qps\n        elif self.download_delay is not None:\n            self.download_delay = float(self.download_delay)\n\n    def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += '?latency={0}'.format(self.latency)\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1\n\n    def parse(self, response):\n        pass\n", "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.", "file_name": "qpsclient.py", "id": "57f23b94e1966bd638b92df5ae73b25a", "language": "Python", "project_name": "scrapy", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/scrapy-scrapy/scrapy-scrapy-6a7cdf9/extras/qpsclient.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:39:41Z", "url": "https://github.com/scrapy/scrapy", "wiki": true}