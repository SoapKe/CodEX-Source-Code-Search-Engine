{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Generates vocabulary and term frequency files for datasets.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\n\n Dependency imports\n\nimport tensorflow as tf\n\nfrom data import data_utils\nfrom data import document_generators\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n Flags controlling input are in document_generators.py\n\nflags.DEFINE_string('output_dir', '',\n                    'Path to save vocab.txt and vocab_freq.txt.')\n\nflags.DEFINE_boolean('use_unlabeled', True, 'Whether to use the '\n                     'unlabeled sentiment dataset in the vocabulary.')\nflags.DEFINE_boolean('include_validation', False, 'Whether to include the '\n                     'validation set in the vocabulary.')\nflags.DEFINE_integer('doc_count_threshold', 1, 'The minimum number of '\n                     'documents a word or bigram should occur in to keep '\n                     'it in the vocabulary.')\n\nMAX_VOCAB_SIZE = 100 * 1000\n\n\ndef fill_vocab_from_doc(doc, vocab_freqs, doc_counts):\n  \"\"\"Fills vocabulary and doc counts with tokens from doc.\n\n  Args:\n    doc: Document to read tokens from.\n    vocab_freqs: dict<token, frequency count>\n    doc_counts: dict<token, document count>\n\n  Returns:\n    None\n  \"\"\"\n  doc_seen = set()\n\n  for token in document_generators.tokens(doc):\n    if doc.add_tokens or token in vocab_freqs:\n      vocab_freqs[token] += 1\n    if token not in doc_seen:\n      doc_counts[token] += 1\n      doc_seen.add(token)\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n  vocab_freqs = defaultdict(int)\n  doc_counts = defaultdict(int)\n\n   Fill vocabulary frequencies map and document counts map\n  for doc in document_generators.documents(\n      dataset='train',\n      include_unlabeled=FLAGS.use_unlabeled,\n      include_validation=FLAGS.include_validation):\n    fill_vocab_from_doc(doc, vocab_freqs, doc_counts)\n\n   Filter out low-occurring terms\n  vocab_freqs = dict((term, freq) for term, freq in vocab_freqs.iteritems()\n                     if doc_counts[term] > FLAGS.doc_count_threshold)\n\n   Sort by frequency\n  ordered_vocab_freqs = data_utils.sort_vocab_by_frequency(vocab_freqs)\n\n   Limit vocab size\n  ordered_vocab_freqs = ordered_vocab_freqs[:MAX_VOCAB_SIZE]\n\n   Add EOS token\n  ordered_vocab_freqs.append((data_utils.EOS_TOKEN, 1))\n\n   Write\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n  data_utils.write_vocab_and_frequency(ordered_vocab_freqs, FLAGS.output_dir)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   generates vocabulary term frequency files datasets       future   import absolute import   future   import division   future   import print function  collections import defaultdict    dependency imports  import tensorflow tf  data import data utils data import document generators  flags   tf app flags flags   flags flags    flags controlling input document generators py  flags define string( output dir                            path save vocab txt vocab freq txt  )  flags define boolean( use unlabeled   true   whether use                         unlabeled sentiment dataset vocabulary  ) flags define boolean( include validation   false   whether include                         validation set vocabulary  ) flags define integer( doc count threshold   1   the minimum number                         documents word bigram occur keep                         vocabulary  )  max vocab size   100   1000   def fill vocab doc(doc  vocab freqs  doc counts)       fills vocabulary doc counts tokens doc     args      doc  document read tokens      vocab freqs  dict token  frequency count      doc counts  dict token  document count     returns      none          copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       dependency imports    flags controlling input document generators py    fill vocabulary frequencies map document counts map    filter low occurring terms    sort frequency    limit vocab size    add eos token    write ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Generates vocabulary and term frequency files for datasets.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\n\n# Dependency imports\n\nimport tensorflow as tf\n\nfrom data import data_utils\nfrom data import document_generators\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Flags controlling input are in document_generators.py\n\nflags.DEFINE_string('output_dir', '',\n                    'Path to save vocab.txt and vocab_freq.txt.')\n\nflags.DEFINE_boolean('use_unlabeled', True, 'Whether to use the '\n                     'unlabeled sentiment dataset in the vocabulary.')\nflags.DEFINE_boolean('include_validation', False, 'Whether to include the '\n                     'validation set in the vocabulary.')\nflags.DEFINE_integer('doc_count_threshold', 1, 'The minimum number of '\n                     'documents a word or bigram should occur in to keep '\n                     'it in the vocabulary.')\n\nMAX_VOCAB_SIZE = 100 * 1000\n\n\ndef fill_vocab_from_doc(doc, vocab_freqs, doc_counts):\n  \"\"\"Fills vocabulary and doc counts with tokens from doc.\n\n  Args:\n    doc: Document to read tokens from.\n    vocab_freqs: dict<token, frequency count>\n    doc_counts: dict<token, document count>\n\n  Returns:\n    None\n  \"\"\"\n  doc_seen = set()\n\n  for token in document_generators.tokens(doc):\n    if doc.add_tokens or token in vocab_freqs:\n      vocab_freqs[token] += 1\n    if token not in doc_seen:\n      doc_counts[token] += 1\n      doc_seen.add(token)\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n  vocab_freqs = defaultdict(int)\n  doc_counts = defaultdict(int)\n\n  # Fill vocabulary frequencies map and document counts map\n  for doc in document_generators.documents(\n      dataset='train',\n      include_unlabeled=FLAGS.use_unlabeled,\n      include_validation=FLAGS.include_validation):\n    fill_vocab_from_doc(doc, vocab_freqs, doc_counts)\n\n  # Filter out low-occurring terms\n  vocab_freqs = dict((term, freq) for term, freq in vocab_freqs.iteritems()\n                     if doc_counts[term] > FLAGS.doc_count_threshold)\n\n  # Sort by frequency\n  ordered_vocab_freqs = data_utils.sort_vocab_by_frequency(vocab_freqs)\n\n  # Limit vocab size\n  ordered_vocab_freqs = ordered_vocab_freqs[:MAX_VOCAB_SIZE]\n\n  # Add EOS token\n  ordered_vocab_freqs.append((data_utils.EOS_TOKEN, 1))\n\n  # Write\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n  data_utils.write_vocab_and_frequency(ordered_vocab_freqs, FLAGS.output_dir)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "gen_vocab.py", "id": "11d976febb591071114c5c4d1a786add", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/adversarial_text/gen_vocab.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}