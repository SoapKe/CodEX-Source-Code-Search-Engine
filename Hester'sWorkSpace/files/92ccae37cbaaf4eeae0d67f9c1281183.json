{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Builds the CIFAR-10 network.\n\nSummary of available functions:\n\n  Compute input images and labels for training. If you would like to run\n  evaluations, use inputs() instead.\n inputs, labels = distorted_inputs()\n\n  Compute inference on the model inputs to make a prediction.\n predictions = inference(inputs)\n\n  Compute the total loss of the prediction with respect to the labels.\n loss = loss(predictions, labels)\n\n  Create a graph to run one step of training with respect to the loss.\n train_op = train(loss, global_step)\n\"\"\"\n pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nimport cifar10_input\n\nFLAGS = tf.app.flags.FLAGS\n\n Basic model parameters.\ntf.app.flags.DEFINE_integer('batch_size', 128,\n                            \"\"\"Number of images to process in a batch.\"\"\")\ntf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',\n                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\ntf.app.flags.DEFINE_boolean('use_fp16', False,\n                            \"\"\"Train the model using fp16.\"\"\")\n\n Global constants describing the CIFAR-10 data set.\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n\n Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999      The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0       Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1   Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1        Initial learning rate.\n\n If a model is trained with multiple GPUs, prefix all Op names with tower_name\n to differentiate the operations. Note that this prefix is removed from the\n names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\nDATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n\n\ndef _activation_summary(x):\n  \"\"\"Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measures the sparsity of activations.\n\n  Args:\n    x: Tensor\n  Returns:\n    nothing\n  \"\"\"\n   Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n   session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n  tf.summary.histogram(tensor_name + '/activations', x)\n  tf.summary.scalar(tensor_name + '/sparsity',\n                                       tf.nn.zero_fraction(x))\n\n\ndef _variable_on_cpu(name, shape, initializer):\n  \"\"\"Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\n\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  \"\"\"Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  var = _variable_on_cpu(\n      name,\n      shape,\n      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n    tf.add_to_collection('losses', weight_decay)\n  return var\n\n\ndef distorted_inputs():\n  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  \"\"\"\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                  batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inputs(eval_data):\n  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  \"\"\"\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n  images, labels = cifar10_input.inputs(eval_data=eval_data,\n                                        data_dir=data_dir,\n                                        batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n   We instantiate all variables using tf.get_variable() instead of\n   tf.Variable() in order to share variables across multiple GPU training runs.\n   If we only ran this model on a single GPU, we could simplify this function\n   by replacing all instances of tf.get_variable() with tf.Variable().\n  \n   conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 3, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv1)\n\n   pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n   norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n   conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 64, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv2)\n\n   norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n   pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n   local3\n  with tf.variable_scope('local3') as scope:\n     Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(pool2, [images.get_shape()[0], -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    _activation_summary(local3)\n\n   local4\n  with tf.variable_scope('local4') as scope:\n    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    _activation_summary(local4)\n\n   linear layer(WX + b),\n   We don't apply softmax here because\n   tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n   and performs the softmax internally for efficiency.\n  with tf.variable_scope('softmax_linear') as scope:\n    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=None)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n\n\ndef loss(logits, labels):\n  \"\"\"Add L2Loss to all the trainable variables.\n\n  Add summary for \"Loss\" and \"Loss/avg\".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n\n  Returns:\n    Loss tensor of type float.\n  \"\"\"\n   Calculate the average cross entropy loss across the batch.\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name='cross_entropy_per_example')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n  tf.add_to_collection('losses', cross_entropy_mean)\n\n   The total loss is defined as the cross entropy loss plus all of the weight\n   decay terms (L2 loss).\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n\n\ndef _add_loss_summaries(total_loss):\n  \"\"\"Add summaries for losses in CIFAR-10 model.\n\n  Generates moving average for all losses and associated summaries for\n  visualizing the performance of the network.\n\n  Args:\n    total_loss: Total loss from loss().\n  Returns:\n    loss_averages_op: op for generating moving averages of losses.\n  \"\"\"\n   Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n  losses = tf.get_collection('losses')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n   Attach a scalar summary to all individual losses and the total loss; do the\n   same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n     Name each loss as '(raw)' and name the moving average version of the loss\n     as the original loss name.\n    tf.summary.scalar(l.op.name + ' (raw)', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n\n  return loss_averages_op\n\n\ndef train(total_loss, global_step):\n  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  \"\"\"\n   Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n   Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar('learning_rate', lr)\n\n   Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n   Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n   Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n   Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n   Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + '/gradients', grad)\n\n   Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n\n  return train_op\n\n\ndef maybe_download_and_extract():\n  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n  dest_directory = FLAGS.data_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = DATA_URL.split('/')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n          float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n  if not os.path.exists(extracted_dir_path):\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n", "comments": "   builds cifar 10 network   summary available functions      compute input images labels training  if would like run    evaluations  use inputs() instead   inputs  labels   distorted inputs()     compute inference model inputs make prediction   predictions   inference(inputs)     compute total loss prediction respect labels   loss   loss(predictions  labels)     create graph run one step training respect loss   train op   train(loss  global step)       pylint  disable missing docstring   future   import absolute import   future   import division   future   import print function  import os import import sys import tarfile  six moves import urllib import tensorflow tf  import cifar10 input  flags   tf app flags flags    basic model parameters  tf app flags define integer( batch size   128                                 number images process batch    ) tf app flags define string( data dir     tmp cifar10 data                                 path cifar 10 data directory    ) tf app flags define boolean( use fp16   false                                 train model using fp16    )    global constants describing cifar 10 data set  image size   cifar10 input image size num classes   cifar10 input num classes num examples per epoch for train   cifar10 input num examples per epoch for train num examples per epoch for eval   cifar10 input num examples per epoch for eval     constants describing training process  moving average decay   0 9999       the decay use moving average  num epochs per decay   350 0        epochs learning rate decays  learning rate decay factor   0 1    learning rate decay factor  initial learning rate   0 1         initial learning rate     if model trained multiple gpus  prefix op names tower name   differentiate operations  note prefix removed   names summaries visualizing model  tower name    tower   data url    https   www cs toronto edu  kriz cifar 10 binary tar gz    def  activation summary(x)       helper create summaries activations     creates summary provides histogram activations    creates summary measures sparsity activations     args      x  tensor   returns      nothing           remove  tower  0 9    name case multi gpu training     session  this helps clarity presentation tensorboard    tensor name   sub(   0 9       tower name      x op name)   tf summary histogram(tensor name     activations   x)   tf summary scalar(tensor name     sparsity                                          tf nn zero fraction(x))   def  variable cpu(name  shape  initializer)       helper create variable stored cpu memory     args      name  name variable     shape  list ints     initializer  initializer variable    returns      variable tensor         tf device(  cpu 0 )      dtype   tf float16 flags use fp16 else tf float32     var   tf get variable(name  shape  initializer initializer  dtype dtype)   return var   def  variable weight decay(name  shape  stddev  wd)       helper create initialized variable weight decay     note variable initialized truncated normal distribution    a weight decay added one specified     args      name  name variable     shape  list ints     stddev  standard deviation truncated gaussian     wd  add l2loss weight decay multiplied float  if none  weight         decay added variable     returns      variable tensor         dtype   tf float16 flags use fp16 else tf float32   var    variable cpu(       name        shape        tf truncated normal initializer(stddev stddev  dtype dtype))   wd none      weight decay   tf multiply(tf nn l2 loss(var)  wd  name  weight loss )     tf add collection( losses   weight decay)   return var   def distorted inputs()       construct distorted input cifar training using reader ops     returns      images  images  4d tensor  batch size  image size  image size  3  size      labels  labels  1d tensor  batch size  size     raises      valueerror  if data dir         flags data dir      raise valueerror( please supply data dir )   data dir   os path join(flags data dir   cifar 10 batches bin )   images  labels   cifar10 input distorted inputs(data dir data dir                                                    batch size flags batch size)   flags use fp16      images   tf cast(images  tf float16)     labels   tf cast(labels  tf float16)   return images  labels   def inputs(eval data)       construct input cifar evaluation using reader ops     args      eval data  bool  indicating one use train eval data set     returns      images  images  4d tensor  batch size  image size  image size  3  size      labels  labels  1d tensor  batch size  size     raises      valueerror  if data dir         flags data dir      raise valueerror( please supply data dir )   data dir   os path join(flags data dir   cifar 10 batches bin )   images  labels   cifar10 input inputs(eval data eval data                                          data dir data dir                                          batch size flags batch size)   flags use fp16      images   tf cast(images  tf float16)     labels   tf cast(labels  tf float16)   return images  labels   def inference(images)       build cifar 10 model     args      images  images returned distorted inputs() inputs()     returns      logits            we instantiate variables using tf get variable() instead     tf variable() order share variables across multiple gpu training runs      if ran model single gpu  could simplify function     replacing instances tf get variable() tf variable()          conv1   tf variable scope( conv1 ) scope      kernel    variable weight decay( weights                                            shape  5  5  3  64                                            stddev 5e 2                                           wd none)     conv   tf nn conv2d(images  kernel   1  1  1  1   padding  same )     biases    variable cpu( biases    64   tf constant initializer(0 0))     pre activation   tf nn bias add(conv  biases)     conv1   tf nn relu(pre activation  name scope name)      activation summary(conv1)      pool1   pool1   tf nn max pool(conv1  ksize  1  3  3  1   strides  1  2  2  1                            padding  same   name  pool1 )     norm1   norm1   tf nn lrn(pool1  4  bias 1 0  alpha 0 001   9 0  beta 0 75                      name  norm1 )      conv2   tf variable scope( conv2 ) scope      kernel    variable weight decay( weights                                            shape  5  5  64  64                                            stddev 5e 2                                           wd none)     conv   tf nn conv2d(norm1  kernel   1  1  1  1   padding  same )     biases    variable cpu( biases    64   tf constant initializer(0 1))     pre activation   tf nn bias add(conv  biases)     conv2   tf nn relu(pre activation  name scope name)      activation summary(conv2)      norm2   norm2   tf nn lrn(conv2  4  bias 1 0  alpha 0 001   9 0  beta 0 75                      name  norm2 )     pool2   pool2   tf nn max pool(norm2  ksize  1  3  3  1                            strides  1  2  2  1   padding  same   name  pool2 )      local3   tf variable scope( local3 ) scope        move everything depth perform single matrix multiply      reshape   tf reshape(pool2   images get shape() 0    1 )     dim   reshape get shape() 1  value     weights    variable weight decay( weights   shape  dim  384                                             stddev 0 04  wd 0 004)     biases    variable cpu( biases    384   tf constant initializer(0 1))     local3   tf nn relu(tf matmul(reshape  weights)   biases  name scope name)      activation summary(local3)      local4   tf variable scope( local4 ) scope      weights    variable weight decay( weights   shape  384  192                                             stddev 0 04  wd 0 004)     biases    variable cpu( biases    192   tf constant initializer(0 1))     local4   tf nn relu(tf matmul(local3  weights)   biases  name scope name)      activation summary(local4)      linear layer(wx   b)      we apply softmax     tf nn sparse softmax cross entropy logits accepts unscaled logits     performs softmax internally efficiency    tf variable scope( softmax linear ) scope      weights    variable weight decay( weights    192  num classes                                             stddev 1 192 0  wd none)     biases    variable cpu( biases    num classes                                 tf constant initializer(0 0))     softmax linear   tf add(tf matmul(local4  weights)  biases  name scope name)      activation summary(softmax linear)    return softmax linear   def loss(logits  labels)       add l2loss trainable variables     add summary  loss   loss avg     args      logits  logits inference()      labels  labels distorted inputs inputs()  1 d tensor             shape  batch size     returns      loss tensor type float            calculate average cross entropy loss across batch    labels   tf cast(labels  tf int64)   cross entropy   tf nn sparse softmax cross entropy logits(       labels labels  logits logits  name  cross entropy per example )   cross entropy mean   tf reduce mean(cross entropy  name  cross entropy )   tf add collection( losses   cross entropy mean)      the total loss defined cross entropy loss plus weight     decay terms (l2 loss)    return tf add n(tf get collection( losses )  name  total loss )   def  add loss summaries(total loss)       add summaries losses cifar 10 model     generates moving average losses associated summaries   visualizing performance network     args      total loss  total loss loss()    returns      loss averages op  op generating moving averages losses            compute moving average individual losses total loss    loss averages   tf train exponentialmovingaverage(0 9  name  avg )   losses   tf get collection( losses )   loss averages op   loss averages apply(losses    total loss )      attach scalar summary individual losses total loss      averaged version losses    l losses    total loss         name loss  (raw)  name moving average version loss       original loss name      tf summary scalar(l op name     (raw)   l)     tf summary scalar(l op name  loss averages average(l))    return loss averages op   def train(total loss  global step)       train cifar 10 model     create optimizer apply trainable variables  add moving   average trainable variables     args      total loss  total loss loss()      global step  integer variable counting number training steps       processed    returns      train op  op training            variables affect learning rate    num batches per epoch   num examples per epoch for train   flags batch size   decay steps   int(num batches per epoch   num epochs per decay)      decay learning rate exponentially based number steps    lr   tf train exponential decay(initial learning rate                                    global step                                    decay steps                                    learning rate decay factor                                    staircase true)   tf summary scalar( learning rate   lr)      generate moving averages losses associated summaries    loss averages op    add loss summaries(total loss)      compute gradients    tf control dependencies( loss averages op )      opt   tf train gradientdescentoptimizer(lr)     grads   opt compute gradients(total loss)      apply gradients    apply gradient op   opt apply gradients(grads  global step global step)      add histograms trainable variables    var tf trainable variables()      tf summary histogram(var op name  var)      add histograms gradients    grad  var grads      grad none        tf summary histogram(var op name     gradients   grad)      track moving averages trainable variables    variable averages   tf train exponentialmovingaverage(       moving average decay  global step)   variables averages op   variable averages apply(tf trainable variables())    tf control dependencies( apply gradient op  variables averages op )      train op   tf op(name  train )    return train op   def maybe download extract()       download extract tarball alex website        copyright 2015 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       compute input images labels training  if would like run    evaluations  use inputs() instead     compute inference model inputs make prediction     compute total loss prediction respect labels     create graph run one step training respect loss     pylint  disable missing docstring    basic model parameters     global constants describing cifar 10 data set     constants describing training process     the decay use moving average     epochs learning rate decays     learning rate decay factor     initial learning rate     if model trained multiple gpus  prefix op names tower name    differentiate operations  note prefix removed    names summaries visualizing model     remove  tower  0 9    name case multi gpu training    session  this helps clarity presentation tensorboard     we instantiate variables using tf get variable() instead    tf variable() order share variables across multiple gpu training runs     if ran model single gpu  could simplify function    replacing instances tf get variable() tf variable()        conv1    pool1    norm1    conv2    norm2    pool2    local3    move everything depth perform single matrix multiply     local4    linear layer(wx   b)     we apply softmax    tf nn sparse softmax cross entropy logits accepts unscaled logits    performs softmax internally efficiency     calculate average cross entropy loss across batch     the total loss defined cross entropy loss plus weight    decay terms (l2 loss)     compute moving average individual losses total loss     attach scalar summary individual losses total loss     averaged version losses     name loss  (raw)  name moving average version loss    original loss name     variables affect learning rate     decay learning rate exponentially based number steps     generate moving averages losses associated summaries     compute gradients     apply gradients     add histograms trainable variables     add histograms gradients     track moving averages trainable variables  ", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Builds the CIFAR-10 network.\n\nSummary of available functions:\n\n # Compute input images and labels for training. If you would like to run\n # evaluations, use inputs() instead.\n inputs, labels = distorted_inputs()\n\n # Compute inference on the model inputs to make a prediction.\n predictions = inference(inputs)\n\n # Compute the total loss of the prediction with respect to the labels.\n loss = loss(predictions, labels)\n\n # Create a graph to run one step of training with respect to the loss.\n train_op = train(loss, global_step)\n\"\"\"\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nimport cifar10_input\n\nFLAGS = tf.app.flags.FLAGS\n\n# Basic model parameters.\ntf.app.flags.DEFINE_integer('batch_size', 128,\n                            \"\"\"Number of images to process in a batch.\"\"\")\ntf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',\n                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\ntf.app.flags.DEFINE_boolean('use_fp16', False,\n                            \"\"\"Train the model using fp16.\"\"\")\n\n# Global constants describing the CIFAR-10 data set.\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n\n# Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n\n# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\nDATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n\n\ndef _activation_summary(x):\n  \"\"\"Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measures the sparsity of activations.\n\n  Args:\n    x: Tensor\n  Returns:\n    nothing\n  \"\"\"\n  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n  # session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n  tf.summary.histogram(tensor_name + '/activations', x)\n  tf.summary.scalar(tensor_name + '/sparsity',\n                                       tf.nn.zero_fraction(x))\n\n\ndef _variable_on_cpu(name, shape, initializer):\n  \"\"\"Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\n\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  \"\"\"Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  var = _variable_on_cpu(\n      name,\n      shape,\n      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n    tf.add_to_collection('losses', weight_decay)\n  return var\n\n\ndef distorted_inputs():\n  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  \"\"\"\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                  batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inputs(eval_data):\n  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  \"\"\"\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n  images, labels = cifar10_input.inputs(eval_data=eval_data,\n                                        data_dir=data_dir,\n                                        batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 3, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 64, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(pool2, [images.get_shape()[0], -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    _activation_summary(local4)\n\n  # linear layer(WX + b),\n  # We don't apply softmax here because\n  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n  # and performs the softmax internally for efficiency.\n  with tf.variable_scope('softmax_linear') as scope:\n    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=None)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n\n\ndef loss(logits, labels):\n  \"\"\"Add L2Loss to all the trainable variables.\n\n  Add summary for \"Loss\" and \"Loss/avg\".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n\n  Returns:\n    Loss tensor of type float.\n  \"\"\"\n  # Calculate the average cross entropy loss across the batch.\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name='cross_entropy_per_example')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n  tf.add_to_collection('losses', cross_entropy_mean)\n\n  # The total loss is defined as the cross entropy loss plus all of the weight\n  # decay terms (L2 loss).\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n\n\ndef _add_loss_summaries(total_loss):\n  \"\"\"Add summaries for losses in CIFAR-10 model.\n\n  Generates moving average for all losses and associated summaries for\n  visualizing the performance of the network.\n\n  Args:\n    total_loss: Total loss from loss().\n  Returns:\n    loss_averages_op: op for generating moving averages of losses.\n  \"\"\"\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n  losses = tf.get_collection('losses')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Name each loss as '(raw)' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name + ' (raw)', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n\n  return loss_averages_op\n\n\ndef train(total_loss, global_step):\n  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  \"\"\"\n  # Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar('learning_rate', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + '/gradients', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n\n  return train_op\n\n\ndef maybe_download_and_extract():\n  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n  dest_directory = FLAGS.data_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = DATA_URL.split('/')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n          float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n  if not os.path.exists(extracted_dir_path):\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n", "description": "Models and examples built with TensorFlow", "file_name": "cifar10.py", "id": "92ccae37cbaaf4eeae0d67f9c1281183", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/tutorials/image/cifar10/cifar10.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}