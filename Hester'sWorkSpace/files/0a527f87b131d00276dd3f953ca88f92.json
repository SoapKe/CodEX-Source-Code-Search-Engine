{"author": "openai", "code": "\"\"\"\nAlgorithmic environments have the following traits in common:\n\n- A 1-d \"input tape\" or 2-d \"input grid\" of characters\n- A target string which is a deterministic function of the input characters\n\nAgents control a read head that moves over the input tape. Observations consist\nof the single character currently under the read head. The read head may fall\noff the end of the tape in any direction. When this happens, agents will observe\na special blank character (with index=env.base) until they get back in bounds.\n\nActions consist of 3 sub-actions:\n    - Direction to move the read head (left or right, plus up and down for 2-d envs)\n    - Whether to write to the output tape\n    - Which character to write (ignored if the above sub-action is 0)\n\nAn episode ends when:\n    - The agent writes the full target string to the output tape.\n    - The agent writes an incorrect character.\n    - The agent runs out the time limit. (Which is fairly conservative.)\n\nReward schedule:\n    write a correct character: +1\n    write a wrong character: -.5\n    run out the clock: -1\n    otherwise: 0\n\nIn the beginning, input strings will be fairly short. After an environment has\nbeen consistently solved over some window of episodes, the environment will \nincrease the average length of generated strings. Typical env specs require\nleveling up many times to reach their reward threshold.\n\"\"\"\nfrom gym import Env, logger\nfrom gym.spaces import Discrete, Tuple\nfrom gym.utils import colorize, seeding\nimport numpy as np\nfrom six import StringIO\nimport sys\nimport math\n\nclass AlgorithmicEnv(Env):\n\n    metadata = {'render.modes': ['human', 'ansi']}\n    \n    \n    MIN_REWARD_SHORTFALL_FOR_PROMOTION = -1.0\n\n    def __init__(self, base=10, chars=False, starting_min_length=2):\n        \"\"\"\n        base: Number of distinct characters. \n        chars: If True, use uppercase alphabet. Otherwise, digits. Only affects\n               rendering.\n        starting_min_length: Minimum input string length. Ramps up as episodes \n                             are consistently solved.\n        \"\"\"\n        self.base = base\n        \n        self.last = 10\n        \n        self.episode_total_reward = None\n        \n        \n        AlgorithmicEnv.reward_shortfalls = []\n        if chars:\n            self.charmap = [chr(ord('A')+i) for i in range(base)]\n        else:\n            self.charmap = [str(i) for i in range(base)]\n        self.charmap.append(' ')\n        \n        \n        \n        AlgorithmicEnv.min_length = starting_min_length\n        \n        #       1. Move read head left or write (or up/down)\n        \n        #       3. Which character to write. (Ignored if should_write=0)\n        self.action_space = Tuple(\n            [Discrete(len(self.MOVEMENTS)), Discrete(2), Discrete(self.base)]\n        )\n        # Can see just what is on the input tape (one of n characters, or nothing)\n        self.observation_space = Discrete(self.base + 1)\n        self.seed()\n        self.reset()\n\n    @classmethod\n    def _movement_idx(kls, movement_name):\n        return kls.MOVEMENTS.index(movement_name)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _get_obs(self, pos=None):\n        \"\"\"Return an observation corresponding to the given read head position\n        (or the current read head position, if none is given).\"\"\"\n        raise NotImplemented\n\n    def _get_str_obs(self, pos=None):\n        ret = self._get_obs(pos)\n        return self.charmap[ret]\n\n    def _get_str_target(self, pos):\n        \"\"\"Return the ith character of the target string (or \" \" if index\n        out of bounds).\"\"\"\n        if pos < 0 or len(self.target) <= pos:\n            return \" \"\n        else:\n            return self.charmap[self.target[pos]]\n\n    def render_observation(self):\n        \"\"\"Return a string representation of the input tape/grid.\"\"\"\n        raise NotImplementedError\n\n    def render(self, mode='human'):\n\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n        inp = \"Total length of input instance: %d, step: %d\\n\" % (self.input_width, self.time)\n        outfile.write(inp)\n        x, y, action = self.read_head_position, self.write_head_position, self.last_action\n        if action is not None:\n            inp_act, out_act, pred = action\n        outfile.write(\"=\" * (len(inp) - 1) + \"\\n\")\n        y_str =      \"Output Tape         : \"\n        target_str = \"Targets             : \"\n        if action is not None:\n            pred_str = self.charmap[pred]\n        x_str = self.render_observation()\n        for i in range(-2, len(self.target) + 2):\n            target_str += self._get_str_target(i)\n            if i < y - 1:\n                y_str += self._get_str_target(i)\n            elif i == (y - 1):\n                if action is not None and out_act == 1:\n                    color = 'green' if pred == self.target[i] else 'red'\n                    y_str += colorize(pred_str, color, highlight=True)\n                else:\n                    y_str += self._get_str_target(i)\n        outfile.write(x_str)\n        outfile.write(y_str + \"\\n\")\n        outfile.write(target_str + \"\\n\\n\")\n\n        if action is not None:\n            outfile.write(\"Current reward      :   %.3f\\n\" % self.last_reward)\n            outfile.write(\"Cumulative reward   :   %.3f\\n\" % self.episode_total_reward)\n            move = self.MOVEMENTS[inp_act]\n            outfile.write(\"Action              :   Tuple(move over input: %s,\\n\" % move)\n            out_act = out_act == 1\n            outfile.write(\"                              write to the output tape: %s,\\n\" % out_act)\n            outfile.write(\"                              prediction: %s)\\n\" % pred_str)\n        else:\n            outfile.write(\"\\n\" * 5)\n        return outfile\n\n    @property\n    def input_width(self):\n        return len(self.input_data)\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        self.last_action = action\n        inp_act, out_act, pred = action\n        done = False\n        reward = 0.0\n        self.time += 1\n        assert 0 <= self.write_head_position\n        if out_act == 1:\n            try:\n                correct = pred == self.target[self.write_head_position]\n            except IndexError:\n                logger.warn(\"It looks like you're calling step() even though this \"+\n                    \"environment has already returned done=True. You should always call \"+\n                    \"reset() once you receive done=True. Any further steps are undefined \"+\n                    \"behaviour.\")\n                correct = False\n            if correct:\n                reward = 1.0\n            else:\n                \n                reward = -0.5\n                done = True\n            self.write_head_position += 1\n            if self.write_head_position >= len(self.target):\n                done = True\n        self._move(inp_act)\n        if self.time > self.time_limit:\n            reward = -1.0\n            done = True\n        obs = self._get_obs()\n        self.last_reward = reward\n        self.episode_total_reward += reward\n        return (obs, reward, done, {})\n\n    @property\n    def time_limit(self):\n        \"\"\"If an agent takes more than this many timesteps, end the episode\n        immediately and return a negative reward.\"\"\"\n        # (Seemingly arbitrary)\n        return self.input_width + len(self.target) + 4\n\n    def _check_levelup(self):\n        \"\"\"Called between episodes. Update our running record of episode rewards \n        and, if appropriate, 'level up' minimum input length.\"\"\"\n        if self.episode_total_reward is None:\n            # This is before the first episode/call to reset(). Nothing to do\n            return\n        AlgorithmicEnv.reward_shortfalls.append(self.episode_total_reward - len(self.target))\n        AlgorithmicEnv.reward_shortfalls = AlgorithmicEnv.reward_shortfalls[-self.last:]\n        if len(AlgorithmicEnv.reward_shortfalls) == self.last and \\\n          min(AlgorithmicEnv.reward_shortfalls) >= self.MIN_REWARD_SHORTFALL_FOR_PROMOTION and \\\n          AlgorithmicEnv.min_length < 30:\n            AlgorithmicEnv.min_length += 1\n            AlgorithmicEnv.reward_shortfalls = []\n        \n\n    def reset(self):\n        self._check_levelup()\n        self.last_action = None\n        self.last_reward = 0\n        self.read_head_position = self.READ_HEAD_START\n        self.write_head_position = 0\n        self.episode_total_reward = 0.0\n        self.time = 0\n        length = self.np_random.randint(3) + AlgorithmicEnv.min_length\n        self.input_data = self.generate_input_data(length)\n        self.target = self.target_from_input_data(self.input_data)\n        return self._get_obs()\n\n    def generate_input_data(self, size):\n        raise NotImplemented\n\n    def target_from_input_data(self, input_data):\n        raise NotImplemented(\"Subclasses must implement\")\n\n    def _move(self, movement):\n        raise NotImplemented\n\nclass TapeAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 1-d input tape.\"\"\"\n    MOVEMENTS = ['left', 'right']\n    READ_HEAD_START = 0\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        self.read_head_position += 1 if named == 'right' else -1\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        if pos < 0:\n            return self.base\n        if isinstance(pos, np.ndarray):\n            pos = pos.item()\n        try:\n            return self.input_data[pos]\n        except IndexError:\n            return self.base\n    \n    def generate_input_data(self, size):\n        return [self.np_random.randint(self.base) for _ in range(size)]\n\n    def render_observation(self):\n        x = self.read_head_position\n        x_str =      \"Observation Tape    : \"\n        for i in range(-2, self.input_width + 2):\n            if i == x:\n                x_str += colorize(self._get_str_obs(np.array([i])), 'green', highlight=True)\n            else:\n                x_str += self._get_str_obs(np.array([i]))\n        x_str += \"\\n\"\n        return x_str\n\nclass GridAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 2-d input grid.\"\"\"\n    MOVEMENTS = ['left', 'right', 'up', 'down']\n    READ_HEAD_START = (0, 0)\n    def __init__(self, rows, *args, **kwargs):\n        self.rows = rows\n        AlgorithmicEnv.__init__(self, *args, **kwargs)\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        x, y = self.read_head_position\n        if named == 'left':\n            x -= 1\n        elif named == 'right':\n            x += 1\n        elif named == 'up':\n            y -= 1\n        elif named == 'down':\n            y += 1\n        else:\n            raise ValueError(\"Unrecognized direction: {}\".format(named))\n        self.read_head_position = x, y\n\n    def generate_input_data(self, size):\n        return [\n            [self.np_random.randint(self.base) for _ in range(self.rows)]\n            for __ in range(size)\n        ]\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        x, y = pos\n        if any(idx < 0 for idx in pos):\n            return self.base\n        try:\n            return self.input_data[x][y]\n        except IndexError:\n            return self.base\n\n    def render_observation(self):\n        x = self.read_head_position\n        label =      \"Observation Grid    : \"\n        x_str = \"\"\n        for j in range(-1, self.rows+1):\n            if j != -1:\n                x_str += \" \" * len(label)\n            for i in range(-2, self.input_width + 2):\n                if i == x[0] and j == x[1]:\n                    x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True)\n                else:\n                    x_str += self._get_str_obs((i, j))\n            x_str += \"\\n\"\n        x_str = label + x_str\n        return x_str\n", "comments": "    algorithmic environments following traits common     a 1  input tape  2  input grid  characters   a target string deterministic function input characters  agents control read head moves input tape  observations consist single character currently read head  the read head may fall end tape direction  when happens  agents observe special blank character (with index env base) get back bounds   actions consist 3 sub actions        direction move read head (left right  plus 2 envs)       whether write output tape       which character write (ignored sub action 0)  an episode ends        the agent writes full target string output tape        the agent writes incorrect character        the agent runs time limit  (which fairly conservative )  reward schedule      write correct character   1     write wrong character    5     run clock   1     otherwise  0  in beginning  input strings fairly short  after environment consistently solved window episodes  environment  increase average length generated strings  typical env specs require leveling many times reach reward threshold      gym import env  logger gym spaces import discrete  tuple gym utils import colorize  seeding import numpy np six import stringio import sys import math  class algorithmicenv(env)       metadata     render modes     human    ansi          only  promote  length generated input strings worst        last n episodes far maximum reward     min reward shortfall for promotion    1 0      def   init  (self  base 10  chars false  starting min length 2)                      base  number distinct characters           chars  if true  use uppercase alphabet  otherwise  digits  only affects                rendering          starting min length  minimum input string length  ramps episodes                               consistently solved                      self base   base           keep track many past episodes         self last   10           cumulative reward earned episode         self episode total reward   none           running tally reward shortfalls  e g  10 points earn           got 8  append  2         algorithmicenv reward shortfalls              chars              self charmap    chr(ord( a ) i) range(base)          else              self charmap    str(i) range(base)          self charmap append(   )           todo  not clear class variable rather instance             could lead spooky action distance someone working           multiple algorithmic envs  also makes testing tricky          algorithmicenv min length   starting min length           three sub actions                  1  move read head left write (or down)                 2  write                 3  which character write  (ignored write 0)         self action space   tuple(              discrete(len(self movements))  discrete(2)  discrete(self base)          )           can see input tape (one n characters  nothing)         self observation space   discrete(self base   1)         self seed()         self reset()       classmethod     def  movement idx(kls  movement name)          return kls movements index(movement name)      def seed(self  seed none)          self np random  seed   seeding np random(seed)         return  seed       def  get obs(self  pos none)             return observation corresponding given read head position         (or current read head position  none given)             raise notimplemented      def  get str obs(self  pos none)          ret   self  get obs(pos)         return self charmap ret       def  get str target(self  pos)             return ith character target string (or     index         bounds)             pos   0 len(self target)    pos              return             else              return self charmap self target pos        def render observation(self)             return string representation input tape grid             raise notimplementederror      def render(self  mode  human )           outfile   stringio() mode     ansi  else sys stdout         inp    total length input instance    step   n    (self input width  self time)         outfile write(inp)         x   action   self read head position  self write head position  self last action         action none              inp act  act  pred   action         outfile write(      (len(inp)   1)     n )         str         output tape                     target str    targets                         action none              pred str   self charmap pred          x str   self render observation()         range( 2  len(self target)   2)              target str    self  get str target(i)                 1                  str    self  get str target(i)             elif    (y   1)                  action none act    1                      color    green  pred    self target  else  red                      str    colorize(pred str  color  highlight true)                 else                      str    self  get str target(i)         outfile write(x str)         outfile write(y str     n )         outfile write(target str     n n )          action none              outfile write( current reward            3f n    self last reward)             outfile write( cumulative reward         3f n    self episode total reward)             move   self movements inp act              outfile write( action                  tuple(move input    n    move)             act   act    1             outfile write(                               write output tape    n    act)             outfile write(                               prediction   s) n    pred str)         else              outfile write(  n    5)         return outfile       property     def input width(self)          return len(self input data)      def step(self  action)          assert self action space contains(action)         self last action   action         inp act  act  pred   action         done   false         reward   0 0         self time    1         assert 0    self write head position         act    1              try                  correct   pred    self target self write head position              except indexerror                  logger warn( it looks like calling step() even though                         environment already returned done true  you always call                         reset() receive done true  any steps undefined                         behaviour  )                 correct   false             correct                  reward   1 0             else                    bail soon wrong character written tape                 reward    0 5                 done   true             self write head position    1             self write head position    len(self target)                  done   true         self  move(inp act)         self time   self time limit              reward    1 0             done   true         obs   self  get obs()         self last reward   reward         self episode total reward    reward         return (obs  reward  done    )       property     def time limit(self)             if agent takes many timesteps  end episode         immediately return negative reward               (seemingly arbitrary)         return self input width   len(self target)   4      def  check levelup(self)             called episodes  update running record episode rewards           appropriate   level  minimum input length             self episode total reward none                this first episode call reset()  nothing             return         algorithmicenv reward shortfalls append(self episode total reward   len(self target))         algorithmicenv reward shortfalls   algorithmicenv reward shortfalls  self last           len(algorithmicenv reward shortfalls)    self last             min(algorithmicenv reward shortfalls)    self min reward shortfall for promotion             algorithmicenv min length   30              algorithmicenv min length    1             algorithmicenv reward shortfalls                    def reset(self)          self  check levelup()         self last action   none         self last reward   0         self read head position   self read head start         self write head position   0         self episode total reward   0 0         self time   0         length   self np random randint(3)   algorithmicenv min length         self input data   self generate input data(length)         self target   self target input data(self input data)         return self  get obs()      def generate input data(self  size)          raise notimplemented      def target input data(self  input data)          raise notimplemented( subclasses must implement )      def  move(self  movement)          raise notimplemented  class tapealgorithmicenv(algorithmicenv)         an algorithmic env 1 input tape         movements     left    right       read head start   0      def  move(self  movement)          named   self movements movement          self read head position    1 named     right  else  1      def  get obs(self  pos none)          pos none              pos   self read head position         pos   0              return self base         isinstance(pos  np ndarray)              pos   pos item()         try              return self input data pos          except indexerror              return self base          def generate input data(self  size)          return  self np random randint(self base)   range(size)       def render observation(self)          x   self read head position         x str         observation tape                range( 2  self input width   2)                 x                  x str    colorize(self  get str obs(np array( ))   green   highlight true)             else                  x str    self  get str obs(np array( ))         x str      n          return x str  class gridalgorithmicenv(algorithmicenv)         an algorithmic env 2 input grid        only  promote  length generated input strings worst     last n episodes far maximum reward    keep track many past episodes    cumulative reward earned episode    running tally reward shortfalls  e g  10 points earn    got 8  append  2    todo  not clear class variable rather instance      could lead spooky action distance someone working    multiple algorithmic envs  also makes testing tricky     three sub actions           1  move read head left write (or down)          2  write          3  which character write  (ignored write 0)    can see input tape (one n characters  nothing)    bail soon wrong character written tape    (seemingly arbitrary)    this first episode call reset()  nothing ", "content": "\"\"\"\nAlgorithmic environments have the following traits in common:\n\n- A 1-d \"input tape\" or 2-d \"input grid\" of characters\n- A target string which is a deterministic function of the input characters\n\nAgents control a read head that moves over the input tape. Observations consist\nof the single character currently under the read head. The read head may fall\noff the end of the tape in any direction. When this happens, agents will observe\na special blank character (with index=env.base) until they get back in bounds.\n\nActions consist of 3 sub-actions:\n    - Direction to move the read head (left or right, plus up and down for 2-d envs)\n    - Whether to write to the output tape\n    - Which character to write (ignored if the above sub-action is 0)\n\nAn episode ends when:\n    - The agent writes the full target string to the output tape.\n    - The agent writes an incorrect character.\n    - The agent runs out the time limit. (Which is fairly conservative.)\n\nReward schedule:\n    write a correct character: +1\n    write a wrong character: -.5\n    run out the clock: -1\n    otherwise: 0\n\nIn the beginning, input strings will be fairly short. After an environment has\nbeen consistently solved over some window of episodes, the environment will \nincrease the average length of generated strings. Typical env specs require\nleveling up many times to reach their reward threshold.\n\"\"\"\nfrom gym import Env, logger\nfrom gym.spaces import Discrete, Tuple\nfrom gym.utils import colorize, seeding\nimport numpy as np\nfrom six import StringIO\nimport sys\nimport math\n\nclass AlgorithmicEnv(Env):\n\n    metadata = {'render.modes': ['human', 'ansi']}\n    # Only 'promote' the length of generated input strings if the worst of the \n    # last n episodes was no more than this far from the maximum reward\n    MIN_REWARD_SHORTFALL_FOR_PROMOTION = -1.0\n\n    def __init__(self, base=10, chars=False, starting_min_length=2):\n        \"\"\"\n        base: Number of distinct characters. \n        chars: If True, use uppercase alphabet. Otherwise, digits. Only affects\n               rendering.\n        starting_min_length: Minimum input string length. Ramps up as episodes \n                             are consistently solved.\n        \"\"\"\n        self.base = base\n        # Keep track of this many past episodes\n        self.last = 10\n        # Cumulative reward earned this episode\n        self.episode_total_reward = None\n        # Running tally of reward shortfalls. e.g. if there were 10 points to earn and\n        # we got 8, we'd append -2\n        AlgorithmicEnv.reward_shortfalls = []\n        if chars:\n            self.charmap = [chr(ord('A')+i) for i in range(base)]\n        else:\n            self.charmap = [str(i) for i in range(base)]\n        self.charmap.append(' ')\n        # TODO: Not clear why this is a class variable rather than instance. \n        # Could lead to some spooky action at a distance if someone is working\n        # with multiple algorithmic envs at once. Also makes testing tricky.\n        AlgorithmicEnv.min_length = starting_min_length\n        # Three sub-actions:\n        #       1. Move read head left or write (or up/down)\n        #       2. Write or not\n        #       3. Which character to write. (Ignored if should_write=0)\n        self.action_space = Tuple(\n            [Discrete(len(self.MOVEMENTS)), Discrete(2), Discrete(self.base)]\n        )\n        # Can see just what is on the input tape (one of n characters, or nothing)\n        self.observation_space = Discrete(self.base + 1)\n        self.seed()\n        self.reset()\n\n    @classmethod\n    def _movement_idx(kls, movement_name):\n        return kls.MOVEMENTS.index(movement_name)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _get_obs(self, pos=None):\n        \"\"\"Return an observation corresponding to the given read head position\n        (or the current read head position, if none is given).\"\"\"\n        raise NotImplemented\n\n    def _get_str_obs(self, pos=None):\n        ret = self._get_obs(pos)\n        return self.charmap[ret]\n\n    def _get_str_target(self, pos):\n        \"\"\"Return the ith character of the target string (or \" \" if index\n        out of bounds).\"\"\"\n        if pos < 0 or len(self.target) <= pos:\n            return \" \"\n        else:\n            return self.charmap[self.target[pos]]\n\n    def render_observation(self):\n        \"\"\"Return a string representation of the input tape/grid.\"\"\"\n        raise NotImplementedError\n\n    def render(self, mode='human'):\n\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n        inp = \"Total length of input instance: %d, step: %d\\n\" % (self.input_width, self.time)\n        outfile.write(inp)\n        x, y, action = self.read_head_position, self.write_head_position, self.last_action\n        if action is not None:\n            inp_act, out_act, pred = action\n        outfile.write(\"=\" * (len(inp) - 1) + \"\\n\")\n        y_str =      \"Output Tape         : \"\n        target_str = \"Targets             : \"\n        if action is not None:\n            pred_str = self.charmap[pred]\n        x_str = self.render_observation()\n        for i in range(-2, len(self.target) + 2):\n            target_str += self._get_str_target(i)\n            if i < y - 1:\n                y_str += self._get_str_target(i)\n            elif i == (y - 1):\n                if action is not None and out_act == 1:\n                    color = 'green' if pred == self.target[i] else 'red'\n                    y_str += colorize(pred_str, color, highlight=True)\n                else:\n                    y_str += self._get_str_target(i)\n        outfile.write(x_str)\n        outfile.write(y_str + \"\\n\")\n        outfile.write(target_str + \"\\n\\n\")\n\n        if action is not None:\n            outfile.write(\"Current reward      :   %.3f\\n\" % self.last_reward)\n            outfile.write(\"Cumulative reward   :   %.3f\\n\" % self.episode_total_reward)\n            move = self.MOVEMENTS[inp_act]\n            outfile.write(\"Action              :   Tuple(move over input: %s,\\n\" % move)\n            out_act = out_act == 1\n            outfile.write(\"                              write to the output tape: %s,\\n\" % out_act)\n            outfile.write(\"                              prediction: %s)\\n\" % pred_str)\n        else:\n            outfile.write(\"\\n\" * 5)\n        return outfile\n\n    @property\n    def input_width(self):\n        return len(self.input_data)\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        self.last_action = action\n        inp_act, out_act, pred = action\n        done = False\n        reward = 0.0\n        self.time += 1\n        assert 0 <= self.write_head_position\n        if out_act == 1:\n            try:\n                correct = pred == self.target[self.write_head_position]\n            except IndexError:\n                logger.warn(\"It looks like you're calling step() even though this \"+\n                    \"environment has already returned done=True. You should always call \"+\n                    \"reset() once you receive done=True. Any further steps are undefined \"+\n                    \"behaviour.\")\n                correct = False\n            if correct:\n                reward = 1.0\n            else:\n                # Bail as soon as a wrong character is written to the tape\n                reward = -0.5\n                done = True\n            self.write_head_position += 1\n            if self.write_head_position >= len(self.target):\n                done = True\n        self._move(inp_act)\n        if self.time > self.time_limit:\n            reward = -1.0\n            done = True\n        obs = self._get_obs()\n        self.last_reward = reward\n        self.episode_total_reward += reward\n        return (obs, reward, done, {})\n\n    @property\n    def time_limit(self):\n        \"\"\"If an agent takes more than this many timesteps, end the episode\n        immediately and return a negative reward.\"\"\"\n        # (Seemingly arbitrary)\n        return self.input_width + len(self.target) + 4\n\n    def _check_levelup(self):\n        \"\"\"Called between episodes. Update our running record of episode rewards \n        and, if appropriate, 'level up' minimum input length.\"\"\"\n        if self.episode_total_reward is None:\n            # This is before the first episode/call to reset(). Nothing to do\n            return\n        AlgorithmicEnv.reward_shortfalls.append(self.episode_total_reward - len(self.target))\n        AlgorithmicEnv.reward_shortfalls = AlgorithmicEnv.reward_shortfalls[-self.last:]\n        if len(AlgorithmicEnv.reward_shortfalls) == self.last and \\\n          min(AlgorithmicEnv.reward_shortfalls) >= self.MIN_REWARD_SHORTFALL_FOR_PROMOTION and \\\n          AlgorithmicEnv.min_length < 30:\n            AlgorithmicEnv.min_length += 1\n            AlgorithmicEnv.reward_shortfalls = []\n        \n\n    def reset(self):\n        self._check_levelup()\n        self.last_action = None\n        self.last_reward = 0\n        self.read_head_position = self.READ_HEAD_START\n        self.write_head_position = 0\n        self.episode_total_reward = 0.0\n        self.time = 0\n        length = self.np_random.randint(3) + AlgorithmicEnv.min_length\n        self.input_data = self.generate_input_data(length)\n        self.target = self.target_from_input_data(self.input_data)\n        return self._get_obs()\n\n    def generate_input_data(self, size):\n        raise NotImplemented\n\n    def target_from_input_data(self, input_data):\n        raise NotImplemented(\"Subclasses must implement\")\n\n    def _move(self, movement):\n        raise NotImplemented\n\nclass TapeAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 1-d input tape.\"\"\"\n    MOVEMENTS = ['left', 'right']\n    READ_HEAD_START = 0\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        self.read_head_position += 1 if named == 'right' else -1\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        if pos < 0:\n            return self.base\n        if isinstance(pos, np.ndarray):\n            pos = pos.item()\n        try:\n            return self.input_data[pos]\n        except IndexError:\n            return self.base\n    \n    def generate_input_data(self, size):\n        return [self.np_random.randint(self.base) for _ in range(size)]\n\n    def render_observation(self):\n        x = self.read_head_position\n        x_str =      \"Observation Tape    : \"\n        for i in range(-2, self.input_width + 2):\n            if i == x:\n                x_str += colorize(self._get_str_obs(np.array([i])), 'green', highlight=True)\n            else:\n                x_str += self._get_str_obs(np.array([i]))\n        x_str += \"\\n\"\n        return x_str\n\nclass GridAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 2-d input grid.\"\"\"\n    MOVEMENTS = ['left', 'right', 'up', 'down']\n    READ_HEAD_START = (0, 0)\n    def __init__(self, rows, *args, **kwargs):\n        self.rows = rows\n        AlgorithmicEnv.__init__(self, *args, **kwargs)\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        x, y = self.read_head_position\n        if named == 'left':\n            x -= 1\n        elif named == 'right':\n            x += 1\n        elif named == 'up':\n            y -= 1\n        elif named == 'down':\n            y += 1\n        else:\n            raise ValueError(\"Unrecognized direction: {}\".format(named))\n        self.read_head_position = x, y\n\n    def generate_input_data(self, size):\n        return [\n            [self.np_random.randint(self.base) for _ in range(self.rows)]\n            for __ in range(size)\n        ]\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        x, y = pos\n        if any(idx < 0 for idx in pos):\n            return self.base\n        try:\n            return self.input_data[x][y]\n        except IndexError:\n            return self.base\n\n    def render_observation(self):\n        x = self.read_head_position\n        label =      \"Observation Grid    : \"\n        x_str = \"\"\n        for j in range(-1, self.rows+1):\n            if j != -1:\n                x_str += \" \" * len(label)\n            for i in range(-2, self.input_width + 2):\n                if i == x[0] and j == x[1]:\n                    x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True)\n                else:\n                    x_str += self._get_str_obs((i, j))\n            x_str += \"\\n\"\n        x_str = label + x_str\n        return x_str\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "algorithmic_env.py", "id": "0a527f87b131d00276dd3f953ca88f92", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/algorithmic/algorithmic_env.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}