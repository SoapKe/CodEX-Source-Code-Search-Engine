{"author": "chiphuyen", "code": "\"\"\" Solution for simple linear regression example using placeholders\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\n\n# Step 2: create placeholders for X (birth rate) and Y (life expectancy)\nX = tf.placeholder(tf.float32, name='X')\nY = tf.placeholder(tf.float32, name='Y')\n\n\nw = tf.get_variable('weights', initializer=tf.constant(0.0))\nb = tf.get_variable('bias', initializer=tf.constant(0.0))\n\n\nY_predicted = w * X + b \n\n\n\nloss = tf.square(Y - Y_predicted, name='loss')\n# loss = utils.huber_loss(Y, Y_predicted)\n\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n\n\nstart = time.time()\nwriter = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph())\nwith tf.Session() as sess:\n\t\n\tsess.run(tf.global_variables_initializer()) \n\t\n\t\n\tfor i in range(100): \n\t\ttotal_loss = 0\n\t\tfor x, y in data:\n\t\t\t\n\t\t\t_, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) \n\t\t\ttotal_loss += l\n\t\tprint('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n\n\t\n\twriter.close() \n\t\n\t\n\tw_out, b_out = sess.run([w, b]) \n\nprint('Took: %f seconds' %(time.time() - start))\n\n\nplt.plot(data[:,0], data[:,1], 'bo', label='Real data')\nplt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data')\nplt.legend()\nplt.show()", "comments": "    solution simple linear regression example using placeholders created chip huyen (chiphuyen cs stanford edu) cs20   tensorflow deep learning research  cs20 stanford edu lecture 03        step 1  read data  txt file    step 2  create placeholders x (birth rate) y (life expectancy)    step 3  create weight bias  initialized 0    step 4  build model predict y    step 5  use squared error loss function    use either mean squared error huber loss    loss   utils huber loss(y  y predicted)    step 6  using gradient descent learning rate 0 001 minimize loss    step 7  initialize necessary variables  case  w b    step 8  train model 100 epochs    session execute optimizer fetch values loss    close writer done using    step 9  output values w b    plot results ", "content": "\"\"\" Solution for simple linear regression example using placeholders\nCreated by Chip Huyen (chiphuyen@cs.stanford.edu)\nCS20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nLecture 03\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport utils\n\nDATA_FILE = 'data/birth_life_2010.txt'\n\n# Step 1: read in data from the .txt file\ndata, n_samples = utils.read_birth_life_data(DATA_FILE)\n\n# Step 2: create placeholders for X (birth rate) and Y (life expectancy)\nX = tf.placeholder(tf.float32, name='X')\nY = tf.placeholder(tf.float32, name='Y')\n\n# Step 3: create weight and bias, initialized to 0\nw = tf.get_variable('weights', initializer=tf.constant(0.0))\nb = tf.get_variable('bias', initializer=tf.constant(0.0))\n\n# Step 4: build model to predict Y\nY_predicted = w * X + b \n\n# Step 5: use the squared error as the loss function\n# you can use either mean squared error or Huber loss\nloss = tf.square(Y - Y_predicted, name='loss')\n# loss = utils.huber_loss(Y, Y_predicted)\n\n# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n\n\nstart = time.time()\nwriter = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph())\nwith tf.Session() as sess:\n\t# Step 7: initialize the necessary variables, in this case, w and b\n\tsess.run(tf.global_variables_initializer()) \n\t\n\t# Step 8: train the model for 100 epochs\n\tfor i in range(100): \n\t\ttotal_loss = 0\n\t\tfor x, y in data:\n\t\t\t# Session execute optimizer and fetch values of loss\n\t\t\t_, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) \n\t\t\ttotal_loss += l\n\t\tprint('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n\n\t# close the writer when you're done using it\n\twriter.close() \n\t\n\t# Step 9: output the values of w and b\n\tw_out, b_out = sess.run([w, b]) \n\nprint('Took: %f seconds' %(time.time() - start))\n\n# plot the results\nplt.plot(data[:,0], data[:,1], 'bo', label='Real data')\nplt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data')\nplt.legend()\nplt.show()", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "03_linreg_placeholder.py", "id": "5d7189e13e9d8a4d21f03a946f324130", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/03_linreg_placeholder.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}