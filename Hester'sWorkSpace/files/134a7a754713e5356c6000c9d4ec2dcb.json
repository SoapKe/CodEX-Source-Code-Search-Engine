{"author": "tflearn", "code": "\n\nimport tensorflow as tf\nimport tflearn\nimport tflearn.variables as va\n\n\nimport tflearn.datasets.mnist as mnist\ntrainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n\n\nwith tf.Graph().as_default():\n\n    \n    X = tf.placeholder(\"float\", [None, 784])\n    Y = tf.placeholder(\"float\", [None, 10])\n\n    \n    def dnn(x):\n        with tf.variable_scope('Layer1'):\n            \n            W1 = va.variable(name='W', shape=[784, 256],\n                             initializer='uniform_scaling',\n                             regularizer='L2')\n            b1 = va.variable(name='b', shape=[256])\n            x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n\n        with tf.variable_scope('Layer2'):\n            W2 = va.variable(name='W', shape=[256, 256],\n                             initializer='uniform_scaling',\n                             regularizer='L2')\n            b2 = va.variable(name='b', shape=[256])\n            x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n\n        with tf.variable_scope('Layer3'):\n            W3 = va.variable(name='W', shape=[256, 10],\n                             initializer='uniform_scaling')\n            b3 = va.variable(name='b', shape=[10])\n            x = tf.add(tf.matmul(x, W3), b3)\n\n        return x\n\n    net = dnn(X)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n        name='acc')\n\n    \n    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n                              metric=accuracy, batch_size=128)\n\n    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=3,\n                              tensorboard_dir='/tmp/tflearn_logs/')\n    \n    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n                n_epoch=10, show_metric=True, run_id='Variables_example')\n", "comments": "    this example introduces use tflearn variables easily implement tensorflow variables custom initialization regularization   note  if using tflearn layers  inititalization regularization directly defined layer definition level applied inner variables         loading mnist dataset    define dnn using tensorflow    model variables    multilayer perceptron    creating variable using tflearn    define train op    training 10 epochs  ", "content": "\"\"\"\nThis example introduces the use of TFLearn variables to easily implement\nTensorflow variables with custom initialization and regularization.\n\nNote: If you are using TFLearn layers, inititalization and regularization\nare directly defined at the layer definition level and applied to inner\nvariables.\n\"\"\"\n\nimport tensorflow as tf\nimport tflearn\nimport tflearn.variables as va\n\n# Loading MNIST dataset\nimport tflearn.datasets.mnist as mnist\ntrainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n\n# Define a dnn using Tensorflow\nwith tf.Graph().as_default():\n\n    # Model variables\n    X = tf.placeholder(\"float\", [None, 784])\n    Y = tf.placeholder(\"float\", [None, 10])\n\n    # Multilayer perceptron\n    def dnn(x):\n        with tf.variable_scope('Layer1'):\n            # Creating variable using TFLearn\n            W1 = va.variable(name='W', shape=[784, 256],\n                             initializer='uniform_scaling',\n                             regularizer='L2')\n            b1 = va.variable(name='b', shape=[256])\n            x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n\n        with tf.variable_scope('Layer2'):\n            W2 = va.variable(name='W', shape=[256, 256],\n                             initializer='uniform_scaling',\n                             regularizer='L2')\n            b2 = va.variable(name='b', shape=[256])\n            x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n\n        with tf.variable_scope('Layer3'):\n            W3 = va.variable(name='W', shape=[256, 10],\n                             initializer='uniform_scaling')\n            b3 = va.variable(name='b', shape=[10])\n            x = tf.add(tf.matmul(x, W3), b3)\n\n        return x\n\n    net = dnn(X)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n        name='acc')\n\n    # Define a train op\n    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n                              metric=accuracy, batch_size=128)\n\n    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=3,\n                              tensorboard_dir='/tmp/tflearn_logs/')\n    # Training for 10 epochs.\n    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n                n_epoch=10, show_metric=True, run_id='Variables_example')\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "variables.py", "id": "134a7a754713e5356c6000c9d4ec2dcb", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/extending_tensorflow/variables.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}