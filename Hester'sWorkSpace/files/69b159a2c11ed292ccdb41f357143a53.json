{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Networks for MNIST example using TFGAN.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nds = tf.contrib.distributions\nlayers = tf.contrib.layers\ntfgan = tf.contrib.gan\n\n\ndef _generator_helper(\n    noise, is_conditional, one_hot_labels, weight_decay, is_training):\n  \"\"\"Core MNIST generator.\n\n  This function is reused between the different GAN modes (unconditional,\n  conditional, etc).\n\n  Args:\n    noise: A 2D Tensor of shape [batch size, noise dim].\n    is_conditional: Whether to condition on labels.\n    one_hot_labels: Optional labels for conditioning.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  with tf.contrib.framework.arg_scope(\n      [layers.fully_connected, layers.conv2d_transpose],\n      activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n      weights_regularizer=layers.l2_regularizer(weight_decay)):\n    with tf.contrib.framework.arg_scope(\n        [layers.batch_norm], is_training=is_training):\n      net = layers.fully_connected(noise, 1024)\n      if is_conditional:\n        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n      net = layers.fully_connected(net, 7 * 7 * 128)\n      net = tf.reshape(net, [-1, 7, 7, 128])\n      net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n      net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n       Make sure that generator output is in the same range as `inputs`\n       ie [-1, 1].\n      net = layers.conv2d(\n          net, 1, [4, 4], normalizer_fn=None, activation_fn=tf.tanh)\n\n      return net\n\n\ndef unconditional_generator(noise, weight_decay=2.5e-5, is_training=True):\n  \"\"\"Generator to produce unconditional MNIST images.\n\n  Args:\n    noise: A single Tensor representing noise.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  return _generator_helper(noise, False, None, weight_decay, is_training)\n\n\ndef conditional_generator(inputs, weight_decay=2.5e-5, is_training=True):\n  \"\"\"Generator to produce MNIST images conditioned on class.\n\n  Args:\n    inputs: A 2-tuple of Tensors (noise, one_hot_labels).\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  noise, one_hot_labels = inputs\n  return _generator_helper(\n      noise, True, one_hot_labels, weight_decay, is_training)\n\n\ndef infogan_generator(inputs, categorical_dim, weight_decay=2.5e-5,\n                      is_training=True):\n  \"\"\"InfoGAN generator network on MNIST digits.\n\n  Based on a paper https://arxiv.org/abs/1606.03657, their code\n  https://github.com/openai/InfoGAN, and code by pooleb@.\n\n  Args:\n    inputs: A 3-tuple of Tensors (unstructured_noise, categorical structured\n      noise, continuous structured noise). `inputs[0]` and `inputs[2]` must be\n      2D, and `inputs[1]` must be 1D. All must have the same first dimension.\n    categorical_dim: Dimensions of the incompressible categorical noise.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  unstructured_noise, cat_noise, cont_noise = inputs\n  cat_noise_onehot = tf.one_hot(cat_noise, categorical_dim)\n  all_noise = tf.concat(\n      [unstructured_noise, cat_noise_onehot, cont_noise], axis=1)\n  return _generator_helper(all_noise, False, None, weight_decay, is_training)\n\n\n_leaky_relu = lambda x: tf.nn.leaky_relu(x, alpha=0.01)\n\n\ndef _discriminator_helper(img, is_conditional, one_hot_labels, weight_decay):\n  \"\"\"Core MNIST discriminator.\n\n  This function is reused between the different GAN modes (unconditional,\n  conditional, etc).\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    is_conditional: Whether to condition on labels.\n    one_hot_labels: Labels to optionally condition the network on.\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Final fully connected discriminator layer. [batch_size, 1024].\n  \"\"\"\n  with tf.contrib.framework.arg_scope(\n      [layers.conv2d, layers.fully_connected],\n      activation_fn=_leaky_relu, normalizer_fn=None,\n      weights_regularizer=layers.l2_regularizer(weight_decay),\n      biases_regularizer=layers.l2_regularizer(weight_decay)):\n    net = layers.conv2d(img, 64, [4, 4], stride=2)\n    net = layers.conv2d(net, 128, [4, 4], stride=2)\n    net = layers.flatten(net)\n    if is_conditional:\n      net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n    net = layers.fully_connected(net, 1024, normalizer_fn=layers.layer_norm)\n\n    return net\n\n\ndef unconditional_discriminator(img, unused_conditioning, weight_decay=2.5e-5):\n  \"\"\"Discriminator network on unconditional MNIST digits.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    unused_conditioning: The TFGAN API can help with conditional GANs, which\n      would require extra `condition` information to both the generator and the\n      discriminator. Since this example is not conditional, we do not use this\n      argument.\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Logits for the probability that the image is real.\n  \"\"\"\n  net = _discriminator_helper(img, False, None, weight_decay)\n  return layers.linear(net, 1)\n\n\ndef conditional_discriminator(img, conditioning, weight_decay=2.5e-5):\n  \"\"\"Conditional discriminator network on MNIST digits.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    conditioning: A 2-tuple of Tensors representing (noise, one_hot_labels).\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Logits for the probability that the image is real.\n  \"\"\"\n  _, one_hot_labels = conditioning\n  net = _discriminator_helper(img, True, one_hot_labels, weight_decay)\n  return layers.linear(net, 1)\n\n\ndef infogan_discriminator(img, unused_conditioning, weight_decay=2.5e-5,\n                          categorical_dim=10, continuous_dim=2):\n  \"\"\"InfoGAN discriminator network on MNIST digits.\n\n  Based on a paper https://arxiv.org/abs/1606.03657, their code\n  https://github.com/openai/InfoGAN, and code by pooleb@.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    unused_conditioning: The TFGAN API can help with conditional GANs, which\n      would require extra `condition` information to both the generator and the\n      discriminator. Since this example is not conditional, we do not use this\n      argument.\n    weight_decay: The L2 weight decay.\n    categorical_dim: Dimensions of the incompressible categorical noise.\n    continuous_dim: Dimensions of the incompressible continuous noise.\n\n  Returns:\n    Logits for the probability that the image is real, and a list of posterior\n    distributions for each of the noise vectors.\n  \"\"\"\n  net = _discriminator_helper(img, False, None, weight_decay)\n  logits_real = layers.fully_connected(net, 1, activation_fn=None)\n\n   Recognition network for latent variables has an additional layer\n  with tf.contrib.framework.arg_scope([layers.batch_norm], is_training=False):\n    encoder = layers.fully_connected(net, 128, normalizer_fn=layers.batch_norm,\n                                     activation_fn=_leaky_relu)\n\n   Compute logits for each category of categorical latent.\n  logits_cat = layers.fully_connected(\n      encoder, categorical_dim, activation_fn=None)\n  q_cat = ds.Categorical(logits_cat)\n\n   Compute mean for Gaussian posterior of continuous latents.\n  mu_cont = layers.fully_connected(encoder, continuous_dim, activation_fn=None)\n  sigma_cont = tf.ones_like(mu_cont)\n  q_cont = ds.Normal(loc=mu_cont, scale=sigma_cont)\n\n  return logits_real, [q_cat, q_cont]\n", "comments": "   networks mnist example using tfgan        future   import absolute import   future   import division   future   import print function  import tensorflow tf  ds   tf contrib distributions layers   tf contrib layers tfgan   tf contrib gan   def  generator helper(     noise  conditional  one hot labels  weight decay  training)       core mnist generator     this function reused different gan modes (unconditional    conditional  etc)     args      noise  a 2d tensor shape  batch size  noise dim       conditional  whether condition labels      one hot labels  optional labels conditioning      weight decay  the value l2 weight decay      training  if  true   batch norm uses batch statistics  if  false   batch       norm uses exponential moving average collected population       statistics     returns      a generated image range   1  1           tf contrib framework arg scope(        layers fully connected  layers conv2d transpose         activation fn tf nn relu  normalizer fn layers batch norm        weights regularizer layers l2 regularizer(weight decay))      tf contrib framework arg scope(          layers batch norm   training training)        net   layers fully connected(noise  1024)       conditional          net   tfgan features condition tensor onehot(net  one hot labels)       net   layers fully connected(net  7   7   128)       net   tf reshape(net    1  7  7  128 )       net   layers conv2d transpose(net  64   4  4   stride 2)       net   layers conv2d transpose(net  32   4  4   stride 2)         make sure generator output range  inputs          ie   1  1         net   layers conv2d(           net  1   4  4   normalizer fn none  activation fn tf tanh)        return net   def unconditional generator(noise  weight decay 2 5e 5  training true)       generator produce unconditional mnist images     args      noise  a single tensor representing noise      weight decay  the value l2 weight decay      training  if  true   batch norm uses batch statistics  if  false   batch       norm uses exponential moving average collected population       statistics     returns      a generated image range   1  1           return  generator helper(noise  false  none  weight decay  training)   def conditional generator(inputs  weight decay 2 5e 5  training true)       generator produce mnist images conditioned class     args      inputs  a 2 tuple tensors (noise  one hot labels)      weight decay  the value l2 weight decay      training  if  true   batch norm uses batch statistics  if  false   batch       norm uses exponential moving average collected population       statistics     returns      a generated image range   1  1           noise  one hot labels   inputs   return  generator helper(       noise  true  one hot labels  weight decay  training)   def infogan generator(inputs  categorical dim  weight decay 2 5e 5                        training true)       infogan generator network mnist digits     based paper https   arxiv org abs 1606 03657  code   https   github com openai infogan  code pooleb      args      inputs  a 3 tuple tensors (unstructured noise  categorical structured       noise  continuous structured noise)   inputs 0    inputs 2   must       2d   inputs 1   must 1d  all must first dimension      categorical dim  dimensions incompressible categorical noise      weight decay  the value l2 weight decay      training  if  true   batch norm uses batch statistics  if  false   batch       norm uses exponential moving average collected population       statistics     returns      a generated image range   1  1           unstructured noise  cat noise  cont noise   inputs   cat noise onehot   tf one hot(cat noise  categorical dim)   noise   tf concat(        unstructured noise  cat noise onehot  cont noise   axis 1)   return  generator helper(all noise  false  none  weight decay  training)    leaky relu   lambda x  tf nn leaky relu(x  alpha 0 01)   def  discriminator helper(img  conditional  one hot labels  weight decay)       core mnist discriminator     this function reused different gan modes (unconditional    conditional  etc)     args      img  real generated mnist digits  should range   1  1       conditional  whether condition labels      one hot labels  labels optionally condition network      weight decay  the l2 weight decay     returns      final fully connected discriminator layer   batch size  1024           tf contrib framework arg scope(        layers conv2d  layers fully connected         activation fn  leaky relu  normalizer fn none        weights regularizer layers l2 regularizer(weight decay)        biases regularizer layers l2 regularizer(weight decay))      net   layers conv2d(img  64   4  4   stride 2)     net   layers conv2d(net  128   4  4   stride 2)     net   layers flatten(net)     conditional        net   tfgan features condition tensor onehot(net  one hot labels)     net   layers fully connected(net  1024  normalizer fn layers layer norm)      return net   def unconditional discriminator(img  unused conditioning  weight decay 2 5e 5)       discriminator network unconditional mnist digits     args      img  real generated mnist digits  should range   1  1       unused conditioning  the tfgan api help conditional gans        would require extra  condition  information generator       discriminator  since example conditional  use       argument      weight decay  the l2 weight decay     returns      logits probability image real          net    discriminator helper(img  false  none  weight decay)   return layers linear(net  1)   def conditional discriminator(img  conditioning  weight decay 2 5e 5)       conditional discriminator network mnist digits     args      img  real generated mnist digits  should range   1  1       conditioning  a 2 tuple tensors representing (noise  one hot labels)      weight decay  the l2 weight decay     returns      logits probability image real             one hot labels   conditioning   net    discriminator helper(img  true  one hot labels  weight decay)   return layers linear(net  1)   def infogan discriminator(img  unused conditioning  weight decay 2 5e 5                            categorical dim 10  continuous dim 2)       infogan discriminator network mnist digits     based paper https   arxiv org abs 1606 03657  code   https   github com openai infogan  code pooleb      args      img  real generated mnist digits  should range   1  1       unused conditioning  the tfgan api help conditional gans        would require extra  condition  information generator       discriminator  since example conditional  use       argument      weight decay  the l2 weight decay      categorical dim  dimensions incompressible categorical noise      continuous dim  dimensions incompressible continuous noise     returns      logits probability image real  list posterior     distributions noise vectors           copyright 2017 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       make sure generator output range  inputs     ie   1  1      recognition network latent variables additional layer    compute logits category categorical latent     compute mean gaussian posterior continuous latents  ", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Networks for MNIST example using TFGAN.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nds = tf.contrib.distributions\nlayers = tf.contrib.layers\ntfgan = tf.contrib.gan\n\n\ndef _generator_helper(\n    noise, is_conditional, one_hot_labels, weight_decay, is_training):\n  \"\"\"Core MNIST generator.\n\n  This function is reused between the different GAN modes (unconditional,\n  conditional, etc).\n\n  Args:\n    noise: A 2D Tensor of shape [batch size, noise dim].\n    is_conditional: Whether to condition on labels.\n    one_hot_labels: Optional labels for conditioning.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  with tf.contrib.framework.arg_scope(\n      [layers.fully_connected, layers.conv2d_transpose],\n      activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n      weights_regularizer=layers.l2_regularizer(weight_decay)):\n    with tf.contrib.framework.arg_scope(\n        [layers.batch_norm], is_training=is_training):\n      net = layers.fully_connected(noise, 1024)\n      if is_conditional:\n        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n      net = layers.fully_connected(net, 7 * 7 * 128)\n      net = tf.reshape(net, [-1, 7, 7, 128])\n      net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n      net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n      # Make sure that generator output is in the same range as `inputs`\n      # ie [-1, 1].\n      net = layers.conv2d(\n          net, 1, [4, 4], normalizer_fn=None, activation_fn=tf.tanh)\n\n      return net\n\n\ndef unconditional_generator(noise, weight_decay=2.5e-5, is_training=True):\n  \"\"\"Generator to produce unconditional MNIST images.\n\n  Args:\n    noise: A single Tensor representing noise.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  return _generator_helper(noise, False, None, weight_decay, is_training)\n\n\ndef conditional_generator(inputs, weight_decay=2.5e-5, is_training=True):\n  \"\"\"Generator to produce MNIST images conditioned on class.\n\n  Args:\n    inputs: A 2-tuple of Tensors (noise, one_hot_labels).\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  noise, one_hot_labels = inputs\n  return _generator_helper(\n      noise, True, one_hot_labels, weight_decay, is_training)\n\n\ndef infogan_generator(inputs, categorical_dim, weight_decay=2.5e-5,\n                      is_training=True):\n  \"\"\"InfoGAN generator network on MNIST digits.\n\n  Based on a paper https://arxiv.org/abs/1606.03657, their code\n  https://github.com/openai/InfoGAN, and code by pooleb@.\n\n  Args:\n    inputs: A 3-tuple of Tensors (unstructured_noise, categorical structured\n      noise, continuous structured noise). `inputs[0]` and `inputs[2]` must be\n      2D, and `inputs[1]` must be 1D. All must have the same first dimension.\n    categorical_dim: Dimensions of the incompressible categorical noise.\n    weight_decay: The value of the l2 weight decay.\n    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n      norm uses the exponential moving average collected from population\n      statistics.\n\n  Returns:\n    A generated image in the range [-1, 1].\n  \"\"\"\n  unstructured_noise, cat_noise, cont_noise = inputs\n  cat_noise_onehot = tf.one_hot(cat_noise, categorical_dim)\n  all_noise = tf.concat(\n      [unstructured_noise, cat_noise_onehot, cont_noise], axis=1)\n  return _generator_helper(all_noise, False, None, weight_decay, is_training)\n\n\n_leaky_relu = lambda x: tf.nn.leaky_relu(x, alpha=0.01)\n\n\ndef _discriminator_helper(img, is_conditional, one_hot_labels, weight_decay):\n  \"\"\"Core MNIST discriminator.\n\n  This function is reused between the different GAN modes (unconditional,\n  conditional, etc).\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    is_conditional: Whether to condition on labels.\n    one_hot_labels: Labels to optionally condition the network on.\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Final fully connected discriminator layer. [batch_size, 1024].\n  \"\"\"\n  with tf.contrib.framework.arg_scope(\n      [layers.conv2d, layers.fully_connected],\n      activation_fn=_leaky_relu, normalizer_fn=None,\n      weights_regularizer=layers.l2_regularizer(weight_decay),\n      biases_regularizer=layers.l2_regularizer(weight_decay)):\n    net = layers.conv2d(img, 64, [4, 4], stride=2)\n    net = layers.conv2d(net, 128, [4, 4], stride=2)\n    net = layers.flatten(net)\n    if is_conditional:\n      net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n    net = layers.fully_connected(net, 1024, normalizer_fn=layers.layer_norm)\n\n    return net\n\n\ndef unconditional_discriminator(img, unused_conditioning, weight_decay=2.5e-5):\n  \"\"\"Discriminator network on unconditional MNIST digits.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    unused_conditioning: The TFGAN API can help with conditional GANs, which\n      would require extra `condition` information to both the generator and the\n      discriminator. Since this example is not conditional, we do not use this\n      argument.\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Logits for the probability that the image is real.\n  \"\"\"\n  net = _discriminator_helper(img, False, None, weight_decay)\n  return layers.linear(net, 1)\n\n\ndef conditional_discriminator(img, conditioning, weight_decay=2.5e-5):\n  \"\"\"Conditional discriminator network on MNIST digits.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    conditioning: A 2-tuple of Tensors representing (noise, one_hot_labels).\n    weight_decay: The L2 weight decay.\n\n  Returns:\n    Logits for the probability that the image is real.\n  \"\"\"\n  _, one_hot_labels = conditioning\n  net = _discriminator_helper(img, True, one_hot_labels, weight_decay)\n  return layers.linear(net, 1)\n\n\ndef infogan_discriminator(img, unused_conditioning, weight_decay=2.5e-5,\n                          categorical_dim=10, continuous_dim=2):\n  \"\"\"InfoGAN discriminator network on MNIST digits.\n\n  Based on a paper https://arxiv.org/abs/1606.03657, their code\n  https://github.com/openai/InfoGAN, and code by pooleb@.\n\n  Args:\n    img: Real or generated MNIST digits. Should be in the range [-1, 1].\n    unused_conditioning: The TFGAN API can help with conditional GANs, which\n      would require extra `condition` information to both the generator and the\n      discriminator. Since this example is not conditional, we do not use this\n      argument.\n    weight_decay: The L2 weight decay.\n    categorical_dim: Dimensions of the incompressible categorical noise.\n    continuous_dim: Dimensions of the incompressible continuous noise.\n\n  Returns:\n    Logits for the probability that the image is real, and a list of posterior\n    distributions for each of the noise vectors.\n  \"\"\"\n  net = _discriminator_helper(img, False, None, weight_decay)\n  logits_real = layers.fully_connected(net, 1, activation_fn=None)\n\n  # Recognition network for latent variables has an additional layer\n  with tf.contrib.framework.arg_scope([layers.batch_norm], is_training=False):\n    encoder = layers.fully_connected(net, 128, normalizer_fn=layers.batch_norm,\n                                     activation_fn=_leaky_relu)\n\n  # Compute logits for each category of categorical latent.\n  logits_cat = layers.fully_connected(\n      encoder, categorical_dim, activation_fn=None)\n  q_cat = ds.Categorical(logits_cat)\n\n  # Compute mean for Gaussian posterior of continuous latents.\n  mu_cont = layers.fully_connected(encoder, continuous_dim, activation_fn=None)\n  sigma_cont = tf.ones_like(mu_cont)\n  q_cont = ds.Normal(loc=mu_cont, scale=sigma_cont)\n\n  return logits_real, [q_cat, q_cont]\n", "description": "Models and examples built with TensorFlow", "file_name": "networks.py", "id": "69b159a2c11ed292ccdb41f357143a53", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/gan/mnist/networks.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}