{"author": "NVIDIA", "code": "import os\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.serialization import load_lua\n\nfrom models import VGGEncoder, VGGDecoder\nfrom photo_wct import PhotoWCT\n\n\ndef weight_assign(lua, pth, maps):\n    for k, v in maps.items():\n        getattr(pth, k).weight = nn.Parameter(lua.get(v).weight.float())\n        getattr(pth, k).bias = nn.Parameter(lua.get(v).bias.float())\n\n\ndef photo_wct_loader(p_wct):\n    p_wct.e1.load_state_dict(torch.load('pth_models/vgg_normalised_conv1.pth'))\n    p_wct.d1.load_state_dict(torch.load('pth_models/feature_invertor_conv1.pth'))\n    p_wct.e2.load_state_dict(torch.load('pth_models/vgg_normalised_conv2.pth'))\n    p_wct.d2.load_state_dict(torch.load('pth_models/feature_invertor_conv2.pth'))\n    p_wct.e3.load_state_dict(torch.load('pth_models/vgg_normalised_conv3.pth'))\n    p_wct.d3.load_state_dict(torch.load('pth_models/feature_invertor_conv3.pth'))\n    p_wct.e4.load_state_dict(torch.load('pth_models/vgg_normalised_conv4.pth'))\n    p_wct.d4.load_state_dict(torch.load('pth_models/feature_invertor_conv4.pth'))\n\n\nif __name__ == '__main__':\n    if not os.path.exists('pth_models'):\n        os.mkdir('pth_models')\n    \n    \n    vgg1 = load_lua('models/vgg_normalised_conv1_1_mask.t7')\n    e1 = VGGEncoder(1)\n    weight_assign(vgg1, e1, {\n        'conv0': 0,\n        'conv1_1': 2,\n    })\n    torch.save(e1.state_dict(), 'pth_models/vgg_normalised_conv1.pth')\n    \n    \n    inv1 = load_lua('models/feature_invertor_conv1_1_mask.t7')\n    d1 = VGGDecoder(1)\n    weight_assign(inv1, d1, {\n        'conv1_1': 1,\n    })\n    torch.save(d1.state_dict(), 'pth_models/feature_invertor_conv1.pth')\n    \n    \n    vgg2 = load_lua('models/vgg_normalised_conv2_1_mask.t7')\n    e2 = VGGEncoder(2)\n    weight_assign(vgg2, e2, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n    })\n    torch.save(e2.state_dict(), 'pth_models/vgg_normalised_conv2.pth')\n    \n    \n    inv2 = load_lua('models/feature_invertor_conv2_1_mask.t7')\n    d2 = VGGDecoder(2)\n    weight_assign(inv2, d2, {\n        'conv2_1': 1,\n        'conv1_2': 5,\n        'conv1_1': 8,\n    })\n    torch.save(d2.state_dict(), 'pth_models/feature_invertor_conv2.pth')\n    \n    \n    vgg3 = load_lua('models/vgg_normalised_conv3_1_mask.t7')\n    e3 = VGGEncoder(3)\n    weight_assign(vgg3, e3, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n        'conv2_2': 12,\n        'conv3_1': 16,\n    })\n    torch.save(e3.state_dict(), 'pth_models/vgg_normalised_conv3.pth')\n    \n    \n    inv3 = load_lua('models/feature_invertor_conv3_1_mask.t7')\n    d3 = VGGDecoder(3)\n    weight_assign(inv3, d3, {\n        'conv3_1': 1,\n        'conv2_2': 5,\n        'conv2_1': 8,\n        'conv1_2': 12,\n        'conv1_1': 15,\n    })\n    torch.save(d3.state_dict(), 'pth_models/feature_invertor_conv3.pth')\n    \n    \n    vgg4 = load_lua('models/vgg_normalised_conv4_1_mask.t7')\n    e4 = VGGEncoder(4)\n    weight_assign(vgg4, e4, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n        'conv2_2': 12,\n        'conv3_1': 16,\n        'conv3_2': 19,\n        'conv3_3': 22,\n        'conv3_4': 25,\n        'conv4_1': 29,\n    })\n    torch.save(e4.state_dict(), 'pth_models/vgg_normalised_conv4.pth')\n    \n    \n    inv4 = load_lua('models/feature_invertor_conv4_1_mask.t7')\n    d4 = VGGDecoder(4)\n    weight_assign(inv4, d4, {\n        'conv4_1': 1,\n        'conv3_4': 5,\n        'conv3_3': 8,\n        'conv3_2': 11,\n        'conv3_1': 14,\n        'conv2_2': 18,\n        'conv2_1': 21,\n        'conv1_2': 25,\n        'conv1_1': 28,\n    })\n    torch.save(d4.state_dict(), 'pth_models/feature_invertor_conv4.pth')\n    \n    p_wct = PhotoWCT()\n    photo_wct_loader(p_wct)\n    torch.save(p_wct.state_dict(), 'PhotoWCTModels/photo_wct.pth')\n", "comments": "   vggencoder1     vggdecoder1     vggencoder2     vggdecoder2     vggencoder3     vggdecoder3     vggencoder4     vggdecoder4 ", "content": "import os\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.serialization import load_lua\n\nfrom models import VGGEncoder, VGGDecoder\nfrom photo_wct import PhotoWCT\n\n\ndef weight_assign(lua, pth, maps):\n    for k, v in maps.items():\n        getattr(pth, k).weight = nn.Parameter(lua.get(v).weight.float())\n        getattr(pth, k).bias = nn.Parameter(lua.get(v).bias.float())\n\n\ndef photo_wct_loader(p_wct):\n    p_wct.e1.load_state_dict(torch.load('pth_models/vgg_normalised_conv1.pth'))\n    p_wct.d1.load_state_dict(torch.load('pth_models/feature_invertor_conv1.pth'))\n    p_wct.e2.load_state_dict(torch.load('pth_models/vgg_normalised_conv2.pth'))\n    p_wct.d2.load_state_dict(torch.load('pth_models/feature_invertor_conv2.pth'))\n    p_wct.e3.load_state_dict(torch.load('pth_models/vgg_normalised_conv3.pth'))\n    p_wct.d3.load_state_dict(torch.load('pth_models/feature_invertor_conv3.pth'))\n    p_wct.e4.load_state_dict(torch.load('pth_models/vgg_normalised_conv4.pth'))\n    p_wct.d4.load_state_dict(torch.load('pth_models/feature_invertor_conv4.pth'))\n\n\nif __name__ == '__main__':\n    if not os.path.exists('pth_models'):\n        os.mkdir('pth_models')\n    \n    ## VGGEncoder1\n    vgg1 = load_lua('models/vgg_normalised_conv1_1_mask.t7')\n    e1 = VGGEncoder(1)\n    weight_assign(vgg1, e1, {\n        'conv0': 0,\n        'conv1_1': 2,\n    })\n    torch.save(e1.state_dict(), 'pth_models/vgg_normalised_conv1.pth')\n    \n    ## VGGDecoder1\n    inv1 = load_lua('models/feature_invertor_conv1_1_mask.t7')\n    d1 = VGGDecoder(1)\n    weight_assign(inv1, d1, {\n        'conv1_1': 1,\n    })\n    torch.save(d1.state_dict(), 'pth_models/feature_invertor_conv1.pth')\n    \n    ## VGGEncoder2\n    vgg2 = load_lua('models/vgg_normalised_conv2_1_mask.t7')\n    e2 = VGGEncoder(2)\n    weight_assign(vgg2, e2, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n    })\n    torch.save(e2.state_dict(), 'pth_models/vgg_normalised_conv2.pth')\n    \n    ## VGGDecoder2\n    inv2 = load_lua('models/feature_invertor_conv2_1_mask.t7')\n    d2 = VGGDecoder(2)\n    weight_assign(inv2, d2, {\n        'conv2_1': 1,\n        'conv1_2': 5,\n        'conv1_1': 8,\n    })\n    torch.save(d2.state_dict(), 'pth_models/feature_invertor_conv2.pth')\n    \n    ## VGGEncoder3\n    vgg3 = load_lua('models/vgg_normalised_conv3_1_mask.t7')\n    e3 = VGGEncoder(3)\n    weight_assign(vgg3, e3, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n        'conv2_2': 12,\n        'conv3_1': 16,\n    })\n    torch.save(e3.state_dict(), 'pth_models/vgg_normalised_conv3.pth')\n    \n    ## VGGDecoder3\n    inv3 = load_lua('models/feature_invertor_conv3_1_mask.t7')\n    d3 = VGGDecoder(3)\n    weight_assign(inv3, d3, {\n        'conv3_1': 1,\n        'conv2_2': 5,\n        'conv2_1': 8,\n        'conv1_2': 12,\n        'conv1_1': 15,\n    })\n    torch.save(d3.state_dict(), 'pth_models/feature_invertor_conv3.pth')\n    \n    ## VGGEncoder4\n    vgg4 = load_lua('models/vgg_normalised_conv4_1_mask.t7')\n    e4 = VGGEncoder(4)\n    weight_assign(vgg4, e4, {\n        'conv0': 0,\n        'conv1_1': 2,\n        'conv1_2': 5,\n        'conv2_1': 9,\n        'conv2_2': 12,\n        'conv3_1': 16,\n        'conv3_2': 19,\n        'conv3_3': 22,\n        'conv3_4': 25,\n        'conv4_1': 29,\n    })\n    torch.save(e4.state_dict(), 'pth_models/vgg_normalised_conv4.pth')\n    \n    ## VGGDecoder4\n    inv4 = load_lua('models/feature_invertor_conv4_1_mask.t7')\n    d4 = VGGDecoder(4)\n    weight_assign(inv4, d4, {\n        'conv4_1': 1,\n        'conv3_4': 5,\n        'conv3_3': 8,\n        'conv3_2': 11,\n        'conv3_1': 14,\n        'conv2_2': 18,\n        'conv2_1': 21,\n        'conv1_2': 25,\n        'conv1_1': 28,\n    })\n    torch.save(d4.state_dict(), 'pth_models/feature_invertor_conv4.pth')\n    \n    p_wct = PhotoWCT()\n    photo_wct_loader(p_wct)\n    torch.save(p_wct.state_dict(), 'PhotoWCTModels/photo_wct.pth')\n", "description": "Style transfer, deep learning, feature transform", "file_name": "converter.py", "id": "30e0fa6bf3a7c796e55744720da005d2", "language": "Python", "project_name": "FastPhotoStyle", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/NVIDIA-FastPhotoStyle/NVIDIA-FastPhotoStyle-208d4f6/converter.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:35:44Z", "url": "https://github.com/NVIDIA/FastPhotoStyle", "wiki": true}