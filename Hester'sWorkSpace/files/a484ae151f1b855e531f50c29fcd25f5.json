{"author": "yunjey", "code": "import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n\ninput_size = 784\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset (Images and Labels)\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Dataset Loader (Input Pipline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nmodel = LogisticRegression(input_size, num_classes)\n\n\n\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n    \nprint('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n\ntorch.save(model.state_dict(), 'model.pkl')", "comments": "  hyper parameters     mnist dataset (images labels)    dataset loader (input pipline)    model    loss optimizer    softmax internally computed     set parameters updated     training model    forward   backward   optimize    test model    save model ", "content": "import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Hyper Parameters \ninput_size = 784\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset (Images and Labels)\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Dataset Loader (Input Pipline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Model\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nmodel = LogisticRegression(input_size, num_classes)\n\n# Loss and Optimizer\n# Softmax is internally computed.\n# Set parameters to be updated.\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n# Training the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n    \nprint('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n# Save the Model\ntorch.save(model.state_dict(), 'model.pkl')", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main.py", "id": "a484ae151f1b855e531f50c29fcd25f5", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/01-basics/logistic_regression/main.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}