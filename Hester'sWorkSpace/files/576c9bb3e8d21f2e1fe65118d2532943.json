{"author": "deepfakes", "code": "from keras.engine import Layer, InputSpec\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\nfrom keras.utils.generic_utils import get_custom_objects\n\nimport numpy as np\n\n\nclass InstanceNormalization(Layer):\n    \"\"\"Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n    Normalize the activations of the previous layer at each step,\n    i.e. applies a transformation that maintains the mean activation\n    close to 0 and the activation standard deviation close to 1.\n    \n        axis: Integer, the axis that should be normalized\n            (typically the features axis).\n            For instance, after a `Conv2D` layer with\n            `data_format=\"channels_first\"`,\n            set `axis=1` in `InstanceNormalization`.\n            Setting `axis=None` will normalize all values in each instance of the batch.\n            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n            When the next layer is linear (also e.g. `nn.relu`),\n            this can be disabled since the scaling\n            will be done by the next layer.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    \n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    \n        Same shape as input.\n    \n        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n        - [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n    \"\"\"\n    def __init__(self,\n                 axis=None,\n                 epsilon=1e-3,\n                 center=True,\n                 scale=True,\n                 beta_initializer='zeros',\n                 gamma_initializer='ones',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        ndim = len(input_shape)\n        if self.axis == 0:\n            raise ValueError('Axis cannot be zero')\n\n        if (self.axis is not None) and (ndim == 2):\n            raise ValueError('Cannot specify axis for rank 1 tensor')\n\n        self.input_spec = InputSpec(ndim=ndim)\n\n        if self.axis is None:\n            shape = (1,)\n        else:\n            shape = (input_shape[self.axis],)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name='gamma',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name='beta',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n        reduction_axes = list(range(0, len(input_shape)))\n\n        if (self.axis is not None):\n            del reduction_axes[self.axis]\n\n        del reduction_axes[0]\n\n        mean = K.mean(inputs, reduction_axes, keepdims=True)\n        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n        normed = (inputs - mean) / stddev\n\n        broadcast_shape = [1] * len(input_shape)\n        if self.axis is not None:\n            broadcast_shape[self.axis] = input_shape[self.axis]\n\n        if self.scale:\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n            normed = normed * broadcast_gamma\n        if self.center:\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n            normed = normed + broadcast_beta\n        return normed\n\n    def get_config(self):\n        config = {\n            'axis': self.axis,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale,\n            'beta_initializer': initializers.serialize(self.beta_initializer),\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n            'beta_constraint': constraints.serialize(self.beta_constraint),\n            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(InstanceNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nget_custom_objects().update({'InstanceNormalization': InstanceNormalization})\n", "comments": "   instance normalization layer (lei ba et al  2016  ulyanov et al   2016)      normalize activations previous layer step      e  applies transformation maintains mean activation     close 0 activation standard deviation close 1        arguments         axis  integer  axis normalized             (typically features axis)              for instance   conv2d  layer              data format  channels first                set  axis 1   instancenormalization               setting  axis none  normalize values instance batch              axis 0 batch dimension   axis  cannot set 0 avoid errors          epsilon  small float added variance avoid dividing zero          center  if true  add offset  beta  normalized tensor              if false   beta  ignored          scale  if true  multiply  gamma               if false   gamma  used              when next layer linear (also e g   nn relu )              disabled since scaling             done next layer          beta initializer  initializer beta weight          gamma initializer  initializer gamma weight          beta regularizer  optional regularizer beta weight          gamma regularizer  optional regularizer gamma weight          beta constraint  optional constraint beta weight          gamma constraint  optional constraint gamma weight        input shape         arbitrary  use keyword argument  input shape          (tuple integers  include samples axis)         using layer first layer model        output shape         same shape input        references            layer normalization (https   arxiv org abs 1607 06450)            instance normalization  the missing ingredient fast stylization (https   arxiv org abs 1607 08022)            arguments    input shape    output shape    references ", "content": "from keras.engine import Layer, InputSpec\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\nfrom keras.utils.generic_utils import get_custom_objects\n\nimport numpy as np\n\n\nclass InstanceNormalization(Layer):\n    \"\"\"Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n    Normalize the activations of the previous layer at each step,\n    i.e. applies a transformation that maintains the mean activation\n    close to 0 and the activation standard deviation close to 1.\n    # Arguments\n        axis: Integer, the axis that should be normalized\n            (typically the features axis).\n            For instance, after a `Conv2D` layer with\n            `data_format=\"channels_first\"`,\n            set `axis=1` in `InstanceNormalization`.\n            Setting `axis=None` will normalize all values in each instance of the batch.\n            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n            When the next layer is linear (also e.g. `nn.relu`),\n            this can be disabled since the scaling\n            will be done by the next layer.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    # Output shape\n        Same shape as input.\n    # References\n        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n        - [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n    \"\"\"\n    def __init__(self,\n                 axis=None,\n                 epsilon=1e-3,\n                 center=True,\n                 scale=True,\n                 beta_initializer='zeros',\n                 gamma_initializer='ones',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        ndim = len(input_shape)\n        if self.axis == 0:\n            raise ValueError('Axis cannot be zero')\n\n        if (self.axis is not None) and (ndim == 2):\n            raise ValueError('Cannot specify axis for rank 1 tensor')\n\n        self.input_spec = InputSpec(ndim=ndim)\n\n        if self.axis is None:\n            shape = (1,)\n        else:\n            shape = (input_shape[self.axis],)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name='gamma',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name='beta',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n        reduction_axes = list(range(0, len(input_shape)))\n\n        if (self.axis is not None):\n            del reduction_axes[self.axis]\n\n        del reduction_axes[0]\n\n        mean = K.mean(inputs, reduction_axes, keepdims=True)\n        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n        normed = (inputs - mean) / stddev\n\n        broadcast_shape = [1] * len(input_shape)\n        if self.axis is not None:\n            broadcast_shape[self.axis] = input_shape[self.axis]\n\n        if self.scale:\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n            normed = normed * broadcast_gamma\n        if self.center:\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n            normed = normed + broadcast_beta\n        return normed\n\n    def get_config(self):\n        config = {\n            'axis': self.axis,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale,\n            'beta_initializer': initializers.serialize(self.beta_initializer),\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n            'beta_constraint': constraints.serialize(self.beta_constraint),\n            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(InstanceNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nget_custom_objects().update({'InstanceNormalization': InstanceNormalization})\n", "description": "Non official project based on original /r/Deepfakes thread. Many thanks to him!", "file_name": "instance_normalization.py", "id": "576c9bb3e8d21f2e1fe65118d2532943", "language": "Python", "project_name": "faceswap", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/deepfakes-faceswap/deepfakes-faceswap-6ff64ef/plugins/Model_GAN/instance_normalization.py", "save_time": "", "source": "", "update_at": "2018-03-18T16:27:43Z", "url": "https://github.com/deepfakes/faceswap", "wiki": true}