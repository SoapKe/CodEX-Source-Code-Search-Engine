{"author": "facebookresearch", "code": "\n\n Copyright (c) 2017-present, Facebook, Inc.\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n\n\"\"\"Script for converting Caffe (<= 1.0) models into the the simple state dict\nformat used by Detectron. For example, this script can convert the orignal\nResNet models released by MSRA.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport cPickle as pickle\nimport numpy as np\nimport os\nimport sys\n\nfrom caffe.proto import caffe_pb2\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import caffe_translator\nfrom caffe2.python import utils\nfrom google.protobuf import text_format\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Dump weights from a Caffe model'\n    )\n    parser.add_argument(\n        '--prototxt',\n        dest='prototxt_file_name',\n        help='Network definition prototxt file path',\n        default=None,\n        type=str\n    )\n    parser.add_argument(\n        '--caffemodel',\n        dest='caffemodel_file_name',\n        help='Pretrained network weights file path',\n        default=None,\n        type=str\n    )\n    parser.add_argument(\n        '--output',\n        dest='out_file_name',\n        help='Output file path',\n        default=None,\n        type=str\n    )\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\ndef normalize_resnet_name(name):\n    if name.find('res') == 0 and name.find('res_') == -1:\n         E.g.,\n          res4b11_branch2c -> res4_11_branch2c\n          res2a_branch1 -> res2_0_branch1\n        chunk = name[len('res'):name.find('_')]\n        name = (\n            'res' + chunk[0] + '_' + str(\n                int(chunk[2:]) if len(chunk) > 2   e.g., \"b1\" -> 1\n                else ord(chunk[1]) - ord('a')\n            ) +   e.g., \"a\" -> 0\n            name[name.find('_'):]\n        )\n    return name\n\n\ndef pickle_weights(out_file_name, weights):\n    blobs = {\n        normalize_resnet_name(blob.name): utils.Caffe2TensorToNumpyArray(blob)\n        for blob in weights.protos\n    }\n    with open(out_file_name, 'w') as f:\n        pickle.dump(blobs, f, protocol=pickle.HIGHEST_PROTOCOL)\n    print('Wrote blobs:')\n    print(sorted(blobs.keys()))\n\n\ndef add_missing_biases(caffenet_weights):\n    for layer in caffenet_weights.layer:\n        if layer.type == 'Convolution' and len(layer.blobs) == 1:\n            num_filters = layer.blobs[0].shape.dim[0]\n            bias_blob = caffe_pb2.BlobProto()\n            bias_blob.data.extend(np.zeros(num_filters))\n            bias_blob.num, bias_blob.channels, bias_blob.height = 1, 1, 1\n            bias_blob.width = num_filters\n            layer.blobs.extend([bias_blob])\n\n\ndef remove_spatial_bn_layers(caffenet, caffenet_weights):\n     Layer types associated with spatial batch norm\n    remove_types = ['BatchNorm', 'Scale']\n\n    def _remove_layers(net):\n        for i in reversed(range(len(net.layer))):\n            if net.layer[i].type in remove_types:\n                net.layer.pop(i)\n\n     First remove layers from caffenet proto\n    _remove_layers(caffenet)\n     We'll return these so we can save the batch norm parameters\n    bn_layers = [\n        layer for layer in caffenet_weights.layer if layer.type in remove_types\n    ]\n    _remove_layers(caffenet_weights)\n\n    def _create_tensor(arr, shape, name):\n        t = caffe2_pb2.TensorProto()\n        t.name = name\n        t.data_type = caffe2_pb2.TensorProto.FLOAT\n        t.dims.extend(shape.dim)\n        t.float_data.extend(arr)\n        assert len(t.float_data) == np.prod(t.dims), 'Data size, shape mismatch'\n        return t\n\n    bn_tensors = []\n    for (bn, scl) in zip(bn_layers[0::2], bn_layers[1::2]):\n        assert bn.name[len('bn'):] == scl.name[len('scale'):], 'Pair mismatch'\n        blob_out = 'res' + bn.name[len('bn'):] + '_bn'\n        bn_mean = np.asarray(bn.blobs[0].data)\n        bn_var = np.asarray(bn.blobs[1].data)\n        scale = np.asarray(scl.blobs[0].data)\n        bias = np.asarray(scl.blobs[1].data)\n        std = np.sqrt(bn_var + 1e-5)\n        new_scale = scale / std\n        new_bias = bias - bn_mean * scale / std\n        new_scale_tensor = _create_tensor(\n            new_scale, bn.blobs[0].shape, blob_out + '_s'\n        )\n        new_bias_tensor = _create_tensor(\n            new_bias, bn.blobs[0].shape, blob_out + '_b'\n        )\n        bn_tensors.extend([new_scale_tensor, new_bias_tensor])\n    return bn_tensors\n\n\ndef remove_layers_without_parameters(caffenet, caffenet_weights):\n    for i in reversed(range(len(caffenet_weights.layer))):\n        if len(caffenet_weights.layer[i].blobs) == 0:\n             Search for the corresponding layer in caffenet and remove it\n            name = caffenet_weights.layer[i].name\n            found = False\n            for j in range(len(caffenet.layer)):\n                if caffenet.layer[j].name == name:\n                    caffenet.layer.pop(j)\n                    found = True\n                    break\n            if not found and name[-len('_split'):] != '_split':\n                print('Warning: layer {} not found in caffenet'.format(name))\n            caffenet_weights.layer.pop(i)\n\n\ndef normalize_shape(caffenet_weights):\n    for layer in caffenet_weights.layer:\n        for blob in layer.blobs:\n            shape = (blob.num, blob.channels, blob.height, blob.width)\n            if len(blob.data) != np.prod(shape):\n                shape = tuple(blob.shape.dim)\n                if len(shape) == 1:\n                     Handle biases\n                    shape = (1, 1, 1, shape[0])\n                if len(shape) == 2:\n                     Handle InnerProduct layers\n                    shape = (1, 1, shape[0], shape[1])\n                assert len(shape) == 4\n                blob.num, blob.channels, blob.height, blob.width = shape\n\n\ndef load_and_convert_caffe_model(prototxt_file_name, caffemodel_file_name):\n    caffenet = caffe_pb2.NetParameter()\n    caffenet_weights = caffe_pb2.NetParameter()\n    text_format.Merge(open(prototxt_file_name).read(), caffenet)\n    caffenet_weights.ParseFromString(open(caffemodel_file_name).read())\n     C2 conv layers current require biases, but they are optional in C1\n     Add zeros as biases is they are missing\n    add_missing_biases(caffenet_weights)\n     We only care about getting parameters, so remove layers w/o parameters\n    remove_layers_without_parameters(caffenet, caffenet_weights)\n     BatchNorm is not implemented in the translator *and* we need to fold Scale\n     layers into the new C2 SpatialBN op, hence we remove the batch norm layers\n     and apply custom translations code\n    bn_weights = remove_spatial_bn_layers(caffenet, caffenet_weights)\n     Set num, channel, height and width for blobs that use shape.dim instead\n    normalize_shape(caffenet_weights)\n     Translate the rest of the model\n    net, pretrained_weights = caffe_translator.TranslateModel(\n        caffenet, caffenet_weights\n    )\n    pretrained_weights.protos.extend(bn_weights)\n    return net, pretrained_weights\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    assert os.path.exists(args.prototxt_file_name), \\\n        'Prototxt file does not exist'\n    assert os.path.exists(args.caffemodel_file_name), \\\n        'Weights file does not exist'\n    net, weights = load_and_convert_caffe_model(\n        args.prototxt_file_name, args.caffemodel_file_name\n    )\n    pickle_weights(args.out_file_name, weights)\n", "comments": "   script converting caffe (   1 0) models simple state dict format used detectron  for example  script convert orignal resnet models released msra          usr bin env python2    copyright (c) 2017 present  facebook  inc        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                     e g       res4b11 branch2c    res4 11 branch2c     res2a branch1    res2 0 branch1    e g    b1     1    e g        0    layer types associated spatial batch norm    first remove layers caffenet proto    we return save batch norm parameters    search corresponding layer caffenet remove    handle biases    handle innerproduct layers    c2 conv layers current require biases  optional c1    add zeros biases missing    we care getting parameters  remove layers w parameters    batchnorm implemented translator   need fold scale    layers new c2 spatialbn op  hence remove batch norm layers    apply custom translations code    set num  channel  height width blobs use shape dim instead    translate rest model ", "content": "#!/usr/bin/env python2\n\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n\"\"\"Script for converting Caffe (<= 1.0) models into the the simple state dict\nformat used by Detectron. For example, this script can convert the orignal\nResNet models released by MSRA.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport cPickle as pickle\nimport numpy as np\nimport os\nimport sys\n\nfrom caffe.proto import caffe_pb2\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import caffe_translator\nfrom caffe2.python import utils\nfrom google.protobuf import text_format\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Dump weights from a Caffe model'\n    )\n    parser.add_argument(\n        '--prototxt',\n        dest='prototxt_file_name',\n        help='Network definition prototxt file path',\n        default=None,\n        type=str\n    )\n    parser.add_argument(\n        '--caffemodel',\n        dest='caffemodel_file_name',\n        help='Pretrained network weights file path',\n        default=None,\n        type=str\n    )\n    parser.add_argument(\n        '--output',\n        dest='out_file_name',\n        help='Output file path',\n        default=None,\n        type=str\n    )\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\ndef normalize_resnet_name(name):\n    if name.find('res') == 0 and name.find('res_') == -1:\n        # E.g.,\n        #  res4b11_branch2c -> res4_11_branch2c\n        #  res2a_branch1 -> res2_0_branch1\n        chunk = name[len('res'):name.find('_')]\n        name = (\n            'res' + chunk[0] + '_' + str(\n                int(chunk[2:]) if len(chunk) > 2  # e.g., \"b1\" -> 1\n                else ord(chunk[1]) - ord('a')\n            ) +  # e.g., \"a\" -> 0\n            name[name.find('_'):]\n        )\n    return name\n\n\ndef pickle_weights(out_file_name, weights):\n    blobs = {\n        normalize_resnet_name(blob.name): utils.Caffe2TensorToNumpyArray(blob)\n        for blob in weights.protos\n    }\n    with open(out_file_name, 'w') as f:\n        pickle.dump(blobs, f, protocol=pickle.HIGHEST_PROTOCOL)\n    print('Wrote blobs:')\n    print(sorted(blobs.keys()))\n\n\ndef add_missing_biases(caffenet_weights):\n    for layer in caffenet_weights.layer:\n        if layer.type == 'Convolution' and len(layer.blobs) == 1:\n            num_filters = layer.blobs[0].shape.dim[0]\n            bias_blob = caffe_pb2.BlobProto()\n            bias_blob.data.extend(np.zeros(num_filters))\n            bias_blob.num, bias_blob.channels, bias_blob.height = 1, 1, 1\n            bias_blob.width = num_filters\n            layer.blobs.extend([bias_blob])\n\n\ndef remove_spatial_bn_layers(caffenet, caffenet_weights):\n    # Layer types associated with spatial batch norm\n    remove_types = ['BatchNorm', 'Scale']\n\n    def _remove_layers(net):\n        for i in reversed(range(len(net.layer))):\n            if net.layer[i].type in remove_types:\n                net.layer.pop(i)\n\n    # First remove layers from caffenet proto\n    _remove_layers(caffenet)\n    # We'll return these so we can save the batch norm parameters\n    bn_layers = [\n        layer for layer in caffenet_weights.layer if layer.type in remove_types\n    ]\n    _remove_layers(caffenet_weights)\n\n    def _create_tensor(arr, shape, name):\n        t = caffe2_pb2.TensorProto()\n        t.name = name\n        t.data_type = caffe2_pb2.TensorProto.FLOAT\n        t.dims.extend(shape.dim)\n        t.float_data.extend(arr)\n        assert len(t.float_data) == np.prod(t.dims), 'Data size, shape mismatch'\n        return t\n\n    bn_tensors = []\n    for (bn, scl) in zip(bn_layers[0::2], bn_layers[1::2]):\n        assert bn.name[len('bn'):] == scl.name[len('scale'):], 'Pair mismatch'\n        blob_out = 'res' + bn.name[len('bn'):] + '_bn'\n        bn_mean = np.asarray(bn.blobs[0].data)\n        bn_var = np.asarray(bn.blobs[1].data)\n        scale = np.asarray(scl.blobs[0].data)\n        bias = np.asarray(scl.blobs[1].data)\n        std = np.sqrt(bn_var + 1e-5)\n        new_scale = scale / std\n        new_bias = bias - bn_mean * scale / std\n        new_scale_tensor = _create_tensor(\n            new_scale, bn.blobs[0].shape, blob_out + '_s'\n        )\n        new_bias_tensor = _create_tensor(\n            new_bias, bn.blobs[0].shape, blob_out + '_b'\n        )\n        bn_tensors.extend([new_scale_tensor, new_bias_tensor])\n    return bn_tensors\n\n\ndef remove_layers_without_parameters(caffenet, caffenet_weights):\n    for i in reversed(range(len(caffenet_weights.layer))):\n        if len(caffenet_weights.layer[i].blobs) == 0:\n            # Search for the corresponding layer in caffenet and remove it\n            name = caffenet_weights.layer[i].name\n            found = False\n            for j in range(len(caffenet.layer)):\n                if caffenet.layer[j].name == name:\n                    caffenet.layer.pop(j)\n                    found = True\n                    break\n            if not found and name[-len('_split'):] != '_split':\n                print('Warning: layer {} not found in caffenet'.format(name))\n            caffenet_weights.layer.pop(i)\n\n\ndef normalize_shape(caffenet_weights):\n    for layer in caffenet_weights.layer:\n        for blob in layer.blobs:\n            shape = (blob.num, blob.channels, blob.height, blob.width)\n            if len(blob.data) != np.prod(shape):\n                shape = tuple(blob.shape.dim)\n                if len(shape) == 1:\n                    # Handle biases\n                    shape = (1, 1, 1, shape[0])\n                if len(shape) == 2:\n                    # Handle InnerProduct layers\n                    shape = (1, 1, shape[0], shape[1])\n                assert len(shape) == 4\n                blob.num, blob.channels, blob.height, blob.width = shape\n\n\ndef load_and_convert_caffe_model(prototxt_file_name, caffemodel_file_name):\n    caffenet = caffe_pb2.NetParameter()\n    caffenet_weights = caffe_pb2.NetParameter()\n    text_format.Merge(open(prototxt_file_name).read(), caffenet)\n    caffenet_weights.ParseFromString(open(caffemodel_file_name).read())\n    # C2 conv layers current require biases, but they are optional in C1\n    # Add zeros as biases is they are missing\n    add_missing_biases(caffenet_weights)\n    # We only care about getting parameters, so remove layers w/o parameters\n    remove_layers_without_parameters(caffenet, caffenet_weights)\n    # BatchNorm is not implemented in the translator *and* we need to fold Scale\n    # layers into the new C2 SpatialBN op, hence we remove the batch norm layers\n    # and apply custom translations code\n    bn_weights = remove_spatial_bn_layers(caffenet, caffenet_weights)\n    # Set num, channel, height and width for blobs that use shape.dim instead\n    normalize_shape(caffenet_weights)\n    # Translate the rest of the model\n    net, pretrained_weights = caffe_translator.TranslateModel(\n        caffenet, caffenet_weights\n    )\n    pretrained_weights.protos.extend(bn_weights)\n    return net, pretrained_weights\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    assert os.path.exists(args.prototxt_file_name), \\\n        'Prototxt file does not exist'\n    assert os.path.exists(args.caffemodel_file_name), \\\n        'Weights file does not exist'\n    net, weights = load_and_convert_caffe_model(\n        args.prototxt_file_name, args.caffemodel_file_name\n    )\n    pickle_weights(args.out_file_name, weights)\n", "description": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.", "file_name": "pickle_caffe_blobs.py", "id": "29c02a7af63ffe8c548e82b1d14cac9e", "language": "Python", "project_name": "Detectron", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/facebookresearch-Detectron/facebookresearch-Detectron-958b0ad/tools/pickle_caffe_blobs.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:01:25Z", "url": "https://github.com/facebookresearch/Detectron", "wiki": false}