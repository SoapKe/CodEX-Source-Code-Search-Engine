{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Author: aneelakantan (Arvind Neelakantan)\n\"\"\"\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport nn_utils\n\n\nclass Graph():\n\n  def __init__(self, utility, batch_size, max_passes, mode=\"train\"):\n    self.utility = utility\n    self.data_type = self.utility.tf_data_type[self.utility.FLAGS.data_type]\n    self.max_elements = self.utility.FLAGS.max_elements\n    max_elements = self.utility.FLAGS.max_elements\n    self.num_cols = self.utility.FLAGS.max_number_cols\n    self.num_word_cols = self.utility.FLAGS.max_word_cols\n    self.question_length = self.utility.FLAGS.question_length\n    self.batch_size = batch_size\n    self.max_passes = max_passes\n    self.mode = mode\n    self.embedding_dims = self.utility.FLAGS.embedding_dims\n    input question and a mask\n    self.batch_question = tf.placeholder(tf.int32,\n                                         [batch_size, self.question_length])\n    self.batch_question_attention_mask = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    ground truth scalar answer and lookup answer\n    self.batch_answer = tf.placeholder(self.data_type, [batch_size])\n    self.batch_print_answer = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    number columns and its processed version\n    self.batch_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements\n                        ])  columns with numeric entries\n    self.batch_processed_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements])\n    self.batch_processed_sorted_index_number_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_cols, max_elements])\n    word columns and its processed version\n    self.batch_processed_word_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols, max_elements])\n    self.batch_processed_sorted_index_word_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    self.batch_word_column_entry_mask = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    names of word and number columns along with their mask\n    self.batch_word_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_word_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_word_column_mask = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols])\n    self.batch_number_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_number_column_mask = tf.placeholder(self.data_type,\n                                                   [batch_size, self.num_cols])\n    exact match and group by max operation\n    self.batch_exact_match = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    self.batch_column_exact_match = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols + self.num_word_cols])\n    self.batch_group_by_max = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    numbers in the question along with their position. This is used to compute arguments to the comparison operations\n    self.batch_question_number = tf.placeholder(self.data_type, [batch_size, 1])\n    self.batch_question_number_one = tf.placeholder(self.data_type,\n                                                    [batch_size, 1])\n    self.batch_question_number_mask = tf.placeholder(\n        self.data_type, [batch_size, max_elements])\n    self.batch_question_number_one_mask = tf.placeholder(self.data_type,\n                                                         [batch_size, 1])\n    self.batch_ordinal_question = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    self.batch_ordinal_question_one = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n\n  def LSTM_question_embedding(self, sentence, sentence_length):\n    LSTM processes the input question\n    lstm_params = \"question_lstm\"\n    hidden_vectors = []\n    sentence = self.batch_question\n    question_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    question_c_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == \"train\"):\n        rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(question_hidden), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        rnn_dropout_mask = tf.ones_like(question_hidden)\n    for question_iterator in range(self.question_length):\n      curr_word = sentence[:, question_iterator]\n      question_vector = nn_utils.apply_dropout(\n          nn_utils.get_embedding(curr_word, self.utility, self.params),\n          self.utility.FLAGS.dropout, self.mode)\n      question_hidden, question_c_hidden = nn_utils.LSTMCell(\n          question_vector, question_hidden, question_c_hidden, lstm_params,\n          self.params)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        question_hidden = question_hidden * rnn_dropout_mask\n      hidden_vectors.append(tf.expand_dims(question_hidden, 0))\n    hidden_vectors = tf.concat(axis=0, values=hidden_vectors)\n    return question_hidden, hidden_vectors\n\n  def history_recurrent_step(self, curr_hprev, hprev):\n    A single RNN step for controller or history RNN\n    return tf.tanh(\n        tf.matmul(\n            tf.concat(axis=1, values=[hprev, curr_hprev]), self.params[\n                \"history_recurrent\"])) + self.params[\"history_recurrent_bias\"]\n\n  def question_number_softmax(self, hidden_vectors):\n    Attention on quetsion to decide the question number to passed to comparison ops\n    def compute_ans(op_embedding, comparison):\n      op_embedding = tf.expand_dims(op_embedding, 0)\n      dot product of operation embedding with hidden state to the left of the number occurrence\n      first = tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question), 2),\n                            [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      second = self.batch_question_number_one_mask + tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question_one), 2\n                            ), [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      question_number_softmax = tf.nn.softmax(tf.concat(axis=1, values=[first, second]))\n      if (self.mode == \"test\"):\n        cond = tf.equal(question_number_softmax,\n                        tf.reshape(\n                            tf.reduce_max(question_number_softmax, 1),\n                            [self.batch_size, 1]))\n        question_number_softmax = tf.where(\n            cond,\n            tf.fill(tf.shape(question_number_softmax), 1.0),\n            tf.fill(tf.shape(question_number_softmax), 0.0))\n        question_number_softmax = tf.cast(question_number_softmax,\n                                          self.data_type)\n      ans = tf.reshape(\n          tf.reduce_sum(question_number_softmax * tf.concat(\n              axis=1, values=[self.batch_question_number, self.batch_question_number_one]),\n                        1), [self.batch_size, 1])\n      return ans\n\n    def compute_op_position(op_name):\n      for i in range(len(self.utility.operations_set)):\n        if (op_name == self.utility.operations_set[i]):\n          return i\n\n    def compute_question_number(op_name):\n      op_embedding = tf.nn.embedding_lookup(self.params_unit,\n                                            compute_op_position(op_name))\n      return compute_ans(op_embedding, op_name)\n\n    curr_greater_question_number = compute_question_number(\"greater\")\n    curr_lesser_question_number = compute_question_number(\"lesser\")\n    curr_geq_question_number = compute_question_number(\"geq\")\n    curr_leq_question_number = compute_question_number(\"leq\")\n    return curr_greater_question_number, curr_lesser_question_number, curr_geq_question_number, curr_leq_question_number\n\n  def perform_attention(self, context_vector, hidden_vectors, length, mask):\n    Performs attention on hiddent_vectors using context vector\n    context_vector = tf.tile(\n        tf.expand_dims(context_vector, 0), [length, 1, 1])  time * bs * d\n    attention_softmax = tf.nn.softmax(\n        tf.transpose(tf.reduce_sum(context_vector * hidden_vectors, 2)) +\n        mask)  batch_size * time\n    attention_softmax = tf.tile(\n        tf.expand_dims(tf.transpose(attention_softmax), 2),\n        [1, 1, self.embedding_dims])\n    ans_vector = tf.reduce_sum(attention_softmax * hidden_vectors, 0)\n    return ans_vector\n\n  computes embeddings for column names using parameters of question module\n  def get_column_hidden_vectors(self):\n    vector representations for the column names\n    self.column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_number_column_names, self.utility,\n                               self.params), 2)\n    self.word_column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_word_column_names, self.utility,\n                               self.params), 2)\n\n  def create_summary_embeddings(self):\n    embeddings for each text entry in the table using parameters of the question module\n    self.summary_text_entry_embeddings = tf.reduce_sum(\n        tf.expand_dims(self.batch_exact_match, 3) * tf.expand_dims(\n            tf.expand_dims(\n                tf.expand_dims(\n                    nn_utils.get_embedding(self.utility.entry_match_token_id,\n                                           self.utility, self.params), 0), 1),\n            2), 2)\n\n  def compute_column_softmax(self, column_controller_vector, time_step):\n    compute softmax over all the columns using column controller vector\n    column_controller_vector = tf.tile(\n        tf.expand_dims(column_controller_vector, 1),\n        [1, self.num_cols + self.num_word_cols, 1])  max_cols * bs * d\n    column_controller_vector = nn_utils.apply_dropout(\n        column_controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.full_column_hidden_vectors = tf.concat(\n        axis=1, values=[self.column_hidden_vectors, self.word_column_hidden_vectors])\n    self.full_column_hidden_vectors += self.summary_text_entry_embeddings\n    self.full_column_hidden_vectors = nn_utils.apply_dropout(\n        self.full_column_hidden_vectors, self.utility.FLAGS.dropout, self.mode)\n    column_logits = tf.reduce_sum(\n        column_controller_vector * self.full_column_hidden_vectors, 2) + (\n            self.params[\"word_match_feature_column_name\"] *\n            self.batch_column_exact_match) + self.full_column_mask\n    column_softmax = tf.nn.softmax(column_logits)  batch_size * max_cols\n    return column_softmax\n\n  def compute_first_or_last(self, select, first=True):\n    perform first ot last operation on row select with probabilistic row selection\n    answer = tf.zeros_like(select)\n    running_sum = tf.zeros([self.batch_size, 1], self.data_type)\n    for i in range(self.max_elements):\n      if (first):\n        current = tf.slice(select, [0, i], [self.batch_size, 1])\n      else:\n        current = tf.slice(select, [0, self.max_elements - 1 - i],\n                           [self.batch_size, 1])\n      curr_prob = current * (1 - running_sum)\n      curr_prob = curr_prob * tf.cast(curr_prob >= 0.0, self.data_type)\n      running_sum += curr_prob\n      temp_ans = []\n      curr_prob = tf.expand_dims(tf.reshape(curr_prob, [self.batch_size]), 0)\n      for i_ans in range(self.max_elements):\n        if (not (first) and i_ans == self.max_elements - 1 - i):\n          temp_ans.append(curr_prob)\n        elif (first and i_ans == i):\n          temp_ans.append(curr_prob)\n        else:\n          temp_ans.append(tf.zeros_like(curr_prob))\n      temp_ans = tf.transpose(tf.concat(axis=0, values=temp_ans))\n      answer += temp_ans\n    return answer\n\n  def make_hard_softmax(self, softmax):\n    converts soft selection to hard selection. used at test time\n    cond = tf.equal(\n        softmax, tf.reshape(tf.reduce_max(softmax, 1), [self.batch_size, 1]))\n    softmax = tf.where(\n        cond, tf.fill(tf.shape(softmax), 1.0), tf.fill(tf.shape(softmax), 0.0))\n    softmax = tf.cast(softmax, self.data_type)\n    return softmax\n\n  def compute_max_or_min(self, select, maxi=True):\n    computes the argmax and argmin of a column with probabilistic row selection\n    answer = tf.zeros([\n        self.batch_size, self.num_cols + self.num_word_cols, self.max_elements\n    ], self.data_type)\n    sum_prob = tf.zeros([self.batch_size, self.num_cols + self.num_word_cols],\n                        self.data_type)\n    for j in range(self.max_elements):\n      if (maxi):\n        curr_pos = j\n      else:\n        curr_pos = self.max_elements - 1 - j\n      select_index = tf.slice(self.full_processed_sorted_index_column,\n                              [0, 0, curr_pos], [self.batch_size, -1, 1])\n      select_mask = tf.equal(\n          tf.tile(\n              tf.expand_dims(\n                  tf.tile(\n                      tf.expand_dims(tf.range(self.max_elements), 0),\n                      [self.batch_size, 1]), 1),\n              [1, self.num_cols + self.num_word_cols, 1]), select_index)\n      curr_prob = tf.expand_dims(select, 1) * tf.cast(\n          select_mask, self.data_type) * self.select_bad_number_mask\n      curr_prob = curr_prob * tf.expand_dims((1 - sum_prob), 2)\n      curr_prob = curr_prob * tf.expand_dims(\n          tf.cast((1 - sum_prob) > 0.0, self.data_type), 2)\n      answer = tf.where(select_mask, curr_prob, answer)\n      sum_prob += tf.reduce_sum(curr_prob, 2)\n    return answer\n\n  def perform_operations(self, softmax, full_column_softmax, select,\n                         prev_select_1, curr_pass):\n    performs all the 15 operations. computes scalar output, lookup answer and row selector\n    column_softmax = tf.slice(full_column_softmax, [0, 0],\n                              [self.batch_size, self.num_cols])\n    word_column_softmax = tf.slice(full_column_softmax, [0, self.num_cols],\n                                   [self.batch_size, self.num_word_cols])\n    init_max = self.compute_max_or_min(select, maxi=True)\n    init_min = self.compute_max_or_min(select, maxi=False)\n    operations that are column  independent\n    count = tf.reshape(tf.reduce_sum(select, 1), [self.batch_size, 1])\n    select_full_column_softmax = tf.tile(\n        tf.expand_dims(full_column_softmax, 2),\n        [1, 1, self.max_elements\n        ])  BS * (max_cols + max_word_cols) * max_elements\n    select_word_column_softmax = tf.tile(\n        tf.expand_dims(word_column_softmax, 2),\n        [1, 1, self.max_elements])  BS * max_word_cols * max_elements\n    select_greater = tf.reduce_sum(\n        self.init_select_greater * select_full_column_softmax,\n        1) * self.batch_question_number_mask  BS * max_elements\n    select_lesser = tf.reduce_sum(\n        self.init_select_lesser * select_full_column_softmax,\n        1) * self.batch_question_number_mask  BS * max_elements\n    select_geq = tf.reduce_sum(\n        self.init_select_geq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  BS * max_elements\n    select_leq = tf.reduce_sum(\n        self.init_select_leq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  BS * max_elements\n    select_max = tf.reduce_sum(init_max * select_full_column_softmax,\n                               1)  BS * max_elements\n    select_min = tf.reduce_sum(init_min * select_full_column_softmax,\n                               1)  BS * max_elements\n    select_prev = tf.concat(axis=1, values=[\n        tf.slice(select, [0, 1], [self.batch_size, self.max_elements - 1]),\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type)\n    ])\n    select_next = tf.concat(axis=1, values=[\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type), tf.slice(\n            select, [0, 0], [self.batch_size, self.max_elements - 1])\n    ])\n    select_last_rs = self.compute_first_or_last(select, False)\n    select_first_rs = self.compute_first_or_last(select, True)\n    select_word_match = tf.reduce_sum(self.batch_exact_match *\n                                      select_full_column_softmax, 1)\n    select_group_by_max = tf.reduce_sum(self.batch_group_by_max *\n                                        select_full_column_softmax, 1)\n    length_content = 1\n    length_select = 13\n    length_print = 1\n    values = tf.concat(axis=1, values=[count])\n    softmax_content = tf.slice(softmax, [0, 0],\n                               [self.batch_size, length_content])\n    compute scalar output\n    output = tf.reduce_sum(tf.multiply(softmax_content, values), 1)\n    compute lookup answer\n    softmax_print = tf.slice(softmax, [0, length_content + length_select],\n                             [self.batch_size, length_print])\n    curr_print = select_full_column_softmax * tf.tile(\n        tf.expand_dims(select, 1),\n        [1, self.num_cols + self.num_word_cols, 1\n        ])  BS * max_cols * max_elements (conisders only column)\n    self.batch_lookup_answer = curr_print * tf.tile(\n        tf.expand_dims(softmax_print, 2),\n        [1, self.num_cols + self.num_word_cols, self.max_elements\n        ])  BS * max_cols * max_elements\n    self.batch_lookup_answer = self.batch_lookup_answer * self.select_full_mask\n    compute row select\n    softmax_select = tf.slice(softmax, [0, length_content],\n                              [self.batch_size, length_select])\n    select_lists = [\n        tf.expand_dims(select_prev, 1), tf.expand_dims(select_next, 1),\n        tf.expand_dims(select_first_rs, 1), tf.expand_dims(select_last_rs, 1),\n        tf.expand_dims(select_group_by_max, 1),\n        tf.expand_dims(select_greater, 1), tf.expand_dims(select_lesser, 1),\n        tf.expand_dims(select_geq, 1), tf.expand_dims(select_leq, 1),\n        tf.expand_dims(select_max, 1), tf.expand_dims(select_min, 1),\n        tf.expand_dims(select_word_match, 1),\n        tf.expand_dims(self.reset_select, 1)\n    ]\n    select = tf.reduce_sum(\n        tf.tile(tf.expand_dims(softmax_select, 2), [1, 1, self.max_elements]) *\n        tf.concat(axis=1, values=select_lists), 1)\n    select = select * self.select_whole_mask\n    return output, select\n\n  def one_pass(self, select, question_embedding, hidden_vectors, hprev,\n               prev_select_1, curr_pass):\n    Performs one timestep which involves selecting an operation and a column\n    attention_vector = self.perform_attention(\n        hprev, hidden_vectors, self.question_length,\n        self.batch_question_attention_mask)  batch_size * embedding_dims\n    controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[\"controller_prev\"]) + tf.matmul(\n            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[\n                \"controller\"]))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[\"column_controller_prev\"]) + tf.matmul(\n            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[\n                \"column_controller\"]))\n    controller_vector = nn_utils.apply_dropout(\n        controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.operation_logits = tf.matmul(controller_vector,\n                                      tf.transpose(self.params_unit))\n    softmax = tf.nn.softmax(self.operation_logits)\n    soft_softmax = softmax\n    compute column softmax: bs * max_columns\n    weighted_op_representation = tf.transpose(\n        tf.matmul(tf.transpose(self.params_unit), tf.transpose(softmax)))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(\n            tf.concat(axis=1, values=[\n                column_controller_vector, weighted_op_representation\n            ]), self.params[\"break_conditional\"]))\n    full_column_softmax = self.compute_column_softmax(column_controller_vector,\n                                                      curr_pass)\n    soft_column_softmax = full_column_softmax\n    if (self.mode == \"test\"):\n      full_column_softmax = self.make_hard_softmax(full_column_softmax)\n      softmax = self.make_hard_softmax(softmax)\n    output, select = self.perform_operations(softmax, full_column_softmax,\n                                             select, prev_select_1, curr_pass)\n    return output, select, softmax, soft_softmax, full_column_softmax, soft_column_softmax\n\n  def compute_lookup_error(self, val):\n    computes lookup error.\n    cond = tf.equal(self.batch_print_answer, val)\n    inter = tf.where(\n        cond, self.init_print_error,\n        tf.tile(\n            tf.reshape(tf.constant(1e10, self.data_type), [1, 1, 1]), [\n                self.batch_size, self.utility.FLAGS.max_word_cols +\n                self.utility.FLAGS.max_number_cols,\n                self.utility.FLAGS.max_elements\n            ]))\n    return tf.reduce_min(tf.reduce_min(inter, 1), 1) * tf.cast(\n        tf.greater(\n            tf.reduce_sum(tf.reduce_sum(tf.cast(cond, self.data_type), 1), 1),\n            0.0), self.data_type)\n\n  def soft_min(self, x, y):\n    return tf.maximum(-1.0 * (1 / (\n        self.utility.FLAGS.soft_min_value + 0.0)) * tf.log(\n            tf.exp(-self.utility.FLAGS.soft_min_value * x) + tf.exp(\n                -self.utility.FLAGS.soft_min_value * y)), tf.zeros_like(x))\n\n  def error_computation(self):\n    computes the error of each example in a batch\n    math_error = 0.5 * tf.square(tf.subtract(self.scalar_output, self.batch_answer))\n    scale math error\n    math_error = math_error / self.rows\n    math_error = tf.minimum(math_error, self.utility.FLAGS.max_math_error *\n                            tf.ones(tf.shape(math_error), self.data_type))\n    self.init_print_error = tf.where(\n        self.batch_gold_select, -1 * tf.log(self.batch_lookup_answer + 1e-300 +\n                                            self.invert_select_full_mask), -1 *\n        tf.log(1 - self.batch_lookup_answer)) * self.select_full_mask\n    print_error_1 = self.init_print_error * tf.cast(\n        tf.equal(self.batch_print_answer, 0.0), self.data_type)\n    print_error = tf.reduce_sum(tf.reduce_sum((print_error_1), 1), 1)\n    for val in range(1, 58):\n      print_error += self.compute_lookup_error(val + 0.0)\n    print_error = print_error * self.utility.FLAGS.print_cost / self.num_entries\n    if (self.mode == \"train\"):\n      error = tf.where(\n          tf.logical_and(\n              tf.not_equal(self.batch_answer, 0.0),\n              tf.not_equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_print_answer, 1), 1),\n                  0.0)),\n          self.soft_min(math_error, print_error),\n          tf.where(\n              tf.not_equal(self.batch_answer, 0.0), math_error, print_error))\n    else:\n      error = tf.where(\n          tf.logical_and(\n              tf.equal(self.scalar_output, 0.0),\n              tf.equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_lookup_answer, 1), 1),\n                  0.0)),\n          tf.ones_like(math_error),\n          tf.where(\n              tf.equal(self.scalar_output, 0.0), print_error, math_error))\n    return error\n\n  def batch_process(self):\n    Computes loss and fraction of correct examples in a batch.\n    self.params_unit = nn_utils.apply_dropout(\n        self.params[\"unit\"], self.utility.FLAGS.dropout, self.mode)\n    batch_size = self.batch_size\n    max_passes = self.max_passes\n    num_timesteps = 1\n    max_elements = self.max_elements\n    select = tf.cast(\n        tf.fill([self.batch_size, max_elements], 1.0), self.data_type)\n    hprev = tf.cast(\n        tf.fill([self.batch_size, self.embedding_dims], 0.0),\n        self.data_type)  running sum of the hidden states of the model\n    output = tf.cast(tf.fill([self.batch_size, 1], 0.0),\n                     self.data_type)  output of the model\n    correct = tf.cast(\n        tf.fill([1], 0.0), self.data_type\n    )  to compute accuracy, returns number of correct examples for this batch\n    total_error = 0.0\n    prev_select_1 = tf.zeros_like(select)\n    self.create_summary_embeddings()\n    self.get_column_hidden_vectors()\n    get question embedding\n    question_embedding, hidden_vectors = self.LSTM_question_embedding(\n        self.batch_question, self.question_length)\n    compute arguments for comparison operation\n    greater_question_number, lesser_question_number, geq_question_number, leq_question_number = self.question_number_softmax(\n        hidden_vectors)\n    self.init_select_greater = tf.cast(\n        tf.greater(self.full_processed_column,\n                   tf.expand_dims(greater_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  bs * max_cols * max_elements\n    self.init_select_lesser = tf.cast(\n        tf.less(self.full_processed_column,\n                tf.expand_dims(lesser_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  bs * max_cols * max_elements\n    self.init_select_geq = tf.cast(\n        tf.greater_equal(self.full_processed_column,\n                         tf.expand_dims(geq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  bs * max_cols * max_elements\n    self.init_select_leq = tf.cast(\n        tf.less_equal(self.full_processed_column,\n                      tf.expand_dims(leq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  bs * max_cols * max_elements\n    self.init_select_word_match = 0\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == \"train\"):\n        history_rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(hprev), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        history_rnn_dropout_mask = tf.ones_like(hprev)\n    select = select * self.select_whole_mask\n    self.batch_log_prob = tf.zeros([self.batch_size], dtype=self.data_type)\n    Perform max_passes and at each  pass select operation and column\n    for curr_pass in range(max_passes):\n      print(\"step: \", curr_pass)\n      output, select, softmax, soft_softmax, column_softmax, soft_column_softmax = self.one_pass(\n          select, question_embedding, hidden_vectors, hprev, prev_select_1,\n          curr_pass)\n      prev_select_1 = select\n      compute input to history RNN\n      input_op = tf.transpose(\n          tf.matmul(\n              tf.transpose(self.params_unit), tf.transpose(\n                  soft_softmax)))  weighted average of emebdding of operations\n      input_col = tf.reduce_sum(\n          tf.expand_dims(soft_column_softmax, 2) *\n          self.full_column_hidden_vectors, 1)\n      history_input = tf.concat(axis=1, values=[input_op, input_col])\n      history_input = nn_utils.apply_dropout(\n          history_input, self.utility.FLAGS.dropout, self.mode)\n      hprev = self.history_recurrent_step(history_input, hprev)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        hprev = hprev * history_rnn_dropout_mask\n    self.scalar_output = output\n    error = self.error_computation()\n    cond = tf.less(error, 0.0001, name=\"cond\")\n    correct_add = tf.where(\n        cond, tf.fill(tf.shape(cond), 1.0), tf.fill(tf.shape(cond), 0.0))\n    correct = tf.reduce_sum(correct_add)\n    error = error / batch_size\n    total_error = tf.reduce_sum(error)\n    total_correct = correct / batch_size\n    return total_error, total_correct\n\n  def compute_error(self):\n    Sets mask variables and performs batch processing\n    self.batch_gold_select = self.batch_print_answer > 0.0\n    self.full_column_mask = tf.concat(\n        axis=1, values=[self.batch_number_column_mask, self.batch_word_column_mask])\n    self.full_processed_column = tf.concat(\n        axis=1,\n        values=[self.batch_processed_number_column, self.batch_processed_word_column])\n    self.full_processed_sorted_index_column = tf.concat(axis=1, values=[\n        self.batch_processed_sorted_index_number_column,\n        self.batch_processed_sorted_index_word_column\n    ])\n    self.select_bad_number_mask = tf.cast(\n        tf.logical_and(\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.pad_int),\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.bad_number_pre_process)),\n        self.data_type)\n    self.select_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int)),\n        self.data_type)\n    self.select_word_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)), self.data_type)\n    self.select_full_mask = tf.concat(\n        axis=1, values=[self.select_mask, self.select_word_mask])\n    self.select_whole_mask = tf.maximum(\n        tf.reshape(\n            tf.slice(self.select_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]),\n        tf.reshape(\n            tf.slice(self.select_word_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]))\n    self.invert_select_full_mask = tf.cast(\n        tf.concat(axis=1, values=[\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int),\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)\n        ]), self.data_type)\n    self.batch_lookup_answer = tf.zeros(tf.shape(self.batch_gold_select))\n    self.reset_select = self.select_whole_mask\n    self.rows = tf.reduce_sum(self.select_whole_mask, 1)\n    self.num_entries = tf.reshape(\n        tf.reduce_sum(tf.reduce_sum(self.select_full_mask, 1), 1),\n        [self.batch_size])\n    self.final_error, self.final_correct = self.batch_process()\n    return self.final_error\n\n  def create_graph(self, params, global_step):\n    Creates the graph to compute error, gradient computation and updates parameters\n    self.params = params\n    batch_size = self.batch_size\n    learning_rate = tf.cast(self.utility.FLAGS.learning_rate, self.data_type)\n    self.total_cost = self.compute_error()\n    optimize_params = self.params.values()\n    optimize_names = self.params.keys()\n    print(\"optimize params \", optimize_names)\n    if (self.utility.FLAGS.l2_regularizer > 0.0):\n      reg_cost = 0.0\n      for ind_param in self.params.keys():\n        reg_cost += tf.nn.l2_loss(self.params[ind_param])\n      self.total_cost += self.utility.FLAGS.l2_regularizer * reg_cost\n    grads = tf.gradients(self.total_cost, optimize_params, name=\"gradients\")\n    grad_norm = 0.0\n    for p, name in zip(grads, optimize_names):\n      print(\"grads: \", p, name)\n      if isinstance(p, tf.IndexedSlices):\n        grad_norm += tf.reduce_sum(p.values * p.values)\n      elif not (p == None):\n        grad_norm += tf.reduce_sum(p * p)\n    grad_norm = tf.sqrt(grad_norm)\n    max_grad_norm = np.float32(self.utility.FLAGS.clip_gradients).astype(\n        self.utility.np_data_type[self.utility.FLAGS.data_type])\n    grad_scale = tf.minimum(\n        tf.cast(1.0, self.data_type), max_grad_norm / grad_norm)\n    clipped_grads = list()\n    for p in grads:\n      if isinstance(p, tf.IndexedSlices):\n        tmp = p.values * grad_scale\n        clipped_grads.append(tf.IndexedSlices(tmp, p.indices))\n      elif not (p == None):\n        clipped_grads.append(p * grad_scale)\n      else:\n        clipped_grads.append(p)\n    grads = clipped_grads\n    self.global_step = global_step\n    params_list = self.params.values()\n    params_list.append(self.global_step)\n    adam = tf.train.AdamOptimizer(\n        learning_rate,\n        epsilon=tf.cast(self.utility.FLAGS.eps, self.data_type),\n        use_locking=True)\n    self.step = adam.apply_gradients(zip(grads, optimize_params),\n\t\t\t\t\tglobal_step=self.global_step)\n    self.init_op = tf.global_variables_initializer()\n", "comments": "   author  aneelakantan (arvind neelakantan)        copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license       http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                      input question mask   ground truth scalar answer lookup answer   number columns processed version   columns numeric entries   word columns processed version   names word number columns along mask   exact match group max operation   numbers question along position  this used compute arguments comparison operations   lstm processes input question   a single rnn step controller history rnn   attention quetsion decide question number passed comparison ops   dot product operation embedding hidden state left number occurrence   performs attention hiddent vectors using context vector   time   bs     batch size   time   computes embeddings column names using parameters question module   vector representations column names   embeddings text entry table using parameters question module   compute softmax columns using column controller vector   max cols   bs     batch size   max cols   perform first ot last operation row select probabilistic row selection   converts soft selection hard selection  used test time   computes argmax argmin column probabilistic row selection   performs 15 operations  computes scalar output  lookup answer row selector   operations column  independent   bs   (max cols   max word cols)   max elements   bs   max word cols   max elements   bs   max elements   bs   max elements   bs   max elements   bs   max elements   bs   max elements   bs   max elements   compute scalar output   compute lookup answer   bs   max cols   max elements (conisders column)   bs   max cols   max elements   compute row select   performs one timestep involves selecting operation column   batch size   embedding dims   compute column softmax  bs   max columns   computes lookup error    computes error example batch   scale math error   computes loss fraction correct examples batch    running sum hidden states model   output model   compute accuracy  returns number correct examples batch   get question embedding   compute arguments comparison operation   bs   max cols   max elements   bs   max cols   max elements   bs   max cols   max elements   bs   max cols   max elements   perform max passes  pass select operation column   compute input history rnn   weighted average emebdding operations   sets mask variables performs batch processing   creates graph compute error  gradient computation updates parameters ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Author: aneelakantan (Arvind Neelakantan)\n\"\"\"\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport nn_utils\n\n\nclass Graph():\n\n  def __init__(self, utility, batch_size, max_passes, mode=\"train\"):\n    self.utility = utility\n    self.data_type = self.utility.tf_data_type[self.utility.FLAGS.data_type]\n    self.max_elements = self.utility.FLAGS.max_elements\n    max_elements = self.utility.FLAGS.max_elements\n    self.num_cols = self.utility.FLAGS.max_number_cols\n    self.num_word_cols = self.utility.FLAGS.max_word_cols\n    self.question_length = self.utility.FLAGS.question_length\n    self.batch_size = batch_size\n    self.max_passes = max_passes\n    self.mode = mode\n    self.embedding_dims = self.utility.FLAGS.embedding_dims\n    #input question and a mask\n    self.batch_question = tf.placeholder(tf.int32,\n                                         [batch_size, self.question_length])\n    self.batch_question_attention_mask = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    #ground truth scalar answer and lookup answer\n    self.batch_answer = tf.placeholder(self.data_type, [batch_size])\n    self.batch_print_answer = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    #number columns and its processed version\n    self.batch_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements\n                        ])  #columns with numeric entries\n    self.batch_processed_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements])\n    self.batch_processed_sorted_index_number_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_cols, max_elements])\n    #word columns and its processed version\n    self.batch_processed_word_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols, max_elements])\n    self.batch_processed_sorted_index_word_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    self.batch_word_column_entry_mask = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    #names of word and number columns along with their mask\n    self.batch_word_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_word_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_word_column_mask = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols])\n    self.batch_number_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_number_column_mask = tf.placeholder(self.data_type,\n                                                   [batch_size, self.num_cols])\n    #exact match and group by max operation\n    self.batch_exact_match = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    self.batch_column_exact_match = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols + self.num_word_cols])\n    self.batch_group_by_max = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    #numbers in the question along with their position. This is used to compute arguments to the comparison operations\n    self.batch_question_number = tf.placeholder(self.data_type, [batch_size, 1])\n    self.batch_question_number_one = tf.placeholder(self.data_type,\n                                                    [batch_size, 1])\n    self.batch_question_number_mask = tf.placeholder(\n        self.data_type, [batch_size, max_elements])\n    self.batch_question_number_one_mask = tf.placeholder(self.data_type,\n                                                         [batch_size, 1])\n    self.batch_ordinal_question = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    self.batch_ordinal_question_one = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n\n  def LSTM_question_embedding(self, sentence, sentence_length):\n    #LSTM processes the input question\n    lstm_params = \"question_lstm\"\n    hidden_vectors = []\n    sentence = self.batch_question\n    question_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    question_c_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == \"train\"):\n        rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(question_hidden), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        rnn_dropout_mask = tf.ones_like(question_hidden)\n    for question_iterator in range(self.question_length):\n      curr_word = sentence[:, question_iterator]\n      question_vector = nn_utils.apply_dropout(\n          nn_utils.get_embedding(curr_word, self.utility, self.params),\n          self.utility.FLAGS.dropout, self.mode)\n      question_hidden, question_c_hidden = nn_utils.LSTMCell(\n          question_vector, question_hidden, question_c_hidden, lstm_params,\n          self.params)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        question_hidden = question_hidden * rnn_dropout_mask\n      hidden_vectors.append(tf.expand_dims(question_hidden, 0))\n    hidden_vectors = tf.concat(axis=0, values=hidden_vectors)\n    return question_hidden, hidden_vectors\n\n  def history_recurrent_step(self, curr_hprev, hprev):\n    #A single RNN step for controller or history RNN\n    return tf.tanh(\n        tf.matmul(\n            tf.concat(axis=1, values=[hprev, curr_hprev]), self.params[\n                \"history_recurrent\"])) + self.params[\"history_recurrent_bias\"]\n\n  def question_number_softmax(self, hidden_vectors):\n    #Attention on quetsion to decide the question number to passed to comparison ops\n    def compute_ans(op_embedding, comparison):\n      op_embedding = tf.expand_dims(op_embedding, 0)\n      #dot product of operation embedding with hidden state to the left of the number occurrence\n      first = tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question), 2),\n                            [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      second = self.batch_question_number_one_mask + tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question_one), 2\n                            ), [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      question_number_softmax = tf.nn.softmax(tf.concat(axis=1, values=[first, second]))\n      if (self.mode == \"test\"):\n        cond = tf.equal(question_number_softmax,\n                        tf.reshape(\n                            tf.reduce_max(question_number_softmax, 1),\n                            [self.batch_size, 1]))\n        question_number_softmax = tf.where(\n            cond,\n            tf.fill(tf.shape(question_number_softmax), 1.0),\n            tf.fill(tf.shape(question_number_softmax), 0.0))\n        question_number_softmax = tf.cast(question_number_softmax,\n                                          self.data_type)\n      ans = tf.reshape(\n          tf.reduce_sum(question_number_softmax * tf.concat(\n              axis=1, values=[self.batch_question_number, self.batch_question_number_one]),\n                        1), [self.batch_size, 1])\n      return ans\n\n    def compute_op_position(op_name):\n      for i in range(len(self.utility.operations_set)):\n        if (op_name == self.utility.operations_set[i]):\n          return i\n\n    def compute_question_number(op_name):\n      op_embedding = tf.nn.embedding_lookup(self.params_unit,\n                                            compute_op_position(op_name))\n      return compute_ans(op_embedding, op_name)\n\n    curr_greater_question_number = compute_question_number(\"greater\")\n    curr_lesser_question_number = compute_question_number(\"lesser\")\n    curr_geq_question_number = compute_question_number(\"geq\")\n    curr_leq_question_number = compute_question_number(\"leq\")\n    return curr_greater_question_number, curr_lesser_question_number, curr_geq_question_number, curr_leq_question_number\n\n  def perform_attention(self, context_vector, hidden_vectors, length, mask):\n    #Performs attention on hiddent_vectors using context vector\n    context_vector = tf.tile(\n        tf.expand_dims(context_vector, 0), [length, 1, 1])  #time * bs * d\n    attention_softmax = tf.nn.softmax(\n        tf.transpose(tf.reduce_sum(context_vector * hidden_vectors, 2)) +\n        mask)  #batch_size * time\n    attention_softmax = tf.tile(\n        tf.expand_dims(tf.transpose(attention_softmax), 2),\n        [1, 1, self.embedding_dims])\n    ans_vector = tf.reduce_sum(attention_softmax * hidden_vectors, 0)\n    return ans_vector\n\n  #computes embeddings for column names using parameters of question module\n  def get_column_hidden_vectors(self):\n    #vector representations for the column names\n    self.column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_number_column_names, self.utility,\n                               self.params), 2)\n    self.word_column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_word_column_names, self.utility,\n                               self.params), 2)\n\n  def create_summary_embeddings(self):\n    #embeddings for each text entry in the table using parameters of the question module\n    self.summary_text_entry_embeddings = tf.reduce_sum(\n        tf.expand_dims(self.batch_exact_match, 3) * tf.expand_dims(\n            tf.expand_dims(\n                tf.expand_dims(\n                    nn_utils.get_embedding(self.utility.entry_match_token_id,\n                                           self.utility, self.params), 0), 1),\n            2), 2)\n\n  def compute_column_softmax(self, column_controller_vector, time_step):\n    #compute softmax over all the columns using column controller vector\n    column_controller_vector = tf.tile(\n        tf.expand_dims(column_controller_vector, 1),\n        [1, self.num_cols + self.num_word_cols, 1])  #max_cols * bs * d\n    column_controller_vector = nn_utils.apply_dropout(\n        column_controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.full_column_hidden_vectors = tf.concat(\n        axis=1, values=[self.column_hidden_vectors, self.word_column_hidden_vectors])\n    self.full_column_hidden_vectors += self.summary_text_entry_embeddings\n    self.full_column_hidden_vectors = nn_utils.apply_dropout(\n        self.full_column_hidden_vectors, self.utility.FLAGS.dropout, self.mode)\n    column_logits = tf.reduce_sum(\n        column_controller_vector * self.full_column_hidden_vectors, 2) + (\n            self.params[\"word_match_feature_column_name\"] *\n            self.batch_column_exact_match) + self.full_column_mask\n    column_softmax = tf.nn.softmax(column_logits)  #batch_size * max_cols\n    return column_softmax\n\n  def compute_first_or_last(self, select, first=True):\n    #perform first ot last operation on row select with probabilistic row selection\n    answer = tf.zeros_like(select)\n    running_sum = tf.zeros([self.batch_size, 1], self.data_type)\n    for i in range(self.max_elements):\n      if (first):\n        current = tf.slice(select, [0, i], [self.batch_size, 1])\n      else:\n        current = tf.slice(select, [0, self.max_elements - 1 - i],\n                           [self.batch_size, 1])\n      curr_prob = current * (1 - running_sum)\n      curr_prob = curr_prob * tf.cast(curr_prob >= 0.0, self.data_type)\n      running_sum += curr_prob\n      temp_ans = []\n      curr_prob = tf.expand_dims(tf.reshape(curr_prob, [self.batch_size]), 0)\n      for i_ans in range(self.max_elements):\n        if (not (first) and i_ans == self.max_elements - 1 - i):\n          temp_ans.append(curr_prob)\n        elif (first and i_ans == i):\n          temp_ans.append(curr_prob)\n        else:\n          temp_ans.append(tf.zeros_like(curr_prob))\n      temp_ans = tf.transpose(tf.concat(axis=0, values=temp_ans))\n      answer += temp_ans\n    return answer\n\n  def make_hard_softmax(self, softmax):\n    #converts soft selection to hard selection. used at test time\n    cond = tf.equal(\n        softmax, tf.reshape(tf.reduce_max(softmax, 1), [self.batch_size, 1]))\n    softmax = tf.where(\n        cond, tf.fill(tf.shape(softmax), 1.0), tf.fill(tf.shape(softmax), 0.0))\n    softmax = tf.cast(softmax, self.data_type)\n    return softmax\n\n  def compute_max_or_min(self, select, maxi=True):\n    #computes the argmax and argmin of a column with probabilistic row selection\n    answer = tf.zeros([\n        self.batch_size, self.num_cols + self.num_word_cols, self.max_elements\n    ], self.data_type)\n    sum_prob = tf.zeros([self.batch_size, self.num_cols + self.num_word_cols],\n                        self.data_type)\n    for j in range(self.max_elements):\n      if (maxi):\n        curr_pos = j\n      else:\n        curr_pos = self.max_elements - 1 - j\n      select_index = tf.slice(self.full_processed_sorted_index_column,\n                              [0, 0, curr_pos], [self.batch_size, -1, 1])\n      select_mask = tf.equal(\n          tf.tile(\n              tf.expand_dims(\n                  tf.tile(\n                      tf.expand_dims(tf.range(self.max_elements), 0),\n                      [self.batch_size, 1]), 1),\n              [1, self.num_cols + self.num_word_cols, 1]), select_index)\n      curr_prob = tf.expand_dims(select, 1) * tf.cast(\n          select_mask, self.data_type) * self.select_bad_number_mask\n      curr_prob = curr_prob * tf.expand_dims((1 - sum_prob), 2)\n      curr_prob = curr_prob * tf.expand_dims(\n          tf.cast((1 - sum_prob) > 0.0, self.data_type), 2)\n      answer = tf.where(select_mask, curr_prob, answer)\n      sum_prob += tf.reduce_sum(curr_prob, 2)\n    return answer\n\n  def perform_operations(self, softmax, full_column_softmax, select,\n                         prev_select_1, curr_pass):\n    #performs all the 15 operations. computes scalar output, lookup answer and row selector\n    column_softmax = tf.slice(full_column_softmax, [0, 0],\n                              [self.batch_size, self.num_cols])\n    word_column_softmax = tf.slice(full_column_softmax, [0, self.num_cols],\n                                   [self.batch_size, self.num_word_cols])\n    init_max = self.compute_max_or_min(select, maxi=True)\n    init_min = self.compute_max_or_min(select, maxi=False)\n    #operations that are column  independent\n    count = tf.reshape(tf.reduce_sum(select, 1), [self.batch_size, 1])\n    select_full_column_softmax = tf.tile(\n        tf.expand_dims(full_column_softmax, 2),\n        [1, 1, self.max_elements\n        ])  #BS * (max_cols + max_word_cols) * max_elements\n    select_word_column_softmax = tf.tile(\n        tf.expand_dims(word_column_softmax, 2),\n        [1, 1, self.max_elements])  #BS * max_word_cols * max_elements\n    select_greater = tf.reduce_sum(\n        self.init_select_greater * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_lesser = tf.reduce_sum(\n        self.init_select_lesser * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_geq = tf.reduce_sum(\n        self.init_select_geq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_leq = tf.reduce_sum(\n        self.init_select_leq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_max = tf.reduce_sum(init_max * select_full_column_softmax,\n                               1)  #BS * max_elements\n    select_min = tf.reduce_sum(init_min * select_full_column_softmax,\n                               1)  #BS * max_elements\n    select_prev = tf.concat(axis=1, values=[\n        tf.slice(select, [0, 1], [self.batch_size, self.max_elements - 1]),\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type)\n    ])\n    select_next = tf.concat(axis=1, values=[\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type), tf.slice(\n            select, [0, 0], [self.batch_size, self.max_elements - 1])\n    ])\n    select_last_rs = self.compute_first_or_last(select, False)\n    select_first_rs = self.compute_first_or_last(select, True)\n    select_word_match = tf.reduce_sum(self.batch_exact_match *\n                                      select_full_column_softmax, 1)\n    select_group_by_max = tf.reduce_sum(self.batch_group_by_max *\n                                        select_full_column_softmax, 1)\n    length_content = 1\n    length_select = 13\n    length_print = 1\n    values = tf.concat(axis=1, values=[count])\n    softmax_content = tf.slice(softmax, [0, 0],\n                               [self.batch_size, length_content])\n    #compute scalar output\n    output = tf.reduce_sum(tf.multiply(softmax_content, values), 1)\n    #compute lookup answer\n    softmax_print = tf.slice(softmax, [0, length_content + length_select],\n                             [self.batch_size, length_print])\n    curr_print = select_full_column_softmax * tf.tile(\n        tf.expand_dims(select, 1),\n        [1, self.num_cols + self.num_word_cols, 1\n        ])  #BS * max_cols * max_elements (conisders only column)\n    self.batch_lookup_answer = curr_print * tf.tile(\n        tf.expand_dims(softmax_print, 2),\n        [1, self.num_cols + self.num_word_cols, self.max_elements\n        ])  #BS * max_cols * max_elements\n    self.batch_lookup_answer = self.batch_lookup_answer * self.select_full_mask\n    #compute row select\n    softmax_select = tf.slice(softmax, [0, length_content],\n                              [self.batch_size, length_select])\n    select_lists = [\n        tf.expand_dims(select_prev, 1), tf.expand_dims(select_next, 1),\n        tf.expand_dims(select_first_rs, 1), tf.expand_dims(select_last_rs, 1),\n        tf.expand_dims(select_group_by_max, 1),\n        tf.expand_dims(select_greater, 1), tf.expand_dims(select_lesser, 1),\n        tf.expand_dims(select_geq, 1), tf.expand_dims(select_leq, 1),\n        tf.expand_dims(select_max, 1), tf.expand_dims(select_min, 1),\n        tf.expand_dims(select_word_match, 1),\n        tf.expand_dims(self.reset_select, 1)\n    ]\n    select = tf.reduce_sum(\n        tf.tile(tf.expand_dims(softmax_select, 2), [1, 1, self.max_elements]) *\n        tf.concat(axis=1, values=select_lists), 1)\n    select = select * self.select_whole_mask\n    return output, select\n\n  def one_pass(self, select, question_embedding, hidden_vectors, hprev,\n               prev_select_1, curr_pass):\n    #Performs one timestep which involves selecting an operation and a column\n    attention_vector = self.perform_attention(\n        hprev, hidden_vectors, self.question_length,\n        self.batch_question_attention_mask)  #batch_size * embedding_dims\n    controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[\"controller_prev\"]) + tf.matmul(\n            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[\n                \"controller\"]))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[\"column_controller_prev\"]) + tf.matmul(\n            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[\n                \"column_controller\"]))\n    controller_vector = nn_utils.apply_dropout(\n        controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.operation_logits = tf.matmul(controller_vector,\n                                      tf.transpose(self.params_unit))\n    softmax = tf.nn.softmax(self.operation_logits)\n    soft_softmax = softmax\n    #compute column softmax: bs * max_columns\n    weighted_op_representation = tf.transpose(\n        tf.matmul(tf.transpose(self.params_unit), tf.transpose(softmax)))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(\n            tf.concat(axis=1, values=[\n                column_controller_vector, weighted_op_representation\n            ]), self.params[\"break_conditional\"]))\n    full_column_softmax = self.compute_column_softmax(column_controller_vector,\n                                                      curr_pass)\n    soft_column_softmax = full_column_softmax\n    if (self.mode == \"test\"):\n      full_column_softmax = self.make_hard_softmax(full_column_softmax)\n      softmax = self.make_hard_softmax(softmax)\n    output, select = self.perform_operations(softmax, full_column_softmax,\n                                             select, prev_select_1, curr_pass)\n    return output, select, softmax, soft_softmax, full_column_softmax, soft_column_softmax\n\n  def compute_lookup_error(self, val):\n    #computes lookup error.\n    cond = tf.equal(self.batch_print_answer, val)\n    inter = tf.where(\n        cond, self.init_print_error,\n        tf.tile(\n            tf.reshape(tf.constant(1e10, self.data_type), [1, 1, 1]), [\n                self.batch_size, self.utility.FLAGS.max_word_cols +\n                self.utility.FLAGS.max_number_cols,\n                self.utility.FLAGS.max_elements\n            ]))\n    return tf.reduce_min(tf.reduce_min(inter, 1), 1) * tf.cast(\n        tf.greater(\n            tf.reduce_sum(tf.reduce_sum(tf.cast(cond, self.data_type), 1), 1),\n            0.0), self.data_type)\n\n  def soft_min(self, x, y):\n    return tf.maximum(-1.0 * (1 / (\n        self.utility.FLAGS.soft_min_value + 0.0)) * tf.log(\n            tf.exp(-self.utility.FLAGS.soft_min_value * x) + tf.exp(\n                -self.utility.FLAGS.soft_min_value * y)), tf.zeros_like(x))\n\n  def error_computation(self):\n    #computes the error of each example in a batch\n    math_error = 0.5 * tf.square(tf.subtract(self.scalar_output, self.batch_answer))\n    #scale math error\n    math_error = math_error / self.rows\n    math_error = tf.minimum(math_error, self.utility.FLAGS.max_math_error *\n                            tf.ones(tf.shape(math_error), self.data_type))\n    self.init_print_error = tf.where(\n        self.batch_gold_select, -1 * tf.log(self.batch_lookup_answer + 1e-300 +\n                                            self.invert_select_full_mask), -1 *\n        tf.log(1 - self.batch_lookup_answer)) * self.select_full_mask\n    print_error_1 = self.init_print_error * tf.cast(\n        tf.equal(self.batch_print_answer, 0.0), self.data_type)\n    print_error = tf.reduce_sum(tf.reduce_sum((print_error_1), 1), 1)\n    for val in range(1, 58):\n      print_error += self.compute_lookup_error(val + 0.0)\n    print_error = print_error * self.utility.FLAGS.print_cost / self.num_entries\n    if (self.mode == \"train\"):\n      error = tf.where(\n          tf.logical_and(\n              tf.not_equal(self.batch_answer, 0.0),\n              tf.not_equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_print_answer, 1), 1),\n                  0.0)),\n          self.soft_min(math_error, print_error),\n          tf.where(\n              tf.not_equal(self.batch_answer, 0.0), math_error, print_error))\n    else:\n      error = tf.where(\n          tf.logical_and(\n              tf.equal(self.scalar_output, 0.0),\n              tf.equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_lookup_answer, 1), 1),\n                  0.0)),\n          tf.ones_like(math_error),\n          tf.where(\n              tf.equal(self.scalar_output, 0.0), print_error, math_error))\n    return error\n\n  def batch_process(self):\n    #Computes loss and fraction of correct examples in a batch.\n    self.params_unit = nn_utils.apply_dropout(\n        self.params[\"unit\"], self.utility.FLAGS.dropout, self.mode)\n    batch_size = self.batch_size\n    max_passes = self.max_passes\n    num_timesteps = 1\n    max_elements = self.max_elements\n    select = tf.cast(\n        tf.fill([self.batch_size, max_elements], 1.0), self.data_type)\n    hprev = tf.cast(\n        tf.fill([self.batch_size, self.embedding_dims], 0.0),\n        self.data_type)  #running sum of the hidden states of the model\n    output = tf.cast(tf.fill([self.batch_size, 1], 0.0),\n                     self.data_type)  #output of the model\n    correct = tf.cast(\n        tf.fill([1], 0.0), self.data_type\n    )  #to compute accuracy, returns number of correct examples for this batch\n    total_error = 0.0\n    prev_select_1 = tf.zeros_like(select)\n    self.create_summary_embeddings()\n    self.get_column_hidden_vectors()\n    #get question embedding\n    question_embedding, hidden_vectors = self.LSTM_question_embedding(\n        self.batch_question, self.question_length)\n    #compute arguments for comparison operation\n    greater_question_number, lesser_question_number, geq_question_number, leq_question_number = self.question_number_softmax(\n        hidden_vectors)\n    self.init_select_greater = tf.cast(\n        tf.greater(self.full_processed_column,\n                   tf.expand_dims(greater_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_lesser = tf.cast(\n        tf.less(self.full_processed_column,\n                tf.expand_dims(lesser_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_geq = tf.cast(\n        tf.greater_equal(self.full_processed_column,\n                         tf.expand_dims(geq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_leq = tf.cast(\n        tf.less_equal(self.full_processed_column,\n                      tf.expand_dims(leq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_word_match = 0\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == \"train\"):\n        history_rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(hprev), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        history_rnn_dropout_mask = tf.ones_like(hprev)\n    select = select * self.select_whole_mask\n    self.batch_log_prob = tf.zeros([self.batch_size], dtype=self.data_type)\n    #Perform max_passes and at each  pass select operation and column\n    for curr_pass in range(max_passes):\n      print(\"step: \", curr_pass)\n      output, select, softmax, soft_softmax, column_softmax, soft_column_softmax = self.one_pass(\n          select, question_embedding, hidden_vectors, hprev, prev_select_1,\n          curr_pass)\n      prev_select_1 = select\n      #compute input to history RNN\n      input_op = tf.transpose(\n          tf.matmul(\n              tf.transpose(self.params_unit), tf.transpose(\n                  soft_softmax)))  #weighted average of emebdding of operations\n      input_col = tf.reduce_sum(\n          tf.expand_dims(soft_column_softmax, 2) *\n          self.full_column_hidden_vectors, 1)\n      history_input = tf.concat(axis=1, values=[input_op, input_col])\n      history_input = nn_utils.apply_dropout(\n          history_input, self.utility.FLAGS.dropout, self.mode)\n      hprev = self.history_recurrent_step(history_input, hprev)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        hprev = hprev * history_rnn_dropout_mask\n    self.scalar_output = output\n    error = self.error_computation()\n    cond = tf.less(error, 0.0001, name=\"cond\")\n    correct_add = tf.where(\n        cond, tf.fill(tf.shape(cond), 1.0), tf.fill(tf.shape(cond), 0.0))\n    correct = tf.reduce_sum(correct_add)\n    error = error / batch_size\n    total_error = tf.reduce_sum(error)\n    total_correct = correct / batch_size\n    return total_error, total_correct\n\n  def compute_error(self):\n    #Sets mask variables and performs batch processing\n    self.batch_gold_select = self.batch_print_answer > 0.0\n    self.full_column_mask = tf.concat(\n        axis=1, values=[self.batch_number_column_mask, self.batch_word_column_mask])\n    self.full_processed_column = tf.concat(\n        axis=1,\n        values=[self.batch_processed_number_column, self.batch_processed_word_column])\n    self.full_processed_sorted_index_column = tf.concat(axis=1, values=[\n        self.batch_processed_sorted_index_number_column,\n        self.batch_processed_sorted_index_word_column\n    ])\n    self.select_bad_number_mask = tf.cast(\n        tf.logical_and(\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.pad_int),\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.bad_number_pre_process)),\n        self.data_type)\n    self.select_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int)),\n        self.data_type)\n    self.select_word_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)), self.data_type)\n    self.select_full_mask = tf.concat(\n        axis=1, values=[self.select_mask, self.select_word_mask])\n    self.select_whole_mask = tf.maximum(\n        tf.reshape(\n            tf.slice(self.select_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]),\n        tf.reshape(\n            tf.slice(self.select_word_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]))\n    self.invert_select_full_mask = tf.cast(\n        tf.concat(axis=1, values=[\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int),\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)\n        ]), self.data_type)\n    self.batch_lookup_answer = tf.zeros(tf.shape(self.batch_gold_select))\n    self.reset_select = self.select_whole_mask\n    self.rows = tf.reduce_sum(self.select_whole_mask, 1)\n    self.num_entries = tf.reshape(\n        tf.reduce_sum(tf.reduce_sum(self.select_full_mask, 1), 1),\n        [self.batch_size])\n    self.final_error, self.final_correct = self.batch_process()\n    return self.final_error\n\n  def create_graph(self, params, global_step):\n    #Creates the graph to compute error, gradient computation and updates parameters\n    self.params = params\n    batch_size = self.batch_size\n    learning_rate = tf.cast(self.utility.FLAGS.learning_rate, self.data_type)\n    self.total_cost = self.compute_error()\n    optimize_params = self.params.values()\n    optimize_names = self.params.keys()\n    print(\"optimize params \", optimize_names)\n    if (self.utility.FLAGS.l2_regularizer > 0.0):\n      reg_cost = 0.0\n      for ind_param in self.params.keys():\n        reg_cost += tf.nn.l2_loss(self.params[ind_param])\n      self.total_cost += self.utility.FLAGS.l2_regularizer * reg_cost\n    grads = tf.gradients(self.total_cost, optimize_params, name=\"gradients\")\n    grad_norm = 0.0\n    for p, name in zip(grads, optimize_names):\n      print(\"grads: \", p, name)\n      if isinstance(p, tf.IndexedSlices):\n        grad_norm += tf.reduce_sum(p.values * p.values)\n      elif not (p == None):\n        grad_norm += tf.reduce_sum(p * p)\n    grad_norm = tf.sqrt(grad_norm)\n    max_grad_norm = np.float32(self.utility.FLAGS.clip_gradients).astype(\n        self.utility.np_data_type[self.utility.FLAGS.data_type])\n    grad_scale = tf.minimum(\n        tf.cast(1.0, self.data_type), max_grad_norm / grad_norm)\n    clipped_grads = list()\n    for p in grads:\n      if isinstance(p, tf.IndexedSlices):\n        tmp = p.values * grad_scale\n        clipped_grads.append(tf.IndexedSlices(tmp, p.indices))\n      elif not (p == None):\n        clipped_grads.append(p * grad_scale)\n      else:\n        clipped_grads.append(p)\n    grads = clipped_grads\n    self.global_step = global_step\n    params_list = self.params.values()\n    params_list.append(self.global_step)\n    adam = tf.train.AdamOptimizer(\n        learning_rate,\n        epsilon=tf.cast(self.utility.FLAGS.eps, self.data_type),\n        use_locking=True)\n    self.step = adam.apply_gradients(zip(grads, optimize_params),\n\t\t\t\t\tglobal_step=self.global_step)\n    self.init_op = tf.global_variables_initializer()\n", "description": "Models and examples built with TensorFlow", "file_name": "model.py", "id": "d00fc02cb5aefff6e3ef2e7653db4d17", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/neural_programmer/model.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}