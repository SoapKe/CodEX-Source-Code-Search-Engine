{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"GRU cell implementation for the skip-thought vectors model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n_layer_norm = tf.contrib.layers.layer_norm\n\n\nclass LayerNormGRUCell(tf.contrib.rnn.RNNCell):\n  \"\"\"GRU cell with layer normalization.\n\n  The layer normalization implementation is based on:\n\n    https://arxiv.org/abs/1607.06450.\n\n  \"Layer Normalization\"\n  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               w_initializer,\n               u_initializer,\n               b_initializer,\n               activation=tf.nn.tanh):\n    \"\"\"Initializes the cell.\n\n    Args:\n      num_units: Number of cell units.\n      w_initializer: Initializer for the \"W\" (input) parameter matrices.\n      u_initializer: Initializer for the \"U\" (recurrent) parameter matrices.\n      b_initializer: Initializer for the \"b\" (bias) parameter vectors.\n      activation: Cell activation function.\n    \"\"\"\n    self._num_units = num_units\n    self._w_initializer = w_initializer\n    self._u_initializer = u_initializer\n    self._b_initializer = b_initializer\n    self._activation = activation\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def _w_h_initializer(self):\n    \"\"\"Returns an initializer for the \"W_h\" parameter matrix.\n\n    See equation (23) in the paper. The \"W_h\" parameter matrix is the\n    concatenation of two parameter submatrices. The matrix returned is\n    [U_z, U_r].\n\n    Returns:\n      A Tensor with shape [num_units, 2 * num_units] as described above.\n    \"\"\"\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n      num_units = self._num_units\n      assert shape == [num_units, 2 * num_units]\n      u_z = self._u_initializer([num_units, num_units], dtype, partition_info)\n      u_r = self._u_initializer([num_units, num_units], dtype, partition_info)\n      return tf.concat([u_z, u_r], 1)\n\n    return _initializer\n\n  def _w_x_initializer(self, input_dim):\n    \"\"\"Returns an initializer for the \"W_x\" parameter matrix.\n\n    See equation (23) in the paper. The \"W_x\" parameter matrix is the\n    concatenation of two parameter submatrices. The matrix returned is\n    [W_z, W_r].\n\n    Args:\n      input_dim: The dimension of the cell inputs.\n\n    Returns:\n      A Tensor with shape [input_dim, 2 * num_units] as described above.\n    \"\"\"\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n      num_units = self._num_units\n      assert shape == [input_dim, 2 * num_units]\n      w_z = self._w_initializer([input_dim, num_units], dtype, partition_info)\n      w_r = self._w_initializer([input_dim, num_units], dtype, partition_info)\n      return tf.concat([w_z, w_r], 1)\n\n    return _initializer\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"GRU cell with layer normalization.\"\"\"\n    input_dim = inputs.get_shape().as_list()[1]\n    num_units = self._num_units\n\n    with tf.variable_scope(scope or \"gru_cell\"):\n      with tf.variable_scope(\"gates\"):\n        w_h = tf.get_variable(\n            \"w_h\", [num_units, 2 * num_units],\n            initializer=self._w_h_initializer())\n        w_x = tf.get_variable(\n            \"w_x\", [input_dim, 2 * num_units],\n            initializer=self._w_x_initializer(input_dim))\n        z_and_r = (_layer_norm(tf.matmul(state, w_h), scope=\"layer_norm/w_h\") +\n                   _layer_norm(tf.matmul(inputs, w_x), scope=\"layer_norm/w_x\"))\n        z, r = tf.split(tf.sigmoid(z_and_r), 2, 1)\n      with tf.variable_scope(\"candidate\"):\n        w = tf.get_variable(\n            \"w\", [input_dim, num_units], initializer=self._w_initializer)\n        u = tf.get_variable(\n            \"u\", [num_units, num_units], initializer=self._u_initializer)\n        h_hat = (r * _layer_norm(tf.matmul(state, u), scope=\"layer_norm/u\") +\n                 _layer_norm(tf.matmul(inputs, w), scope=\"layer_norm/w\"))\n      new_h = (1 - z) * state + z * self._activation(h_hat)\n    return new_h, new_h\n", "comments": "   gru cell implementation skip thought vectors model        future   import absolute import   future   import division   future   import print function   import tensorflow tf   layer norm   tf contrib layers layer norm   class layernormgrucell(tf contrib rnn rnncell)       gru cell layer normalization     the layer normalization implementation based       https   arxiv org abs 1607 06450      layer normalization    jimmy lei ba  jamie ryan kiros  geoffrey e  hinton          def   init  (self                 num units                 w initializer                 u initializer                 b initializer                 activation tf nn tanh)         initializes cell       args        num units  number cell units        w initializer  initializer  w  (input) parameter matrices        u initializer  initializer  u  (recurrent) parameter matrices        b initializer  initializer  b  (bias) parameter vectors        activation  cell activation function              self  num units   num units     self  w initializer   w initializer     self  u initializer   u initializer     self  b initializer   b initializer     self  activation   activation     property   def state size(self)      return self  num units     property   def output size(self)      return self  num units    def  w h initializer(self)         returns initializer  w h  parameter matrix       see equation (23) paper  the  w h  parameter matrix     concatenation two parameter submatrices  the matrix returned      u z  u r        returns        a tensor shape  num units  2   num units  described               def  initializer(shape  dtype tf float32  partition info none)        num units   self  num units       assert shape     num units  2   num units        u z   self  u initializer( num units  num units   dtype  partition info)       u r   self  u initializer( num units  num units   dtype  partition info)       return tf concat( u z  u r   1)      return  initializer    def  w x initializer(self  input dim)         returns initializer  w x  parameter matrix       see equation (23) paper  the  w x  parameter matrix     concatenation two parameter submatrices  the matrix returned      w z  w r        args        input dim  the dimension cell inputs       returns        a tensor shape  input dim  2   num units  described               def  initializer(shape  dtype tf float32  partition info none)        num units   self  num units       assert shape     input dim  2   num units        w z   self  w initializer( input dim  num units   dtype  partition info)       w r   self  w initializer( input dim  num units   dtype  partition info)       return tf concat( w z  w r   1)      return  initializer    def   call  (self  inputs  state  scope none)         gru cell layer normalization        copyright 2017 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                    ", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"GRU cell implementation for the skip-thought vectors model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n_layer_norm = tf.contrib.layers.layer_norm\n\n\nclass LayerNormGRUCell(tf.contrib.rnn.RNNCell):\n  \"\"\"GRU cell with layer normalization.\n\n  The layer normalization implementation is based on:\n\n    https://arxiv.org/abs/1607.06450.\n\n  \"Layer Normalization\"\n  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               w_initializer,\n               u_initializer,\n               b_initializer,\n               activation=tf.nn.tanh):\n    \"\"\"Initializes the cell.\n\n    Args:\n      num_units: Number of cell units.\n      w_initializer: Initializer for the \"W\" (input) parameter matrices.\n      u_initializer: Initializer for the \"U\" (recurrent) parameter matrices.\n      b_initializer: Initializer for the \"b\" (bias) parameter vectors.\n      activation: Cell activation function.\n    \"\"\"\n    self._num_units = num_units\n    self._w_initializer = w_initializer\n    self._u_initializer = u_initializer\n    self._b_initializer = b_initializer\n    self._activation = activation\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def _w_h_initializer(self):\n    \"\"\"Returns an initializer for the \"W_h\" parameter matrix.\n\n    See equation (23) in the paper. The \"W_h\" parameter matrix is the\n    concatenation of two parameter submatrices. The matrix returned is\n    [U_z, U_r].\n\n    Returns:\n      A Tensor with shape [num_units, 2 * num_units] as described above.\n    \"\"\"\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n      num_units = self._num_units\n      assert shape == [num_units, 2 * num_units]\n      u_z = self._u_initializer([num_units, num_units], dtype, partition_info)\n      u_r = self._u_initializer([num_units, num_units], dtype, partition_info)\n      return tf.concat([u_z, u_r], 1)\n\n    return _initializer\n\n  def _w_x_initializer(self, input_dim):\n    \"\"\"Returns an initializer for the \"W_x\" parameter matrix.\n\n    See equation (23) in the paper. The \"W_x\" parameter matrix is the\n    concatenation of two parameter submatrices. The matrix returned is\n    [W_z, W_r].\n\n    Args:\n      input_dim: The dimension of the cell inputs.\n\n    Returns:\n      A Tensor with shape [input_dim, 2 * num_units] as described above.\n    \"\"\"\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n      num_units = self._num_units\n      assert shape == [input_dim, 2 * num_units]\n      w_z = self._w_initializer([input_dim, num_units], dtype, partition_info)\n      w_r = self._w_initializer([input_dim, num_units], dtype, partition_info)\n      return tf.concat([w_z, w_r], 1)\n\n    return _initializer\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"GRU cell with layer normalization.\"\"\"\n    input_dim = inputs.get_shape().as_list()[1]\n    num_units = self._num_units\n\n    with tf.variable_scope(scope or \"gru_cell\"):\n      with tf.variable_scope(\"gates\"):\n        w_h = tf.get_variable(\n            \"w_h\", [num_units, 2 * num_units],\n            initializer=self._w_h_initializer())\n        w_x = tf.get_variable(\n            \"w_x\", [input_dim, 2 * num_units],\n            initializer=self._w_x_initializer(input_dim))\n        z_and_r = (_layer_norm(tf.matmul(state, w_h), scope=\"layer_norm/w_h\") +\n                   _layer_norm(tf.matmul(inputs, w_x), scope=\"layer_norm/w_x\"))\n        z, r = tf.split(tf.sigmoid(z_and_r), 2, 1)\n      with tf.variable_scope(\"candidate\"):\n        w = tf.get_variable(\n            \"w\", [input_dim, num_units], initializer=self._w_initializer)\n        u = tf.get_variable(\n            \"u\", [num_units, num_units], initializer=self._u_initializer)\n        h_hat = (r * _layer_norm(tf.matmul(state, u), scope=\"layer_norm/u\") +\n                 _layer_norm(tf.matmul(inputs, w), scope=\"layer_norm/w\"))\n      new_h = (1 - z) * state + z * self._activation(h_hat)\n    return new_h, new_h\n", "description": "Models and examples built with TensorFlow", "file_name": "gru_cell.py", "id": "e0af6ac30e091fdcbf6a0f6f159ea379", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/skip_thoughts/skip_thoughts/ops/gru_cell.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}