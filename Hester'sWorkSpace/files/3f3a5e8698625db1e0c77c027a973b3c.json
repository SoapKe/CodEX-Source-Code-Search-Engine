{"author": "yunjey", "code": "import torch\nimport os\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n    \n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \nclass Corpus(object):\n    def __init__(self, path='./data'):\n        self.dictionary = Dictionary()\n\n    def get_data(self, path, batch_size=20):\n        \n        with open(path, 'r') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                tokens += len(words)\n                for word in words: \n                    self.dictionary.add_word(word)  \n        \n        \n        ids = torch.LongTensor(tokens)\n        token = 0\n        with open(path, 'r') as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n        num_batches = ids.size(0) // batch_size\n        ids = ids[:num_batches*batch_size]\n        return ids.view(batch_size, -1)\n", "comments": "  add words dictionary    tokenize file content ", "content": "import torch\nimport os\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n    \n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \nclass Corpus(object):\n    def __init__(self, path='./data'):\n        self.dictionary = Dictionary()\n\n    def get_data(self, path, batch_size=20):\n        # Add words to the dictionary\n        with open(path, 'r') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                tokens += len(words)\n                for word in words: \n                    self.dictionary.add_word(word)  \n        \n        # Tokenize the file content\n        ids = torch.LongTensor(tokens)\n        token = 0\n        with open(path, 'r') as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n        num_batches = ids.size(0) // batch_size\n        ids = ids[:num_batches*batch_size]\n        return ids.view(batch_size, -1)\n", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "data_utils.py", "id": "3f3a5e8698625db1e0c77c027a973b3c", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/02-intermediate/language_model/data_utils.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}