{"author": "HelloZeroNet", "code": "import time\nimport os\nimport subprocess\nimport shutil\nimport collections\nimport gevent\nimport math\n\nimport msgpack\n\nfrom Plugin import PluginManager\nfrom Debug import Debug\nfrom Crypt import CryptHash\nfrom lib import merkletools\nfrom util import helper\nimport util\nfrom BigfilePiecefield import BigfilePiecefield, BigfilePiecefieldPacked\n\n\n\n@PluginManager.afterLoad\ndef importPluginnedClasses():\n    global VerifyError, config\n    from Content.ContentManager import VerifyError\n    from Config import config\n\nif \"upload_nonces\" not in locals():\n    upload_nonces = {}\n\n\n@PluginManager.registerTo(\"UiRequest\")\nclass UiRequestPlugin(object):\n    def isCorsAllowed(self, path):\n        if path == \"/ZeroNet-Internal/BigfileUpload\":\n            return True\n        else:\n            return super(UiRequestPlugin, self).isCorsAllowed(path)\n\n    def actionBigfileUpload(self):\n        nonce = self.get.get(\"upload_nonce\")\n        if nonce not in upload_nonces:\n            return self.error403(\"Upload nonce error.\")\n\n        upload_info = upload_nonces[nonce]\n        del upload_nonces[nonce]\n\n        self.sendHeader(200, \"text/html\", noscript=True, extra_headers={\n            \"Access-Control-Allow-Origin\": \"null\",\n            \"Access-Control-Allow-Credentials\": \"true\"\n        })\n\n        self.readMultipartHeaders(self.env['wsgi.input'])  \n\n        site = upload_info[\"site\"]\n        inner_path = upload_info[\"inner_path\"]\n\n        with site.storage.open(inner_path, \"wb\", create_dirs=True) as out_file:\n            merkle_root, piece_size, piecemap_info = site.content_manager.hashBigfile(\n                self.env['wsgi.input'], upload_info[\"size\"], upload_info[\"piece_size\"], out_file\n            )\n\n        if len(piecemap_info[\"sha512_pieces\"]) == 1:  \n            hash = piecemap_info[\"sha512_pieces\"][0].encode(\"hex\")\n            site.content_manager.optionalDownloaded(inner_path, hash, upload_info[\"size\"], own=True)\n\n        else:  \n            file_name = helper.getFilename(inner_path)\n            msgpack.pack({file_name: piecemap_info}, site.storage.open(upload_info[\"piecemap\"], \"wb\"))\n\n            \n            file_info = site.content_manager.getFileInfo(inner_path, new_file=True)\n            content_inner_path_dir = helper.getDirname(file_info[\"content_inner_path\"])\n            piecemap_relative_path = upload_info[\"piecemap\"][len(content_inner_path_dir):]\n            file_relative_path = inner_path[len(content_inner_path_dir):]\n\n            \n            if site.storage.isFile(file_info[\"content_inner_path\"]):\n                content = site.storage.loadJson(file_info[\"content_inner_path\"])\n            else:\n                content = {}\n            if \"files_optional\" not in content:\n                content[\"files_optional\"] = {}\n\n            content[\"files_optional\"][file_relative_path] = {\n                \"sha512\": merkle_root,\n                \"size\": upload_info[\"size\"],\n                \"piecemap\": piecemap_relative_path,\n                \"piece_size\": piece_size\n            }\n\n            site.content_manager.optionalDownloaded(inner_path, merkle_root, upload_info[\"size\"], own=True)\n            site.storage.writeJson(file_info[\"content_inner_path\"], content)\n\n            site.content_manager.contents.loadItem(file_info[\"content_inner_path\"])  \n\n        return {\n            \"merkle_root\": merkle_root,\n            \"piece_num\": len(piecemap_info[\"sha512_pieces\"]),\n            \"piece_size\": piece_size,\n            \"inner_path\": inner_path\n        }\n\n    def readMultipartHeaders(self, wsgi_input):\n        for i in range(100):\n            line = wsgi_input.readline()\n            if line == \"\\r\\n\":\n                break\n        return i\n\n    def actionFile(self, file_path, *args, **kwargs):\n        if kwargs.get(\"file_size\", 0) > 1024 * 1024 and kwargs.get(\"path_parts\"):  \n            path_parts = kwargs[\"path_parts\"]\n            site = self.server.site_manager.get(path_parts[\"address\"])\n            kwargs[\"file_obj\"] = site.storage.openBigfile(path_parts[\"inner_path\"], prebuffer=2 * 1024 * 1024)\n\n        return super(UiRequestPlugin, self).actionFile(file_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"UiWebsocket\")\nclass UiWebsocketPlugin(object):\n    def actionBigfileUploadInit(self, to, inner_path, size):\n        valid_signers = self.site.content_manager.getValidSigners(inner_path)\n        auth_address = self.user.getAuthAddress(self.site.address)\n        if not self.site.settings[\"own\"] and auth_address not in valid_signers:\n            self.log.error(\"FileWrite forbidden %s not in valid_signers %s\" % (auth_address, valid_signers))\n            return self.response(to, {\"error\": \"Forbidden, you can only modify your own files\"})\n\n        nonce = CryptHash.random()\n        piece_size = 1024 * 1024\n        inner_path = self.site.content_manager.sanitizePath(inner_path)\n        file_info = self.site.content_manager.getFileInfo(inner_path, new_file=True)\n\n        content_inner_path_dir = helper.getDirname(file_info[\"content_inner_path\"])\n        file_relative_path = inner_path[len(content_inner_path_dir):]\n\n        upload_nonces[nonce] = {\n            \"added\": time.time(),\n            \"site\": self.site,\n            \"inner_path\": inner_path,\n            \"websocket_client\": self,\n            \"size\": size,\n            \"piece_size\": piece_size,\n            \"piecemap\": inner_path + \".piecemap.msgpack\"\n        }\n        return {\n            \"url\": \"/ZeroNet-Internal/BigfileUpload?upload_nonce=\" + nonce,\n            \"piece_size\": piece_size,\n            \"inner_path\": inner_path,\n            \"file_relative_path\": file_relative_path\n        }\n\n    def actionSiteSetAutodownloadBigfileLimit(self, to, limit):\n        permissions = self.getPermissions(to)\n        if \"ADMIN\" not in permissions:\n            return self.response(to, \"You don't have permission to run this command\")\n\n        self.site.settings[\"autodownload_bigfile_size_limit\"] = int(limit)\n        self.response(to, \"ok\")\n\n\n@PluginManager.registerTo(\"ContentManager\")\nclass ContentManagerPlugin(object):\n    def getFileInfo(self, inner_path, *args, **kwargs):\n        if \"|\" not in inner_path:\n            return super(ContentManagerPlugin, self).getFileInfo(inner_path, *args, **kwargs)\n\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n        file_info = super(ContentManagerPlugin, self).getFileInfo(inner_path, *args, **kwargs)\n        return file_info\n\n    def readFile(self, file_in, size, buff_size=1024 * 64):\n        part_num = 0\n        recv_left = size\n\n        while 1:\n            part_num += 1\n            read_size = min(buff_size, recv_left)\n            part = file_in.read(read_size)\n\n            if not part:\n                break\n            yield part\n\n            if part_num % 100 == 0:  \n                time.sleep(0.001)\n\n            recv_left -= read_size\n            if recv_left <= 0:\n                break\n\n    def hashBigfile(self, file_in, size, piece_size=1024 * 1024, file_out=None):\n        self.site.settings[\"has_bigfile\"] = True\n\n        recv = 0\n        try:\n            piece_hash = CryptHash.sha512t()\n            piece_hashes = []\n            piece_recv = 0\n\n            mt = merkletools.MerkleTools()\n            mt.hash_function = CryptHash.sha512t\n\n            part = \"\"\n            for part in self.readFile(file_in, size):\n                if file_out:\n                    file_out.write(part)\n\n                recv += len(part)\n                piece_recv += len(part)\n                piece_hash.update(part)\n                if piece_recv >= piece_size:\n                    piece_digest = piece_hash.digest()\n                    piece_hashes.append(piece_digest)\n                    mt.leaves.append(piece_digest)\n                    piece_hash = CryptHash.sha512t()\n                    piece_recv = 0\n\n                    if len(piece_hashes) % 100 == 0 or recv == size:\n                        self.log.info(\"- [HASHING:%.0f%%] Pieces: %s, %.1fMB/%.1fMB\" % (\n                            float(recv) / size * 100, len(piece_hashes), recv / 1024 / 1024, size / 1024 / 1024\n                        ))\n                        part = \"\"\n            if len(part) > 0:\n                piece_digest = piece_hash.digest()\n                piece_hashes.append(piece_digest)\n                mt.leaves.append(piece_digest)\n        except Exception as err:\n            raise err\n        finally:\n            if file_out:\n                file_out.close()\n\n        mt.make_tree()\n        return mt.get_merkle_root(), piece_size, {\n            \"sha512_pieces\": piece_hashes\n        }\n\n    def hashFile(self, dir_inner_path, file_relative_path, optional=False):\n        inner_path = dir_inner_path + file_relative_path\n\n        file_size = self.site.storage.getSize(inner_path)\n        \n        if not optional or file_size < 1 * 1024 * 1024:\n            return super(ContentManagerPlugin, self).hashFile(dir_inner_path, file_relative_path, optional)\n\n        back = {}\n        content = self.contents.get(dir_inner_path + \"content.json\")\n\n        hash = None\n        piecemap_relative_path = None\n        piece_size = None\n\n        \n        if content and file_relative_path in content.get(\"files_optional\", {}):\n            file_node = content[\"files_optional\"][file_relative_path]\n            if file_node[\"size\"] == file_size:\n                self.log.info(\"- [SAME SIZE] %s\" % file_relative_path)\n                hash = file_node.get(\"sha512\")\n                piecemap_relative_path = file_node.get(\"piecemap\")\n                piece_size = file_node.get(\"piece_size\")\n\n        if not hash or not piecemap_relative_path:  \n            if file_size < 5 * 1024 * 1024:  \n                return super(ContentManagerPlugin, self).hashFile(dir_inner_path, file_relative_path, optional)\n\n            self.log.info(\"- [HASHING] %s\" % file_relative_path)\n            merkle_root, piece_size, piecemap_info = self.hashBigfile(self.site.storage.open(inner_path, \"rb\"), file_size)\n            if not hash:\n                hash = merkle_root\n\n            if not piecemap_relative_path:\n                file_name = helper.getFilename(file_relative_path)\n                piecemap_relative_path = file_relative_path + \".piecemap.msgpack\"\n                piecemap_inner_path = inner_path + \".piecemap.msgpack\"\n\n                msgpack.pack({file_name: piecemap_info}, self.site.storage.open(piecemap_inner_path, \"wb\"))\n\n                back.update(super(ContentManagerPlugin, self).hashFile(dir_inner_path, piecemap_relative_path, optional=True))\n\n        piece_num = int(math.ceil(float(file_size) / piece_size))\n\n        \n        self.optionalDownloaded(inner_path, hash, file_size, own=True)\n        self.site.storage.piecefields[hash].fromstring(\"1\" * piece_num)\n\n        back[file_relative_path] = {\"sha512\": hash, \"size\": file_size, \"piecemap\": piecemap_relative_path, \"piece_size\": piece_size}\n        return back\n\n    def getPiecemap(self, inner_path):\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        piecemap_inner_path = helper.getDirname(file_info[\"content_inner_path\"]) + file_info[\"piecemap\"]\n        self.site.needFile(piecemap_inner_path, priority=20)\n        piecemap = msgpack.unpack(self.site.storage.open(piecemap_inner_path))[helper.getFilename(inner_path)]\n        piecemap[\"piece_size\"] = file_info[\"piece_size\"]\n        return piecemap\n\n    def verifyPiece(self, inner_path, pos, piece):\n        piecemap = self.getPiecemap(inner_path)\n        piece_i = pos / piecemap[\"piece_size\"]\n        if CryptHash.sha512sum(piece, format=\"digest\") != piecemap[\"sha512_pieces\"][piece_i]:\n            raise VerifyError(\"Invalid hash\")\n        return True\n\n    def verifyFile(self, inner_path, file, ignore_same=True):\n        if \"|\" not in inner_path:\n            return super(ContentManagerPlugin, self).verifyFile(inner_path, file, ignore_same)\n\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n\n        return self.verifyPiece(inner_path, pos_from, file)\n\n    def optionalDownloaded(self, inner_path, hash, size=None, own=False):\n        if \"|\" in inner_path:\n            inner_path, file_range = inner_path.split(\"|\")\n            pos_from, pos_to = map(int, file_range.split(\"-\"))\n            file_info = self.getFileInfo(inner_path)\n\n            \n            piece_i = pos_from / file_info[\"piece_size\"]\n            self.site.storage.piecefields[file_info[\"sha512\"]][piece_i] = True\n\n            \n            if hash in self.hashfield:\n                size = 0\n        elif size > 1024 * 1024:\n            file_info = self.getFileInfo(inner_path)\n            if file_info:  \n                sha512 = file_info[\"sha512\"]\n                if sha512 not in self.site.storage.piecefields:\n                    self.site.storage.checkBigfile(inner_path)\n\n        return super(ContentManagerPlugin, self).optionalDownloaded(inner_path, hash, size, own)\n\n    def optionalRemove(self, inner_path, hash, size=None):\n        if size and size > 1024 * 1024:\n            file_info = self.getFileInfo(inner_path)\n            sha512 = file_info[\"sha512\"]\n            if sha512 in self.site.storage.piecefields:\n                del self.site.storage.piecefields[sha512]\n\n            \n            for key in self.site.bad_files.keys():\n                if key.startswith(inner_path + \"|\"):\n                    del self.site.bad_files[key]\n        return super(ContentManagerPlugin, self).optionalRemove(inner_path, hash, size)\n\n\n@PluginManager.registerTo(\"SiteStorage\")\nclass SiteStoragePlugin(object):\n    def __init__(self, *args, **kwargs):\n        super(SiteStoragePlugin, self).__init__(*args, **kwargs)\n        self.piecefields = collections.defaultdict(BigfilePiecefield)\n        if \"piecefields\" in self.site.settings.get(\"cache\", {}):\n            for sha512, piecefield_packed in self.site.settings[\"cache\"].get(\"piecefields\").iteritems():\n                if piecefield_packed:\n                    self.piecefields[sha512].unpack(piecefield_packed.decode(\"base64\"))\n            self.site.settings[\"cache\"][\"piecefields\"] = {}\n\n    def createSparseFile(self, inner_path, size, sha512=None):\n        file_path = self.getPath(inner_path)\n\n        file_dir = os.path.dirname(file_path)\n        if not os.path.isdir(file_dir):\n            os.makedirs(file_dir)\n\n        f = open(file_path, 'wb')\n        f.truncate(size)\n        f.close()\n        if os.name == \"nt\":\n            startupinfo = subprocess.STARTUPINFO()\n            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n            subprocess.call([\"fsutil\", \"sparse\", \"setflag\", file_path], close_fds=True, startupinfo=startupinfo)\n\n        if sha512 and sha512 in self.piecefields:\n            self.log.debug(\"%s: File not exists, but has piecefield. Deleting piecefield.\" % inner_path)\n            del self.piecefields[sha512]\n\n    def write(self, inner_path, content):\n        if \"|\" not in inner_path:\n            return super(SiteStoragePlugin, self).write(inner_path, content)\n\n        |\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n        file_path = self.getPath(inner_path)\n\n        \n        file_dir = os.path.dirname(file_path)\n        if not os.path.isdir(file_dir):\n            os.makedirs(file_dir)\n\n        if not os.path.isfile(file_path):\n            file_info = self.site.content_manager.getFileInfo(inner_path)\n            self.createSparseFile(inner_path, file_info[\"size\"])\n\n        \n        with open(file_path, \"rb+\") as file:\n            file.seek(pos_from)\n            if hasattr(content, 'read'):  \n                shutil.copyfileobj(content, file)  \n            else:  \n                file.write(content)\n        del content\n        self.onUpdated(inner_path)\n\n    def checkBigfile(self, inner_path):\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        if not file_info or (file_info and \"piecemap\" not in file_info):  \n            return False\n        file_path = self.getPath(inner_path)\n        sha512 = file_info[\"sha512\"]\n        piece_num = int(math.ceil(float(file_info[\"size\"]) / file_info[\"piece_size\"]))\n        if os.path.isfile(file_path):\n            if sha512 not in self.piecefields:\n                if open(file_path).read(128) == \"\\0\" * 128:\n                    piece_data = \"0\"\n                else:\n                    piece_data = \"1\"\n                self.log.debug(\"%s: File exists, but not in piecefield. Filling piecefiled with %s * %s.\" % (inner_path, piece_num, piece_data))\n                self.piecefields[sha512].fromstring(piece_data * piece_num)\n        else:\n            self.log.debug(\"Creating bigfile: %s\" % inner_path)\n            self.createSparseFile(inner_path, file_info[\"size\"], sha512)\n            self.piecefields[sha512].fromstring(\"0\" * piece_num)\n        return True\n\n    def openBigfile(self, inner_path, prebuffer=0):\n        if not self.checkBigfile(inner_path):\n            return False\n        self.site.needFile(inner_path, blocking=False)  \n        return BigFile(self.site, inner_path, prebuffer=prebuffer)\n\n\nclass BigFile(object):\n    def __init__(self, site, inner_path, prebuffer=0):\n        self.site = site\n        self.inner_path = inner_path\n        file_path = site.storage.getPath(inner_path)\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        self.piece_size = file_info[\"piece_size\"]\n        self.sha512 = file_info[\"sha512\"]\n        self.size = file_info[\"size\"]\n        self.prebuffer = prebuffer\n        self.read_bytes = 0\n\n        self.piecefield = self.site.storage.piecefields[self.sha512]\n        self.f = open(file_path, \"rb+\")\n\n    def read(self, buff=64 * 1024):\n        pos = self.f.tell()\n        read_until = min(self.size, pos + buff)\n        requests = []\n        \n        while 1:\n            piece_i = pos / self.piece_size\n            if piece_i * self.piece_size >= read_until:\n                break\n            pos_from = piece_i * self.piece_size\n            pos_to = pos_from + self.piece_size\n            if not self.piecefield[piece_i]:\n                requests.append(self.site.needFile(\"%s|%s-%s\" % (self.inner_path, pos_from, pos_to), blocking=False, update=True, priority=10))\n            pos += self.piece_size\n\n        if not all(requests):\n            return None\n\n        \n        if self.prebuffer:\n            prebuffer_until = min(self.size, read_until + self.prebuffer)\n            priority = 3\n            while 1:\n                piece_i = pos / self.piece_size\n                if piece_i * self.piece_size >= prebuffer_until:\n                    break\n                pos_from = piece_i * self.piece_size\n                pos_to = pos_from + self.piece_size\n                if not self.piecefield[piece_i]:\n                    self.site.needFile(\"%s|%s-%s\" % (self.inner_path, pos_from, pos_to), blocking=False, update=True, priority=max(0, priority))\n                priority -= 1\n                pos += self.piece_size\n\n        gevent.joinall(requests)\n        self.read_bytes += buff\n\n        \n        if self.read_bytes > 7 * 1024 * 1024 and self.prebuffer < 5 * 1024 * 1024:\n            self.site.log.debug(\"%s: Increasing bigfile buffer size to 5MB...\" % self.inner_path)\n            self.prebuffer = 5 * 1024 * 1024\n\n        return self.f.read(buff)\n\n    def seek(self, pos):\n        return self.f.seek(pos)\n\n    def tell(self):\n        self.f.tell()\n\n    def close(self):\n        self.f.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\n@PluginManager.registerTo(\"WorkerManager\")\nclass WorkerManagerPlugin(object):\n    def addTask(self, inner_path, *args, **kwargs):\n        file_info = kwargs.get(\"file_info\")\n        if file_info and \"piecemap\" in file_info:  \n            self.site.settings[\"has_bigfile\"] = True\n\n            piecemap_inner_path = helper.getDirname(file_info[\"content_inner_path\"]) + file_info[\"piecemap\"]\n            piecemap_task = None\n            if not self.site.storage.isFile(piecemap_inner_path):\n                \n                piecemap_task = super(WorkerManagerPlugin, self).addTask(piecemap_inner_path, priority=30)\n                autodownload_bigfile_size_limit = self.site.settings.get(\"autodownload_bigfile_size_limit\", config.autodownload_bigfile_size_limit)\n                if \"|\" not in inner_path and self.site.isDownloadable(inner_path) and file_info[\"size\"] / 1024 / 1024 <= autodownload_bigfile_size_limit:\n                    gevent.spawn_later(0.1, self.site.needFile, inner_path + \"|all\")  \n\n            if \"|\" in inner_path:\n                \n                task = super(WorkerManagerPlugin, self).addTask(inner_path, *args, **kwargs)\n\n                inner_path, file_range = inner_path.split(\"|\")\n                pos_from, pos_to = map(int, file_range.split(\"-\"))\n                task[\"piece_i\"] = pos_from / file_info[\"piece_size\"]\n                task[\"sha512\"] = file_info[\"sha512\"]\n            else:\n                if inner_path in self.site.bad_files:\n                    del self.site.bad_files[inner_path]\n                if piecemap_task:\n                    task = piecemap_task\n                else:\n                    fake_evt = gevent.event.AsyncResult()  \n                    fake_evt.set(True)\n                    task = {\"evt\": fake_evt}\n\n            if not self.site.storage.isFile(inner_path):\n                self.site.storage.createSparseFile(inner_path, file_info[\"size\"], file_info[\"sha512\"])\n                piece_num = int(math.ceil(float(file_info[\"size\"]) / file_info[\"piece_size\"]))\n                self.site.storage.piecefields[file_info[\"sha512\"]].fromstring(\"0\" * piece_num)\n        else:\n            task = super(WorkerManagerPlugin, self).addTask(inner_path, *args, **kwargs)\n        return task\n\n    def taskAddPeer(self, task, peer):\n        if \"piece_i\" in task:\n            if not peer.piecefields[task[\"sha512\"]][task[\"piece_i\"]]:\n                if task[\"sha512\"] not in peer.piecefields:\n                    gevent.spawn(peer.updatePiecefields, force=True)\n                elif not task[\"peers\"]:\n                    gevent.spawn(peer.updatePiecefields)\n\n                return False  \n        return super(WorkerManagerPlugin, self).taskAddPeer(task, peer)\n\n\n@PluginManager.registerTo(\"FileRequest\")\nclass FileRequestPlugin(object):\n    def isReadable(self, site, inner_path, file, pos):\n        \n        if file.read(10) == \"\\0\" * 10:\n            \n            file_info = site.content_manager.getFileInfo(inner_path)\n            piece_i = pos / file_info[\"piece_size\"]\n            if not site.storage.piecefields[file_info[\"sha512\"]][piece_i]:\n                return False\n        \n        file.seek(pos)\n        return super(FileRequestPlugin, self).isReadable(site, inner_path, file, pos)\n\n    def actionGetPiecefields(self, params):\n        site = self.sites.get(params[\"site\"])\n        if not site or not site.settings[\"serving\"]:  \n            self.response({\"error\": \"Unknown site\"})\n            return False\n\n        \n        peer = site.addPeer(self.connection.ip, self.connection.port, return_peer=True)\n        if not peer.connection:  \n            peer.connect(self.connection)  \n\n        piecefields_packed = {sha512: piecefield.pack() for sha512, piecefield in site.storage.piecefields.iteritems()}\n        self.response({\"piecefields_packed\": piecefields_packed})\n\n    def actionSetPiecefields(self, params):\n        site = self.sites.get(params[\"site\"])\n        if not site or not site.settings[\"serving\"]:  \n            self.response({\"error\": \"Unknown site\"})\n            self.connection.badAction(5)\n            return False\n\n        \n        peer = site.addPeer(self.connection.ip, self.connection.port, return_peer=True, connection=self.connection)\n        if not peer.connection:\n            peer.connect(self.connection)\n\n        peer.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n        for sha512, piecefield_packed in params[\"piecefields_packed\"].iteritems():\n            peer.piecefields[sha512].unpack(piecefield_packed)\n        site.settings[\"has_bigfile\"] = True\n\n        self.response({\"ok\": \"Updated\"})\n\n\n@PluginManager.registerTo(\"Peer\")\nclass PeerPlugin(object):\n    def __getattr__(self, key):\n        if key == \"piecefields\":\n            self.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n            return self.piecefields\n        elif key == \"time_piecefields_updated\":\n            self.time_piecefields_updated = None\n            return self.time_piecefields_updated\n        else:\n            return super(PeerPlugin, self).__getattr__(key)\n\n    @util.Noparallel(ignore_args=True)\n    def updatePiecefields(self, force=False):\n        if self.connection and self.connection.handshake.get(\"rev\", 0) < 2190:\n            return False  \n\n        \n        if self.time_piecefields_updated and time.time() - self.time_piecefields_updated < 60 and not force:\n            return False\n\n        self.time_piecefields_updated = time.time()\n        res = self.request(\"getPiecefields\", {\"site\": self.site.address})\n        if not res or \"error\" in res:\n            return False\n\n        self.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n        try:\n            for sha512, piecefield_packed in res[\"piecefields_packed\"].iteritems():\n                self.piecefields[sha512].unpack(piecefield_packed)\n        except Exception as err:\n            self.log(\"Invalid updatePiecefields response: %s\" % Debug.formatException(err))\n\n        return self.piecefields\n\n    def sendMyHashfield(self, *args, **kwargs):\n        return super(PeerPlugin, self).sendMyHashfield(*args, **kwargs)\n\n    def updateHashfield(self, *args, **kwargs):\n        if self.site.settings.get(\"has_bigfile\"):\n            thread = gevent.spawn(self.updatePiecefields, *args, **kwargs)\n            back = super(PeerPlugin, self).updateHashfield(*args, **kwargs)\n            thread.join()\n            return back\n        else:\n            return super(PeerPlugin, self).updateHashfield(*args, **kwargs)\n\n    def getFile(self, site, inner_path, *args, **kwargs):\n        if \"|\" in inner_path:\n            inner_path, file_range = inner_path.split(\"|\")\n            pos_from, pos_to = map(int, file_range.split(\"-\"))\n            kwargs[\"pos_from\"] = pos_from\n            kwargs[\"pos_to\"] = pos_to\n        return super(PeerPlugin, self).getFile(site, inner_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"Site\")\nclass SitePlugin(object):\n    def isFileDownloadAllowed(self, inner_path, file_info):\n        if \"piecemap\" in file_info:\n            file_info = file_info.copy()\n            file_info[\"size\"] = file_info[\"piece_size\"]\n        return super(SitePlugin, self).isFileDownloadAllowed(inner_path, file_info)\n\n    def getSettingsCache(self):\n        back = super(SitePlugin, self).getSettingsCache()\n        if self.storage.piecefields:\n            back[\"piecefields\"] = {sha512: piecefield.pack().encode(\"base64\") for sha512, piecefield in self.storage.piecefields.iteritems()}\n        return back\n\n    def needFile(self, inner_path, *args, **kwargs):\n        if inner_path.endswith(\"|all\"):\n            @util.Pooled(20)\n            def pooledNeedBigfile(*args, **kwargs):\n                return self.needFile(*args, **kwargs)\n\n            inner_path = inner_path.replace(\"|all\", \"\")\n            file_info = self.needFileInfo(inner_path)\n            file_size = file_info[\"size\"]\n            piece_size = file_info[\"piece_size\"]\n\n            piece_num = int(math.ceil(float(file_size) / piece_size))\n\n            file_threads = []\n\n            piecefield = self.storage.piecefields.get(file_info[\"sha512\"])\n\n            for piece_i in range(piece_num):\n                piece_from = piece_i * piece_size\n                piece_to = min(file_size, piece_from + piece_size)\n                if not piecefield or not piecefield[piece_i]:\n                    res = pooledNeedBigfile(\"%s|%s-%s\" % (inner_path, piece_from, piece_to), blocking=False)\n                    if res is not True and res is not False:\n                        file_threads.append(res)\n            gevent.joinall(file_threads)\n        else:\n            return super(SitePlugin, self).needFile(inner_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"ConfigPlugin\")\nclass ConfigPlugin(object):\n    def createArguments(self):\n        group = self.parser.add_argument_group(\"Bigfile plugin\")\n        group.add_argument('--autodownload_bigfile_size_limit', help='Also download bigfiles smaller than this limit if help distribute option is checked', default=1, metavar=\"MB\", type=int)\n\n        return super(ConfigPlugin, self).createArguments()\n", "comments": "  we import plugin host clases plugins loaded    skip http headers    small file  split    big file    find piecemap file relative path content json    add file content json    reload cache    only check files larger 1mb    avoid blocking zeronet execution upload    only care optional files  1mb    don hash already content json    not content json yet    don create piecemap automatically files smaller 5mb    add merkle root hashfield    mark piece downloaded    only add site size first request    we already file  piecefield    also remove pieces file download queue    write specific position passing   pos  filename    create dir exist    write file    file like object    write buff disk    simple string    it big file    download piecemap    request required blocks    request prebuffer    increase buffer long reads    bigfile    start download piecemap    download pieces    start download piece    don download anything range specified    deny add peers task file piecefield    peek file    looks empty  makes sures piece    seek back position want read    site unknown serving    add peer site added    just added    assign current connection peer    site unknown serving    add get peer    not supported    don update piecefield 1 min ", "content": "import time\nimport os\nimport subprocess\nimport shutil\nimport collections\nimport gevent\nimport math\n\nimport msgpack\n\nfrom Plugin import PluginManager\nfrom Debug import Debug\nfrom Crypt import CryptHash\nfrom lib import merkletools\nfrom util import helper\nimport util\nfrom BigfilePiecefield import BigfilePiecefield, BigfilePiecefieldPacked\n\n\n# We can only import plugin host clases after the plugins are loaded\n@PluginManager.afterLoad\ndef importPluginnedClasses():\n    global VerifyError, config\n    from Content.ContentManager import VerifyError\n    from Config import config\n\nif \"upload_nonces\" not in locals():\n    upload_nonces = {}\n\n\n@PluginManager.registerTo(\"UiRequest\")\nclass UiRequestPlugin(object):\n    def isCorsAllowed(self, path):\n        if path == \"/ZeroNet-Internal/BigfileUpload\":\n            return True\n        else:\n            return super(UiRequestPlugin, self).isCorsAllowed(path)\n\n    def actionBigfileUpload(self):\n        nonce = self.get.get(\"upload_nonce\")\n        if nonce not in upload_nonces:\n            return self.error403(\"Upload nonce error.\")\n\n        upload_info = upload_nonces[nonce]\n        del upload_nonces[nonce]\n\n        self.sendHeader(200, \"text/html\", noscript=True, extra_headers={\n            \"Access-Control-Allow-Origin\": \"null\",\n            \"Access-Control-Allow-Credentials\": \"true\"\n        })\n\n        self.readMultipartHeaders(self.env['wsgi.input'])  # Skip http headers\n\n        site = upload_info[\"site\"]\n        inner_path = upload_info[\"inner_path\"]\n\n        with site.storage.open(inner_path, \"wb\", create_dirs=True) as out_file:\n            merkle_root, piece_size, piecemap_info = site.content_manager.hashBigfile(\n                self.env['wsgi.input'], upload_info[\"size\"], upload_info[\"piece_size\"], out_file\n            )\n\n        if len(piecemap_info[\"sha512_pieces\"]) == 1:  # Small file, don't split\n            hash = piecemap_info[\"sha512_pieces\"][0].encode(\"hex\")\n            site.content_manager.optionalDownloaded(inner_path, hash, upload_info[\"size\"], own=True)\n\n        else:  # Big file\n            file_name = helper.getFilename(inner_path)\n            msgpack.pack({file_name: piecemap_info}, site.storage.open(upload_info[\"piecemap\"], \"wb\"))\n\n            # Find piecemap and file relative path to content.json\n            file_info = site.content_manager.getFileInfo(inner_path, new_file=True)\n            content_inner_path_dir = helper.getDirname(file_info[\"content_inner_path\"])\n            piecemap_relative_path = upload_info[\"piecemap\"][len(content_inner_path_dir):]\n            file_relative_path = inner_path[len(content_inner_path_dir):]\n\n            # Add file to content.json\n            if site.storage.isFile(file_info[\"content_inner_path\"]):\n                content = site.storage.loadJson(file_info[\"content_inner_path\"])\n            else:\n                content = {}\n            if \"files_optional\" not in content:\n                content[\"files_optional\"] = {}\n\n            content[\"files_optional\"][file_relative_path] = {\n                \"sha512\": merkle_root,\n                \"size\": upload_info[\"size\"],\n                \"piecemap\": piecemap_relative_path,\n                \"piece_size\": piece_size\n            }\n\n            site.content_manager.optionalDownloaded(inner_path, merkle_root, upload_info[\"size\"], own=True)\n            site.storage.writeJson(file_info[\"content_inner_path\"], content)\n\n            site.content_manager.contents.loadItem(file_info[\"content_inner_path\"])  # reload cache\n\n        return {\n            \"merkle_root\": merkle_root,\n            \"piece_num\": len(piecemap_info[\"sha512_pieces\"]),\n            \"piece_size\": piece_size,\n            \"inner_path\": inner_path\n        }\n\n    def readMultipartHeaders(self, wsgi_input):\n        for i in range(100):\n            line = wsgi_input.readline()\n            if line == \"\\r\\n\":\n                break\n        return i\n\n    def actionFile(self, file_path, *args, **kwargs):\n        if kwargs.get(\"file_size\", 0) > 1024 * 1024 and kwargs.get(\"path_parts\"):  # Only check files larger than 1MB\n            path_parts = kwargs[\"path_parts\"]\n            site = self.server.site_manager.get(path_parts[\"address\"])\n            kwargs[\"file_obj\"] = site.storage.openBigfile(path_parts[\"inner_path\"], prebuffer=2 * 1024 * 1024)\n\n        return super(UiRequestPlugin, self).actionFile(file_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"UiWebsocket\")\nclass UiWebsocketPlugin(object):\n    def actionBigfileUploadInit(self, to, inner_path, size):\n        valid_signers = self.site.content_manager.getValidSigners(inner_path)\n        auth_address = self.user.getAuthAddress(self.site.address)\n        if not self.site.settings[\"own\"] and auth_address not in valid_signers:\n            self.log.error(\"FileWrite forbidden %s not in valid_signers %s\" % (auth_address, valid_signers))\n            return self.response(to, {\"error\": \"Forbidden, you can only modify your own files\"})\n\n        nonce = CryptHash.random()\n        piece_size = 1024 * 1024\n        inner_path = self.site.content_manager.sanitizePath(inner_path)\n        file_info = self.site.content_manager.getFileInfo(inner_path, new_file=True)\n\n        content_inner_path_dir = helper.getDirname(file_info[\"content_inner_path\"])\n        file_relative_path = inner_path[len(content_inner_path_dir):]\n\n        upload_nonces[nonce] = {\n            \"added\": time.time(),\n            \"site\": self.site,\n            \"inner_path\": inner_path,\n            \"websocket_client\": self,\n            \"size\": size,\n            \"piece_size\": piece_size,\n            \"piecemap\": inner_path + \".piecemap.msgpack\"\n        }\n        return {\n            \"url\": \"/ZeroNet-Internal/BigfileUpload?upload_nonce=\" + nonce,\n            \"piece_size\": piece_size,\n            \"inner_path\": inner_path,\n            \"file_relative_path\": file_relative_path\n        }\n\n    def actionSiteSetAutodownloadBigfileLimit(self, to, limit):\n        permissions = self.getPermissions(to)\n        if \"ADMIN\" not in permissions:\n            return self.response(to, \"You don't have permission to run this command\")\n\n        self.site.settings[\"autodownload_bigfile_size_limit\"] = int(limit)\n        self.response(to, \"ok\")\n\n\n@PluginManager.registerTo(\"ContentManager\")\nclass ContentManagerPlugin(object):\n    def getFileInfo(self, inner_path, *args, **kwargs):\n        if \"|\" not in inner_path:\n            return super(ContentManagerPlugin, self).getFileInfo(inner_path, *args, **kwargs)\n\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n        file_info = super(ContentManagerPlugin, self).getFileInfo(inner_path, *args, **kwargs)\n        return file_info\n\n    def readFile(self, file_in, size, buff_size=1024 * 64):\n        part_num = 0\n        recv_left = size\n\n        while 1:\n            part_num += 1\n            read_size = min(buff_size, recv_left)\n            part = file_in.read(read_size)\n\n            if not part:\n                break\n            yield part\n\n            if part_num % 100 == 0:  # Avoid blocking ZeroNet execution during upload\n                time.sleep(0.001)\n\n            recv_left -= read_size\n            if recv_left <= 0:\n                break\n\n    def hashBigfile(self, file_in, size, piece_size=1024 * 1024, file_out=None):\n        self.site.settings[\"has_bigfile\"] = True\n\n        recv = 0\n        try:\n            piece_hash = CryptHash.sha512t()\n            piece_hashes = []\n            piece_recv = 0\n\n            mt = merkletools.MerkleTools()\n            mt.hash_function = CryptHash.sha512t\n\n            part = \"\"\n            for part in self.readFile(file_in, size):\n                if file_out:\n                    file_out.write(part)\n\n                recv += len(part)\n                piece_recv += len(part)\n                piece_hash.update(part)\n                if piece_recv >= piece_size:\n                    piece_digest = piece_hash.digest()\n                    piece_hashes.append(piece_digest)\n                    mt.leaves.append(piece_digest)\n                    piece_hash = CryptHash.sha512t()\n                    piece_recv = 0\n\n                    if len(piece_hashes) % 100 == 0 or recv == size:\n                        self.log.info(\"- [HASHING:%.0f%%] Pieces: %s, %.1fMB/%.1fMB\" % (\n                            float(recv) / size * 100, len(piece_hashes), recv / 1024 / 1024, size / 1024 / 1024\n                        ))\n                        part = \"\"\n            if len(part) > 0:\n                piece_digest = piece_hash.digest()\n                piece_hashes.append(piece_digest)\n                mt.leaves.append(piece_digest)\n        except Exception as err:\n            raise err\n        finally:\n            if file_out:\n                file_out.close()\n\n        mt.make_tree()\n        return mt.get_merkle_root(), piece_size, {\n            \"sha512_pieces\": piece_hashes\n        }\n\n    def hashFile(self, dir_inner_path, file_relative_path, optional=False):\n        inner_path = dir_inner_path + file_relative_path\n\n        file_size = self.site.storage.getSize(inner_path)\n        # Only care about optional files >1MB\n        if not optional or file_size < 1 * 1024 * 1024:\n            return super(ContentManagerPlugin, self).hashFile(dir_inner_path, file_relative_path, optional)\n\n        back = {}\n        content = self.contents.get(dir_inner_path + \"content.json\")\n\n        hash = None\n        piecemap_relative_path = None\n        piece_size = None\n\n        # Don't re-hash if it's already in content.json\n        if content and file_relative_path in content.get(\"files_optional\", {}):\n            file_node = content[\"files_optional\"][file_relative_path]\n            if file_node[\"size\"] == file_size:\n                self.log.info(\"- [SAME SIZE] %s\" % file_relative_path)\n                hash = file_node.get(\"sha512\")\n                piecemap_relative_path = file_node.get(\"piecemap\")\n                piece_size = file_node.get(\"piece_size\")\n\n        if not hash or not piecemap_relative_path:  # Not in content.json yet\n            if file_size < 5 * 1024 * 1024:  # Don't create piecemap automatically for files smaller than 5MB\n                return super(ContentManagerPlugin, self).hashFile(dir_inner_path, file_relative_path, optional)\n\n            self.log.info(\"- [HASHING] %s\" % file_relative_path)\n            merkle_root, piece_size, piecemap_info = self.hashBigfile(self.site.storage.open(inner_path, \"rb\"), file_size)\n            if not hash:\n                hash = merkle_root\n\n            if not piecemap_relative_path:\n                file_name = helper.getFilename(file_relative_path)\n                piecemap_relative_path = file_relative_path + \".piecemap.msgpack\"\n                piecemap_inner_path = inner_path + \".piecemap.msgpack\"\n\n                msgpack.pack({file_name: piecemap_info}, self.site.storage.open(piecemap_inner_path, \"wb\"))\n\n                back.update(super(ContentManagerPlugin, self).hashFile(dir_inner_path, piecemap_relative_path, optional=True))\n\n        piece_num = int(math.ceil(float(file_size) / piece_size))\n\n        # Add the merkle root to hashfield\n        self.optionalDownloaded(inner_path, hash, file_size, own=True)\n        self.site.storage.piecefields[hash].fromstring(\"1\" * piece_num)\n\n        back[file_relative_path] = {\"sha512\": hash, \"size\": file_size, \"piecemap\": piecemap_relative_path, \"piece_size\": piece_size}\n        return back\n\n    def getPiecemap(self, inner_path):\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        piecemap_inner_path = helper.getDirname(file_info[\"content_inner_path\"]) + file_info[\"piecemap\"]\n        self.site.needFile(piecemap_inner_path, priority=20)\n        piecemap = msgpack.unpack(self.site.storage.open(piecemap_inner_path))[helper.getFilename(inner_path)]\n        piecemap[\"piece_size\"] = file_info[\"piece_size\"]\n        return piecemap\n\n    def verifyPiece(self, inner_path, pos, piece):\n        piecemap = self.getPiecemap(inner_path)\n        piece_i = pos / piecemap[\"piece_size\"]\n        if CryptHash.sha512sum(piece, format=\"digest\") != piecemap[\"sha512_pieces\"][piece_i]:\n            raise VerifyError(\"Invalid hash\")\n        return True\n\n    def verifyFile(self, inner_path, file, ignore_same=True):\n        if \"|\" not in inner_path:\n            return super(ContentManagerPlugin, self).verifyFile(inner_path, file, ignore_same)\n\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n\n        return self.verifyPiece(inner_path, pos_from, file)\n\n    def optionalDownloaded(self, inner_path, hash, size=None, own=False):\n        if \"|\" in inner_path:\n            inner_path, file_range = inner_path.split(\"|\")\n            pos_from, pos_to = map(int, file_range.split(\"-\"))\n            file_info = self.getFileInfo(inner_path)\n\n            # Mark piece downloaded\n            piece_i = pos_from / file_info[\"piece_size\"]\n            self.site.storage.piecefields[file_info[\"sha512\"]][piece_i] = True\n\n            # Only add to site size on first request\n            if hash in self.hashfield:\n                size = 0\n        elif size > 1024 * 1024:\n            file_info = self.getFileInfo(inner_path)\n            if file_info:  # We already have the file, but not in piecefield\n                sha512 = file_info[\"sha512\"]\n                if sha512 not in self.site.storage.piecefields:\n                    self.site.storage.checkBigfile(inner_path)\n\n        return super(ContentManagerPlugin, self).optionalDownloaded(inner_path, hash, size, own)\n\n    def optionalRemove(self, inner_path, hash, size=None):\n        if size and size > 1024 * 1024:\n            file_info = self.getFileInfo(inner_path)\n            sha512 = file_info[\"sha512\"]\n            if sha512 in self.site.storage.piecefields:\n                del self.site.storage.piecefields[sha512]\n\n            # Also remove other pieces of the file from download queue\n            for key in self.site.bad_files.keys():\n                if key.startswith(inner_path + \"|\"):\n                    del self.site.bad_files[key]\n        return super(ContentManagerPlugin, self).optionalRemove(inner_path, hash, size)\n\n\n@PluginManager.registerTo(\"SiteStorage\")\nclass SiteStoragePlugin(object):\n    def __init__(self, *args, **kwargs):\n        super(SiteStoragePlugin, self).__init__(*args, **kwargs)\n        self.piecefields = collections.defaultdict(BigfilePiecefield)\n        if \"piecefields\" in self.site.settings.get(\"cache\", {}):\n            for sha512, piecefield_packed in self.site.settings[\"cache\"].get(\"piecefields\").iteritems():\n                if piecefield_packed:\n                    self.piecefields[sha512].unpack(piecefield_packed.decode(\"base64\"))\n            self.site.settings[\"cache\"][\"piecefields\"] = {}\n\n    def createSparseFile(self, inner_path, size, sha512=None):\n        file_path = self.getPath(inner_path)\n\n        file_dir = os.path.dirname(file_path)\n        if not os.path.isdir(file_dir):\n            os.makedirs(file_dir)\n\n        f = open(file_path, 'wb')\n        f.truncate(size)\n        f.close()\n        if os.name == \"nt\":\n            startupinfo = subprocess.STARTUPINFO()\n            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n            subprocess.call([\"fsutil\", \"sparse\", \"setflag\", file_path], close_fds=True, startupinfo=startupinfo)\n\n        if sha512 and sha512 in self.piecefields:\n            self.log.debug(\"%s: File not exists, but has piecefield. Deleting piecefield.\" % inner_path)\n            del self.piecefields[sha512]\n\n    def write(self, inner_path, content):\n        if \"|\" not in inner_path:\n            return super(SiteStoragePlugin, self).write(inner_path, content)\n\n        # Write to specific position by passing |{pos} after the filename\n        inner_path, file_range = inner_path.split(\"|\")\n        pos_from, pos_to = map(int, file_range.split(\"-\"))\n        file_path = self.getPath(inner_path)\n\n        # Create dir if not exist\n        file_dir = os.path.dirname(file_path)\n        if not os.path.isdir(file_dir):\n            os.makedirs(file_dir)\n\n        if not os.path.isfile(file_path):\n            file_info = self.site.content_manager.getFileInfo(inner_path)\n            self.createSparseFile(inner_path, file_info[\"size\"])\n\n        # Write file\n        with open(file_path, \"rb+\") as file:\n            file.seek(pos_from)\n            if hasattr(content, 'read'):  # File-like object\n                shutil.copyfileobj(content, file)  # Write buff to disk\n            else:  # Simple string\n                file.write(content)\n        del content\n        self.onUpdated(inner_path)\n\n    def checkBigfile(self, inner_path):\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        if not file_info or (file_info and \"piecemap\" not in file_info):  # It's not a big file\n            return False\n        file_path = self.getPath(inner_path)\n        sha512 = file_info[\"sha512\"]\n        piece_num = int(math.ceil(float(file_info[\"size\"]) / file_info[\"piece_size\"]))\n        if os.path.isfile(file_path):\n            if sha512 not in self.piecefields:\n                if open(file_path).read(128) == \"\\0\" * 128:\n                    piece_data = \"0\"\n                else:\n                    piece_data = \"1\"\n                self.log.debug(\"%s: File exists, but not in piecefield. Filling piecefiled with %s * %s.\" % (inner_path, piece_num, piece_data))\n                self.piecefields[sha512].fromstring(piece_data * piece_num)\n        else:\n            self.log.debug(\"Creating bigfile: %s\" % inner_path)\n            self.createSparseFile(inner_path, file_info[\"size\"], sha512)\n            self.piecefields[sha512].fromstring(\"0\" * piece_num)\n        return True\n\n    def openBigfile(self, inner_path, prebuffer=0):\n        if not self.checkBigfile(inner_path):\n            return False\n        self.site.needFile(inner_path, blocking=False)  # Download piecemap\n        return BigFile(self.site, inner_path, prebuffer=prebuffer)\n\n\nclass BigFile(object):\n    def __init__(self, site, inner_path, prebuffer=0):\n        self.site = site\n        self.inner_path = inner_path\n        file_path = site.storage.getPath(inner_path)\n        file_info = self.site.content_manager.getFileInfo(inner_path)\n        self.piece_size = file_info[\"piece_size\"]\n        self.sha512 = file_info[\"sha512\"]\n        self.size = file_info[\"size\"]\n        self.prebuffer = prebuffer\n        self.read_bytes = 0\n\n        self.piecefield = self.site.storage.piecefields[self.sha512]\n        self.f = open(file_path, \"rb+\")\n\n    def read(self, buff=64 * 1024):\n        pos = self.f.tell()\n        read_until = min(self.size, pos + buff)\n        requests = []\n        # Request all required blocks\n        while 1:\n            piece_i = pos / self.piece_size\n            if piece_i * self.piece_size >= read_until:\n                break\n            pos_from = piece_i * self.piece_size\n            pos_to = pos_from + self.piece_size\n            if not self.piecefield[piece_i]:\n                requests.append(self.site.needFile(\"%s|%s-%s\" % (self.inner_path, pos_from, pos_to), blocking=False, update=True, priority=10))\n            pos += self.piece_size\n\n        if not all(requests):\n            return None\n\n        # Request prebuffer\n        if self.prebuffer:\n            prebuffer_until = min(self.size, read_until + self.prebuffer)\n            priority = 3\n            while 1:\n                piece_i = pos / self.piece_size\n                if piece_i * self.piece_size >= prebuffer_until:\n                    break\n                pos_from = piece_i * self.piece_size\n                pos_to = pos_from + self.piece_size\n                if not self.piecefield[piece_i]:\n                    self.site.needFile(\"%s|%s-%s\" % (self.inner_path, pos_from, pos_to), blocking=False, update=True, priority=max(0, priority))\n                priority -= 1\n                pos += self.piece_size\n\n        gevent.joinall(requests)\n        self.read_bytes += buff\n\n        # Increase buffer for long reads\n        if self.read_bytes > 7 * 1024 * 1024 and self.prebuffer < 5 * 1024 * 1024:\n            self.site.log.debug(\"%s: Increasing bigfile buffer size to 5MB...\" % self.inner_path)\n            self.prebuffer = 5 * 1024 * 1024\n\n        return self.f.read(buff)\n\n    def seek(self, pos):\n        return self.f.seek(pos)\n\n    def tell(self):\n        self.f.tell()\n\n    def close(self):\n        self.f.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\n@PluginManager.registerTo(\"WorkerManager\")\nclass WorkerManagerPlugin(object):\n    def addTask(self, inner_path, *args, **kwargs):\n        file_info = kwargs.get(\"file_info\")\n        if file_info and \"piecemap\" in file_info:  # Bigfile\n            self.site.settings[\"has_bigfile\"] = True\n\n            piecemap_inner_path = helper.getDirname(file_info[\"content_inner_path\"]) + file_info[\"piecemap\"]\n            piecemap_task = None\n            if not self.site.storage.isFile(piecemap_inner_path):\n                # Start download piecemap\n                piecemap_task = super(WorkerManagerPlugin, self).addTask(piecemap_inner_path, priority=30)\n                autodownload_bigfile_size_limit = self.site.settings.get(\"autodownload_bigfile_size_limit\", config.autodownload_bigfile_size_limit)\n                if \"|\" not in inner_path and self.site.isDownloadable(inner_path) and file_info[\"size\"] / 1024 / 1024 <= autodownload_bigfile_size_limit:\n                    gevent.spawn_later(0.1, self.site.needFile, inner_path + \"|all\")  # Download all pieces\n\n            if \"|\" in inner_path:\n                # Start download piece\n                task = super(WorkerManagerPlugin, self).addTask(inner_path, *args, **kwargs)\n\n                inner_path, file_range = inner_path.split(\"|\")\n                pos_from, pos_to = map(int, file_range.split(\"-\"))\n                task[\"piece_i\"] = pos_from / file_info[\"piece_size\"]\n                task[\"sha512\"] = file_info[\"sha512\"]\n            else:\n                if inner_path in self.site.bad_files:\n                    del self.site.bad_files[inner_path]\n                if piecemap_task:\n                    task = piecemap_task\n                else:\n                    fake_evt = gevent.event.AsyncResult()  # Don't download anything if no range specified\n                    fake_evt.set(True)\n                    task = {\"evt\": fake_evt}\n\n            if not self.site.storage.isFile(inner_path):\n                self.site.storage.createSparseFile(inner_path, file_info[\"size\"], file_info[\"sha512\"])\n                piece_num = int(math.ceil(float(file_info[\"size\"]) / file_info[\"piece_size\"]))\n                self.site.storage.piecefields[file_info[\"sha512\"]].fromstring(\"0\" * piece_num)\n        else:\n            task = super(WorkerManagerPlugin, self).addTask(inner_path, *args, **kwargs)\n        return task\n\n    def taskAddPeer(self, task, peer):\n        if \"piece_i\" in task:\n            if not peer.piecefields[task[\"sha512\"]][task[\"piece_i\"]]:\n                if task[\"sha512\"] not in peer.piecefields:\n                    gevent.spawn(peer.updatePiecefields, force=True)\n                elif not task[\"peers\"]:\n                    gevent.spawn(peer.updatePiecefields)\n\n                return False  # Deny to add peers to task if file not in piecefield\n        return super(WorkerManagerPlugin, self).taskAddPeer(task, peer)\n\n\n@PluginManager.registerTo(\"FileRequest\")\nclass FileRequestPlugin(object):\n    def isReadable(self, site, inner_path, file, pos):\n        # Peek into file\n        if file.read(10) == \"\\0\" * 10:\n            # Looks empty, but makes sures we don't have that piece\n            file_info = site.content_manager.getFileInfo(inner_path)\n            piece_i = pos / file_info[\"piece_size\"]\n            if not site.storage.piecefields[file_info[\"sha512\"]][piece_i]:\n                return False\n        # Seek back to position we want to read\n        file.seek(pos)\n        return super(FileRequestPlugin, self).isReadable(site, inner_path, file, pos)\n\n    def actionGetPiecefields(self, params):\n        site = self.sites.get(params[\"site\"])\n        if not site or not site.settings[\"serving\"]:  # Site unknown or not serving\n            self.response({\"error\": \"Unknown site\"})\n            return False\n\n        # Add peer to site if not added before\n        peer = site.addPeer(self.connection.ip, self.connection.port, return_peer=True)\n        if not peer.connection:  # Just added\n            peer.connect(self.connection)  # Assign current connection to peer\n\n        piecefields_packed = {sha512: piecefield.pack() for sha512, piecefield in site.storage.piecefields.iteritems()}\n        self.response({\"piecefields_packed\": piecefields_packed})\n\n    def actionSetPiecefields(self, params):\n        site = self.sites.get(params[\"site\"])\n        if not site or not site.settings[\"serving\"]:  # Site unknown or not serving\n            self.response({\"error\": \"Unknown site\"})\n            self.connection.badAction(5)\n            return False\n\n        # Add or get peer\n        peer = site.addPeer(self.connection.ip, self.connection.port, return_peer=True, connection=self.connection)\n        if not peer.connection:\n            peer.connect(self.connection)\n\n        peer.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n        for sha512, piecefield_packed in params[\"piecefields_packed\"].iteritems():\n            peer.piecefields[sha512].unpack(piecefield_packed)\n        site.settings[\"has_bigfile\"] = True\n\n        self.response({\"ok\": \"Updated\"})\n\n\n@PluginManager.registerTo(\"Peer\")\nclass PeerPlugin(object):\n    def __getattr__(self, key):\n        if key == \"piecefields\":\n            self.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n            return self.piecefields\n        elif key == \"time_piecefields_updated\":\n            self.time_piecefields_updated = None\n            return self.time_piecefields_updated\n        else:\n            return super(PeerPlugin, self).__getattr__(key)\n\n    @util.Noparallel(ignore_args=True)\n    def updatePiecefields(self, force=False):\n        if self.connection and self.connection.handshake.get(\"rev\", 0) < 2190:\n            return False  # Not supported\n\n        # Don't update piecefield again in 1 min\n        if self.time_piecefields_updated and time.time() - self.time_piecefields_updated < 60 and not force:\n            return False\n\n        self.time_piecefields_updated = time.time()\n        res = self.request(\"getPiecefields\", {\"site\": self.site.address})\n        if not res or \"error\" in res:\n            return False\n\n        self.piecefields = collections.defaultdict(BigfilePiecefieldPacked)\n        try:\n            for sha512, piecefield_packed in res[\"piecefields_packed\"].iteritems():\n                self.piecefields[sha512].unpack(piecefield_packed)\n        except Exception as err:\n            self.log(\"Invalid updatePiecefields response: %s\" % Debug.formatException(err))\n\n        return self.piecefields\n\n    def sendMyHashfield(self, *args, **kwargs):\n        return super(PeerPlugin, self).sendMyHashfield(*args, **kwargs)\n\n    def updateHashfield(self, *args, **kwargs):\n        if self.site.settings.get(\"has_bigfile\"):\n            thread = gevent.spawn(self.updatePiecefields, *args, **kwargs)\n            back = super(PeerPlugin, self).updateHashfield(*args, **kwargs)\n            thread.join()\n            return back\n        else:\n            return super(PeerPlugin, self).updateHashfield(*args, **kwargs)\n\n    def getFile(self, site, inner_path, *args, **kwargs):\n        if \"|\" in inner_path:\n            inner_path, file_range = inner_path.split(\"|\")\n            pos_from, pos_to = map(int, file_range.split(\"-\"))\n            kwargs[\"pos_from\"] = pos_from\n            kwargs[\"pos_to\"] = pos_to\n        return super(PeerPlugin, self).getFile(site, inner_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"Site\")\nclass SitePlugin(object):\n    def isFileDownloadAllowed(self, inner_path, file_info):\n        if \"piecemap\" in file_info:\n            file_info = file_info.copy()\n            file_info[\"size\"] = file_info[\"piece_size\"]\n        return super(SitePlugin, self).isFileDownloadAllowed(inner_path, file_info)\n\n    def getSettingsCache(self):\n        back = super(SitePlugin, self).getSettingsCache()\n        if self.storage.piecefields:\n            back[\"piecefields\"] = {sha512: piecefield.pack().encode(\"base64\") for sha512, piecefield in self.storage.piecefields.iteritems()}\n        return back\n\n    def needFile(self, inner_path, *args, **kwargs):\n        if inner_path.endswith(\"|all\"):\n            @util.Pooled(20)\n            def pooledNeedBigfile(*args, **kwargs):\n                return self.needFile(*args, **kwargs)\n\n            inner_path = inner_path.replace(\"|all\", \"\")\n            file_info = self.needFileInfo(inner_path)\n            file_size = file_info[\"size\"]\n            piece_size = file_info[\"piece_size\"]\n\n            piece_num = int(math.ceil(float(file_size) / piece_size))\n\n            file_threads = []\n\n            piecefield = self.storage.piecefields.get(file_info[\"sha512\"])\n\n            for piece_i in range(piece_num):\n                piece_from = piece_i * piece_size\n                piece_to = min(file_size, piece_from + piece_size)\n                if not piecefield or not piecefield[piece_i]:\n                    res = pooledNeedBigfile(\"%s|%s-%s\" % (inner_path, piece_from, piece_to), blocking=False)\n                    if res is not True and res is not False:\n                        file_threads.append(res)\n            gevent.joinall(file_threads)\n        else:\n            return super(SitePlugin, self).needFile(inner_path, *args, **kwargs)\n\n\n@PluginManager.registerTo(\"ConfigPlugin\")\nclass ConfigPlugin(object):\n    def createArguments(self):\n        group = self.parser.add_argument_group(\"Bigfile plugin\")\n        group.add_argument('--autodownload_bigfile_size_limit', help='Also download bigfiles smaller than this limit if help distribute option is checked', default=1, metavar=\"MB\", type=int)\n\n        return super(ConfigPlugin, self).createArguments()\n", "description": "ZeroNet - Decentralized websites using Bitcoin crypto and BitTorrent network", "file_name": "BigfilePlugin.py", "id": "0e6c13746f7b0176795ab1d96169a684", "language": "Python", "project_name": "ZeroNet", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/HelloZeroNet-ZeroNet/HelloZeroNet-ZeroNet-8828629/plugins/Bigfile/BigfilePlugin.py", "save_time": "", "source": "", "update_at": "2018-03-18T12:17:52Z", "url": "https://github.com/HelloZeroNet/ZeroNet", "wiki": true}