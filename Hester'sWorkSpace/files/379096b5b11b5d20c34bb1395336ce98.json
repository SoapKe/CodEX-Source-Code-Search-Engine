{"author": "deepfakes", "code": "import atexit\nimport numpy as np\nimport os\nimport cv2\nimport dlib\nimport keras\nfrom keras import backend as K\n\ndlib_detectors = []\nkeras_model = None\nis_initialized = False\n\n@atexit.register\ndef onExit():\n    global dlib_detectors\n    global keras_model\n    \n    if keras_model is not None:\n        del keras_model\n        K.clear_session()\n        \n    for detector in dlib_detectors:\n        del detector\n        \nclass TorchBatchNorm2D(keras.engine.topology.Layer):\n    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):\n        super(TorchBatchNorm2D, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.momentum = momentum\n        self.epsilon = epsilon\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of ' 'input tensor should have a defined dimension ' 'but the layer received an input with shape ' + str(input_shape) + '.')\n        shape = (dim,)\n        self.gamma = self.add_weight(shape=shape, name='gamma', initializer='ones', regularizer=None, constraint=None)\n        self.beta = self.add_weight(shape=shape, name='beta', initializer='zeros', regularizer=None, constraint=None)\n        self.moving_mean = self.add_weight(shape=shape, name='moving_mean', initializer='zeros', trainable=False)            \n        self.moving_variance = self.add_weight(shape=shape, name='moving_variance', initializer='ones', trainable=False)            \n        self.built = True\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n        \n        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)\n        broadcast_moving_variance = K.reshape(self.moving_variance, broadcast_shape)\n        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n        broadcast_beta = K.reshape(self.beta, broadcast_shape)        \n        invstd = K.ones (shape=broadcast_shape, dtype='float32') / K.sqrt(broadcast_moving_variance + K.constant(self.epsilon, dtype='float32'))\n        \n        return (inputs - broadcast_moving_mean) * invstd * broadcast_gamma + broadcast_beta\n       \n    def get_config(self):\n        config = { 'axis': self.axis, 'momentum': self.momentum, 'epsilon': self.epsilon }\n        base_config = super(TorchBatchNorm2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\ndef transform(point, center, scale, resolution):\n    pt = np.array ( [point[0], point[1], 1.0] )            \n    h = 200.0 * scale\n    m = np.eye(3)\n    m[0,0] = resolution / h\n    m[1,1] = resolution / h\n    m[0,2] = resolution * ( -center[0] / h + 0.5 )\n    m[1,2] = resolution * ( -center[1] / h + 0.5 )\n    m = np.linalg.inv(m)\n    return np.matmul (m, pt)[0:2]\n    \ndef crop(image, center, scale, resolution=256.0):\n    ul = transform([1, 1], center, scale, resolution).astype( np.int )\n    br = transform([resolution, resolution], center, scale, resolution).astype( np.int )\n    if image.ndim > 2:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0], image.shape[2]], dtype=np.int32)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    else:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    ht = image.shape[0]\n    wd = image.shape[1]\n    newX = np.array([max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)\n    newY = np.array([max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)\n    oldX = np.array([max(1, ul[0] + 1), min(br[0], wd)], dtype=np.int32)\n    oldY = np.array([max(1, ul[1] + 1), min(br[1], ht)], dtype=np.int32)\n    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1] ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]\n    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)), interpolation=cv2.INTER_LINEAR)\n    return newImg\n           \ndef get_pts_from_predict(a, center, scale):\n    b = a.reshape ( (a.shape[0], a.shape[1]*a.shape[2]) )    \n    c = b.argmax(1).reshape ( (a.shape[0], 1) ).repeat(2, axis=1).astype(np.float)\n    c[:,0] %= a.shape[2]    \n    c[:,1] = np.apply_along_axis ( lambda x: np.floor(x / a.shape[2]), 0, c[:,1] )\n\n    for i in range(a.shape[0]):\n        pX, pY = int(c[i,0]), int(c[i,1])\n        if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n            diff = np.array ( [a[i,pY,pX+1]-a[i,pY,pX-1], a[i,pY+1,pX]-a[i,pY-1,pX]] )\n            c[i] += np.sign(diff)*0.25\n   \n    c += 0.5\n    return [ transform (c[i], center, scale, a.shape[2]) for i in range(a.shape[0]) ]\n\ndef initialize(detector, scale_to=2048):\n    global dlib_detectors\n    global keras_model\n    global is_initialized\n    if not is_initialized:\n        dlib_cnn_face_detector_path = os.path.join(os.path.dirname(__file__), \"mmod_human_face_detector.dat\")\n        if not os.path.exists(dlib_cnn_face_detector_path):\n            raise Exception (\"Error: Unable to find %s, reinstall the lib !\" % (dlib_cnn_face_detector_path) )\n        \n        if detector == 'cnn' or detector == \"all\":\n            dlib_cnn_face_detector = dlib.cnn_face_detection_model_v1(dlib_cnn_face_detector_path)            \n            \n            dlib_cnn_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 ) \n            dlib_detectors.append(dlib_cnn_face_detector)\n        \n        if detector == \"hog\" or detector == \"all\":\n            dlib_face_detector = dlib.get_frontal_face_detector()\n            dlib_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 )\n            dlib_detectors.append(dlib_face_detector)        \n    \n        keras_model_path = os.path.join( os.path.dirname(__file__) , \"2DFAN-4.h5\" )\n        if not os.path.exists(keras_model_path):\n            print (\"Error: Unable to find %s, reinstall the lib !\" % (keras_model_path) )\n        else:\n            print (\"Info: initializing keras model...\")\n            keras_model = keras.models.load_model (keras_model_path, custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D} ) \n            \n        is_initialized = True\n\n\ndef extract(input_image, detector, verbose, all_faces=True, scale_to=2048):\n    initialize(detector, scale_to)\n    global dlib_detectors\n    global keras_model\n    \n    (h, w, ch) = input_image.shape\n\n    input_scale = scale_to / (w if w > h else h)\n    input_image = cv2.resize (input_image, ( int(w*input_scale), int(h*input_scale) ), interpolation=cv2.INTER_LINEAR)\n    input_image_bgr = input_image[:,:,::-1].copy() \n    input_images = [input_image, input_image_bgr]\n \n    detected_faces = []\n    for current_detector, input_image in ((current_detector, input_image) for current_detector in dlib_detectors for input_image in input_images):\n        detected_faces = current_detector(input_image, 0)\n        if len(detected_faces) != 0:\n            break\n\n    landmarks = []\n    if len(detected_faces) > 0:        \n        for i, d_rect in enumerate(detected_faces):\n            if i > 0 and not all_faces:\n                break\n        \n            if type(d_rect) == dlib.mmod_rectangle:\n                d_rect = d_rect.rect\n            \n            left, top, right, bottom = d_rect.left(), d_rect.top(), d_rect.right(), d_rect.bottom()\n            del d_rect\n    \n            center = np.array( [ (left + right) / 2.0, (top + bottom) / 2.0] )\n            center[1] -= (bottom - top) * 0.12\n            scale = (right - left + bottom - top) / 195.0\n        \n            image = crop(input_image, center, scale).transpose ( (2,0,1) ).astype(np.float32) / 255.0\n            image = np.expand_dims(image, 0)\n            \n            pts_img = get_pts_from_predict ( keras_model.predict (image)[-1][0], center, scale)\n            pts_img = [ ( int(pt[0]/input_scale), int(pt[1]/input_scale) ) for pt in pts_img ]             \n            landmarks.append ( ((  int(left/input_scale), int(top/input_scale), int(right/input_scale), int(bottom/input_scale) ),pts_img) )\n    elif verbose:\n        print(\"Warning: No faces were detected.\")\n        \n    return landmarks\n", "comments": " dlib tf competiting vram  dlib must first allocation prevent oom error    scale 2048 dlib upsamples 0 3gb vram windows 10 users           cv2 numpy inputs differs rgb bgr order  affects chance dlib face detection ", "content": "import atexit\nimport numpy as np\nimport os\nimport cv2\nimport dlib\nimport keras\nfrom keras import backend as K\n\ndlib_detectors = []\nkeras_model = None\nis_initialized = False\n\n@atexit.register\ndef onExit():\n    global dlib_detectors\n    global keras_model\n    \n    if keras_model is not None:\n        del keras_model\n        K.clear_session()\n        \n    for detector in dlib_detectors:\n        del detector\n        \nclass TorchBatchNorm2D(keras.engine.topology.Layer):\n    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):\n        super(TorchBatchNorm2D, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.momentum = momentum\n        self.epsilon = epsilon\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of ' 'input tensor should have a defined dimension ' 'but the layer received an input with shape ' + str(input_shape) + '.')\n        shape = (dim,)\n        self.gamma = self.add_weight(shape=shape, name='gamma', initializer='ones', regularizer=None, constraint=None)\n        self.beta = self.add_weight(shape=shape, name='beta', initializer='zeros', regularizer=None, constraint=None)\n        self.moving_mean = self.add_weight(shape=shape, name='moving_mean', initializer='zeros', trainable=False)            \n        self.moving_variance = self.add_weight(shape=shape, name='moving_variance', initializer='ones', trainable=False)            \n        self.built = True\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n        \n        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)\n        broadcast_moving_variance = K.reshape(self.moving_variance, broadcast_shape)\n        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n        broadcast_beta = K.reshape(self.beta, broadcast_shape)        \n        invstd = K.ones (shape=broadcast_shape, dtype='float32') / K.sqrt(broadcast_moving_variance + K.constant(self.epsilon, dtype='float32'))\n        \n        return (inputs - broadcast_moving_mean) * invstd * broadcast_gamma + broadcast_beta\n       \n    def get_config(self):\n        config = { 'axis': self.axis, 'momentum': self.momentum, 'epsilon': self.epsilon }\n        base_config = super(TorchBatchNorm2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\ndef transform(point, center, scale, resolution):\n    pt = np.array ( [point[0], point[1], 1.0] )            \n    h = 200.0 * scale\n    m = np.eye(3)\n    m[0,0] = resolution / h\n    m[1,1] = resolution / h\n    m[0,2] = resolution * ( -center[0] / h + 0.5 )\n    m[1,2] = resolution * ( -center[1] / h + 0.5 )\n    m = np.linalg.inv(m)\n    return np.matmul (m, pt)[0:2]\n    \ndef crop(image, center, scale, resolution=256.0):\n    ul = transform([1, 1], center, scale, resolution).astype( np.int )\n    br = transform([resolution, resolution], center, scale, resolution).astype( np.int )\n    if image.ndim > 2:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0], image.shape[2]], dtype=np.int32)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    else:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    ht = image.shape[0]\n    wd = image.shape[1]\n    newX = np.array([max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)\n    newY = np.array([max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)\n    oldX = np.array([max(1, ul[0] + 1), min(br[0], wd)], dtype=np.int32)\n    oldY = np.array([max(1, ul[1] + 1), min(br[1], ht)], dtype=np.int32)\n    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1] ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]\n    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)), interpolation=cv2.INTER_LINEAR)\n    return newImg\n           \ndef get_pts_from_predict(a, center, scale):\n    b = a.reshape ( (a.shape[0], a.shape[1]*a.shape[2]) )    \n    c = b.argmax(1).reshape ( (a.shape[0], 1) ).repeat(2, axis=1).astype(np.float)\n    c[:,0] %= a.shape[2]    \n    c[:,1] = np.apply_along_axis ( lambda x: np.floor(x / a.shape[2]), 0, c[:,1] )\n\n    for i in range(a.shape[0]):\n        pX, pY = int(c[i,0]), int(c[i,1])\n        if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n            diff = np.array ( [a[i,pY,pX+1]-a[i,pY,pX-1], a[i,pY+1,pX]-a[i,pY-1,pX]] )\n            c[i] += np.sign(diff)*0.25\n   \n    c += 0.5\n    return [ transform (c[i], center, scale, a.shape[2]) for i in range(a.shape[0]) ]\n\ndef initialize(detector, scale_to=2048):\n    global dlib_detectors\n    global keras_model\n    global is_initialized\n    if not is_initialized:\n        dlib_cnn_face_detector_path = os.path.join(os.path.dirname(__file__), \"mmod_human_face_detector.dat\")\n        if not os.path.exists(dlib_cnn_face_detector_path):\n            raise Exception (\"Error: Unable to find %s, reinstall the lib !\" % (dlib_cnn_face_detector_path) )\n        \n        if detector == 'cnn' or detector == \"all\":\n            dlib_cnn_face_detector = dlib.cnn_face_detection_model_v1(dlib_cnn_face_detector_path)            \n            #DLIB and TF competiting for VRAM, so dlib must do first allocation to prevent OOM error \n            dlib_cnn_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 ) \n            dlib_detectors.append(dlib_cnn_face_detector)\n        \n        if detector == \"hog\" or detector == \"all\":\n            dlib_face_detector = dlib.get_frontal_face_detector()\n            dlib_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 )\n            dlib_detectors.append(dlib_face_detector)        \n    \n        keras_model_path = os.path.join( os.path.dirname(__file__) , \"2DFAN-4.h5\" )\n        if not os.path.exists(keras_model_path):\n            print (\"Error: Unable to find %s, reinstall the lib !\" % (keras_model_path) )\n        else:\n            print (\"Info: initializing keras model...\")\n            keras_model = keras.models.load_model (keras_model_path, custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D} ) \n            \n        is_initialized = True\n\n#scale_to=2048 with dlib upsamples=0 for 3GB VRAM Windows 10 users        \ndef extract(input_image, detector, verbose, all_faces=True, scale_to=2048):\n    initialize(detector, scale_to)\n    global dlib_detectors\n    global keras_model\n    \n    (h, w, ch) = input_image.shape\n\n    input_scale = scale_to / (w if w > h else h)\n    input_image = cv2.resize (input_image, ( int(w*input_scale), int(h*input_scale) ), interpolation=cv2.INTER_LINEAR)\n    input_image_bgr = input_image[:,:,::-1].copy() #cv2 and numpy inputs differs in rgb-bgr order, this affects chance of dlib face detection\n    input_images = [input_image, input_image_bgr]\n \n    detected_faces = []\n    for current_detector, input_image in ((current_detector, input_image) for current_detector in dlib_detectors for input_image in input_images):\n        detected_faces = current_detector(input_image, 0)\n        if len(detected_faces) != 0:\n            break\n\n    landmarks = []\n    if len(detected_faces) > 0:        \n        for i, d_rect in enumerate(detected_faces):\n            if i > 0 and not all_faces:\n                break\n        \n            if type(d_rect) == dlib.mmod_rectangle:\n                d_rect = d_rect.rect\n            \n            left, top, right, bottom = d_rect.left(), d_rect.top(), d_rect.right(), d_rect.bottom()\n            del d_rect\n    \n            center = np.array( [ (left + right) / 2.0, (top + bottom) / 2.0] )\n            center[1] -= (bottom - top) * 0.12\n            scale = (right - left + bottom - top) / 195.0\n        \n            image = crop(input_image, center, scale).transpose ( (2,0,1) ).astype(np.float32) / 255.0\n            image = np.expand_dims(image, 0)\n            \n            pts_img = get_pts_from_predict ( keras_model.predict (image)[-1][0], center, scale)\n            pts_img = [ ( int(pt[0]/input_scale), int(pt[1]/input_scale) ) for pt in pts_img ]             \n            landmarks.append ( ((  int(left/input_scale), int(top/input_scale), int(right/input_scale), int(bottom/input_scale) ),pts_img) )\n    elif verbose:\n        print(\"Warning: No faces were detected.\")\n        \n    return landmarks\n", "description": "Non official project based on original /r/Deepfakes thread. Many thanks to him!", "file_name": "FaceLandmarksExtractor.py", "id": "379096b5b11b5d20c34bb1395336ce98", "language": "Python", "project_name": "faceswap", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/deepfakes-faceswap/deepfakes-faceswap-6ff64ef/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py", "save_time": "", "source": "", "update_at": "2018-03-18T16:27:43Z", "url": "https://github.com/deepfakes/faceswap", "wiki": true}