{"author": "donnemartin", "code": "\nimport numpy\n\nimport theano\nimport theano.tensor as T\n\nfrom theano import config\nfrom theano.tensor.nnet import categorical_crossentropy\n\n\nfloatX = theano.config.floatX\n\n\n\n\nclass SimpleRNN(object):\n    def __init__(self, input_dim, recurrent_dim):\n        w_xh = numpy.random.normal(0, .01, (input_dim, recurrent_dim))\n        w_hh = numpy.random.normal(0, .02, (recurrent_dim, recurrent_dim))\n        self.w_xh = theano.shared(numpy.asarray(w_xh, dtype=floatX), name='w_xh')\n        self.w_hh = theano.shared(numpy.asarray(w_hh, dtype=floatX), name='w_hh')\n        self.b_h = theano.shared(numpy.zeros((recurrent_dim,), dtype=floatX), name='b_h')\n        self.parameters = [self.w_xh, self.w_hh, self.b_h]\n\n    def _step(self, input_t, previous):\n        return T.tanh(T.dot(previous, self.w_hh) + input_t)\n\n    def __call__(self, x):\n        x_w_xh = T.dot(x, self.w_xh) + self.b_h\n        result, updates = theano.scan(self._step,\n                                      sequences=[x_w_xh],\n                                      outputs_info=[T.zeros_like(self.b_h)])\n        return result\n\n\nw_ho_np = numpy.random.normal(0, .01, (15, 1))\nw_ho = theano.shared(numpy.asarray(w_ho_np, dtype=floatX), name='w_ho')\nb_o = theano.shared(numpy.zeros((1,), dtype=floatX), name='b_o')\n\nx = T.matrix('x')\nmy_rnn = SimpleRNN(1, 15)\nhidden = my_rnn(x)\nprediction = T.dot(hidden, w_ho) + b_o\nparameters = my_rnn.parameters + [w_ho, b_o]\nl2 = sum((p**2).sum() for p in parameters)\nmse = T.mean((prediction[:-1] - x[1:])**2)\ncost = mse + .0001 * l2\ngradient = T.grad(cost, wrt=parameters)\n\nlr = .3\nupdates = [(par, par - lr * gra) for par, gra in zip(parameters, gradient)]\nupdate_model = theano.function([x], cost, updates=updates)\nget_cost = theano.function([x], mse)\npredict = theano.function([x], prediction)\nget_hidden = theano.function([x], hidden)\nget_gradient = theano.function([x], gradient)\n\npredict = theano.function([x], prediction)\n\n\n\nx_t = T.vector()\nh_p = T.vector()\npreactivation = T.dot(x_t, my_rnn.w_xh) + my_rnn.b_h\nh_t = my_rnn._step(preactivation, h_p)\no_t = T.dot(h_t, w_ho) + b_o\n\nsingle_step = theano.function([x_t, h_p], [o_t, h_t])\n\n\n\ndef gauss_weight(rng, ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = rng.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        \n        self.W_i = theano.shared(gauss_weight(rng, n_in, n_h), 'W_i', borrow=True)\n        self.W_f = theano.shared(gauss_weight(rng, n_in, n_h), 'W_f', borrow=True)\n        self.W_c = theano.shared(gauss_weight(rng, n_in, n_h), 'W_c', borrow=True)\n        self.W_o = theano.shared(gauss_weight(rng, n_in, n_h), 'W_o', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(rng, n_h), 'U_i', borrow=True)\n        self.U_f = theano.shared(gauss_weight(rng, n_h), 'U_f', borrow=True)\n        self.U_c = theano.shared(gauss_weight(rng, n_h), 'U_c', borrow=True)\n        self.U_o = theano.shared(gauss_weight(rng, n_h), 'U_o', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_i', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_f', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_c', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_o', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (length, batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\nclass LogisticRegression(object):\n\n    def __init__(self, rng, input, n_in, n_out):\n\n        W = gauss_weight(rng, n_in, n_out)\n        self.W = theano.shared(value=numpy.asarray(W, dtype=theano.config.floatX),\n                               name='W', borrow=True)\n        \n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name='b', borrow=True)\n\n        \n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, axis=2, keepdims=True))\n        pmf = energy_exp / energy_exp.sum(axis=2, keepdims=True)\n        self.p_y_given_x = pmf\n        self.params = [self.W, self.b]\n\nbatch_size = 100\nn_h = 50\n\n\n\nrng = numpy.random.RandomState(12345)\n\nx = T.lmatrix('x')\nmask = T.matrix('mask')\n\n\nrecurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\nlogreg_layer = LogisticRegression(rng=rng, input=recurrent_layer.output[:-1],\n                                  n_in=n_h, n_out=111)\n\n\ncost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                         x[1:],\n                                         mask[1:]) / batch_size\n\n\nparams = logreg_layer.params + recurrent_layer.params\n\n\ngrads = T.grad(cost, params)\n\nlearning_rate = 0.1\nupdates = [\n    (param_i, param_i - learning_rate * grad_i)\n    for param_i, grad_i in zip(params, grads)\n]\n\nupdate_model = theano.function([x, mask], cost, updates=updates)\n\nevaluate_model = theano.function([x, mask], cost)\n\n\nx_t = T.iscalar()\nh_p = T.vector()\nc_p = T.vector()\nh_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\nenergy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\nenergy_exp = T.exp(energy - T.max(energy, axis=1, keepdims=True))\n\noutput = energy_exp / energy_exp.sum(axis=1, keepdims=True)\nsingle_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n", "comments": "   this file speed execution notebooks   it contains subset code defined simple rnn ipynb lstm text ipynb  particular code compiling theano function  executing script first populate cache compiled c code  make subsequent compilations faster   the use case run script background demo vm one nvidia qwiklabs  compilation phase started notebooks faster          simple rnn ipynb    generating sequences    lstm text ipynb    init params    self output format (length  batchsize  n h)    initialize biases b vector n 0s    compute vector class membership probabilities symbolic form    the theano graph    set random number generator  seeds consistency    construct lstm layer logistic regression layer    define cost variable optimize    create list model parameters fit gradient descent    create list gradients model parameters    generating sequences ", "content": "\"\"\"This file is only here to speed up the execution of notebooks.\n\nIt contains a subset of the code defined in simple_rnn.ipynb and\nlstm_text.ipynb, in particular the code compiling Theano function.\nExecuting this script first will populate the cache of compiled C code,\nwhich will make subsequent compilations faster.\n\nThe use case is to run this script in the background when a demo VM\nsuch as the one for NVIDIA's qwikLABS, so that the compilation phase\nstarted from the notebooks is faster.\n\n\"\"\"\nimport numpy\n\nimport theano\nimport theano.tensor as T\n\nfrom theano import config\nfrom theano.tensor.nnet import categorical_crossentropy\n\n\nfloatX = theano.config.floatX\n\n\n# simple_rnn.ipynb\n\nclass SimpleRNN(object):\n    def __init__(self, input_dim, recurrent_dim):\n        w_xh = numpy.random.normal(0, .01, (input_dim, recurrent_dim))\n        w_hh = numpy.random.normal(0, .02, (recurrent_dim, recurrent_dim))\n        self.w_xh = theano.shared(numpy.asarray(w_xh, dtype=floatX), name='w_xh')\n        self.w_hh = theano.shared(numpy.asarray(w_hh, dtype=floatX), name='w_hh')\n        self.b_h = theano.shared(numpy.zeros((recurrent_dim,), dtype=floatX), name='b_h')\n        self.parameters = [self.w_xh, self.w_hh, self.b_h]\n\n    def _step(self, input_t, previous):\n        return T.tanh(T.dot(previous, self.w_hh) + input_t)\n\n    def __call__(self, x):\n        x_w_xh = T.dot(x, self.w_xh) + self.b_h\n        result, updates = theano.scan(self._step,\n                                      sequences=[x_w_xh],\n                                      outputs_info=[T.zeros_like(self.b_h)])\n        return result\n\n\nw_ho_np = numpy.random.normal(0, .01, (15, 1))\nw_ho = theano.shared(numpy.asarray(w_ho_np, dtype=floatX), name='w_ho')\nb_o = theano.shared(numpy.zeros((1,), dtype=floatX), name='b_o')\n\nx = T.matrix('x')\nmy_rnn = SimpleRNN(1, 15)\nhidden = my_rnn(x)\nprediction = T.dot(hidden, w_ho) + b_o\nparameters = my_rnn.parameters + [w_ho, b_o]\nl2 = sum((p**2).sum() for p in parameters)\nmse = T.mean((prediction[:-1] - x[1:])**2)\ncost = mse + .0001 * l2\ngradient = T.grad(cost, wrt=parameters)\n\nlr = .3\nupdates = [(par, par - lr * gra) for par, gra in zip(parameters, gradient)]\nupdate_model = theano.function([x], cost, updates=updates)\nget_cost = theano.function([x], mse)\npredict = theano.function([x], prediction)\nget_hidden = theano.function([x], hidden)\nget_gradient = theano.function([x], gradient)\n\npredict = theano.function([x], prediction)\n\n# Generating sequences\n\nx_t = T.vector()\nh_p = T.vector()\npreactivation = T.dot(x_t, my_rnn.w_xh) + my_rnn.b_h\nh_t = my_rnn._step(preactivation, h_p)\no_t = T.dot(h_t, w_ho) + b_o\n\nsingle_step = theano.function([x_t, h_p], [o_t, h_t])\n\n# lstm_text.ipynb\n\ndef gauss_weight(rng, ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = rng.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        # Init params\n        self.W_i = theano.shared(gauss_weight(rng, n_in, n_h), 'W_i', borrow=True)\n        self.W_f = theano.shared(gauss_weight(rng, n_in, n_h), 'W_f', borrow=True)\n        self.W_c = theano.shared(gauss_weight(rng, n_in, n_h), 'W_c', borrow=True)\n        self.W_o = theano.shared(gauss_weight(rng, n_in, n_h), 'W_o', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(rng, n_h), 'U_i', borrow=True)\n        self.U_f = theano.shared(gauss_weight(rng, n_h), 'U_f', borrow=True)\n        self.U_c = theano.shared(gauss_weight(rng, n_h), 'U_c', borrow=True)\n        self.U_o = theano.shared(gauss_weight(rng, n_h), 'U_o', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_i', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_f', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_c', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_o', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (length, batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\nclass LogisticRegression(object):\n\n    def __init__(self, rng, input, n_in, n_out):\n\n        W = gauss_weight(rng, n_in, n_out)\n        self.W = theano.shared(value=numpy.asarray(W, dtype=theano.config.floatX),\n                               name='W', borrow=True)\n        # initialize the biases b as a vector of n_out 0s\n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name='b', borrow=True)\n\n        # compute vector of class-membership probabilities in symbolic form\n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, axis=2, keepdims=True))\n        pmf = energy_exp / energy_exp.sum(axis=2, keepdims=True)\n        self.p_y_given_x = pmf\n        self.params = [self.W, self.b]\n\nbatch_size = 100\nn_h = 50\n\n# The Theano graph\n# Set the random number generator' seeds for consistency\nrng = numpy.random.RandomState(12345)\n\nx = T.lmatrix('x')\nmask = T.matrix('mask')\n\n# Construct an LSTM layer and a logistic regression layer\nrecurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\nlogreg_layer = LogisticRegression(rng=rng, input=recurrent_layer.output[:-1],\n                                  n_in=n_h, n_out=111)\n\n# define a cost variable to optimize\ncost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                         x[1:],\n                                         mask[1:]) / batch_size\n\n# create a list of all model parameters to be fit by gradient descent\nparams = logreg_layer.params + recurrent_layer.params\n\n# create a list of gradients for all model parameters\ngrads = T.grad(cost, params)\n\nlearning_rate = 0.1\nupdates = [\n    (param_i, param_i - learning_rate * grad_i)\n    for param_i, grad_i in zip(params, grads)\n]\n\nupdate_model = theano.function([x, mask], cost, updates=updates)\n\nevaluate_model = theano.function([x, mask], cost)\n\n# Generating Sequences\nx_t = T.iscalar()\nh_p = T.vector()\nc_p = T.vector()\nh_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\nenergy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\nenergy_exp = T.exp(energy - T.max(energy, axis=1, keepdims=True))\n\noutput = energy_exp / energy_exp.sum(axis=1, keepdims=True)\nsingle_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "rnn_precompile.py", "id": "f9f924aacf56b26e8719a125478d6480", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/deep-learning/theano-tutorial/rnn_tutorial/rnn_precompile.py", "save_time": "", "source": "", "update_at": "2018-03-18T12:16:56Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}