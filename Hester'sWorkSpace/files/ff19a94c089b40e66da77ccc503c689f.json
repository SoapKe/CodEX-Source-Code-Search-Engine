{"author": "yunjey", "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torchvision\n\n\ndataset = datasets.MNIST(root='./data',\n                         train=True,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n\ndata_loader = torch.utils.data.DataLoader(dataset=dataset,\n                                          batch_size=100, \n                                          shuffle=True)\n\ndef to_var(x):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x)\n\n\nclass VAE(nn.Module):\n    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(image_size, h_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(h_dim, z_dim*2))  \n        \n        self.decoder = nn.Sequential(\n            nn.Linear(z_dim, h_dim),\n            nn.ReLU(),\n            nn.Linear(h_dim, image_size),\n            nn.Sigmoid())\n    \n    def reparameterize(self, mu, log_var):\n        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n        eps = to_var(torch.randn(mu.size(0), mu.size(1)))\n        z = mu + eps * torch.exp(log_var/2)    \n        return z\n                     \n    def forward(self, x):\n        h = self.encoder(x)\n        mu, log_var = torch.chunk(h, 2, dim=1)  \n        z = self.reparameterize(mu, log_var)\n        out = self.decoder(z)\n        return out, mu, log_var\n    \n    def sample(self, z):\n        return self.decoder(z)\n    \nvae = VAE()\n\nif torch.cuda.is_available():\n    vae.cuda()\n    \noptimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\niter_per_epoch = len(data_loader)\ndata_iter = iter(data_loader)\n\n\nfixed_z = to_var(torch.randn(100, 20))\nfixed_x, _ = next(data_iter)\ntorchvision.utils.save_image(fixed_x.cpu(), './data/real_images.png')\nfixed_x = to_var(fixed_x.view(fixed_x.size(0), -1))\n\nfor epoch in range(50):\n    for i, (images, _) in enumerate(data_loader):\n        \n        images = to_var(images.view(images.size(0), -1))\n        out, mu, log_var = vae(images)\n        \n        \n        \n        reconst_loss = F.binary_cross_entropy(out, images, size_average=False)\n        kl_divergence = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n        \n        # Backprop + Optimize\n        total_loss = reconst_loss + kl_divergence\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        \n        if i % 100 == 0:\n            print (\"Epoch[%d/%d], Step [%d/%d], Total Loss: %.4f, \"\n                   \"Reconst Loss: %.4f, KL Div: %.7f\" \n                   %(epoch+1, 50, i+1, iter_per_epoch, total_loss.data[0], \n                     reconst_loss.data[0], kl_divergence.data[0]))\n    \n    \n    reconst_images, _, _ = vae(fixed_x)\n    reconst_images = reconst_images.view(reconst_images.size(0), 1, 28, 28)\n    torchvision.utils.save_image(reconst_images.data.cpu(), \n        './data/reconst_images_%d.png' %(epoch+1))\n", "comments": "    z   mean   eps   sigma eps sampled n(0  1)        mnist dataset    data loader    vae model    2 mean variance     2 convert var std    mean log variance     fixed inputs debugging    compute reconstruction loss kl divergence    for kl divergence  see appendix b paper http   yunjey47 tistory com 43    backprop   optimize    save reconstructed images ", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torchvision\n\n# MNIST dataset\ndataset = datasets.MNIST(root='./data',\n                         train=True,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# Data loader\ndata_loader = torch.utils.data.DataLoader(dataset=dataset,\n                                          batch_size=100, \n                                          shuffle=True)\n\ndef to_var(x):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x)\n\n# VAE model\nclass VAE(nn.Module):\n    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(image_size, h_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(h_dim, z_dim*2))  # 2 for mean and variance.\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(z_dim, h_dim),\n            nn.ReLU(),\n            nn.Linear(h_dim, image_size),\n            nn.Sigmoid())\n    \n    def reparameterize(self, mu, log_var):\n        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n        eps = to_var(torch.randn(mu.size(0), mu.size(1)))\n        z = mu + eps * torch.exp(log_var/2)    # 2 for convert var to std\n        return z\n                     \n    def forward(self, x):\n        h = self.encoder(x)\n        mu, log_var = torch.chunk(h, 2, dim=1)  # mean and log variance.\n        z = self.reparameterize(mu, log_var)\n        out = self.decoder(z)\n        return out, mu, log_var\n    \n    def sample(self, z):\n        return self.decoder(z)\n    \nvae = VAE()\n\nif torch.cuda.is_available():\n    vae.cuda()\n    \noptimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\niter_per_epoch = len(data_loader)\ndata_iter = iter(data_loader)\n\n# fixed inputs for debugging\nfixed_z = to_var(torch.randn(100, 20))\nfixed_x, _ = next(data_iter)\ntorchvision.utils.save_image(fixed_x.cpu(), './data/real_images.png')\nfixed_x = to_var(fixed_x.view(fixed_x.size(0), -1))\n\nfor epoch in range(50):\n    for i, (images, _) in enumerate(data_loader):\n        \n        images = to_var(images.view(images.size(0), -1))\n        out, mu, log_var = vae(images)\n        \n        # Compute reconstruction loss and kl divergence\n        # For kl_divergence, see Appendix B in the paper or http://yunjey47.tistory.com/43\n        reconst_loss = F.binary_cross_entropy(out, images, size_average=False)\n        kl_divergence = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n        \n        # Backprop + Optimize\n        total_loss = reconst_loss + kl_divergence\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        \n        if i % 100 == 0:\n            print (\"Epoch[%d/%d], Step [%d/%d], Total Loss: %.4f, \"\n                   \"Reconst Loss: %.4f, KL Div: %.7f\" \n                   %(epoch+1, 50, i+1, iter_per_epoch, total_loss.data[0], \n                     reconst_loss.data[0], kl_divergence.data[0]))\n    \n    # Save the reconstructed images\n    reconst_images, _, _ = vae(fixed_x)\n    reconst_images = reconst_images.view(reconst_images.size(0), 1, 28, 28)\n    torchvision.utils.save_image(reconst_images.data.cpu(), \n        './data/reconst_images_%d.png' %(epoch+1))\n", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main.py", "id": "ff19a94c089b40e66da77ccc503c689f", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/03-advanced/variational_auto_encoder/main.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}