{"author": "donnemartin", "code": "\nimport time\nfrom mrjob.job import MRJob\nfrom mrjob.protocol import RawValueProtocol, ReprProtocol\nimport re\n\n\nclass MrS3LogParser(MRJob):\n    \"\"\"Parses the logs from S3 based on the S3 logging format:\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n    \n    Aggregates a user's daily requests by user agent and operation\n    \n    Outputs date_time, requester, user_agent, operation, count\n    \"\"\"\n\n    LOGPATS  = r'(\\S+) (\\S+) \\[(.*?)\\] (\\S+) (\\S+) ' \\\n               r'(\\S+) (\\S+) (\\S+) (\"([^\"]+)\"|-) ' \\\n               r'(\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) ' \\\n               r'(\"([^\"]+)\"|-) (\"([^\"]+)\"|-)'\n    NUM_ENTRIES_PER_LINE = 17\n    logpat = re.compile(LOGPATS)\n\n    (S3_LOG_BUCKET_OWNER, \n     S3_LOG_BUCKET, \n     S3_LOG_DATE_TIME,\n     S3_LOG_IP, \n     S3_LOG_REQUESTER_ID, \n     S3_LOG_REQUEST_ID,\n     S3_LOG_OPERATION, \n     S3_LOG_KEY, \n     S3_LOG_HTTP_METHOD,\n     S3_LOG_HTTP_STATUS, \n     S3_LOG_S3_ERROR, \n     S3_LOG_BYTES_SENT,\n     S3_LOG_OBJECT_SIZE, \n     S3_LOG_TOTAL_TIME, \n     S3_LOG_TURN_AROUND_TIME,\n     S3_LOG_REFERER, \n     S3_LOG_USER_AGENT) = range(NUM_ENTRIES_PER_LINE)\n\n    DELIMITER = '\\t'\n\n    \n    \n    INPUT_PROTOCOL = RawValueProtocol\n\n    \n    # instead of (k, v) pairs\n    OUTPUT_PROTOCOL = RawValueProtocol\n\n    # Encode the intermediate records using repr() instead of JSON, so the\n    \n    INTERNAL_PROTOCOL = ReprProtocol\n\n    def clean_date_time_zone(self, raw_date_time_zone):\n        \"\"\"Converts entry 22/Jul/2013:21:04:17 +0000 to the format\n        'YYYY-MM-DD HH:MM:SS' which is more suitable for loading into\n        a database such as Redshift or RDS\n\n        Note: requires the chars \"[ ]\" to be stripped prior to input\n        Returns the converted datetime annd timezone\n        or None for both values if failed\n\n        TODO: Needs to combine timezone with date as one field\n        \"\"\"\n        date_time = None\n        time_zone_parsed = None\n\n        \n        date_parsed = raw_date_time_zone[:raw_date_time_zone.find(\":\")]\n        time_parsed = raw_date_time_zone[raw_date_time_zone.find(\":\") + 1:\n                                         raw_date_time_zone.find(\"+\") - 1]\n        time_zone_parsed = raw_date_time_zone[raw_date_time_zone.find(\"+\"):]\n\n        try:\n            date_struct = time.strptime(date_parsed, \"%d/%b/%Y\")\n            converted_date = time.strftime(\"%Y-%m-%d\", date_struct)\n            date_time = converted_date + \" \" + time_parsed\n\n        \n        \n        except ValueError as error:\n            raise ValueError(error)\n        else:\n            return converted_date, date_time, time_zone_parsed\n\n    def mapper(self, _, line):\n        line = line.strip()\n        match = self.logpat.search(line)\n\n        date_time = None\n        requester = None\n        user_agent = None\n        operation = None\n\n        try:\n            for n in range(self.NUM_ENTRIES_PER_LINE):\n                group = match.group(1 + n)\n\n                if n == self.S3_LOG_DATE_TIME:\n                    date, date_time, time_zone_parsed = \\\n                        self.clean_date_time_zone(group)\n                    \n                    \n                    date_time = date + \" 00:00:00\"\n                elif n == self.S3_LOG_REQUESTER_ID:\n                    requester = group\n                elif n == self.S3_LOG_USER_AGENT:\n                    user_agent = group\n                elif n == self.S3_LOG_OPERATION:\n                    operation = group\n                else:\n                    pass\n\n        except Exception:\n            yield ((\"Error while parsing line: %s\", line), 1)\n        else:\n            yield ((date_time, requester, user_agent, operation), 1)\n\n    def reducer(self, key, values):\n        output = list(key)\n        output = self.DELIMITER.join(output) + \\\n                 self.DELIMITER + \\\n                 str(sum(values))\n\n        yield None, output\n\n    def steps(self):\n        return [\n            self.mr(mapper=self.mapper,\n                    reducer=self.reducer)\n        ]\n\n\nif __name__ == '__main__':\n    MrS3LogParser.run()", "comments": "   parses logs s3 based s3 logging format      http   docs aws amazon com amazons3 latest dev logformat html          aggregates user daily requests user agent operation          outputs date time  requester  user agent  operation  count              logpats    r ( s ) ( s )   (   )   ( s ) ( s )                    r ( s ) ( s ) ( s ) ( (     )   )                    r ( s ) ( s ) ( s ) ( s ) ( s ) ( s )                    r ( (     )   ) ( (     )   )      num entries per line   17     logpat   compile(logpats)      (s3 log bucket owner        s3 log bucket        s3 log date time       s3 log ip        s3 log requester id        s3 log request id       s3 log operation        s3 log key        s3 log http method       s3 log http status        s3 log s3 error        s3 log bytes sent       s3 log object size        s3 log total time        s3 log turn around time       s3 log referer        s3 log user agent)   range(num entries per line)      delimiter             we use rawvalueprotocol input format agnostic       avoid type parsing errors     input protocol   rawvalueprotocol        we use rawvalueprotocol output output raw lines       instead (k  v) pairs     output protocol   rawvalueprotocol        encode intermediate records using repr() instead json        record get unicode encoded     internal protocol   reprprotocol      def clean date time zone(self  raw date time zone)             converts entry 22 jul 2013 21 04 17  0000 format          yyyy mm dd hh mm ss  suitable loading         database redshift rds          note  requires chars       stripped prior input         returns converted datetime annd timezone         none values failed          todo  needs combine timezone date one field                we use rawvalueprotocol input format agnostic    avoid type parsing errors    we use rawvalueprotocol output output raw lines    instead (k  v) pairs    encode intermediate records using repr() instead json     record get unicode encoded    todo  probably cleaner parse regex    throws valueerror exception operation fails    caught calling function handled appropriately    leave following line code     want aggregate date ", "content": "\nimport time\nfrom mrjob.job import MRJob\nfrom mrjob.protocol import RawValueProtocol, ReprProtocol\nimport re\n\n\nclass MrS3LogParser(MRJob):\n    \"\"\"Parses the logs from S3 based on the S3 logging format:\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n    \n    Aggregates a user's daily requests by user agent and operation\n    \n    Outputs date_time, requester, user_agent, operation, count\n    \"\"\"\n\n    LOGPATS  = r'(\\S+) (\\S+) \\[(.*?)\\] (\\S+) (\\S+) ' \\\n               r'(\\S+) (\\S+) (\\S+) (\"([^\"]+)\"|-) ' \\\n               r'(\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) ' \\\n               r'(\"([^\"]+)\"|-) (\"([^\"]+)\"|-)'\n    NUM_ENTRIES_PER_LINE = 17\n    logpat = re.compile(LOGPATS)\n\n    (S3_LOG_BUCKET_OWNER, \n     S3_LOG_BUCKET, \n     S3_LOG_DATE_TIME,\n     S3_LOG_IP, \n     S3_LOG_REQUESTER_ID, \n     S3_LOG_REQUEST_ID,\n     S3_LOG_OPERATION, \n     S3_LOG_KEY, \n     S3_LOG_HTTP_METHOD,\n     S3_LOG_HTTP_STATUS, \n     S3_LOG_S3_ERROR, \n     S3_LOG_BYTES_SENT,\n     S3_LOG_OBJECT_SIZE, \n     S3_LOG_TOTAL_TIME, \n     S3_LOG_TURN_AROUND_TIME,\n     S3_LOG_REFERER, \n     S3_LOG_USER_AGENT) = range(NUM_ENTRIES_PER_LINE)\n\n    DELIMITER = '\\t'\n\n    # We use RawValueProtocol for input to be format agnostic\n    # and avoid any type of parsing errors\n    INPUT_PROTOCOL = RawValueProtocol\n\n    # We use RawValueProtocol for output so we can output raw lines\n    # instead of (k, v) pairs\n    OUTPUT_PROTOCOL = RawValueProtocol\n\n    # Encode the intermediate records using repr() instead of JSON, so the\n    # record doesn't get Unicode-encoded\n    INTERNAL_PROTOCOL = ReprProtocol\n\n    def clean_date_time_zone(self, raw_date_time_zone):\n        \"\"\"Converts entry 22/Jul/2013:21:04:17 +0000 to the format\n        'YYYY-MM-DD HH:MM:SS' which is more suitable for loading into\n        a database such as Redshift or RDS\n\n        Note: requires the chars \"[ ]\" to be stripped prior to input\n        Returns the converted datetime annd timezone\n        or None for both values if failed\n\n        TODO: Needs to combine timezone with date as one field\n        \"\"\"\n        date_time = None\n        time_zone_parsed = None\n\n        # TODO: Probably cleaner to parse this with a regex\n        date_parsed = raw_date_time_zone[:raw_date_time_zone.find(\":\")]\n        time_parsed = raw_date_time_zone[raw_date_time_zone.find(\":\") + 1:\n                                         raw_date_time_zone.find(\"+\") - 1]\n        time_zone_parsed = raw_date_time_zone[raw_date_time_zone.find(\"+\"):]\n\n        try:\n            date_struct = time.strptime(date_parsed, \"%d/%b/%Y\")\n            converted_date = time.strftime(\"%Y-%m-%d\", date_struct)\n            date_time = converted_date + \" \" + time_parsed\n\n        # Throws a ValueError exception if the operation fails that is\n        # caught by the calling function and is handled appropriately\n        except ValueError as error:\n            raise ValueError(error)\n        else:\n            return converted_date, date_time, time_zone_parsed\n\n    def mapper(self, _, line):\n        line = line.strip()\n        match = self.logpat.search(line)\n\n        date_time = None\n        requester = None\n        user_agent = None\n        operation = None\n\n        try:\n            for n in range(self.NUM_ENTRIES_PER_LINE):\n                group = match.group(1 + n)\n\n                if n == self.S3_LOG_DATE_TIME:\n                    date, date_time, time_zone_parsed = \\\n                        self.clean_date_time_zone(group)\n                    # Leave the following line of code if \n                    # you want to aggregate by date\n                    date_time = date + \" 00:00:00\"\n                elif n == self.S3_LOG_REQUESTER_ID:\n                    requester = group\n                elif n == self.S3_LOG_USER_AGENT:\n                    user_agent = group\n                elif n == self.S3_LOG_OPERATION:\n                    operation = group\n                else:\n                    pass\n\n        except Exception:\n            yield ((\"Error while parsing line: %s\", line), 1)\n        else:\n            yield ((date_time, requester, user_agent, operation), 1)\n\n    def reducer(self, key, values):\n        output = list(key)\n        output = self.DELIMITER.join(output) + \\\n                 self.DELIMITER + \\\n                 str(sum(values))\n\n        yield None, output\n\n    def steps(self):\n        return [\n            self.mr(mapper=self.mapper,\n                    reducer=self.reducer)\n        ]\n\n\nif __name__ == '__main__':\n    MrS3LogParser.run()", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "mr_s3_log_parser.py", "id": "617353a69f45e405debb4b2f03e124ec", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/mapreduce/mr_s3_log_parser.py", "save_time": "", "source": "", "update_at": "2018-03-13T23:30:30Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}