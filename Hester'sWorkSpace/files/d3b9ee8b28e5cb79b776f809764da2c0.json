{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Trainer for coordinating single or multi-replica training.\n\nMain point of entry for running models.  Specifies most of\nthe parameters used by different algorithms.\n\"\"\"\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport os\nimport pickle\n\nfrom six.moves import xrange\nimport controller\nimport model\nimport policy\nimport baseline\nimport objective\nimport full_episode_objective\nimport trust_region\nimport optimizers\nimport replay_buffer\nimport expert_paths\nimport gym_wrapper\nimport env_spec\n\napp = tf.app\nflags = tf.flags\nlogging = tf.logging\ngfile = tf.gfile\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('env', 'Copy-v0', 'environment name')\nflags.DEFINE_integer('batch_size', 100, 'batch size')\nflags.DEFINE_integer('replay_batch_size', None, 'replay batch size; defaults to batch_size')\nflags.DEFINE_integer('num_samples', 1,\n                     'number of samples from each random seed initialization')\nflags.DEFINE_integer('max_step', 200, 'max number of steps to train on')\nflags.DEFINE_integer('cutoff_agent', 0,\n                     'number of steps at which to cut-off agent. '\n                     'Defaults to always cutoff')\nflags.DEFINE_integer('num_steps', 100000, 'number of training steps')\nflags.DEFINE_integer('validation_frequency', 100,\n                     'every so many steps, output some stats')\n\nflags.DEFINE_float('target_network_lag', 0.95,\n                   'This exponential decay on online network yields target '\n                   'network')\nflags.DEFINE_string('sample_from', 'online',\n                    'Sample actions from \"online\" network or \"target\" network')\n\nflags.DEFINE_string('objective', 'pcl',\n                    'pcl/upcl/a3c/trpo/reinforce/urex')\nflags.DEFINE_bool('trust_region_p', False,\n                  'use trust region for policy optimization')\nflags.DEFINE_string('value_opt', None,\n                    'leave as None to optimize it along with policy '\n                    '(using critic_weight). Otherwise set to '\n                    '\"best_fit\" (least squares regression), \"lbfgs\", or \"grad\"')\nflags.DEFINE_float('max_divergence', 0.01,\n                   'max divergence (i.e. KL) to allow during '\n                   'trust region optimization')\n\nflags.DEFINE_float('learning_rate', 0.01, 'learning rate')\nflags.DEFINE_float('clip_norm', 5.0, 'clip norm')\nflags.DEFINE_float('clip_adv', 0.0, 'Clip advantages at this value.  '\n                   'Leave as 0 to not clip at all.')\nflags.DEFINE_float('critic_weight', 0.1, 'critic weight')\nflags.DEFINE_float('tau', 0.1, 'entropy regularizer.'\n                   'If using decaying tau, this is the final value.')\nflags.DEFINE_float('tau_decay', None,\n                   'decay tau by this much every 100 steps')\nflags.DEFINE_float('tau_start', 0.1,\n                   'start tau at this value')\nflags.DEFINE_float('eps_lambda', 0.0, 'relative entropy regularizer.')\nflags.DEFINE_bool('update_eps_lambda', False,\n                  'Update lambda automatically based on last 100 episodes.')\nflags.DEFINE_float('gamma', 1.0, 'discount')\nflags.DEFINE_integer('rollout', 10, 'rollout')\nflags.DEFINE_bool('use_target_values', False,\n                  'use target network for value estimates')\nflags.DEFINE_bool('fixed_std', True,\n                  'fix the std in Gaussian distributions')\nflags.DEFINE_bool('input_prev_actions', True,\n                  'input previous actions to policy network')\nflags.DEFINE_bool('recurrent', True,\n                  'use recurrent connections')\nflags.DEFINE_bool('input_time_step', False,\n                  'input time step into value calucations')\n\nflags.DEFINE_bool('use_online_batch', True, 'train on batches as they are sampled')\nflags.DEFINE_bool('batch_by_steps', False,\n                  'ensure each training batch has batch_size * max_step steps')\nflags.DEFINE_bool('unify_episodes', False,\n                  'Make sure replay buffer holds entire episodes, '\n                  'even across distinct sampling steps')\nflags.DEFINE_integer('replay_buffer_size', 5000, 'replay buffer size')\nflags.DEFINE_float('replay_buffer_alpha', 0.5, 'replay buffer alpha param')\nflags.DEFINE_integer('replay_buffer_freq', 0,\n                     'replay buffer frequency (only supports -1/0/1)')\nflags.DEFINE_string('eviction', 'rand',\n                    'how to evict from replay buffer: rand/rank/fifo')\nflags.DEFINE_string('prioritize_by', 'rewards',\n                    'Prioritize replay buffer by \"rewards\" or \"step\"')\nflags.DEFINE_integer('num_expert_paths', 0,\n                     'number of expert paths to seed replay buffer with')\n\nflags.DEFINE_integer('internal_dim', 256, 'RNN internal dim')\nflags.DEFINE_integer('value_hidden_layers', 0,\n                     'number of hidden layers in value estimate')\nflags.DEFINE_integer('tf_seed', 42, 'random seed for tensorflow')\n\nflags.DEFINE_string('save_trajectories_dir', None,\n                    'directory to save trajectories to, if desired')\nflags.DEFINE_string('load_trajectories_file', None,\n                    'file to load expert trajectories from')\n\n supervisor flags\nflags.DEFINE_bool('supervisor', False, 'use supervisor training')\nflags.DEFINE_integer('task_id', 0, 'task id')\nflags.DEFINE_integer('ps_tasks', 0, 'number of ps tasks')\nflags.DEFINE_integer('num_replicas', 1, 'number of replicas used')\nflags.DEFINE_string('master', 'local', 'name of master')\nflags.DEFINE_string('save_dir', '', 'directory to save model to')\nflags.DEFINE_string('load_path', '', 'path of saved model to load (if none in save_dir)')\n\n\nclass Trainer(object):\n  \"\"\"Coordinates single or multi-replica training.\"\"\"\n\n  def __init__(self):\n    self.batch_size = FLAGS.batch_size\n    self.replay_batch_size = FLAGS.replay_batch_size\n    if self.replay_batch_size is None:\n      self.replay_batch_size = self.batch_size\n    self.num_samples = FLAGS.num_samples\n\n    self.env_str = FLAGS.env\n    self.env = gym_wrapper.GymWrapper(self.env_str,\n                                      distinct=FLAGS.batch_size // self.num_samples,\n                                      count=self.num_samples)\n    self.eval_env = gym_wrapper.GymWrapper(\n        self.env_str,\n        distinct=FLAGS.batch_size // self.num_samples,\n        count=self.num_samples)\n    self.env_spec = env_spec.EnvSpec(self.env.get_one())\n\n    self.max_step = FLAGS.max_step\n    self.cutoff_agent = FLAGS.cutoff_agent\n    self.num_steps = FLAGS.num_steps\n    self.validation_frequency = FLAGS.validation_frequency\n\n    self.target_network_lag = FLAGS.target_network_lag\n    self.sample_from = FLAGS.sample_from\n    assert self.sample_from in ['online', 'target']\n\n    self.critic_weight = FLAGS.critic_weight\n    self.objective = FLAGS.objective\n    self.trust_region_p = FLAGS.trust_region_p\n    self.value_opt = FLAGS.value_opt\n    assert not self.trust_region_p or self.objective in ['pcl', 'trpo']\n    assert self.objective != 'trpo' or self.trust_region_p\n    assert self.value_opt is None or self.value_opt == 'None' or \\\n        self.critic_weight == 0.0\n    self.max_divergence = FLAGS.max_divergence\n\n    self.learning_rate = FLAGS.learning_rate\n    self.clip_norm = FLAGS.clip_norm\n    self.clip_adv = FLAGS.clip_adv\n    self.tau = FLAGS.tau\n    self.tau_decay = FLAGS.tau_decay\n    self.tau_start = FLAGS.tau_start\n    self.eps_lambda = FLAGS.eps_lambda\n    self.update_eps_lambda = FLAGS.update_eps_lambda\n    self.gamma = FLAGS.gamma\n    self.rollout = FLAGS.rollout\n    self.use_target_values = FLAGS.use_target_values\n    self.fixed_std = FLAGS.fixed_std\n    self.input_prev_actions = FLAGS.input_prev_actions\n    self.recurrent = FLAGS.recurrent\n    assert not self.trust_region_p or not self.recurrent\n    self.input_time_step = FLAGS.input_time_step\n    assert not self.input_time_step or (self.cutoff_agent <= self.max_step)\n\n    self.use_online_batch = FLAGS.use_online_batch\n    self.batch_by_steps = FLAGS.batch_by_steps\n    self.unify_episodes = FLAGS.unify_episodes\n    if self.unify_episodes:\n      assert self.batch_size == 1\n\n    self.replay_buffer_size = FLAGS.replay_buffer_size\n    self.replay_buffer_alpha = FLAGS.replay_buffer_alpha\n    self.replay_buffer_freq = FLAGS.replay_buffer_freq\n    assert self.replay_buffer_freq in [-1, 0, 1]\n    self.eviction = FLAGS.eviction\n    self.prioritize_by = FLAGS.prioritize_by\n    assert self.prioritize_by in ['rewards', 'step']\n    self.num_expert_paths = FLAGS.num_expert_paths\n\n    self.internal_dim = FLAGS.internal_dim\n    self.value_hidden_layers = FLAGS.value_hidden_layers\n    self.tf_seed = FLAGS.tf_seed\n\n    self.save_trajectories_dir = FLAGS.save_trajectories_dir\n    self.save_trajectories_file = (\n        os.path.join(\n            self.save_trajectories_dir, self.env_str.replace('-', '_'))\n        if self.save_trajectories_dir else None)\n    self.load_trajectories_file = FLAGS.load_trajectories_file\n\n    self.hparams = dict((attr, getattr(self, attr))\n                        for attr in dir(self)\n                        if not attr.startswith('__') and\n                        not callable(getattr(self, attr)))\n\n  def hparams_string(self):\n    return '\\n'.join('%s: %s' % item for item in sorted(self.hparams.items()))\n\n  def get_objective(self):\n    tau = self.tau\n    if self.tau_decay is not None:\n      assert self.tau_start >= self.tau\n      tau = tf.maximum(\n          tf.train.exponential_decay(\n              self.tau_start, self.global_step, 100, self.tau_decay),\n          self.tau)\n\n    if self.objective in ['pcl', 'a3c', 'trpo', 'upcl']:\n      cls = (objective.PCL if self.objective in ['pcl', 'upcl'] else\n             objective.TRPO if self.objective == 'trpo' else\n             objective.ActorCritic)\n      policy_weight = 1.0\n\n      return cls(self.learning_rate,\n                 clip_norm=self.clip_norm,\n                 policy_weight=policy_weight,\n                 critic_weight=self.critic_weight,\n                 tau=tau, gamma=self.gamma, rollout=self.rollout,\n                 eps_lambda=self.eps_lambda, clip_adv=self.clip_adv,\n                 use_target_values=self.use_target_values)\n    elif self.objective in ['reinforce', 'urex']:\n      cls = (full_episode_objective.Reinforce\n             if self.objective == 'reinforce' else\n             full_episode_objective.UREX)\n      return cls(self.learning_rate,\n                 clip_norm=self.clip_norm,\n                 num_samples=self.num_samples,\n                 tau=tau, bonus_weight=1.0)   TODO: bonus weight?\n    else:\n      assert False, 'Unknown objective %s' % self.objective\n\n  def get_policy(self):\n    if self.recurrent:\n      cls = policy.Policy\n    else:\n      cls = policy.MLPPolicy\n    return cls(self.env_spec, self.internal_dim,\n               fixed_std=self.fixed_std,\n               recurrent=self.recurrent,\n               input_prev_actions=self.input_prev_actions)\n\n  def get_baseline(self):\n    cls = (baseline.UnifiedBaseline if self.objective == 'upcl' else\n           baseline.Baseline)\n    return cls(self.env_spec, self.internal_dim,\n               input_prev_actions=self.input_prev_actions,\n               input_time_step=self.input_time_step,\n               input_policy_state=self.recurrent,   may want to change this\n               n_hidden_layers=self.value_hidden_layers,\n               hidden_dim=self.internal_dim,\n               tau=self.tau)\n\n  def get_trust_region_p_opt(self):\n    if self.trust_region_p:\n      return trust_region.TrustRegionOptimization(\n          max_divergence=self.max_divergence)\n    else:\n      return None\n\n  def get_value_opt(self):\n    if self.value_opt == 'grad':\n      return optimizers.GradOptimization(\n          learning_rate=self.learning_rate, max_iter=5, mix_frac=0.05)\n    elif self.value_opt == 'lbfgs':\n      return optimizers.LbfgsOptimization(max_iter=25, mix_frac=0.1)\n    elif self.value_opt == 'best_fit':\n      return optimizers.BestFitOptimization(mix_frac=1.0)\n    else:\n      return None\n\n  def get_model(self):\n    cls = model.Model\n    return cls(self.env_spec, self.global_step,\n               target_network_lag=self.target_network_lag,\n               sample_from=self.sample_from,\n               get_policy=self.get_policy,\n               get_baseline=self.get_baseline,\n               get_objective=self.get_objective,\n               get_trust_region_p_opt=self.get_trust_region_p_opt,\n               get_value_opt=self.get_value_opt)\n\n  def get_replay_buffer(self):\n    if self.replay_buffer_freq <= 0:\n      return None\n    else:\n      assert self.objective in ['pcl', 'upcl'], 'Can\\'t use replay buffer with %s' % (\n          self.objective)\n    cls = replay_buffer.PrioritizedReplayBuffer\n    return cls(self.replay_buffer_size,\n               alpha=self.replay_buffer_alpha,\n               eviction_strategy=self.eviction)\n\n  def get_buffer_seeds(self):\n    return expert_paths.sample_expert_paths(\n        self.num_expert_paths, self.env_str, self.env_spec,\n        load_trajectories_file=self.load_trajectories_file)\n\n  def get_controller(self, env):\n    \"\"\"Get controller.\"\"\"\n    cls = controller.Controller\n    return cls(env, self.env_spec, self.internal_dim,\n               use_online_batch=self.use_online_batch,\n               batch_by_steps=self.batch_by_steps,\n               unify_episodes=self.unify_episodes,\n               replay_batch_size=self.replay_batch_size,\n               max_step=self.max_step,\n               cutoff_agent=self.cutoff_agent,\n               save_trajectories_file=self.save_trajectories_file,\n               use_trust_region=self.trust_region_p,\n               use_value_opt=self.value_opt not in [None, 'None'],\n               update_eps_lambda=self.update_eps_lambda,\n               prioritize_by=self.prioritize_by,\n               get_model=self.get_model,\n               get_replay_buffer=self.get_replay_buffer,\n               get_buffer_seeds=self.get_buffer_seeds)\n\n  def do_before_step(self, step):\n    pass\n\n  def run(self):\n    \"\"\"Run training.\"\"\"\n    is_chief = FLAGS.task_id == 0 or not FLAGS.supervisor\n    sv = None\n\n    def init_fn(sess, saver):\n      ckpt = None\n      if FLAGS.save_dir and sv is None:\n        load_dir = FLAGS.save_dir\n        ckpt = tf.train.get_checkpoint_state(load_dir)\n      if ckpt and ckpt.model_checkpoint_path:\n        logging.info('restoring from %s', ckpt.model_checkpoint_path)\n        saver.restore(sess, ckpt.model_checkpoint_path)\n      elif FLAGS.load_path:\n        logging.info('restoring from %s', FLAGS.load_path)\n        saver.restore(sess, FLAGS.load_path)\n\n    if FLAGS.supervisor:\n      with tf.device(tf.ReplicaDeviceSetter(FLAGS.ps_tasks, merge_devices=True)):\n        self.global_step = tf.contrib.framework.get_or_create_global_step()\n        tf.set_random_seed(FLAGS.tf_seed)\n        self.controller = self.get_controller(self.env)\n        self.model = self.controller.model\n        self.controller.setup()\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n          self.eval_controller = self.get_controller(self.eval_env)\n          self.eval_controller.setup(train=False)\n\n        saver = tf.train.Saver(max_to_keep=10)\n        step = self.model.global_step\n        sv = tf.Supervisor(logdir=FLAGS.save_dir,\n                           is_chief=is_chief,\n                           saver=saver,\n                           save_model_secs=600,\n                           summary_op=None,   we define it ourselves\n                           save_summaries_secs=60,\n                           global_step=step,\n                           init_fn=lambda sess: init_fn(sess, saver))\n        sess = sv.PrepareSession(FLAGS.master)\n    else:\n      tf.set_random_seed(FLAGS.tf_seed)\n      self.global_step = tf.contrib.framework.get_or_create_global_step()\n      self.controller = self.get_controller(self.env)\n      self.model = self.controller.model\n      self.controller.setup()\n      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        self.eval_controller = self.get_controller(self.eval_env)\n        self.eval_controller.setup(train=False)\n\n      saver = tf.train.Saver(max_to_keep=10)\n      sess = tf.Session()\n      sess.run(tf.initialize_all_variables())\n      init_fn(sess, saver)\n\n    self.sv = sv\n    self.sess = sess\n\n    logging.info('hparams:\\n%s', self.hparams_string())\n\n    model_step = sess.run(self.model.global_step)\n    if model_step >= self.num_steps:\n      logging.info('training has reached final step')\n      return\n\n    losses = []\n    rewards = []\n    all_ep_rewards = []\n    for step in xrange(1 + self.num_steps):\n\n      if sv is not None and sv.ShouldStop():\n        logging.info('stopping supervisor')\n        break\n\n      self.do_before_step(step)\n\n      (loss, summary,\n       total_rewards, episode_rewards) = self.controller.train(sess)\n      _, greedy_episode_rewards = self.eval_controller.eval(sess)\n      self.controller.greedy_episode_rewards = greedy_episode_rewards\n      losses.append(loss)\n      rewards.append(total_rewards)\n      all_ep_rewards.extend(episode_rewards)\n\n      if (random.random() < 0.1 and summary and episode_rewards and\n          is_chief and sv and sv._summary_writer):\n        sv.summary_computed(sess, summary)\n\n      model_step = sess.run(self.model.global_step)\n      if is_chief and step % self.validation_frequency == 0:\n        logging.info('at training step %d, model step %d: '\n                     'avg loss %f, avg reward %f, '\n                     'episode rewards: %f, greedy rewards: %f',\n                     step, model_step,\n                     np.mean(losses), np.mean(rewards),\n                     np.mean(all_ep_rewards),\n                     np.mean(greedy_episode_rewards))\n\n        losses = []\n        rewards = []\n        all_ep_rewards = []\n\n      if model_step >= self.num_steps:\n        logging.info('training has reached final step')\n        break\n\n    if is_chief and sv is not None:\n      logging.info('saving final model to %s', sv.save_path)\n      sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  trainer = Trainer()\n  trainer.run()\n\n\nif __name__ == '__main__':\n  app.run()\n", "comments": "   trainer coordinating single multi replica training   main point entry running models   specifies parameters used different algorithms       import tensorflow tf import numpy np import random import os import pickle  six moves import xrange import controller import model import policy import baseline import objective import full episode objective import trust region import optimizers import replay buffer import expert paths import gym wrapper import env spec  app   tf app flags   tf flags logging   tf logging gfile   tf gfile  flags   flags flags  flags define string( env    copy v0    environment name ) flags define integer( batch size   100   batch size ) flags define integer( replay batch size   none   replay batch size  defaults batch size ) flags define integer( num samples   1                        number samples random seed initialization ) flags define integer( max step   200   max number steps train ) flags define integer( cutoff agent   0                        number steps cut agent                          defaults always cutoff ) flags define integer( num steps   100000   number training steps ) flags define integer( validation frequency   100                        every many steps  output stats )  flags define float( target network lag   0 95                      this exponential decay online network yields target                       network ) flags define string( sample    online                        sample actions  online  network  target  network )  flags define string( objective    pcl                        pcl upcl a3c trpo reinforce urex ) flags define bool( trust region p   false                     use trust region policy optimization ) flags define string( value opt   none                       leave none optimize along policy                        (using critic weight)  otherwise set                         best fit  (least squares regression)   lbfgs    grad  ) flags define float( max divergence   0 01                      max divergence (i e  kl) allow                       trust region optimization )  flags define float( learning rate   0 01   learning rate ) flags define float( clip norm   5 0   clip norm ) flags define float( clip adv   0 0   clip advantages value                         leave 0 clip  ) flags define float( critic weight   0 1   critic weight ) flags define float( tau   0 1   entropy regularizer                       if using decaying tau  final value  ) flags define float( tau decay   none                      decay tau much every 100 steps ) flags define float( tau start   0 1                      start tau value ) flags define float( eps lambda   0 0   relative entropy regularizer  ) flags define bool( update eps lambda   false                     update lambda automatically based last 100 episodes  ) flags define float( gamma   1 0   discount ) flags define integer( rollout   10   rollout ) flags define bool( use target values   false                     use target network value estimates ) flags define bool( fixed std   true                     fix std gaussian distributions ) flags define bool( input prev actions   true                     input previous actions policy network ) flags define bool( recurrent   true                     use recurrent connections ) flags define bool( input time step   false                     input time step value calucations )  flags define bool( use online batch   true   train batches sampled ) flags define bool( batch steps   false                     ensure training batch batch size   max step steps ) flags define bool( unify episodes   false                     make sure replay buffer holds entire episodes                       even across distinct sampling steps ) flags define integer( replay buffer size   5000   replay buffer size ) flags define float( replay buffer alpha   0 5   replay buffer alpha param ) flags define integer( replay buffer freq   0                        replay buffer frequency (only supports  1 0 1) ) flags define string( eviction    rand                        evict replay buffer  rand rank fifo ) flags define string( prioritize    rewards                        prioritize replay buffer  rewards   step  ) flags define integer( num expert paths   0                        number expert paths seed replay buffer )  flags define integer( internal dim   256   rnn internal dim ) flags define integer( value hidden layers   0                        number hidden layers value estimate ) flags define integer( tf seed   42   random seed tensorflow )  flags define string( save trajectories dir   none                       directory save trajectories  desired ) flags define string( load trajectories file   none                       file load expert trajectories )    supervisor flags flags define bool( supervisor   false   use supervisor training ) flags define integer( task id   0   task id ) flags define integer( ps tasks   0   number ps tasks ) flags define integer( num replicas   1   number replicas used ) flags define string( master    local    name master ) flags define string( save dir        directory save model ) flags define string( load path        path saved model load (if none save dir) )   class trainer(object)       coordinates single multi replica training        def   init  (self)      self batch size   flags batch size     self replay batch size   flags replay batch size     self replay batch size none        self replay batch size   self batch size     self num samples   flags num samples      self env str   flags env     self env   gym wrapper gymwrapper(self env str                                        distinct flags batch size    self num samples                                        count self num samples)     self eval env   gym wrapper gymwrapper(         self env str          distinct flags batch size    self num samples          count self num samples)     self env spec   env spec envspec(self env get one())      self max step   flags max step     self cutoff agent   flags cutoff agent     self num steps   flags num steps     self validation frequency   flags validation frequency      self target network lag   flags target network lag     self sample   flags sample     assert self sample   online    target        self critic weight   flags critic weight     self objective   flags objective     self trust region p   flags trust region p     self value opt   flags value opt     assert self trust region p self objective   pcl    trpo       assert self objective     trpo  self trust region p     assert self value opt none self value opt     none            self critic weight    0 0     self max divergence   flags max divergence      self learning rate   flags learning rate     self clip norm   flags clip norm     self clip adv   flags clip adv     self tau   flags tau     self tau decay   flags tau decay     self tau start   flags tau start     self eps lambda   flags eps lambda     self update eps lambda   flags update eps lambda     self gamma   flags gamma     self rollout   flags rollout     self use target values   flags use target values     self fixed std   flags fixed std     self input prev actions   flags input prev actions     self recurrent   flags recurrent     assert self trust region p self recurrent     self input time step   flags input time step     assert self input time step (self cutoff agent    self max step)      self use online batch   flags use online batch     self batch steps   flags batch steps     self unify episodes   flags unify episodes     self unify episodes        assert self batch size    1      self replay buffer size   flags replay buffer size     self replay buffer alpha   flags replay buffer alpha     self replay buffer freq   flags replay buffer freq     assert self replay buffer freq   1  0  1      self eviction   flags eviction     self prioritize   flags prioritize     assert self prioritize   rewards    step       self num expert paths   flags num expert paths      self internal dim   flags internal dim     self value hidden layers   flags value hidden layers     self tf seed   flags tf seed      self save trajectories dir   flags save trajectories dir     self save trajectories file   (         os path join(             self save trajectories dir  self env str replace(        ))         self save trajectories dir else none)     self load trajectories file   flags load trajectories file      self hparams   dict((attr  getattr(self  attr))                         attr dir(self)                         attr startswith(    )                         callable(getattr(self  attr)))    def hparams string(self)      return   n  join(       item item sorted(self hparams items()))    def get objective(self)      tau   self tau     self tau decay none        assert self tau start    self tau       tau   tf maximum(           tf train exponential decay(               self tau start  self global step  100  self tau decay)            self tau)      self objective   pcl    a3c    trpo    upcl          cls   (objective pcl self objective   pcl    upcl   else              objective trpo self objective     trpo  else              objective actorcritic)       policy weight   1 0        return cls(self learning rate                   clip norm self clip norm                   policy weight policy weight                   critic weight self critic weight                   tau tau  gamma self gamma  rollout self rollout                   eps lambda self eps lambda  clip adv self clip adv                   use target values self use target values)     elif self objective   reinforce    urex          cls   (full episode objective reinforce              self objective     reinforce  else              full episode objective urex)       return cls(self learning rate                   clip norm self clip norm                   num samples self num samples                   tau tau  bonus weight 1 0)    todo  bonus weight      else        assert false   unknown objective     self objective    def get policy(self)      self recurrent        cls   policy policy     else        cls   policy mlppolicy     return cls(self env spec  self internal dim                 fixed std self fixed std                 recurrent self recurrent                 input prev actions self input prev actions)    def get baseline(self)      cls   (baseline unifiedbaseline self objective     upcl  else            baseline baseline)     return cls(self env spec  self internal dim                 input prev actions self input prev actions                 input time step self input time step                 input policy state self recurrent     may want change                n hidden layers self value hidden layers                 hidden dim self internal dim                 tau self tau)    def get trust region p opt(self)      self trust region p        return trust region trustregionoptimization(           max divergence self max divergence)     else        return none    def get value opt(self)      self value opt     grad         return optimizers gradoptimization(           learning rate self learning rate  max iter 5  mix frac 0 05)     elif self value opt     lbfgs         return optimizers lbfgsoptimization(max iter 25  mix frac 0 1)     elif self value opt     best fit         return optimizers bestfitoptimization(mix frac 1 0)     else        return none    def get model(self)      cls   model model     return cls(self env spec  self global step                 target network lag self target network lag                 sample self sample                 get policy self get policy                 get baseline self get baseline                 get objective self get objective                 get trust region p opt self get trust region p opt                 get value opt self get value opt)    def get replay buffer(self)      self replay buffer freq    0        return none     else        assert self objective   pcl    upcl     can  use replay buffer     (           self objective)     cls   replay buffer prioritizedreplaybuffer     return cls(self replay buffer size                 alpha self replay buffer alpha                 eviction strategy self eviction)    def get buffer seeds(self)      return expert paths sample expert paths(         self num expert paths  self env str  self env spec          load trajectories file self load trajectories file)    def get controller(self  env)         get controller         cls   controller controller     return cls(env  self env spec  self internal dim                 use online batch self use online batch                 batch steps self batch steps                 unify episodes self unify episodes                 replay batch size self replay batch size                 max step self max step                 cutoff agent self cutoff agent                 save trajectories file self save trajectories file                 use trust region self trust region p                 use value opt self value opt  none   none                   update eps lambda self update eps lambda                 prioritize self prioritize                 get model self get model                 get replay buffer self get replay buffer                 get buffer seeds self get buffer seeds)    def step(self  step)      pass    def run(self)         run training        copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       supervisor flags    todo  bonus weight     may want change    define ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Trainer for coordinating single or multi-replica training.\n\nMain point of entry for running models.  Specifies most of\nthe parameters used by different algorithms.\n\"\"\"\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport os\nimport pickle\n\nfrom six.moves import xrange\nimport controller\nimport model\nimport policy\nimport baseline\nimport objective\nimport full_episode_objective\nimport trust_region\nimport optimizers\nimport replay_buffer\nimport expert_paths\nimport gym_wrapper\nimport env_spec\n\napp = tf.app\nflags = tf.flags\nlogging = tf.logging\ngfile = tf.gfile\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('env', 'Copy-v0', 'environment name')\nflags.DEFINE_integer('batch_size', 100, 'batch size')\nflags.DEFINE_integer('replay_batch_size', None, 'replay batch size; defaults to batch_size')\nflags.DEFINE_integer('num_samples', 1,\n                     'number of samples from each random seed initialization')\nflags.DEFINE_integer('max_step', 200, 'max number of steps to train on')\nflags.DEFINE_integer('cutoff_agent', 0,\n                     'number of steps at which to cut-off agent. '\n                     'Defaults to always cutoff')\nflags.DEFINE_integer('num_steps', 100000, 'number of training steps')\nflags.DEFINE_integer('validation_frequency', 100,\n                     'every so many steps, output some stats')\n\nflags.DEFINE_float('target_network_lag', 0.95,\n                   'This exponential decay on online network yields target '\n                   'network')\nflags.DEFINE_string('sample_from', 'online',\n                    'Sample actions from \"online\" network or \"target\" network')\n\nflags.DEFINE_string('objective', 'pcl',\n                    'pcl/upcl/a3c/trpo/reinforce/urex')\nflags.DEFINE_bool('trust_region_p', False,\n                  'use trust region for policy optimization')\nflags.DEFINE_string('value_opt', None,\n                    'leave as None to optimize it along with policy '\n                    '(using critic_weight). Otherwise set to '\n                    '\"best_fit\" (least squares regression), \"lbfgs\", or \"grad\"')\nflags.DEFINE_float('max_divergence', 0.01,\n                   'max divergence (i.e. KL) to allow during '\n                   'trust region optimization')\n\nflags.DEFINE_float('learning_rate', 0.01, 'learning rate')\nflags.DEFINE_float('clip_norm', 5.0, 'clip norm')\nflags.DEFINE_float('clip_adv', 0.0, 'Clip advantages at this value.  '\n                   'Leave as 0 to not clip at all.')\nflags.DEFINE_float('critic_weight', 0.1, 'critic weight')\nflags.DEFINE_float('tau', 0.1, 'entropy regularizer.'\n                   'If using decaying tau, this is the final value.')\nflags.DEFINE_float('tau_decay', None,\n                   'decay tau by this much every 100 steps')\nflags.DEFINE_float('tau_start', 0.1,\n                   'start tau at this value')\nflags.DEFINE_float('eps_lambda', 0.0, 'relative entropy regularizer.')\nflags.DEFINE_bool('update_eps_lambda', False,\n                  'Update lambda automatically based on last 100 episodes.')\nflags.DEFINE_float('gamma', 1.0, 'discount')\nflags.DEFINE_integer('rollout', 10, 'rollout')\nflags.DEFINE_bool('use_target_values', False,\n                  'use target network for value estimates')\nflags.DEFINE_bool('fixed_std', True,\n                  'fix the std in Gaussian distributions')\nflags.DEFINE_bool('input_prev_actions', True,\n                  'input previous actions to policy network')\nflags.DEFINE_bool('recurrent', True,\n                  'use recurrent connections')\nflags.DEFINE_bool('input_time_step', False,\n                  'input time step into value calucations')\n\nflags.DEFINE_bool('use_online_batch', True, 'train on batches as they are sampled')\nflags.DEFINE_bool('batch_by_steps', False,\n                  'ensure each training batch has batch_size * max_step steps')\nflags.DEFINE_bool('unify_episodes', False,\n                  'Make sure replay buffer holds entire episodes, '\n                  'even across distinct sampling steps')\nflags.DEFINE_integer('replay_buffer_size', 5000, 'replay buffer size')\nflags.DEFINE_float('replay_buffer_alpha', 0.5, 'replay buffer alpha param')\nflags.DEFINE_integer('replay_buffer_freq', 0,\n                     'replay buffer frequency (only supports -1/0/1)')\nflags.DEFINE_string('eviction', 'rand',\n                    'how to evict from replay buffer: rand/rank/fifo')\nflags.DEFINE_string('prioritize_by', 'rewards',\n                    'Prioritize replay buffer by \"rewards\" or \"step\"')\nflags.DEFINE_integer('num_expert_paths', 0,\n                     'number of expert paths to seed replay buffer with')\n\nflags.DEFINE_integer('internal_dim', 256, 'RNN internal dim')\nflags.DEFINE_integer('value_hidden_layers', 0,\n                     'number of hidden layers in value estimate')\nflags.DEFINE_integer('tf_seed', 42, 'random seed for tensorflow')\n\nflags.DEFINE_string('save_trajectories_dir', None,\n                    'directory to save trajectories to, if desired')\nflags.DEFINE_string('load_trajectories_file', None,\n                    'file to load expert trajectories from')\n\n# supervisor flags\nflags.DEFINE_bool('supervisor', False, 'use supervisor training')\nflags.DEFINE_integer('task_id', 0, 'task id')\nflags.DEFINE_integer('ps_tasks', 0, 'number of ps tasks')\nflags.DEFINE_integer('num_replicas', 1, 'number of replicas used')\nflags.DEFINE_string('master', 'local', 'name of master')\nflags.DEFINE_string('save_dir', '', 'directory to save model to')\nflags.DEFINE_string('load_path', '', 'path of saved model to load (if none in save_dir)')\n\n\nclass Trainer(object):\n  \"\"\"Coordinates single or multi-replica training.\"\"\"\n\n  def __init__(self):\n    self.batch_size = FLAGS.batch_size\n    self.replay_batch_size = FLAGS.replay_batch_size\n    if self.replay_batch_size is None:\n      self.replay_batch_size = self.batch_size\n    self.num_samples = FLAGS.num_samples\n\n    self.env_str = FLAGS.env\n    self.env = gym_wrapper.GymWrapper(self.env_str,\n                                      distinct=FLAGS.batch_size // self.num_samples,\n                                      count=self.num_samples)\n    self.eval_env = gym_wrapper.GymWrapper(\n        self.env_str,\n        distinct=FLAGS.batch_size // self.num_samples,\n        count=self.num_samples)\n    self.env_spec = env_spec.EnvSpec(self.env.get_one())\n\n    self.max_step = FLAGS.max_step\n    self.cutoff_agent = FLAGS.cutoff_agent\n    self.num_steps = FLAGS.num_steps\n    self.validation_frequency = FLAGS.validation_frequency\n\n    self.target_network_lag = FLAGS.target_network_lag\n    self.sample_from = FLAGS.sample_from\n    assert self.sample_from in ['online', 'target']\n\n    self.critic_weight = FLAGS.critic_weight\n    self.objective = FLAGS.objective\n    self.trust_region_p = FLAGS.trust_region_p\n    self.value_opt = FLAGS.value_opt\n    assert not self.trust_region_p or self.objective in ['pcl', 'trpo']\n    assert self.objective != 'trpo' or self.trust_region_p\n    assert self.value_opt is None or self.value_opt == 'None' or \\\n        self.critic_weight == 0.0\n    self.max_divergence = FLAGS.max_divergence\n\n    self.learning_rate = FLAGS.learning_rate\n    self.clip_norm = FLAGS.clip_norm\n    self.clip_adv = FLAGS.clip_adv\n    self.tau = FLAGS.tau\n    self.tau_decay = FLAGS.tau_decay\n    self.tau_start = FLAGS.tau_start\n    self.eps_lambda = FLAGS.eps_lambda\n    self.update_eps_lambda = FLAGS.update_eps_lambda\n    self.gamma = FLAGS.gamma\n    self.rollout = FLAGS.rollout\n    self.use_target_values = FLAGS.use_target_values\n    self.fixed_std = FLAGS.fixed_std\n    self.input_prev_actions = FLAGS.input_prev_actions\n    self.recurrent = FLAGS.recurrent\n    assert not self.trust_region_p or not self.recurrent\n    self.input_time_step = FLAGS.input_time_step\n    assert not self.input_time_step or (self.cutoff_agent <= self.max_step)\n\n    self.use_online_batch = FLAGS.use_online_batch\n    self.batch_by_steps = FLAGS.batch_by_steps\n    self.unify_episodes = FLAGS.unify_episodes\n    if self.unify_episodes:\n      assert self.batch_size == 1\n\n    self.replay_buffer_size = FLAGS.replay_buffer_size\n    self.replay_buffer_alpha = FLAGS.replay_buffer_alpha\n    self.replay_buffer_freq = FLAGS.replay_buffer_freq\n    assert self.replay_buffer_freq in [-1, 0, 1]\n    self.eviction = FLAGS.eviction\n    self.prioritize_by = FLAGS.prioritize_by\n    assert self.prioritize_by in ['rewards', 'step']\n    self.num_expert_paths = FLAGS.num_expert_paths\n\n    self.internal_dim = FLAGS.internal_dim\n    self.value_hidden_layers = FLAGS.value_hidden_layers\n    self.tf_seed = FLAGS.tf_seed\n\n    self.save_trajectories_dir = FLAGS.save_trajectories_dir\n    self.save_trajectories_file = (\n        os.path.join(\n            self.save_trajectories_dir, self.env_str.replace('-', '_'))\n        if self.save_trajectories_dir else None)\n    self.load_trajectories_file = FLAGS.load_trajectories_file\n\n    self.hparams = dict((attr, getattr(self, attr))\n                        for attr in dir(self)\n                        if not attr.startswith('__') and\n                        not callable(getattr(self, attr)))\n\n  def hparams_string(self):\n    return '\\n'.join('%s: %s' % item for item in sorted(self.hparams.items()))\n\n  def get_objective(self):\n    tau = self.tau\n    if self.tau_decay is not None:\n      assert self.tau_start >= self.tau\n      tau = tf.maximum(\n          tf.train.exponential_decay(\n              self.tau_start, self.global_step, 100, self.tau_decay),\n          self.tau)\n\n    if self.objective in ['pcl', 'a3c', 'trpo', 'upcl']:\n      cls = (objective.PCL if self.objective in ['pcl', 'upcl'] else\n             objective.TRPO if self.objective == 'trpo' else\n             objective.ActorCritic)\n      policy_weight = 1.0\n\n      return cls(self.learning_rate,\n                 clip_norm=self.clip_norm,\n                 policy_weight=policy_weight,\n                 critic_weight=self.critic_weight,\n                 tau=tau, gamma=self.gamma, rollout=self.rollout,\n                 eps_lambda=self.eps_lambda, clip_adv=self.clip_adv,\n                 use_target_values=self.use_target_values)\n    elif self.objective in ['reinforce', 'urex']:\n      cls = (full_episode_objective.Reinforce\n             if self.objective == 'reinforce' else\n             full_episode_objective.UREX)\n      return cls(self.learning_rate,\n                 clip_norm=self.clip_norm,\n                 num_samples=self.num_samples,\n                 tau=tau, bonus_weight=1.0)  # TODO: bonus weight?\n    else:\n      assert False, 'Unknown objective %s' % self.objective\n\n  def get_policy(self):\n    if self.recurrent:\n      cls = policy.Policy\n    else:\n      cls = policy.MLPPolicy\n    return cls(self.env_spec, self.internal_dim,\n               fixed_std=self.fixed_std,\n               recurrent=self.recurrent,\n               input_prev_actions=self.input_prev_actions)\n\n  def get_baseline(self):\n    cls = (baseline.UnifiedBaseline if self.objective == 'upcl' else\n           baseline.Baseline)\n    return cls(self.env_spec, self.internal_dim,\n               input_prev_actions=self.input_prev_actions,\n               input_time_step=self.input_time_step,\n               input_policy_state=self.recurrent,  # may want to change this\n               n_hidden_layers=self.value_hidden_layers,\n               hidden_dim=self.internal_dim,\n               tau=self.tau)\n\n  def get_trust_region_p_opt(self):\n    if self.trust_region_p:\n      return trust_region.TrustRegionOptimization(\n          max_divergence=self.max_divergence)\n    else:\n      return None\n\n  def get_value_opt(self):\n    if self.value_opt == 'grad':\n      return optimizers.GradOptimization(\n          learning_rate=self.learning_rate, max_iter=5, mix_frac=0.05)\n    elif self.value_opt == 'lbfgs':\n      return optimizers.LbfgsOptimization(max_iter=25, mix_frac=0.1)\n    elif self.value_opt == 'best_fit':\n      return optimizers.BestFitOptimization(mix_frac=1.0)\n    else:\n      return None\n\n  def get_model(self):\n    cls = model.Model\n    return cls(self.env_spec, self.global_step,\n               target_network_lag=self.target_network_lag,\n               sample_from=self.sample_from,\n               get_policy=self.get_policy,\n               get_baseline=self.get_baseline,\n               get_objective=self.get_objective,\n               get_trust_region_p_opt=self.get_trust_region_p_opt,\n               get_value_opt=self.get_value_opt)\n\n  def get_replay_buffer(self):\n    if self.replay_buffer_freq <= 0:\n      return None\n    else:\n      assert self.objective in ['pcl', 'upcl'], 'Can\\'t use replay buffer with %s' % (\n          self.objective)\n    cls = replay_buffer.PrioritizedReplayBuffer\n    return cls(self.replay_buffer_size,\n               alpha=self.replay_buffer_alpha,\n               eviction_strategy=self.eviction)\n\n  def get_buffer_seeds(self):\n    return expert_paths.sample_expert_paths(\n        self.num_expert_paths, self.env_str, self.env_spec,\n        load_trajectories_file=self.load_trajectories_file)\n\n  def get_controller(self, env):\n    \"\"\"Get controller.\"\"\"\n    cls = controller.Controller\n    return cls(env, self.env_spec, self.internal_dim,\n               use_online_batch=self.use_online_batch,\n               batch_by_steps=self.batch_by_steps,\n               unify_episodes=self.unify_episodes,\n               replay_batch_size=self.replay_batch_size,\n               max_step=self.max_step,\n               cutoff_agent=self.cutoff_agent,\n               save_trajectories_file=self.save_trajectories_file,\n               use_trust_region=self.trust_region_p,\n               use_value_opt=self.value_opt not in [None, 'None'],\n               update_eps_lambda=self.update_eps_lambda,\n               prioritize_by=self.prioritize_by,\n               get_model=self.get_model,\n               get_replay_buffer=self.get_replay_buffer,\n               get_buffer_seeds=self.get_buffer_seeds)\n\n  def do_before_step(self, step):\n    pass\n\n  def run(self):\n    \"\"\"Run training.\"\"\"\n    is_chief = FLAGS.task_id == 0 or not FLAGS.supervisor\n    sv = None\n\n    def init_fn(sess, saver):\n      ckpt = None\n      if FLAGS.save_dir and sv is None:\n        load_dir = FLAGS.save_dir\n        ckpt = tf.train.get_checkpoint_state(load_dir)\n      if ckpt and ckpt.model_checkpoint_path:\n        logging.info('restoring from %s', ckpt.model_checkpoint_path)\n        saver.restore(sess, ckpt.model_checkpoint_path)\n      elif FLAGS.load_path:\n        logging.info('restoring from %s', FLAGS.load_path)\n        saver.restore(sess, FLAGS.load_path)\n\n    if FLAGS.supervisor:\n      with tf.device(tf.ReplicaDeviceSetter(FLAGS.ps_tasks, merge_devices=True)):\n        self.global_step = tf.contrib.framework.get_or_create_global_step()\n        tf.set_random_seed(FLAGS.tf_seed)\n        self.controller = self.get_controller(self.env)\n        self.model = self.controller.model\n        self.controller.setup()\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n          self.eval_controller = self.get_controller(self.eval_env)\n          self.eval_controller.setup(train=False)\n\n        saver = tf.train.Saver(max_to_keep=10)\n        step = self.model.global_step\n        sv = tf.Supervisor(logdir=FLAGS.save_dir,\n                           is_chief=is_chief,\n                           saver=saver,\n                           save_model_secs=600,\n                           summary_op=None,  # we define it ourselves\n                           save_summaries_secs=60,\n                           global_step=step,\n                           init_fn=lambda sess: init_fn(sess, saver))\n        sess = sv.PrepareSession(FLAGS.master)\n    else:\n      tf.set_random_seed(FLAGS.tf_seed)\n      self.global_step = tf.contrib.framework.get_or_create_global_step()\n      self.controller = self.get_controller(self.env)\n      self.model = self.controller.model\n      self.controller.setup()\n      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        self.eval_controller = self.get_controller(self.eval_env)\n        self.eval_controller.setup(train=False)\n\n      saver = tf.train.Saver(max_to_keep=10)\n      sess = tf.Session()\n      sess.run(tf.initialize_all_variables())\n      init_fn(sess, saver)\n\n    self.sv = sv\n    self.sess = sess\n\n    logging.info('hparams:\\n%s', self.hparams_string())\n\n    model_step = sess.run(self.model.global_step)\n    if model_step >= self.num_steps:\n      logging.info('training has reached final step')\n      return\n\n    losses = []\n    rewards = []\n    all_ep_rewards = []\n    for step in xrange(1 + self.num_steps):\n\n      if sv is not None and sv.ShouldStop():\n        logging.info('stopping supervisor')\n        break\n\n      self.do_before_step(step)\n\n      (loss, summary,\n       total_rewards, episode_rewards) = self.controller.train(sess)\n      _, greedy_episode_rewards = self.eval_controller.eval(sess)\n      self.controller.greedy_episode_rewards = greedy_episode_rewards\n      losses.append(loss)\n      rewards.append(total_rewards)\n      all_ep_rewards.extend(episode_rewards)\n\n      if (random.random() < 0.1 and summary and episode_rewards and\n          is_chief and sv and sv._summary_writer):\n        sv.summary_computed(sess, summary)\n\n      model_step = sess.run(self.model.global_step)\n      if is_chief and step % self.validation_frequency == 0:\n        logging.info('at training step %d, model step %d: '\n                     'avg loss %f, avg reward %f, '\n                     'episode rewards: %f, greedy rewards: %f',\n                     step, model_step,\n                     np.mean(losses), np.mean(rewards),\n                     np.mean(all_ep_rewards),\n                     np.mean(greedy_episode_rewards))\n\n        losses = []\n        rewards = []\n        all_ep_rewards = []\n\n      if model_step >= self.num_steps:\n        logging.info('training has reached final step')\n        break\n\n    if is_chief and sv is not None:\n      logging.info('saving final model to %s', sv.save_path)\n      sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  trainer = Trainer()\n  trainer.run()\n\n\nif __name__ == '__main__':\n  app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "trainer.py", "id": "d3b9ee8b28e5cb79b776f809764da2c0", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/pcl_rl/trainer.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}