{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Evaluate the model.\n\nThis script should be run concurrently with training so that summaries show up\nin TensorBoard.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os.path\nimport time\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import show_and_tell_model\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(\"input_file_pattern\", \"\",\n                       \"File pattern of sharded TFRecord input files.\")\ntf.flags.DEFINE_string(\"checkpoint_dir\", \"\",\n                       \"Directory containing model checkpoints.\")\ntf.flags.DEFINE_string(\"eval_dir\", \"\", \"Directory to write event logs.\")\n\ntf.flags.DEFINE_integer(\"eval_interval_secs\", 600,\n                        \"Interval between evaluation runs.\")\ntf.flags.DEFINE_integer(\"num_eval_examples\", 10132,\n                        \"Number of examples for evaluation.\")\n\ntf.flags.DEFINE_integer(\"min_global_step\", 5000,\n                        \"Minimum global step to run evaluation.\")\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef evaluate_model(sess, model, global_step, summary_writer, summary_op):\n  \"\"\"Computes perplexity-per-word over the evaluation dataset.\n\n  Summaries and perplexity-per-word are written out to the eval directory.\n\n  Args:\n    sess: Session object.\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    global_step: Integer; global step of the model checkpoint.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"\n   Log model summaries on a single batch.\n  summary_str = sess.run(summary_op)\n  summary_writer.add_summary(summary_str, global_step)\n\n   Compute perplexity over the entire dataset.\n  num_eval_batches = int(\n      math.ceil(FLAGS.num_eval_examples / model.config.batch_size))\n\n  start_time = time.time()\n  sum_losses = 0.\n  sum_weights = 0.\n  for i in range(num_eval_batches):\n    cross_entropy_losses, weights = sess.run([\n        model.target_cross_entropy_losses,\n        model.target_cross_entropy_loss_weights\n    ])\n    sum_losses += np.sum(cross_entropy_losses * weights)\n    sum_weights += np.sum(weights)\n    if not i % 100:\n      tf.logging.info(\"Computed losses for %d of %d batches.\", i + 1,\n                      num_eval_batches)\n  eval_time = time.time() - start_time\n\n  perplexity = math.exp(sum_losses / sum_weights)\n  tf.logging.info(\"Perplexity = %f (%.2g sec)\", perplexity, eval_time)\n\n   Log perplexity to the FileWriter.\n  summary = tf.Summary()\n  value = summary.value.add()\n  value.simple_value = perplexity\n  value.tag = \"Perplexity\"\n  summary_writer.add_summary(summary, global_step)\n\n   Write the Events file to the eval directory.\n  summary_writer.flush()\n  tf.logging.info(\"Finished processing evaluation at global step %d.\",\n                  global_step)\n\n\ndef run_once(model, saver, summary_writer, summary_op):\n  \"\"\"Evaluates the latest model checkpoint.\n\n  Args:\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    saver: Instance of tf.train.Saver for restoring model Variables.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"\n  model_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n  if not model_path:\n    tf.logging.info(\"Skipping evaluation. No checkpoint found in: %s\",\n                    FLAGS.checkpoint_dir)\n    return\n\n  with tf.Session() as sess:\n     Load model from checkpoint.\n    tf.logging.info(\"Loading model from checkpoint: %s\", model_path)\n    saver.restore(sess, model_path)\n    global_step = tf.train.global_step(sess, model.global_step.name)\n    tf.logging.info(\"Successfully loaded %s at global step = %d.\",\n                    os.path.basename(model_path), global_step)\n    if global_step < FLAGS.min_global_step:\n      tf.logging.info(\"Skipping evaluation. Global step = %d < %d\", global_step,\n                      FLAGS.min_global_step)\n      return\n\n     Start the queue runners.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n     Run evaluation on the latest checkpoint.\n    try:\n      evaluate_model(\n          sess=sess,\n          model=model,\n          global_step=global_step,\n          summary_writer=summary_writer,\n          summary_op=summary_op)\n    except Exception as e:   pylint: disable=broad-except\n      tf.logging.error(\"Evaluation failed.\")\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef run():\n  \"\"\"Runs evaluation in a loop, and logs summaries to TensorBoard.\"\"\"\n   Create the evaluation directory if it doesn't exist.\n  eval_dir = FLAGS.eval_dir\n  if not tf.gfile.IsDirectory(eval_dir):\n    tf.logging.info(\"Creating eval directory: %s\", eval_dir)\n    tf.gfile.MakeDirs(eval_dir)\n\n  g = tf.Graph()\n  with g.as_default():\n     Build the model for evaluation.\n    model_config = configuration.ModelConfig()\n    model_config.input_file_pattern = FLAGS.input_file_pattern\n    model = show_and_tell_model.ShowAndTellModel(model_config, mode=\"eval\")\n    model.build()\n\n     Create the Saver to restore model Variables.\n    saver = tf.train.Saver()\n\n     Create the summary operation and the summary writer.\n    summary_op = tf.summary.merge_all()\n    summary_writer = tf.summary.FileWriter(eval_dir)\n\n    g.finalize()\n\n     Run a new evaluation run every eval_interval_secs.\n    while True:\n      start = time.time()\n      tf.logging.info(\"Starting evaluation at \" + time.strftime(\n          \"%Y-%m-%d-%H:%M:%S\", time.localtime()))\n      run_once(model, saver, summary_writer, summary_op)\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)\n\n\ndef main(unused_argv):\n  assert FLAGS.input_file_pattern, \"--input_file_pattern is required\"\n  assert FLAGS.checkpoint_dir, \"--checkpoint_dir is required\"\n  assert FLAGS.eval_dir, \"--eval_dir is required\"\n  run()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "comments": "   evaluate model   this script run concurrently training summaries show tensorboard         future   import absolute import   future   import division   future   import print function  import math import os path import time   import numpy np import tensorflow tf  im2txt import configuration im2txt import show tell model  flags   tf flags flags  tf flags define string( input file pattern                               file pattern sharded tfrecord input files  ) tf flags define string( checkpoint dir                               directory containing model checkpoints  ) tf flags define string( eval dir        directory write event logs  )  tf flags define integer( eval interval secs   600                           interval evaluation runs  ) tf flags define integer( num eval examples   10132                           number examples evaluation  )  tf flags define integer( min global step   5000                           minimum global step run evaluation  )  tf logging set verbosity(tf logging info)   def evaluate model(sess  model  global step  summary writer  summary op)       computes perplexity per word evaluation dataset     summaries perplexity per word written eval directory     args      sess  session object      model  instance showandtellmodel  model evaluate      global step  integer  global step model checkpoint      summary writer  instance filewriter      summary op  op generating model summaries            log model summaries single batch    summary str   sess run(summary op)   summary writer add summary(summary str  global step)      compute perplexity entire dataset    num eval batches   int(       math ceil(flags num eval examples   model config batch size))    start time   time time()   sum losses   0    sum weights   0    range(num eval batches)      cross entropy losses  weights   sess run(          model target cross entropy losses          model target cross entropy loss weights      )     sum losses    np sum(cross entropy losses   weights)     sum weights    np sum(weights)       100        tf logging info( computed losses   batches      1                        num eval batches)   eval time   time time()   start time    perplexity   math exp(sum losses   sum weights)   tf logging info( perplexity    f (  2g sec)   perplexity  eval time)      log perplexity filewriter    summary   tf summary()   value   summary value add()   value simple value   perplexity   value tag    perplexity    summary writer add summary(summary  global step)      write events file eval directory    summary writer flush()   tf logging info( finished processing evaluation global step                       global step)   def run once(model  saver  summary writer  summary op)       evaluates latest model checkpoint     args      model  instance showandtellmodel  model evaluate      saver  instance tf train saver restoring model variables      summary writer  instance filewriter      summary op  op generating model summaries          model path   tf train latest checkpoint(flags checkpoint dir)   model path      tf logging info( skipping evaluation  no checkpoint found                         flags checkpoint dir)     return    tf session() sess        load model checkpoint      tf logging info( loading model checkpoint     model path)     saver restore(sess  model path)     global step   tf train global step(sess  model global step name)     tf logging info( successfully loaded  global step                           os path basename(model path)  global step)     global step   flags min global step        tf logging info( skipping evaluation  global step         global step                        flags min global step)       return        start queue runners      coord   tf train coordinator()     threads   tf train start queue runners(coord coord)        run evaluation latest checkpoint      try        evaluate model(           sess sess            model model            global step global step            summary writer summary writer            summary op summary op)     except exception e     pylint  disable broad except       tf logging error( evaluation failed  )       coord request stop(e)      coord request stop()     coord join(threads  stop grace period secs 10)   def run()       runs evaluation loop  logs summaries tensorboard        copyright 2016 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       log model summaries single batch     compute perplexity entire dataset     log perplexity filewriter     write events file eval directory     load model checkpoint     start queue runners     run evaluation latest checkpoint     pylint  disable broad except    create evaluation directory exist     build model evaluation     create saver restore model variables     create summary operation summary writer     run new evaluation run every eval interval secs  ", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Evaluate the model.\n\nThis script should be run concurrently with training so that summaries show up\nin TensorBoard.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os.path\nimport time\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import show_and_tell_model\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(\"input_file_pattern\", \"\",\n                       \"File pattern of sharded TFRecord input files.\")\ntf.flags.DEFINE_string(\"checkpoint_dir\", \"\",\n                       \"Directory containing model checkpoints.\")\ntf.flags.DEFINE_string(\"eval_dir\", \"\", \"Directory to write event logs.\")\n\ntf.flags.DEFINE_integer(\"eval_interval_secs\", 600,\n                        \"Interval between evaluation runs.\")\ntf.flags.DEFINE_integer(\"num_eval_examples\", 10132,\n                        \"Number of examples for evaluation.\")\n\ntf.flags.DEFINE_integer(\"min_global_step\", 5000,\n                        \"Minimum global step to run evaluation.\")\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef evaluate_model(sess, model, global_step, summary_writer, summary_op):\n  \"\"\"Computes perplexity-per-word over the evaluation dataset.\n\n  Summaries and perplexity-per-word are written out to the eval directory.\n\n  Args:\n    sess: Session object.\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    global_step: Integer; global step of the model checkpoint.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"\n  # Log model summaries on a single batch.\n  summary_str = sess.run(summary_op)\n  summary_writer.add_summary(summary_str, global_step)\n\n  # Compute perplexity over the entire dataset.\n  num_eval_batches = int(\n      math.ceil(FLAGS.num_eval_examples / model.config.batch_size))\n\n  start_time = time.time()\n  sum_losses = 0.\n  sum_weights = 0.\n  for i in range(num_eval_batches):\n    cross_entropy_losses, weights = sess.run([\n        model.target_cross_entropy_losses,\n        model.target_cross_entropy_loss_weights\n    ])\n    sum_losses += np.sum(cross_entropy_losses * weights)\n    sum_weights += np.sum(weights)\n    if not i % 100:\n      tf.logging.info(\"Computed losses for %d of %d batches.\", i + 1,\n                      num_eval_batches)\n  eval_time = time.time() - start_time\n\n  perplexity = math.exp(sum_losses / sum_weights)\n  tf.logging.info(\"Perplexity = %f (%.2g sec)\", perplexity, eval_time)\n\n  # Log perplexity to the FileWriter.\n  summary = tf.Summary()\n  value = summary.value.add()\n  value.simple_value = perplexity\n  value.tag = \"Perplexity\"\n  summary_writer.add_summary(summary, global_step)\n\n  # Write the Events file to the eval directory.\n  summary_writer.flush()\n  tf.logging.info(\"Finished processing evaluation at global step %d.\",\n                  global_step)\n\n\ndef run_once(model, saver, summary_writer, summary_op):\n  \"\"\"Evaluates the latest model checkpoint.\n\n  Args:\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    saver: Instance of tf.train.Saver for restoring model Variables.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"\n  model_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n  if not model_path:\n    tf.logging.info(\"Skipping evaluation. No checkpoint found in: %s\",\n                    FLAGS.checkpoint_dir)\n    return\n\n  with tf.Session() as sess:\n    # Load model from checkpoint.\n    tf.logging.info(\"Loading model from checkpoint: %s\", model_path)\n    saver.restore(sess, model_path)\n    global_step = tf.train.global_step(sess, model.global_step.name)\n    tf.logging.info(\"Successfully loaded %s at global step = %d.\",\n                    os.path.basename(model_path), global_step)\n    if global_step < FLAGS.min_global_step:\n      tf.logging.info(\"Skipping evaluation. Global step = %d < %d\", global_step,\n                      FLAGS.min_global_step)\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n    # Run evaluation on the latest checkpoint.\n    try:\n      evaluate_model(\n          sess=sess,\n          model=model,\n          global_step=global_step,\n          summary_writer=summary_writer,\n          summary_op=summary_op)\n    except Exception as e:  # pylint: disable=broad-except\n      tf.logging.error(\"Evaluation failed.\")\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef run():\n  \"\"\"Runs evaluation in a loop, and logs summaries to TensorBoard.\"\"\"\n  # Create the evaluation directory if it doesn't exist.\n  eval_dir = FLAGS.eval_dir\n  if not tf.gfile.IsDirectory(eval_dir):\n    tf.logging.info(\"Creating eval directory: %s\", eval_dir)\n    tf.gfile.MakeDirs(eval_dir)\n\n  g = tf.Graph()\n  with g.as_default():\n    # Build the model for evaluation.\n    model_config = configuration.ModelConfig()\n    model_config.input_file_pattern = FLAGS.input_file_pattern\n    model = show_and_tell_model.ShowAndTellModel(model_config, mode=\"eval\")\n    model.build()\n\n    # Create the Saver to restore model Variables.\n    saver = tf.train.Saver()\n\n    # Create the summary operation and the summary writer.\n    summary_op = tf.summary.merge_all()\n    summary_writer = tf.summary.FileWriter(eval_dir)\n\n    g.finalize()\n\n    # Run a new evaluation run every eval_interval_secs.\n    while True:\n      start = time.time()\n      tf.logging.info(\"Starting evaluation at \" + time.strftime(\n          \"%Y-%m-%d-%H:%M:%S\", time.localtime()))\n      run_once(model, saver, summary_writer, summary_op)\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)\n\n\ndef main(unused_argv):\n  assert FLAGS.input_file_pattern, \"--input_file_pattern is required\"\n  assert FLAGS.checkpoint_dir, \"--checkpoint_dir is required\"\n  assert FLAGS.eval_dir, \"--eval_dir is required\"\n  run()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "evaluate.py", "id": "8ca3b8f404ff94b7fcd9d205ad93fa34", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/im2txt/im2txt/evaluate.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}