{"author": "openai", "code": "\n\n\nfrom __future__ import unicode_literals\nimport json\nimport hashlib\nimport os\n\nimport pytest\nfrom gym import spaces, logger\nfrom gym.envs.tests.spec_list import spec_list\n\nDATA_DIR = os.path.dirname(__file__)\nROLLOUT_STEPS = 100\nepisodes = ROLLOUT_STEPS\nsteps = ROLLOUT_STEPS\n\nROLLOUT_FILE = os.path.join(DATA_DIR, 'rollout.json')\n\nif not os.path.isfile(ROLLOUT_FILE):\n\twith open(ROLLOUT_FILE, \"w\") as outfile:\n\t\tjson.dump({}, outfile, indent=2)\n\ndef hash_object(unhashed):\n\treturn hashlib.sha256(str(unhashed).encode('utf-16')).hexdigest() \n\ndef generate_rollout_hash(spec):\n\tspaces.seed(0)\n\tenv = spec.make()\n\tenv.seed(0)\n\n\tobservation_list = []\n\taction_list = []\n\treward_list = []\n\tdone_list = []\n\n\ttotal_steps = 0\n\tfor episode in range(episodes):\n\t\tif total_steps >= ROLLOUT_STEPS: break\n\t\tobservation = env.reset()\n\n\t\tfor step in range(steps):\n\t\t\taction = env.action_space.sample()\n\t\t\tobservation, reward, done, _ = env.step(action)\n\n\t\t\taction_list.append(action)\n\t\t\tobservation_list.append(observation)\n\t\t\treward_list.append(reward)\n\t\t\tdone_list.append(done)\n\n\t\t\ttotal_steps += 1\n\t\t\tif total_steps >= ROLLOUT_STEPS: break\n\n\t\t\tif done: break\n\n\tobservations_hash = hash_object(observation_list)\n\tactions_hash = hash_object(action_list)\n\trewards_hash = hash_object(reward_list)\n\tdones_hash = hash_object(done_list)\n\n\tenv.close()\n\treturn observations_hash, actions_hash, rewards_hash, dones_hash\n\n@pytest.mark.parametrize(\"spec\", spec_list)\ndef test_env_semantics(spec):\n\tlogger.warn(\"Skipping this test. Existing hashes were generated in a bad way\")\t\n\treturn\n\twith open(ROLLOUT_FILE) as data_file:\n\t\trollout_dict = json.load(data_file)\n\n\tif spec.id not in rollout_dict:\n\t\tif not spec.nondeterministic:\n\t\t\tlogger.warn(\"Rollout does not exist for {}, run generate_json.py to generate rollouts for new envs\".format(spec.id))\n\t\treturn\n\n\tlogger.info(\"Testing rollout for {} environment...\".format(spec.id))\n\n\tobservations_now, actions_now, rewards_now, dones_now = generate_rollout_hash(spec)\n\n\terrors = []\n\tif rollout_dict[spec.id]['observations'] != observations_now:\n\t\terrors.append('Observations not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['observations'], observations_now))\n\tif rollout_dict[spec.id]['actions'] != actions_now:\n\t\terrors.append('Actions not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['actions'], actions_now))\n\tif rollout_dict[spec.id]['rewards'] != rewards_now:\n\t\terrors.append('Rewards not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['rewards'], rewards_now))\n\tif rollout_dict[spec.id]['dones'] != dones_now:\n\t\terrors.append('Dones not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['dones'], dones_now))\n\tif len(errors):\n\t\tfor error in errors:\n\t\t\tlogger.warn(error)\n\t\traise ValueError(errors)\n", "comments": "    currently disabled since done poor way hashed str representation objects        this really bad  str could values change ", "content": "\"\"\"\nCurrently disabled since this was done in a very poor way\nHashed str representation of objects\n\"\"\"\n\n\nfrom __future__ import unicode_literals\nimport json\nimport hashlib\nimport os\n\nimport pytest\nfrom gym import spaces, logger\nfrom gym.envs.tests.spec_list import spec_list\n\nDATA_DIR = os.path.dirname(__file__)\nROLLOUT_STEPS = 100\nepisodes = ROLLOUT_STEPS\nsteps = ROLLOUT_STEPS\n\nROLLOUT_FILE = os.path.join(DATA_DIR, 'rollout.json')\n\nif not os.path.isfile(ROLLOUT_FILE):\n\twith open(ROLLOUT_FILE, \"w\") as outfile:\n\t\tjson.dump({}, outfile, indent=2)\n\ndef hash_object(unhashed):\n\treturn hashlib.sha256(str(unhashed).encode('utf-16')).hexdigest() # This is really bad, str could be same while values change\n\ndef generate_rollout_hash(spec):\n\tspaces.seed(0)\n\tenv = spec.make()\n\tenv.seed(0)\n\n\tobservation_list = []\n\taction_list = []\n\treward_list = []\n\tdone_list = []\n\n\ttotal_steps = 0\n\tfor episode in range(episodes):\n\t\tif total_steps >= ROLLOUT_STEPS: break\n\t\tobservation = env.reset()\n\n\t\tfor step in range(steps):\n\t\t\taction = env.action_space.sample()\n\t\t\tobservation, reward, done, _ = env.step(action)\n\n\t\t\taction_list.append(action)\n\t\t\tobservation_list.append(observation)\n\t\t\treward_list.append(reward)\n\t\t\tdone_list.append(done)\n\n\t\t\ttotal_steps += 1\n\t\t\tif total_steps >= ROLLOUT_STEPS: break\n\n\t\t\tif done: break\n\n\tobservations_hash = hash_object(observation_list)\n\tactions_hash = hash_object(action_list)\n\trewards_hash = hash_object(reward_list)\n\tdones_hash = hash_object(done_list)\n\n\tenv.close()\n\treturn observations_hash, actions_hash, rewards_hash, dones_hash\n\n@pytest.mark.parametrize(\"spec\", spec_list)\ndef test_env_semantics(spec):\n\tlogger.warn(\"Skipping this test. Existing hashes were generated in a bad way\")\t\n\treturn\n\twith open(ROLLOUT_FILE) as data_file:\n\t\trollout_dict = json.load(data_file)\n\n\tif spec.id not in rollout_dict:\n\t\tif not spec.nondeterministic:\n\t\t\tlogger.warn(\"Rollout does not exist for {}, run generate_json.py to generate rollouts for new envs\".format(spec.id))\n\t\treturn\n\n\tlogger.info(\"Testing rollout for {} environment...\".format(spec.id))\n\n\tobservations_now, actions_now, rewards_now, dones_now = generate_rollout_hash(spec)\n\n\terrors = []\n\tif rollout_dict[spec.id]['observations'] != observations_now:\n\t\terrors.append('Observations not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['observations'], observations_now))\n\tif rollout_dict[spec.id]['actions'] != actions_now:\n\t\terrors.append('Actions not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['actions'], actions_now))\n\tif rollout_dict[spec.id]['rewards'] != rewards_now:\n\t\terrors.append('Rewards not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['rewards'], rewards_now))\n\tif rollout_dict[spec.id]['dones'] != dones_now:\n\t\terrors.append('Dones not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['dones'], dones_now))\n\tif len(errors):\n\t\tfor error in errors:\n\t\t\tlogger.warn(error)\n\t\traise ValueError(errors)\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "test_envs_semantics.py", "id": "5e1c2803e893deb09190eb214e651a0d", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/tests/test_envs_semantics.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}