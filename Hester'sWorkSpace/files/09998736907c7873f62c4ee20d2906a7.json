{"author": "tflearn", "code": "# -*- coding: utf-8 -*-\n\"\"\"\nSimple example using a Dynamic RNN (LSTM) to classify IMDB sentiment dataset.\nDynamic computation are performed over sequences with variable length.\n\nReferences:\n    - Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural\n    Computation 9(8): 1735-1780, 1997.\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n\nLinks:\n    - http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\n\n\ntrain, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n                                valid_portion=0.1)\ntrainX, trainY = train\ntestX, testY = test\n\n\n\n\n\n\n\ntrainX = pad_sequences(trainX, maxlen=100, value=0.)\ntestX = pad_sequences(testX, maxlen=100, value=0.)\n\ntrainY = to_categorical(trainY)\ntestY = to_categorical(testY)\n\n\nnet = tflearn.input_data([None, 100])\n\n\nnet = tflearn.embedding(net, input_dim=10000, output_dim=128)\nnet = tflearn.lstm(net, 128, dropout=0.8, dynamic=True)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n                         loss='categorical_crossentropy')\n\n\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n          batch_size=32)\n", "comments": "    simple example using dynamic rnn (lstm) classify imdb sentiment dataset  dynamic computation performed sequences variable length   references        long short term memory  sepp hochreiter   jurgen schmidhuber  neural     computation 9(8)  1735 1780  1997        andrew l  maas  raymond e  daly  peter t  pham  dan huang  andrew y  ng      christopher potts  (2011)  learning word vectors sentiment     analysis  the 49th annual meeting association computational     linguistics (acl 2011)   links        http   deeplearning cs cmu edu pdfs hochreiter97 lstm pdf       http   ai stanford edu  amaas data sentiment              coding  utf 8        imdb dataset loading    data preprocessing    note  padding required dimension consistency  this pad sequences    0 end  reaches max sequence length  0 used    masking value dynamic rnns tflearn  sequence length    retrieved counting non zero elements sequence  then dynamic rnn step    computation performed according length     converting labels binary vectors    network building    masking required embedding  sequence length computed prior    embedding op assigned  seq length  attribute returned tensor     training ", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nSimple example using a Dynamic RNN (LSTM) to classify IMDB sentiment dataset.\nDynamic computation are performed over sequences with variable length.\n\nReferences:\n    - Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural\n    Computation 9(8): 1735-1780, 1997.\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n\nLinks:\n    - http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\n\n# IMDB Dataset loading\ntrain, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n                                valid_portion=0.1)\ntrainX, trainY = train\ntestX, testY = test\n\n# Data preprocessing\n# NOTE: Padding is required for dimension consistency. This will pad sequences\n# with 0 at the end, until it reaches the max sequence length. 0 is used as a\n# masking value by dynamic RNNs in TFLearn; a sequence length will be\n# retrieved by counting non zero elements in a sequence. Then dynamic RNN step\n# computation is performed according to that length.\ntrainX = pad_sequences(trainX, maxlen=100, value=0.)\ntestX = pad_sequences(testX, maxlen=100, value=0.)\n# Converting labels to binary vectors\ntrainY = to_categorical(trainY)\ntestY = to_categorical(testY)\n\n# Network building\nnet = tflearn.input_data([None, 100])\n# Masking is not required for embedding, sequence length is computed prior to\n# the embedding op and assigned as 'seq_length' attribute to the returned Tensor.\nnet = tflearn.embedding(net, input_dim=10000, output_dim=128)\nnet = tflearn.lstm(net, 128, dropout=0.8, dynamic=True)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n                         loss='categorical_crossentropy')\n\n# Training\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n          batch_size=32)\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "dynamic_lstm.py", "id": "09998736907c7873f62c4ee20d2906a7", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/nlp/dynamic_lstm.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}