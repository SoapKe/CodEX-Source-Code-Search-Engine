{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Default configuration for model architecture and training.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass _HParams(object):\n  \"\"\"Wrapper for configuration parameters.\"\"\"\n  pass\n\n\ndef model_config(input_file_pattern=None,\n                 input_queue_capacity=640000,\n                 num_input_reader_threads=1,\n                 shuffle_input_data=True,\n                 uniform_init_scale=0.1,\n                 vocab_size=20000,\n                 batch_size=128,\n                 word_embedding_dim=620,\n                 bidirectional_encoder=False,\n                 encoder_dim=2400):\n  \"\"\"Creates a model configuration object.\n\n  Args:\n    input_file_pattern: File pattern of sharded TFRecord files containing\n      tf.Example protobufs.\n    input_queue_capacity: Number of examples to keep in the input queue.\n    num_input_reader_threads: Number of threads for prefetching input\n      tf.Examples.\n    shuffle_input_data: Whether to shuffle the input data.\n    uniform_init_scale: Scale of random uniform initializer.\n    vocab_size: Number of unique words in the vocab.\n    batch_size: Batch size (training and evaluation only).\n    word_embedding_dim: Word embedding dimension.\n    bidirectional_encoder: Whether to use a bidirectional or unidirectional\n      encoder RNN.\n    encoder_dim: Number of output dimensions of the sentence encoder.\n\n  Returns:\n    An object containing model configuration parameters.\n  \"\"\"\n  config = _HParams()\n  config.input_file_pattern = input_file_pattern\n  config.input_queue_capacity = input_queue_capacity\n  config.num_input_reader_threads = num_input_reader_threads\n  config.shuffle_input_data = shuffle_input_data\n  config.uniform_init_scale = uniform_init_scale\n  config.vocab_size = vocab_size\n  config.batch_size = batch_size\n  config.word_embedding_dim = word_embedding_dim\n  config.bidirectional_encoder = bidirectional_encoder\n  config.encoder_dim = encoder_dim\n  return config\n\n\ndef training_config(learning_rate=0.0008,\n                    learning_rate_decay_factor=0.5,\n                    learning_rate_decay_steps=400000,\n                    number_of_steps=500000,\n                    clip_gradient_norm=5.0,\n                    save_model_secs=600,\n                    save_summaries_secs=600):\n  \"\"\"Creates a training configuration object.\n\n  Args:\n    learning_rate: Initial learning rate.\n    learning_rate_decay_factor: If > 0, the learning rate decay factor.\n    learning_rate_decay_steps: The number of steps before the learning rate\n      decays by learning_rate_decay_factor.\n    number_of_steps: The total number of training steps to run. Passing None\n      will cause the training script to run indefinitely.\n    clip_gradient_norm: If not None, then clip gradients to this value.\n    save_model_secs: How often (in seconds) to save model checkpoints.\n    save_summaries_secs: How often (in seconds) to save model summaries.\n\n  Returns:\n    An object containing training configuration parameters.\n\n  Raises:\n    ValueError: If learning_rate_decay_factor is set and\n      learning_rate_decay_steps is unset.\n  \"\"\"\n  if learning_rate_decay_factor and not learning_rate_decay_steps:\n    raise ValueError(\n        \"learning_rate_decay_factor requires learning_rate_decay_steps.\")\n\n  config = _HParams()\n  config.learning_rate = learning_rate\n  config.learning_rate_decay_factor = learning_rate_decay_factor\n  config.learning_rate_decay_steps = learning_rate_decay_steps\n  config.number_of_steps = number_of_steps\n  config.clip_gradient_norm = clip_gradient_norm\n  config.save_model_secs = save_model_secs\n  config.save_summaries_secs = save_summaries_secs\n  return config\n", "comments": "   default configuration model architecture training        future   import absolute import   future   import division   future   import print function   class  hparams(object)       wrapper configuration parameters       pass   def model config(input file pattern none                   input queue capacity 640000                   num input reader threads 1                   shuffle input data true                   uniform init scale 0 1                   vocab size 20000                   batch size 128                   word embedding dim 620                   bidirectional encoder false                   encoder dim 2400)       creates model configuration object     args      input file pattern  file pattern sharded tfrecord files containing       tf example protobufs      input queue capacity  number examples keep input queue      num input reader threads  number threads prefetching input       tf examples      shuffle input data  whether shuffle input data      uniform init scale  scale random uniform initializer      vocab size  number unique words vocab      batch size  batch size (training evaluation only)      word embedding dim  word embedding dimension      bidirectional encoder  whether use bidirectional unidirectional       encoder rnn      encoder dim  number output dimensions sentence encoder     returns      an object containing model configuration parameters          config    hparams()   config input file pattern   input file pattern   config input queue capacity   input queue capacity   config num input reader threads   num input reader threads   config shuffle input data   shuffle input data   config uniform init scale   uniform init scale   config vocab size   vocab size   config batch size   batch size   config word embedding dim   word embedding dim   config bidirectional encoder   bidirectional encoder   config encoder dim   encoder dim   return config   def training config(learning rate 0 0008                      learning rate decay factor 0 5                      learning rate decay steps 400000                      number steps 500000                      clip gradient norm 5 0                      save model secs 600                      save summaries secs 600)       creates training configuration object     args      learning rate  initial learning rate      learning rate decay factor  if   0  learning rate decay factor      learning rate decay steps  the number steps learning rate       decays learning rate decay factor      number steps  the total number training steps run  passing none       cause training script run indefinitely      clip gradient norm  if none  clip gradients value      save model secs  how often (in seconds) save model checkpoints      save summaries secs  how often (in seconds) save model summaries     returns      an object containing training configuration parameters     raises      valueerror  if learning rate decay factor set       learning rate decay steps unset           copyright 2017 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                    ", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Default configuration for model architecture and training.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass _HParams(object):\n  \"\"\"Wrapper for configuration parameters.\"\"\"\n  pass\n\n\ndef model_config(input_file_pattern=None,\n                 input_queue_capacity=640000,\n                 num_input_reader_threads=1,\n                 shuffle_input_data=True,\n                 uniform_init_scale=0.1,\n                 vocab_size=20000,\n                 batch_size=128,\n                 word_embedding_dim=620,\n                 bidirectional_encoder=False,\n                 encoder_dim=2400):\n  \"\"\"Creates a model configuration object.\n\n  Args:\n    input_file_pattern: File pattern of sharded TFRecord files containing\n      tf.Example protobufs.\n    input_queue_capacity: Number of examples to keep in the input queue.\n    num_input_reader_threads: Number of threads for prefetching input\n      tf.Examples.\n    shuffle_input_data: Whether to shuffle the input data.\n    uniform_init_scale: Scale of random uniform initializer.\n    vocab_size: Number of unique words in the vocab.\n    batch_size: Batch size (training and evaluation only).\n    word_embedding_dim: Word embedding dimension.\n    bidirectional_encoder: Whether to use a bidirectional or unidirectional\n      encoder RNN.\n    encoder_dim: Number of output dimensions of the sentence encoder.\n\n  Returns:\n    An object containing model configuration parameters.\n  \"\"\"\n  config = _HParams()\n  config.input_file_pattern = input_file_pattern\n  config.input_queue_capacity = input_queue_capacity\n  config.num_input_reader_threads = num_input_reader_threads\n  config.shuffle_input_data = shuffle_input_data\n  config.uniform_init_scale = uniform_init_scale\n  config.vocab_size = vocab_size\n  config.batch_size = batch_size\n  config.word_embedding_dim = word_embedding_dim\n  config.bidirectional_encoder = bidirectional_encoder\n  config.encoder_dim = encoder_dim\n  return config\n\n\ndef training_config(learning_rate=0.0008,\n                    learning_rate_decay_factor=0.5,\n                    learning_rate_decay_steps=400000,\n                    number_of_steps=500000,\n                    clip_gradient_norm=5.0,\n                    save_model_secs=600,\n                    save_summaries_secs=600):\n  \"\"\"Creates a training configuration object.\n\n  Args:\n    learning_rate: Initial learning rate.\n    learning_rate_decay_factor: If > 0, the learning rate decay factor.\n    learning_rate_decay_steps: The number of steps before the learning rate\n      decays by learning_rate_decay_factor.\n    number_of_steps: The total number of training steps to run. Passing None\n      will cause the training script to run indefinitely.\n    clip_gradient_norm: If not None, then clip gradients to this value.\n    save_model_secs: How often (in seconds) to save model checkpoints.\n    save_summaries_secs: How often (in seconds) to save model summaries.\n\n  Returns:\n    An object containing training configuration parameters.\n\n  Raises:\n    ValueError: If learning_rate_decay_factor is set and\n      learning_rate_decay_steps is unset.\n  \"\"\"\n  if learning_rate_decay_factor and not learning_rate_decay_steps:\n    raise ValueError(\n        \"learning_rate_decay_factor requires learning_rate_decay_steps.\")\n\n  config = _HParams()\n  config.learning_rate = learning_rate\n  config.learning_rate_decay_factor = learning_rate_decay_factor\n  config.learning_rate_decay_steps = learning_rate_decay_steps\n  config.number_of_steps = number_of_steps\n  config.clip_gradient_norm = clip_gradient_norm\n  config.save_model_secs = save_model_secs\n  config.save_summaries_secs = save_summaries_secs\n  return config\n", "description": "Models and examples built with TensorFlow", "file_name": "configuration.py", "id": "f7001fa5e825486fbc1f2dcbdfdfdf68", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/skip_thoughts/skip_thoughts/configuration.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}