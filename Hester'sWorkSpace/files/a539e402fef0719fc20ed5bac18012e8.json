{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\nimport numpy as np\n\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nimport logging\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\nfrom src import utils \nimport src.file_utils as fu\nimport tfcode.nav_utils as nu \nfrom tfcode import tf_utils\n\nsetup_train_step_kwargs = nu.default_train_step_kwargs\ncompute_losses_multi_or = nu.compute_losses_multi_or\nget_repr_from_image = nu.get_repr_from_image\n\n_save_d_at_t = nu.save_d_at_t\n_save_all = nu.save_all\n_eval_ap = nu.eval_ap\n_eval_dist = nu.eval_dist\n_plot_trajectories = nu.plot_trajectories\n\ndef lstm_online(cell_fn, num_steps, inputs, state, varscope):\n   inputs is B x num_steps x C, C channels.\n   state is 2 tuple with B x 1 x C1, B x 1 x C2 \n   Output state is always B x 1 x C\n  inputs = tf.unstack(inputs, axis=1, num=num_steps)\n  state = tf.unstack(state, axis=1, num=1)[0]\n  outputs = [] \n  \n  if num_steps > 1: \n    varscope.reuse_variables()\n  \n  for s in range(num_steps):\n    output, state = cell_fn(inputs[s], state)\n    outputs.append(output)\n  outputs = tf.stack(outputs, axis=1)\n  state = tf.stack([state], axis=1)\n  return outputs, state\n\ndef _inputs(problem, lstm_states, lstm_state_dims):\n   Set up inputs.\n  with tf.name_scope('inputs'):\n    n_views = problem.n_views\n\n    inputs = []\n    inputs.append(('orig_maps', tf.float32,\n                   (problem.batch_size, 1, None, None, 1)))\n    inputs.append(('goal_loc', tf.float32,\n                   (problem.batch_size, problem.num_goals, 2)))\n\n     For initing LSTM.\n    inputs.append(('rel_goal_loc_at_start', tf.float32,\n                   (problem.batch_size, problem.num_goals,\n                    problem.rel_goal_loc_dim)))\n    common_input_data, _ = tf_utils.setup_inputs(inputs)\n\n    inputs = []\n    inputs.append(('imgs', tf.float32, (problem.batch_size, None, n_views,\n                                        problem.img_height, problem.img_width,\n                                        problem.img_channels)))\n     Goal location as a tuple of delta location and delta theta.\n    inputs.append(('rel_goal_loc', tf.float32, (problem.batch_size, None,\n                                                problem.rel_goal_loc_dim)))\n    if problem.outputs.visit_count:\n      inputs.append(('visit_count', tf.int32, (problem.batch_size, None, 1)))\n      inputs.append(('last_visit', tf.int32, (problem.batch_size, None, 1)))\n\n    for i, (state, dim) in enumerate(zip(lstm_states, lstm_state_dims)):\n      inputs.append((state, tf.float32, (problem.batch_size, 1, dim)))\n\n    if problem.outputs.egomotion:\n      inputs.append(('incremental_locs', tf.float32,\n                     (problem.batch_size, None, 2)))\n      inputs.append(('incremental_thetas', tf.float32,\n                     (problem.batch_size, None, 1)))\n\n    inputs.append(('step_number', tf.int32, (1, None, 1)))\n    inputs.append(('node_ids', tf.int32, (problem.batch_size, None,\n                                          problem.node_ids_dim)))\n    inputs.append(('perturbs', tf.float32, (problem.batch_size, None,\n                                            problem.perturbs_dim)))\n\n     For plotting result plots\n    inputs.append(('loc_on_map', tf.float32, (problem.batch_size, None, 2)))\n    inputs.append(('gt_dist_to_goal', tf.float32, (problem.batch_size, None, 1)))\n    step_input_data, _ = tf_utils.setup_inputs(inputs)\n\n    inputs = []\n    inputs.append(('executed_actions', tf.int32, (problem.batch_size, None)))\n    inputs.append(('rewards', tf.float32, (problem.batch_size, None)))\n    inputs.append(('action_sample_wts', tf.float32, (problem.batch_size, None)))\n    inputs.append(('action', tf.int32, (problem.batch_size, None,\n                                        problem.num_actions)))\n    train_data, _ = tf_utils.setup_inputs(inputs)\n    train_data.update(step_input_data)\n    train_data.update(common_input_data)\n  return common_input_data, step_input_data, train_data\n\n\ndef _add_summaries(m, summary_mode, arop_full_summary_iters):\n  summarize_ops = [m.lr_op, m.global_step_op, m.sample_gt_prob_op,\n                   m.total_loss_op, m.data_loss_op, m.reg_loss_op] + m.acc_ops\n  summarize_names = ['lr', 'global_step', 'sample_gt_prob_op', 'total_loss',\n                     'data_loss', 'reg_loss'] + \\\n                    ['acc_{:d}'.format(i) for i in range(len(m.acc_ops))]\n  to_aggregate = [0, 0, 0, 1, 1, 1] + [1]*len(m.acc_ops)\n\n  scope_name = 'summary'\n  with tf.name_scope(scope_name):\n    s_ops = nu.add_default_summaries(summary_mode, arop_full_summary_iters,\n                                     summarize_ops, summarize_names,\n                                     to_aggregate, m.action_prob_op,\n                                     m.input_tensors, scope_name=scope_name)\n    m.summary_ops = {summary_mode: s_ops}\n\ndef visit_count_fc(visit_count, last_visit, embed_neurons, wt_decay, fc_dropout):\n  with tf.variable_scope('embed_visit_count'):\n    visit_count = tf.reshape(visit_count, shape=[-1])\n    last_visit = tf.reshape(last_visit, shape=[-1])\n    \n    visit_count = tf.clip_by_value(visit_count, clip_value_min=-1,\n                                   clip_value_max=15)\n    last_visit = tf.clip_by_value(last_visit, clip_value_min=-1,\n                                   clip_value_max=15)\n    visit_count = tf.one_hot(visit_count, depth=16, axis=1, dtype=tf.float32,\n                             on_value=10., off_value=0.)\n    last_visit = tf.one_hot(last_visit, depth=16, axis=1, dtype=tf.float32,\n                             on_value=10., off_value=0.)\n    f = tf.concat([visit_count, last_visit], 1)\n    x, _ = tf_utils.fc_network(\n        f, neurons=embed_neurons, wt_decay=wt_decay, name='visit_count_embed',\n        offset=0, batch_norm_param=None, dropout_ratio=fc_dropout,\n        is_training=is_training)\n  return x\n\ndef lstm_setup(name, x, batch_size, is_single_step, lstm_dim, lstm_out,\n               num_steps, state_input_op):\n   returns state_name, state_init_op, updated_state_op, out_op \n  with tf.name_scope('reshape_'+name):\n    sh = x.get_shape().as_list()\n    x = tf.reshape(x, shape=[batch_size, -1, sh[-1]])\n\n  with tf.variable_scope(name) as varscope:\n    cell = tf.contrib.rnn.LSTMCell(\n      num_units=lstm_dim, forget_bias=1.0, state_is_tuple=False,\n      num_proj=lstm_out, use_peepholes=True,\n      initializer=tf.random_uniform_initializer(-0.01, 0.01, seed=0),\n      cell_clip=None, proj_clip=None)\n\n    sh = [batch_size, 1, lstm_dim+lstm_out]\n    state_init_op = tf.constant(0., dtype=tf.float32, shape=sh)\n\n    fn = lambda ns: lstm_online(cell, ns, x, state_input_op, varscope)\n    out_op, updated_state_op = tf.cond(is_single_step, lambda: fn(1), lambda:\n                                       fn(num_steps))\n\n  return name, state_init_op, updated_state_op, out_op \n\ndef combine_setup(name, combine_type, embed_img, embed_goal, num_img_neuorons=None,\n                  num_goal_neurons=None):\n  with tf.name_scope(name + '_' + combine_type):\n    if combine_type == 'add':\n       Simple concat features from goal and image\n      out = embed_img + embed_goal\n\n    elif combine_type == 'multiply':\n       Multiply things together\n      re_embed_img = tf.reshape(\n          embed_img, shape=[-1, num_img_neuorons / num_goal_neurons,\n                            num_goal_neurons])\n      re_embed_goal = tf.reshape(embed_goal, shape=[-1, num_goal_neurons, 1])\n      x = tf.matmul(re_embed_img, re_embed_goal, transpose_a=False, transpose_b=False)\n      out = slim.flatten(x)\n    elif combine_type == 'none' or combine_type == 'imgonly':\n      out = embed_img\n    elif combine_type == 'goalonly':\n      out = embed_goal\n    else:\n      logging.fatal('Undefined combine_type: %s', combine_type)\n  return out\n\n\ndef preprocess_egomotion(locs, thetas):\n  with tf.name_scope('pre_ego'):\n    pre_ego = tf.concat([locs, tf.sin(thetas), tf.cos(thetas)], 2)\n    sh = pre_ego.get_shape().as_list()\n    pre_ego = tf.reshape(pre_ego, [-1, sh[-1]])\n  return pre_ego\n\ndef setup_to_run(m, args, is_training, batch_norm_is_training, summary_mode):\n   Set up the model.\n  tf.set_random_seed(args.solver.seed)\n  task_params = args.navtask.task_params\n  num_steps = task_params.num_steps\n  num_goals = task_params.num_goals\n  num_actions = task_params.num_actions\n  num_actions_ = num_actions\n\n  n_views = task_params.n_views\n\n  batch_norm_is_training_op = \\\n      tf.placeholder_with_default(batch_norm_is_training, shape=[],\n                                  name='batch_norm_is_training_op') \n   Setup the inputs\n  m.input_tensors = {}\n  lstm_states = []; lstm_state_dims = [];\n  state_names = []; updated_state_ops = []; init_state_ops = [];\n  if args.arch.lstm_output:\n    lstm_states += ['lstm_output']\n    lstm_state_dims += [args.arch.lstm_output_dim+task_params.num_actions]\n  if args.arch.lstm_ego:\n    lstm_states += ['lstm_ego']\n    lstm_state_dims += [args.arch.lstm_ego_dim + args.arch.lstm_ego_out]\n    lstm_states += ['lstm_img']\n    lstm_state_dims += [args.arch.lstm_img_dim + args.arch.lstm_img_out]\n  elif args.arch.lstm_img:\n     An LSTM only on the image\n    lstm_states += ['lstm_img']\n    lstm_state_dims += [args.arch.lstm_img_dim + args.arch.lstm_img_out]\n  else:\n     No LSTMs involved here.\n    None\n\n  m.input_tensors['common'], m.input_tensors['step'], m.input_tensors['train'] = \\\n      _inputs(task_params, lstm_states, lstm_state_dims)\n\n  with tf.name_scope('check_size'):\n    is_single_step = tf.equal(tf.unstack(tf.shape(m.input_tensors['step']['imgs']), \n                                        num=6)[1], 1)\n\n  images_reshaped = tf.reshape(m.input_tensors['step']['imgs'], \n      shape=[-1, task_params.img_height, task_params.img_width,\n             task_params.img_channels], name='re_image')\n\n  rel_goal_loc_reshaped = tf.reshape(m.input_tensors['step']['rel_goal_loc'], \n      shape=[-1, task_params.rel_goal_loc_dim], name='re_rel_goal_loc')\n\n  x, vars_ = get_repr_from_image(\n      images_reshaped, task_params.modalities, task_params.data_augment,\n      args.arch.encoder, args.solver.freeze_conv, args.solver.wt_decay,\n      is_training)\n\n   Reshape into nice things so that these can be accumulated over time steps\n   for faster backprop.\n  sh_before = x.get_shape().as_list()\n  m.encoder_output = tf.reshape(\n      x, shape=[task_params.batch_size, -1, n_views] + sh_before[1:])\n  x = tf.reshape(m.encoder_output, shape=[-1] + sh_before[1:])\n\n   Add a layer to reduce dimensions for a fc layer.\n  if args.arch.dim_reduce_neurons > 0:\n    ks = 1; neurons = args.arch.dim_reduce_neurons;\n    init_var = np.sqrt(2.0/(ks**2)/neurons)\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.conv_feat = slim.conv2d(\n        x, neurons, kernel_size=ks, stride=1, normalizer_fn=slim.batch_norm,\n        normalizer_params=batch_norm_param, padding='SAME', scope='dim_reduce',\n        weights_regularizer=slim.l2_regularizer(args.solver.wt_decay),\n        weights_initializer=tf.random_normal_initializer(stddev=init_var))\n    reshape_conv_feat = slim.flatten(m.conv_feat)\n    sh = reshape_conv_feat.get_shape().as_list()\n    m.reshape_conv_feat = tf.reshape(reshape_conv_feat, \n                                     shape=[-1, sh[1]*n_views])\n\n   Restore these from a checkpoint.\n  if args.solver.pretrained_path is not None:\n    m.init_fn = slim.assign_from_checkpoint_fn(args.solver.pretrained_path,\n                                               vars_)\n  else:\n    m.init_fn = None\n\n   Hit the goal_location with a bunch of fully connected layers, to embed it\n   into some space.\n  with tf.variable_scope('embed_goal'):\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.embed_goal, _ = tf_utils.fc_network(\n        rel_goal_loc_reshaped, neurons=args.arch.goal_embed_neurons,\n        wt_decay=args.solver.wt_decay, name='goal_embed', offset=0,\n        batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n        is_training=is_training)\n  \n  if args.arch.embed_goal_for_state:\n    with tf.variable_scope('embed_goal_for_state'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      m.embed_goal_for_state, _ = tf_utils.fc_network(\n          m.input_tensors['common']['rel_goal_loc_at_start'][:,0,:],\n          neurons=args.arch.goal_embed_neurons, wt_decay=args.solver.wt_decay,\n          name='goal_embed', offset=0, batch_norm_param=batch_norm_param,\n          dropout_ratio=args.arch.fc_dropout, is_training=is_training)\n\n   Hit the goal_location with a bunch of fully connected layers, to embed it\n   into some space.\n  with tf.variable_scope('embed_img'):\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.embed_img, _ = tf_utils.fc_network(\n        m.reshape_conv_feat, neurons=args.arch.img_embed_neurons,\n        wt_decay=args.solver.wt_decay, name='img_embed', offset=0,\n        batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n        is_training=is_training)\n\n   For lstm_ego, and lstm_image, embed the ego motion, accumulate it into an\n   LSTM, combine with image features and accumulate those in an LSTM. Finally\n   combine what you get from the image LSTM with the goal to output an action.\n  if args.arch.lstm_ego:\n    ego_reshaped = preprocess_egomotion(m.input_tensors['step']['incremental_locs'], \n                                        m.input_tensors['step']['incremental_thetas'])\n    with tf.variable_scope('embed_ego'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      m.embed_ego, _ = tf_utils.fc_network(\n          ego_reshaped, neurons=args.arch.ego_embed_neurons,\n          wt_decay=args.solver.wt_decay, name='ego_embed', offset=0,\n          batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n          is_training=is_training)\n\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_ego', m.embed_ego, task_params.batch_size, is_single_step, \n        args.arch.lstm_ego_dim, args.arch.lstm_ego_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_ego'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n\n     Combine the output with the vision features.\n    m.img_ego_op = combine_setup('img_ego', args.arch.combine_type_ego,\n                                 m.embed_img, out_op,\n                                 args.arch.img_embed_neurons[-1],\n                                 args.arch.lstm_ego_out)\n\n     LSTM on these vision features.\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_img', m.img_ego_op, task_params.batch_size, is_single_step, \n        args.arch.lstm_img_dim, args.arch.lstm_img_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_img'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n\n    m.img_for_goal = out_op\n    num_img_for_goal_neurons = args.arch.lstm_img_out\n\n  elif args.arch.lstm_img:\n     LSTM on just the image features.\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_img', m.embed_img, task_params.batch_size, is_single_step,\n        args.arch.lstm_img_dim, args.arch.lstm_img_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_img'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n    m.img_for_goal = out_op\n    num_img_for_goal_neurons = args.arch.lstm_img_out\n\n  else:\n    m.img_for_goal = m.embed_img\n    num_img_for_goal_neurons = args.arch.img_embed_neurons[-1]\n\n\n  if args.arch.use_visit_count:\n    m.embed_visit_count = visit_count_fc(\n        m.input_tensors['step']['visit_count'],\n        m.input_tensors['step']['last_visit'], args.arch.goal_embed_neurons,\n        args.solver.wt_decay, args.arch.fc_dropout, is_training=is_training)\n    m.embed_goal = m.embed_goal + m.embed_visit_count\n  \n  m.combined_f = combine_setup('img_goal', args.arch.combine_type,\n                               m.img_for_goal, m.embed_goal,\n                               num_img_for_goal_neurons,\n                               args.arch.goal_embed_neurons[-1])\n\n   LSTM on the combined representation.\n  if args.arch.lstm_output:\n    name = 'lstm_output'\n     A few fully connected layers here.\n    with tf.variable_scope('action_pred'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      x, _ = tf_utils.fc_network(\n          m.combined_f, neurons=args.arch.pred_neurons,\n          wt_decay=args.solver.wt_decay, name='pred', offset=0,\n          batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout)\n\n    if args.arch.lstm_output_init_state_from_goal:\n       Use the goal embedding to initialize the LSTM state.\n       UGLY CLUGGY HACK: if this is doing computation for a single time step\n       then this will not involve back prop, so we can use the state input from\n       the feed dict, otherwise we compute the state representation from the\n       goal and feed that in. Necessary for using goal location to generate the\n       state representation.\n      m.embed_goal_for_state = tf.expand_dims(m.embed_goal_for_state, dim=1)\n      state_op = tf.cond(is_single_step, lambda: m.input_tensors['step'][name],\n                         lambda: m.embed_goal_for_state)\n      state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n          name, x, task_params.batch_size, is_single_step,\n          args.arch.lstm_output_dim,\n          num_actions_,\n          num_steps*num_goals, state_op)\n      init_state_ops += [m.embed_goal_for_state]\n    else:\n      state_op = m.input_tensors['step'][name]\n      state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n          name, x, task_params.batch_size, is_single_step,\n          args.arch.lstm_output_dim,\n          num_actions_, num_steps*num_goals, state_op)\n      init_state_ops += [state_init_op]\n\n    state_names += [state_name]\n    updated_state_ops += [updated_state_op]\n\n    out_op = tf.reshape(out_op, shape=[-1, num_actions_])\n    if num_actions_ > num_actions:\n      m.action_logits_op = out_op[:,:num_actions]\n      m.baseline_op = out_op[:,num_actions:]\n    else:\n      m.action_logits_op = out_op\n      m.baseline_op = None\n    m.action_prob_op = tf.nn.softmax(m.action_logits_op)\n\n  else:\n     A few fully connected layers here.\n    with tf.variable_scope('action_pred'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      out_op, _ = tf_utils.fc_network(\n          m.combined_f, neurons=args.arch.pred_neurons,\n          wt_decay=args.solver.wt_decay, name='pred', offset=0,\n          num_pred=num_actions_,\n          batch_norm_param=batch_norm_param,\n          dropout_ratio=args.arch.fc_dropout, is_training=is_training)\n      if num_actions_ > num_actions:\n        m.action_logits_op = out_op[:,:num_actions]\n        m.baseline_op = out_op[:,num_actions:]\n      else:\n        m.action_logits_op = out_op \n        m.baseline_op = None\n      m.action_prob_op = tf.nn.softmax(m.action_logits_op)\n\n  m.train_ops = {}\n  m.train_ops['step'] = m.action_prob_op\n  m.train_ops['common'] = [m.input_tensors['common']['orig_maps'],\n                           m.input_tensors['common']['goal_loc'],\n                           m.input_tensors['common']['rel_goal_loc_at_start']]\n  m.train_ops['state_names'] = state_names\n  m.train_ops['init_state'] = init_state_ops\n  m.train_ops['updated_state'] = updated_state_ops\n  m.train_ops['batch_norm_is_training_op'] = batch_norm_is_training_op\n\n   Flat list of ops which cache the step data.\n  m.train_ops['step_data_cache'] = [tf.no_op()]\n\n  if args.solver.freeze_conv:\n    m.train_ops['step_data_cache'] = [m.encoder_output]\n  else:\n    m.train_ops['step_data_cache'] = []\n\n  ewma_decay = 0.99 if is_training else 0.0\n  weight = tf.ones_like(m.input_tensors['train']['action'], dtype=tf.float32,\n                        name='weight')\n\n  m.reg_loss_op, m.data_loss_op, m.total_loss_op, m.acc_ops = \\\n    compute_losses_multi_or(\n        m.action_logits_op, m.input_tensors['train']['action'],\n        weights=weight, num_actions=num_actions,\n        data_loss_wt=args.solver.data_loss_wt,\n        reg_loss_wt=args.solver.reg_loss_wt, ewma_decay=ewma_decay)\n\n\n  if args.solver.freeze_conv:\n    vars_to_optimize = list(set(tf.trainable_variables()) - set(vars_))\n  else:\n    vars_to_optimize = None\n\n  m.lr_op, m.global_step_op, m.train_op, m.should_stop_op, m.optimizer, \\\n  m.sync_optimizer = tf_utils.setup_training(\n      m.total_loss_op, \n      args.solver.initial_learning_rate, \n      args.solver.steps_per_decay,\n      args.solver.learning_rate_decay, \n      args.solver.momentum,\n      args.solver.max_steps, \n      args.solver.sync, \n      args.solver.adjust_lr_sync,\n      args.solver.num_workers, \n      args.solver.task,\n      vars_to_optimize=vars_to_optimize,\n      clip_gradient_norm=args.solver.clip_gradient_norm,\n      typ=args.solver.typ, momentum2=args.solver.momentum2,\n      adam_eps=args.solver.adam_eps)\n  \n  \n  if args.arch.sample_gt_prob_type == 'inverse_sigmoid_decay':\n    m.sample_gt_prob_op = tf_utils.inverse_sigmoid_decay(args.arch.isd_k,\n                                                         m.global_step_op)\n  elif args.arch.sample_gt_prob_type == 'zero':\n    m.sample_gt_prob_op = tf.constant(-1.0, dtype=tf.float32)\n  elif args.arch.sample_gt_prob_type.split('_')[0] == 'step':\n    step = int(args.arch.sample_gt_prob_type.split('_')[1])\n    m.sample_gt_prob_op = tf_utils.step_gt_prob(\n        step, m.input_tensors['step']['step_number'][0,0,0])\n  \n  m.sample_action_type = args.arch.action_sample_type\n  m.sample_action_combine_type = args.arch.action_sample_combine_type\n  _add_summaries(m, summary_mode, args.summary.arop_full_summary_iters)\n  \n  m.init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n  m.saver_op = tf.train.Saver(keep_checkpoint_every_n_hours=4,\n                              write_version=tf.train.SaverDef.V2)\n  \n  return m\n", "comments": "  copyright 2016 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       inputs b x num steps x c  c channels     state 2 tuple b x 1 x c1  b x 1 x c2     output state always b x 1 x c    set inputs     for initing lstm     goal location tuple delta location delta theta     for plotting result plots    returns state name  state init op  updated state op  op     simple concat features goal image    multiply things together    set model     setup inputs    an lstm image    no lstms involved     reshape nice things accumulated time steps    faster backprop     add layer reduce dimensions fc layer     restore checkpoint     hit goal location bunch fully connected layers  embed    space     hit goal location bunch fully connected layers  embed    space     for lstm ego  lstm image  embed ego motion  accumulate    lstm  combine image features accumulate lstm  finally    combine get image lstm goal output action     combine output vision features     lstm vision features     lstm image features     lstm combined representation     a fully connected layers     use goal embedding initialize lstm state     ugly cluggy hack  computation single time step    involve back prop  use state input    feed dict  otherwise compute state representation    goal feed  necessary using goal location generate    state representation     a fully connected layers     flat list ops cache step data  ", "content": "# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\n\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nimport logging\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\nfrom src import utils \nimport src.file_utils as fu\nimport tfcode.nav_utils as nu \nfrom tfcode import tf_utils\n\nsetup_train_step_kwargs = nu.default_train_step_kwargs\ncompute_losses_multi_or = nu.compute_losses_multi_or\nget_repr_from_image = nu.get_repr_from_image\n\n_save_d_at_t = nu.save_d_at_t\n_save_all = nu.save_all\n_eval_ap = nu.eval_ap\n_eval_dist = nu.eval_dist\n_plot_trajectories = nu.plot_trajectories\n\ndef lstm_online(cell_fn, num_steps, inputs, state, varscope):\n  # inputs is B x num_steps x C, C channels.\n  # state is 2 tuple with B x 1 x C1, B x 1 x C2 \n  # Output state is always B x 1 x C\n  inputs = tf.unstack(inputs, axis=1, num=num_steps)\n  state = tf.unstack(state, axis=1, num=1)[0]\n  outputs = [] \n  \n  if num_steps > 1: \n    varscope.reuse_variables()\n  \n  for s in range(num_steps):\n    output, state = cell_fn(inputs[s], state)\n    outputs.append(output)\n  outputs = tf.stack(outputs, axis=1)\n  state = tf.stack([state], axis=1)\n  return outputs, state\n\ndef _inputs(problem, lstm_states, lstm_state_dims):\n  # Set up inputs.\n  with tf.name_scope('inputs'):\n    n_views = problem.n_views\n\n    inputs = []\n    inputs.append(('orig_maps', tf.float32,\n                   (problem.batch_size, 1, None, None, 1)))\n    inputs.append(('goal_loc', tf.float32,\n                   (problem.batch_size, problem.num_goals, 2)))\n\n    # For initing LSTM.\n    inputs.append(('rel_goal_loc_at_start', tf.float32,\n                   (problem.batch_size, problem.num_goals,\n                    problem.rel_goal_loc_dim)))\n    common_input_data, _ = tf_utils.setup_inputs(inputs)\n\n    inputs = []\n    inputs.append(('imgs', tf.float32, (problem.batch_size, None, n_views,\n                                        problem.img_height, problem.img_width,\n                                        problem.img_channels)))\n    # Goal location as a tuple of delta location and delta theta.\n    inputs.append(('rel_goal_loc', tf.float32, (problem.batch_size, None,\n                                                problem.rel_goal_loc_dim)))\n    if problem.outputs.visit_count:\n      inputs.append(('visit_count', tf.int32, (problem.batch_size, None, 1)))\n      inputs.append(('last_visit', tf.int32, (problem.batch_size, None, 1)))\n\n    for i, (state, dim) in enumerate(zip(lstm_states, lstm_state_dims)):\n      inputs.append((state, tf.float32, (problem.batch_size, 1, dim)))\n\n    if problem.outputs.egomotion:\n      inputs.append(('incremental_locs', tf.float32,\n                     (problem.batch_size, None, 2)))\n      inputs.append(('incremental_thetas', tf.float32,\n                     (problem.batch_size, None, 1)))\n\n    inputs.append(('step_number', tf.int32, (1, None, 1)))\n    inputs.append(('node_ids', tf.int32, (problem.batch_size, None,\n                                          problem.node_ids_dim)))\n    inputs.append(('perturbs', tf.float32, (problem.batch_size, None,\n                                            problem.perturbs_dim)))\n\n    # For plotting result plots\n    inputs.append(('loc_on_map', tf.float32, (problem.batch_size, None, 2)))\n    inputs.append(('gt_dist_to_goal', tf.float32, (problem.batch_size, None, 1)))\n    step_input_data, _ = tf_utils.setup_inputs(inputs)\n\n    inputs = []\n    inputs.append(('executed_actions', tf.int32, (problem.batch_size, None)))\n    inputs.append(('rewards', tf.float32, (problem.batch_size, None)))\n    inputs.append(('action_sample_wts', tf.float32, (problem.batch_size, None)))\n    inputs.append(('action', tf.int32, (problem.batch_size, None,\n                                        problem.num_actions)))\n    train_data, _ = tf_utils.setup_inputs(inputs)\n    train_data.update(step_input_data)\n    train_data.update(common_input_data)\n  return common_input_data, step_input_data, train_data\n\n\ndef _add_summaries(m, summary_mode, arop_full_summary_iters):\n  summarize_ops = [m.lr_op, m.global_step_op, m.sample_gt_prob_op,\n                   m.total_loss_op, m.data_loss_op, m.reg_loss_op] + m.acc_ops\n  summarize_names = ['lr', 'global_step', 'sample_gt_prob_op', 'total_loss',\n                     'data_loss', 'reg_loss'] + \\\n                    ['acc_{:d}'.format(i) for i in range(len(m.acc_ops))]\n  to_aggregate = [0, 0, 0, 1, 1, 1] + [1]*len(m.acc_ops)\n\n  scope_name = 'summary'\n  with tf.name_scope(scope_name):\n    s_ops = nu.add_default_summaries(summary_mode, arop_full_summary_iters,\n                                     summarize_ops, summarize_names,\n                                     to_aggregate, m.action_prob_op,\n                                     m.input_tensors, scope_name=scope_name)\n    m.summary_ops = {summary_mode: s_ops}\n\ndef visit_count_fc(visit_count, last_visit, embed_neurons, wt_decay, fc_dropout):\n  with tf.variable_scope('embed_visit_count'):\n    visit_count = tf.reshape(visit_count, shape=[-1])\n    last_visit = tf.reshape(last_visit, shape=[-1])\n    \n    visit_count = tf.clip_by_value(visit_count, clip_value_min=-1,\n                                   clip_value_max=15)\n    last_visit = tf.clip_by_value(last_visit, clip_value_min=-1,\n                                   clip_value_max=15)\n    visit_count = tf.one_hot(visit_count, depth=16, axis=1, dtype=tf.float32,\n                             on_value=10., off_value=0.)\n    last_visit = tf.one_hot(last_visit, depth=16, axis=1, dtype=tf.float32,\n                             on_value=10., off_value=0.)\n    f = tf.concat([visit_count, last_visit], 1)\n    x, _ = tf_utils.fc_network(\n        f, neurons=embed_neurons, wt_decay=wt_decay, name='visit_count_embed',\n        offset=0, batch_norm_param=None, dropout_ratio=fc_dropout,\n        is_training=is_training)\n  return x\n\ndef lstm_setup(name, x, batch_size, is_single_step, lstm_dim, lstm_out,\n               num_steps, state_input_op):\n  # returns state_name, state_init_op, updated_state_op, out_op \n  with tf.name_scope('reshape_'+name):\n    sh = x.get_shape().as_list()\n    x = tf.reshape(x, shape=[batch_size, -1, sh[-1]])\n\n  with tf.variable_scope(name) as varscope:\n    cell = tf.contrib.rnn.LSTMCell(\n      num_units=lstm_dim, forget_bias=1.0, state_is_tuple=False,\n      num_proj=lstm_out, use_peepholes=True,\n      initializer=tf.random_uniform_initializer(-0.01, 0.01, seed=0),\n      cell_clip=None, proj_clip=None)\n\n    sh = [batch_size, 1, lstm_dim+lstm_out]\n    state_init_op = tf.constant(0., dtype=tf.float32, shape=sh)\n\n    fn = lambda ns: lstm_online(cell, ns, x, state_input_op, varscope)\n    out_op, updated_state_op = tf.cond(is_single_step, lambda: fn(1), lambda:\n                                       fn(num_steps))\n\n  return name, state_init_op, updated_state_op, out_op \n\ndef combine_setup(name, combine_type, embed_img, embed_goal, num_img_neuorons=None,\n                  num_goal_neurons=None):\n  with tf.name_scope(name + '_' + combine_type):\n    if combine_type == 'add':\n      # Simple concat features from goal and image\n      out = embed_img + embed_goal\n\n    elif combine_type == 'multiply':\n      # Multiply things together\n      re_embed_img = tf.reshape(\n          embed_img, shape=[-1, num_img_neuorons / num_goal_neurons,\n                            num_goal_neurons])\n      re_embed_goal = tf.reshape(embed_goal, shape=[-1, num_goal_neurons, 1])\n      x = tf.matmul(re_embed_img, re_embed_goal, transpose_a=False, transpose_b=False)\n      out = slim.flatten(x)\n    elif combine_type == 'none' or combine_type == 'imgonly':\n      out = embed_img\n    elif combine_type == 'goalonly':\n      out = embed_goal\n    else:\n      logging.fatal('Undefined combine_type: %s', combine_type)\n  return out\n\n\ndef preprocess_egomotion(locs, thetas):\n  with tf.name_scope('pre_ego'):\n    pre_ego = tf.concat([locs, tf.sin(thetas), tf.cos(thetas)], 2)\n    sh = pre_ego.get_shape().as_list()\n    pre_ego = tf.reshape(pre_ego, [-1, sh[-1]])\n  return pre_ego\n\ndef setup_to_run(m, args, is_training, batch_norm_is_training, summary_mode):\n  # Set up the model.\n  tf.set_random_seed(args.solver.seed)\n  task_params = args.navtask.task_params\n  num_steps = task_params.num_steps\n  num_goals = task_params.num_goals\n  num_actions = task_params.num_actions\n  num_actions_ = num_actions\n\n  n_views = task_params.n_views\n\n  batch_norm_is_training_op = \\\n      tf.placeholder_with_default(batch_norm_is_training, shape=[],\n                                  name='batch_norm_is_training_op') \n  # Setup the inputs\n  m.input_tensors = {}\n  lstm_states = []; lstm_state_dims = [];\n  state_names = []; updated_state_ops = []; init_state_ops = [];\n  if args.arch.lstm_output:\n    lstm_states += ['lstm_output']\n    lstm_state_dims += [args.arch.lstm_output_dim+task_params.num_actions]\n  if args.arch.lstm_ego:\n    lstm_states += ['lstm_ego']\n    lstm_state_dims += [args.arch.lstm_ego_dim + args.arch.lstm_ego_out]\n    lstm_states += ['lstm_img']\n    lstm_state_dims += [args.arch.lstm_img_dim + args.arch.lstm_img_out]\n  elif args.arch.lstm_img:\n    # An LSTM only on the image\n    lstm_states += ['lstm_img']\n    lstm_state_dims += [args.arch.lstm_img_dim + args.arch.lstm_img_out]\n  else:\n    # No LSTMs involved here.\n    None\n\n  m.input_tensors['common'], m.input_tensors['step'], m.input_tensors['train'] = \\\n      _inputs(task_params, lstm_states, lstm_state_dims)\n\n  with tf.name_scope('check_size'):\n    is_single_step = tf.equal(tf.unstack(tf.shape(m.input_tensors['step']['imgs']), \n                                        num=6)[1], 1)\n\n  images_reshaped = tf.reshape(m.input_tensors['step']['imgs'], \n      shape=[-1, task_params.img_height, task_params.img_width,\n             task_params.img_channels], name='re_image')\n\n  rel_goal_loc_reshaped = tf.reshape(m.input_tensors['step']['rel_goal_loc'], \n      shape=[-1, task_params.rel_goal_loc_dim], name='re_rel_goal_loc')\n\n  x, vars_ = get_repr_from_image(\n      images_reshaped, task_params.modalities, task_params.data_augment,\n      args.arch.encoder, args.solver.freeze_conv, args.solver.wt_decay,\n      is_training)\n\n  # Reshape into nice things so that these can be accumulated over time steps\n  # for faster backprop.\n  sh_before = x.get_shape().as_list()\n  m.encoder_output = tf.reshape(\n      x, shape=[task_params.batch_size, -1, n_views] + sh_before[1:])\n  x = tf.reshape(m.encoder_output, shape=[-1] + sh_before[1:])\n\n  # Add a layer to reduce dimensions for a fc layer.\n  if args.arch.dim_reduce_neurons > 0:\n    ks = 1; neurons = args.arch.dim_reduce_neurons;\n    init_var = np.sqrt(2.0/(ks**2)/neurons)\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.conv_feat = slim.conv2d(\n        x, neurons, kernel_size=ks, stride=1, normalizer_fn=slim.batch_norm,\n        normalizer_params=batch_norm_param, padding='SAME', scope='dim_reduce',\n        weights_regularizer=slim.l2_regularizer(args.solver.wt_decay),\n        weights_initializer=tf.random_normal_initializer(stddev=init_var))\n    reshape_conv_feat = slim.flatten(m.conv_feat)\n    sh = reshape_conv_feat.get_shape().as_list()\n    m.reshape_conv_feat = tf.reshape(reshape_conv_feat, \n                                     shape=[-1, sh[1]*n_views])\n\n  # Restore these from a checkpoint.\n  if args.solver.pretrained_path is not None:\n    m.init_fn = slim.assign_from_checkpoint_fn(args.solver.pretrained_path,\n                                               vars_)\n  else:\n    m.init_fn = None\n\n  # Hit the goal_location with a bunch of fully connected layers, to embed it\n  # into some space.\n  with tf.variable_scope('embed_goal'):\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.embed_goal, _ = tf_utils.fc_network(\n        rel_goal_loc_reshaped, neurons=args.arch.goal_embed_neurons,\n        wt_decay=args.solver.wt_decay, name='goal_embed', offset=0,\n        batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n        is_training=is_training)\n  \n  if args.arch.embed_goal_for_state:\n    with tf.variable_scope('embed_goal_for_state'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      m.embed_goal_for_state, _ = tf_utils.fc_network(\n          m.input_tensors['common']['rel_goal_loc_at_start'][:,0,:],\n          neurons=args.arch.goal_embed_neurons, wt_decay=args.solver.wt_decay,\n          name='goal_embed', offset=0, batch_norm_param=batch_norm_param,\n          dropout_ratio=args.arch.fc_dropout, is_training=is_training)\n\n  # Hit the goal_location with a bunch of fully connected layers, to embed it\n  # into some space.\n  with tf.variable_scope('embed_img'):\n    batch_norm_param = args.arch.batch_norm_param\n    batch_norm_param['is_training'] = batch_norm_is_training_op\n    m.embed_img, _ = tf_utils.fc_network(\n        m.reshape_conv_feat, neurons=args.arch.img_embed_neurons,\n        wt_decay=args.solver.wt_decay, name='img_embed', offset=0,\n        batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n        is_training=is_training)\n\n  # For lstm_ego, and lstm_image, embed the ego motion, accumulate it into an\n  # LSTM, combine with image features and accumulate those in an LSTM. Finally\n  # combine what you get from the image LSTM with the goal to output an action.\n  if args.arch.lstm_ego:\n    ego_reshaped = preprocess_egomotion(m.input_tensors['step']['incremental_locs'], \n                                        m.input_tensors['step']['incremental_thetas'])\n    with tf.variable_scope('embed_ego'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      m.embed_ego, _ = tf_utils.fc_network(\n          ego_reshaped, neurons=args.arch.ego_embed_neurons,\n          wt_decay=args.solver.wt_decay, name='ego_embed', offset=0,\n          batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout,\n          is_training=is_training)\n\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_ego', m.embed_ego, task_params.batch_size, is_single_step, \n        args.arch.lstm_ego_dim, args.arch.lstm_ego_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_ego'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n\n    # Combine the output with the vision features.\n    m.img_ego_op = combine_setup('img_ego', args.arch.combine_type_ego,\n                                 m.embed_img, out_op,\n                                 args.arch.img_embed_neurons[-1],\n                                 args.arch.lstm_ego_out)\n\n    # LSTM on these vision features.\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_img', m.img_ego_op, task_params.batch_size, is_single_step, \n        args.arch.lstm_img_dim, args.arch.lstm_img_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_img'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n\n    m.img_for_goal = out_op\n    num_img_for_goal_neurons = args.arch.lstm_img_out\n\n  elif args.arch.lstm_img:\n    # LSTM on just the image features.\n    state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n        'lstm_img', m.embed_img, task_params.batch_size, is_single_step,\n        args.arch.lstm_img_dim, args.arch.lstm_img_out, num_steps*num_goals,\n        m.input_tensors['step']['lstm_img'])\n    state_names += [state_name]\n    init_state_ops += [state_init_op]\n    updated_state_ops += [updated_state_op]\n    m.img_for_goal = out_op\n    num_img_for_goal_neurons = args.arch.lstm_img_out\n\n  else:\n    m.img_for_goal = m.embed_img\n    num_img_for_goal_neurons = args.arch.img_embed_neurons[-1]\n\n\n  if args.arch.use_visit_count:\n    m.embed_visit_count = visit_count_fc(\n        m.input_tensors['step']['visit_count'],\n        m.input_tensors['step']['last_visit'], args.arch.goal_embed_neurons,\n        args.solver.wt_decay, args.arch.fc_dropout, is_training=is_training)\n    m.embed_goal = m.embed_goal + m.embed_visit_count\n  \n  m.combined_f = combine_setup('img_goal', args.arch.combine_type,\n                               m.img_for_goal, m.embed_goal,\n                               num_img_for_goal_neurons,\n                               args.arch.goal_embed_neurons[-1])\n\n  # LSTM on the combined representation.\n  if args.arch.lstm_output:\n    name = 'lstm_output'\n    # A few fully connected layers here.\n    with tf.variable_scope('action_pred'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      x, _ = tf_utils.fc_network(\n          m.combined_f, neurons=args.arch.pred_neurons,\n          wt_decay=args.solver.wt_decay, name='pred', offset=0,\n          batch_norm_param=batch_norm_param, dropout_ratio=args.arch.fc_dropout)\n\n    if args.arch.lstm_output_init_state_from_goal:\n      # Use the goal embedding to initialize the LSTM state.\n      # UGLY CLUGGY HACK: if this is doing computation for a single time step\n      # then this will not involve back prop, so we can use the state input from\n      # the feed dict, otherwise we compute the state representation from the\n      # goal and feed that in. Necessary for using goal location to generate the\n      # state representation.\n      m.embed_goal_for_state = tf.expand_dims(m.embed_goal_for_state, dim=1)\n      state_op = tf.cond(is_single_step, lambda: m.input_tensors['step'][name],\n                         lambda: m.embed_goal_for_state)\n      state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n          name, x, task_params.batch_size, is_single_step,\n          args.arch.lstm_output_dim,\n          num_actions_,\n          num_steps*num_goals, state_op)\n      init_state_ops += [m.embed_goal_for_state]\n    else:\n      state_op = m.input_tensors['step'][name]\n      state_name, state_init_op, updated_state_op, out_op = lstm_setup(\n          name, x, task_params.batch_size, is_single_step,\n          args.arch.lstm_output_dim,\n          num_actions_, num_steps*num_goals, state_op)\n      init_state_ops += [state_init_op]\n\n    state_names += [state_name]\n    updated_state_ops += [updated_state_op]\n\n    out_op = tf.reshape(out_op, shape=[-1, num_actions_])\n    if num_actions_ > num_actions:\n      m.action_logits_op = out_op[:,:num_actions]\n      m.baseline_op = out_op[:,num_actions:]\n    else:\n      m.action_logits_op = out_op\n      m.baseline_op = None\n    m.action_prob_op = tf.nn.softmax(m.action_logits_op)\n\n  else:\n    # A few fully connected layers here.\n    with tf.variable_scope('action_pred'):\n      batch_norm_param = args.arch.batch_norm_param\n      batch_norm_param['is_training'] = batch_norm_is_training_op\n      out_op, _ = tf_utils.fc_network(\n          m.combined_f, neurons=args.arch.pred_neurons,\n          wt_decay=args.solver.wt_decay, name='pred', offset=0,\n          num_pred=num_actions_,\n          batch_norm_param=batch_norm_param,\n          dropout_ratio=args.arch.fc_dropout, is_training=is_training)\n      if num_actions_ > num_actions:\n        m.action_logits_op = out_op[:,:num_actions]\n        m.baseline_op = out_op[:,num_actions:]\n      else:\n        m.action_logits_op = out_op \n        m.baseline_op = None\n      m.action_prob_op = tf.nn.softmax(m.action_logits_op)\n\n  m.train_ops = {}\n  m.train_ops['step'] = m.action_prob_op\n  m.train_ops['common'] = [m.input_tensors['common']['orig_maps'],\n                           m.input_tensors['common']['goal_loc'],\n                           m.input_tensors['common']['rel_goal_loc_at_start']]\n  m.train_ops['state_names'] = state_names\n  m.train_ops['init_state'] = init_state_ops\n  m.train_ops['updated_state'] = updated_state_ops\n  m.train_ops['batch_norm_is_training_op'] = batch_norm_is_training_op\n\n  # Flat list of ops which cache the step data.\n  m.train_ops['step_data_cache'] = [tf.no_op()]\n\n  if args.solver.freeze_conv:\n    m.train_ops['step_data_cache'] = [m.encoder_output]\n  else:\n    m.train_ops['step_data_cache'] = []\n\n  ewma_decay = 0.99 if is_training else 0.0\n  weight = tf.ones_like(m.input_tensors['train']['action'], dtype=tf.float32,\n                        name='weight')\n\n  m.reg_loss_op, m.data_loss_op, m.total_loss_op, m.acc_ops = \\\n    compute_losses_multi_or(\n        m.action_logits_op, m.input_tensors['train']['action'],\n        weights=weight, num_actions=num_actions,\n        data_loss_wt=args.solver.data_loss_wt,\n        reg_loss_wt=args.solver.reg_loss_wt, ewma_decay=ewma_decay)\n\n\n  if args.solver.freeze_conv:\n    vars_to_optimize = list(set(tf.trainable_variables()) - set(vars_))\n  else:\n    vars_to_optimize = None\n\n  m.lr_op, m.global_step_op, m.train_op, m.should_stop_op, m.optimizer, \\\n  m.sync_optimizer = tf_utils.setup_training(\n      m.total_loss_op, \n      args.solver.initial_learning_rate, \n      args.solver.steps_per_decay,\n      args.solver.learning_rate_decay, \n      args.solver.momentum,\n      args.solver.max_steps, \n      args.solver.sync, \n      args.solver.adjust_lr_sync,\n      args.solver.num_workers, \n      args.solver.task,\n      vars_to_optimize=vars_to_optimize,\n      clip_gradient_norm=args.solver.clip_gradient_norm,\n      typ=args.solver.typ, momentum2=args.solver.momentum2,\n      adam_eps=args.solver.adam_eps)\n  \n  \n  if args.arch.sample_gt_prob_type == 'inverse_sigmoid_decay':\n    m.sample_gt_prob_op = tf_utils.inverse_sigmoid_decay(args.arch.isd_k,\n                                                         m.global_step_op)\n  elif args.arch.sample_gt_prob_type == 'zero':\n    m.sample_gt_prob_op = tf.constant(-1.0, dtype=tf.float32)\n  elif args.arch.sample_gt_prob_type.split('_')[0] == 'step':\n    step = int(args.arch.sample_gt_prob_type.split('_')[1])\n    m.sample_gt_prob_op = tf_utils.step_gt_prob(\n        step, m.input_tensors['step']['step_number'][0,0,0])\n  \n  m.sample_action_type = args.arch.action_sample_type\n  m.sample_action_combine_type = args.arch.action_sample_combine_type\n  _add_summaries(m, summary_mode, args.summary.arop_full_summary_iters)\n  \n  m.init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n  m.saver_op = tf.train.Saver(keep_checkpoint_every_n_hours=4,\n                              write_version=tf.train.SaverDef.V2)\n  \n  return m\n", "description": "Models and examples built with TensorFlow", "file_name": "vision_baseline_lstm.py", "id": "a539e402fef0719fc20ed5bac18012e8", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/cognitive_mapping_and_planning/tfcode/vision_baseline_lstm.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}