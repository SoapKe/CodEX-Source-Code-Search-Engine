{"author": "Rochester-NRT", "code": "from AlphaGo.training.reinforcement_policy_trainer import run_training\nfrom AlphaGo.models.policy import CNNPolicy\nimport os\nfrom cProfile import Profile\n\n\narchitecture = {'filters_per_layer': 32, 'layers': 4, 'board': 7}\nfeatures = ['board', 'ones', 'turns_since', 'liberties', 'capture_size',\n            'self_atari_size', 'liberties_after', 'sensibleness']\npolicy = CNNPolicy(features, **architecture)\n\ndatadir = os.path.join('benchmarks', 'data')\nmodelfile = os.path.join(datadir, 'mini_rl_model.json')\nweights = os.path.join(datadir, 'init_weights.hdf5')\noutdir = os.path.join(datadir, 'rl_output')\nstats_file = os.path.join(datadir, 'reinforcement_policy_trainer.prof')\n\nif not os.path.exists(datadir):\n    os.makedirs(datadir)\nif not os.path.exists(weights):\n    policy.model.save_weights(weights)\npolicy.save_model(modelfile)\n\nprofile = Profile()\narguments = (modelfile, weights, outdir, '--learning-rate', '0.001', '--save-every', '2',\n             '--game-batch', '20', '--iterations', '10', '--verbose')\n\nprofile.runcall(run_training, arguments)\nprofile.dump_stats(stats_file)\n", "comments": "  make miniature model playing miniature 7x7 board ", "content": "from AlphaGo.training.reinforcement_policy_trainer import run_training\nfrom AlphaGo.models.policy import CNNPolicy\nimport os\nfrom cProfile import Profile\n\n# make a miniature model for playing on a miniature 7x7 board\narchitecture = {'filters_per_layer': 32, 'layers': 4, 'board': 7}\nfeatures = ['board', 'ones', 'turns_since', 'liberties', 'capture_size',\n            'self_atari_size', 'liberties_after', 'sensibleness']\npolicy = CNNPolicy(features, **architecture)\n\ndatadir = os.path.join('benchmarks', 'data')\nmodelfile = os.path.join(datadir, 'mini_rl_model.json')\nweights = os.path.join(datadir, 'init_weights.hdf5')\noutdir = os.path.join(datadir, 'rl_output')\nstats_file = os.path.join(datadir, 'reinforcement_policy_trainer.prof')\n\nif not os.path.exists(datadir):\n    os.makedirs(datadir)\nif not os.path.exists(weights):\n    policy.model.save_weights(weights)\npolicy.save_model(modelfile)\n\nprofile = Profile()\narguments = (modelfile, weights, outdir, '--learning-rate', '0.001', '--save-every', '2',\n             '--game-batch', '20', '--iterations', '10', '--verbose')\n\nprofile.runcall(run_training, arguments)\nprofile.dump_stats(stats_file)\n", "description": "An independent, student-led replication of DeepMind's 2016 Nature publication, \"Mastering the game of Go with deep neural networks and tree search\" (Nature 529, 484-489, 28 Jan 2016), details of which can be found on their website https://deepmind.com/publications.html.", "file_name": "reinforcement_policy_training_benchmark.py", "id": "55347449ffbab7d88059a322a63c9d45", "language": "Python", "project_name": "RocAlphaGo", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/Rochester-NRT-RocAlphaGo/Rochester-NRT-RocAlphaGo-a39cac2/benchmarks/reinforcement_policy_training_benchmark.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:14:30Z", "url": "https://github.com/Rochester-NRT/RocAlphaGo", "wiki": true}