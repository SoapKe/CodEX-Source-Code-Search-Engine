{"author": "tensorflow", "code": "import pandas as pd\nimport tensorflow as tf\n\nTRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\nTEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n                    'PetalLength', 'PetalWidth', 'Species']\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\n\ndef maybe_download():\n    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n\n    return train_path, test_path\n\ndef load_data(y_name='Species'):\n    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n    train_path, test_path = maybe_download()\n\n    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n    train_x, train_y = train, train.pop(y_name)\n\n    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n    test_x, test_y = test, test.pop(y_name)\n\n    return (train_x, train_y), (test_x, test_y)\n\n\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    \n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    \n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    \n    return dataset\n\n\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        \n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    \n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    \n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    \n    return dataset\n\n\n\n\n\n\n\nCSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n\ndef _parse_line(line):\n    \n    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n\n    \n    features = dict(zip(CSV_COLUMN_NAMES, fields))\n\n    \n    label = features.pop('Species')\n\n    return features, label\n\n\ndef csv_input_fn(csv_path, batch_size):\n    \n    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n\n    \n    dataset = dataset.map(_parse_line)\n\n    \n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    \n    return dataset\n", "comments": "   returns iris dataset (train x  train y)  (test x  test y)         train path  test path   maybe download()      train   pd read csv(train path  names csv column names  header 0)     train x  train   train  train pop(y name)      test   pd read csv(test path  names csv column names  header 0)     test x  test   test  test pop(y name)      return (train x  train y)  (test x  test y)   def train input fn(features  labels  batch size)         an input function training          convert inputs dataset      dataset   tf data dataset tensor slices((dict(features)  labels))        shuffle  repeat  batch examples      dataset   dataset shuffle(1000) repeat() batch(batch size)        return dataset      return dataset   def eval input fn(features  labels  batch size)         an input function evaluation prediction       convert inputs dataset     shuffle  repeat  batch examples     return dataset     no labels  use features     convert inputs dataset     batch examples    return dataset     the remainder file contains simple example csv parser         implemented using  dataset  class      tf parse csv  sets types outputs match examples given         record defaults  argument     decode line fields    pack result dictionary    separate label features    create dataset containing text lines     parse line     shuffle  repeat  batch examples     return dataset  ", "content": "import pandas as pd\nimport tensorflow as tf\n\nTRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\nTEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n                    'PetalLength', 'PetalWidth', 'Species']\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\n\ndef maybe_download():\n    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n\n    return train_path, test_path\n\ndef load_data(y_name='Species'):\n    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n    train_path, test_path = maybe_download()\n\n    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n    train_x, train_y = train, train.pop(y_name)\n\n    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n    test_x, test_y = test, test.pop(y_name)\n\n    return (train_x, train_y), (test_x, test_y)\n\n\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\n\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\n\n# The remainder of this file contains a simple example of a csv parser,\n#     implemented using a the `Dataset` class.\n\n# `tf.parse_csv` sets the types of the outputs to match the examples given in\n#     the `record_defaults` argument.\nCSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n\ndef _parse_line(line):\n    # Decode the line into its fields\n    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n\n    # Pack the result into a dictionary\n    features = dict(zip(CSV_COLUMN_NAMES, fields))\n\n    # Separate the label from the features\n    label = features.pop('Species')\n\n    return features, label\n\n\ndef csv_input_fn(csv_path, batch_size):\n    # Create a dataset containing the text lines.\n    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n\n    # Parse each line.\n    dataset = dataset.map(_parse_line)\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n", "description": "Models and examples built with TensorFlow", "file_name": "iris_data.py", "id": "40becbe976c5b1c0e1627a42bbf62a0b", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/samples/core/get_started/iris_data.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}