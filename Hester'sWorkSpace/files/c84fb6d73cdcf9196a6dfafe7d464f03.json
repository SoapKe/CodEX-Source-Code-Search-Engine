{"author": "tensorflow", "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder_models.VariationalAutoencoder import VariationalAutoencoder\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n\ndef min_max_scale(X_train, X_test):\n    preprocessor = prep.MinMaxScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\n\nX_train, X_test = min_max_scale(mnist.train.images, mnist.test.images)\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 20\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = VariationalAutoencoder(\n    n_input=784,\n    n_hidden=200,\n    optimizer=tf.train.AdamOptimizer(learning_rate = 0.001))\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    \n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        \n        cost = autoencoder.partial_fit(batch_xs)\n        \n        avg_cost += cost / n_samples * batch_size\n\n    \n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%d,' % (epoch + 1),\n              \"Cost:\", \"{:.9f}\".format(avg_cost))\n\nprint(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\n", "comments": "  loop batches    fit training using batch data    compute average loss    display logs per epoch step ", "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder_models.VariationalAutoencoder import VariationalAutoencoder\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n\ndef min_max_scale(X_train, X_test):\n    preprocessor = prep.MinMaxScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\n\nX_train, X_test = min_max_scale(mnist.train.images, mnist.test.images)\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 20\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = VariationalAutoencoder(\n    n_input=784,\n    n_hidden=200,\n    optimizer=tf.train.AdamOptimizer(learning_rate = 0.001))\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%d,' % (epoch + 1),\n              \"Cost:\", \"{:.9f}\".format(avg_cost))\n\nprint(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\n", "description": "Models and examples built with TensorFlow", "file_name": "VariationalAutoencoderRunner.py", "id": "c84fb6d73cdcf9196a6dfafe7d464f03", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/autoencoder/VariationalAutoencoderRunner.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}