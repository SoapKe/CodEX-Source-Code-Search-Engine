{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Implementation of objectives for training stochastic latent variable models.\n\nContains implementations of the Importance Weighted Autoencoder objective (IWAE)\nand the Filtering Variational objective (FIVO).\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport nested_utils as nested\n\n\ndef iwae(cell,\n         inputs,\n         seq_lengths,\n         num_samples=1,\n         parallel_iterations=30,\n         swap_memory=True):\n  \"\"\"Computes the IWAE lower bound on the log marginal probability.\n\n  This method accepts a stochastic latent variable model and some observations\n  and computes a stochastic lower bound on the log marginal probability of the\n  observations. The IWAE estimator is defined by averaging multiple importance\n  weights. For more details see \"Importance Weighted Autoencoders\" by Burda\n  et al. https://arxiv.org/abs/1509.00519.\n\n  When num_samples = 1, this bound becomes the evidence lower bound (ELBO).\n\n  Args:\n    cell: A callable that implements one timestep of the model. See\n      models/vrnn.py for an example.\n    inputs: The inputs to the model. A potentially nested list or tuple of\n      Tensors each of shape [max_seq_len, batch_size, ...]. The Tensors must\n      have a rank at least two and have matching shapes in the first two\n      dimensions, which represent time and the batch respectively. At each\n      timestep 'cell' will be called with a slice of the Tensors in inputs.\n    seq_lengths: A [batch_size] Tensor of ints encoding the length of each\n      sequence in the batch (sequences can be padded to a common length).\n    num_samples: The number of samples to use.\n    parallel_iterations: The number of parallel iterations to use for the\n      internal while loop.\n    swap_memory: Whether GPU-CPU memory swapping should be enabled for the\n      internal while loop.\n\n  Returns:\n    log_p_hat: A Tensor of shape [batch_size] containing IWAE's estimate of the\n      log marginal probability of the observations.\n    kl: A Tensor of shape [batch_size] containing the kl divergence\n      from q(z|x) to p(z), averaged over samples.\n    log_weights: A Tensor of shape [max_seq_len, batch_size, num_samples]\n      containing the log weights at each timestep. Will not be valid for\n      timesteps past the end of a sequence.\n    log_ess: A Tensor of shape [max_seq_len, batch_size] containing the log\n      effective sample size at each timestep. Will not be valid for timesteps\n      past the end of a sequence.\n  \"\"\"\n  batch_size = tf.shape(seq_lengths)[0]\n  max_seq_len = tf.reduce_max(seq_lengths)\n  seq_mask = tf.transpose(\n      tf.sequence_mask(seq_lengths, maxlen=max_seq_len, dtype=tf.float32),\n      perm=[1, 0])\n  if num_samples > 1:\n    inputs, seq_mask = nested.tile_tensors([inputs, seq_mask], [1, num_samples])\n  inputs_ta, mask_ta = nested.tas_for_tensors([inputs, seq_mask], max_seq_len)\n\n  t0 = tf.constant(0, tf.int32)\n  init_states = cell.zero_state(batch_size * num_samples, tf.float32)\n  ta_names = ['log_weights', 'log_ess']\n  tas = [tf.TensorArray(tf.float32, max_seq_len, name='%s_ta' % n)\n         for n in ta_names]\n  log_weights_acc = tf.zeros([num_samples, batch_size], dtype=tf.float32)\n  kl_acc = tf.zeros([num_samples * batch_size], dtype=tf.float32)\n  accs = (log_weights_acc, kl_acc)\n\n  def while_predicate(t, *unused_args):\n    return t < max_seq_len\n\n  def while_step(t, rnn_state, tas, accs):\n    \"\"\"Implements one timestep of IWAE computation.\"\"\"\n    log_weights_acc, kl_acc = accs\n    cur_inputs, cur_mask = nested.read_tas([inputs_ta, mask_ta], t)\n     Run the cell for one step.\n    log_q_z, log_p_z, log_p_x_given_z, kl, new_state = cell(\n        cur_inputs,\n        rnn_state,\n        cur_mask,\n    )\n     Compute the incremental weight and use it to update the current\n     accumulated weight.\n    kl_acc += kl * cur_mask\n    log_alpha = (log_p_x_given_z + log_p_z - log_q_z) * cur_mask\n    log_alpha = tf.reshape(log_alpha, [num_samples, batch_size])\n    log_weights_acc += log_alpha\n     Calculate the effective sample size.\n    ess_num = 2 * tf.reduce_logsumexp(log_weights_acc, axis=0)\n    ess_denom = tf.reduce_logsumexp(2 * log_weights_acc, axis=0)\n    log_ess = ess_num - ess_denom\n     Update the  Tensorarrays and accumulators.\n    ta_updates = [log_weights_acc, log_ess]\n    new_tas = [ta.write(t, x) for ta, x in zip(tas, ta_updates)]\n    new_accs = (log_weights_acc, kl_acc)\n    return t + 1, new_state, new_tas, new_accs\n\n  _, _, tas, accs = tf.while_loop(\n      while_predicate,\n      while_step,\n      loop_vars=(t0, init_states, tas, accs),\n      parallel_iterations=parallel_iterations,\n      swap_memory=swap_memory)\n\n  log_weights, log_ess = [x.stack() for x in tas]\n  final_log_weights, kl = accs\n  log_p_hat = (tf.reduce_logsumexp(final_log_weights, axis=0) -\n               tf.log(tf.to_float(num_samples)))\n  kl = tf.reduce_mean(tf.reshape(kl, [num_samples, batch_size]), axis=0)\n  log_weights = tf.transpose(log_weights, perm=[0, 2, 1])\n  return log_p_hat, kl, log_weights, log_ess\n\n\ndef ess_criterion(num_samples, log_ess, unused_t):\n  \"\"\"A criterion that resamples based on effective sample size.\"\"\"\n  return log_ess <= tf.log(num_samples / 2.0)\n\n\ndef never_resample_criterion(unused_num_samples, log_ess, unused_t):\n  \"\"\"A criterion that never resamples.\"\"\"\n  return tf.cast(tf.zeros_like(log_ess), tf.bool)\n\n\ndef always_resample_criterion(unused_num_samples, log_ess, unused_t):\n  \"\"\"A criterion resamples at every timestep.\"\"\"\n  return tf.cast(tf.ones_like(log_ess), tf.bool)\n\n\ndef fivo(cell,\n         inputs,\n         seq_lengths,\n         num_samples=1,\n         resampling_criterion=ess_criterion,\n         parallel_iterations=30,\n         swap_memory=True,\n         random_seed=None):\n  \"\"\"Computes the FIVO lower bound on the log marginal probability.\n\n  This method accepts a stochastic latent variable model and some observations\n  and computes a stochastic lower bound on the log marginal probability of the\n  observations. The lower bound is defined by a particle filter's unbiased\n  estimate of the marginal probability of the observations. For more details see\n  \"Filtering Variational Objectives\" by Maddison et al.\n  https://arxiv.org/abs/1705.09279.\n\n  When the resampling criterion is \"never resample\", this bound becomes IWAE.\n\n  Args:\n    cell: A callable that implements one timestep of the model. See\n      models/vrnn.py for an example.\n    inputs: The inputs to the model. A potentially nested list or tuple of\n      Tensors each of shape [max_seq_len, batch_size, ...]. The Tensors must\n      have a rank at least two and have matching shapes in the first two\n      dimensions, which represent time and the batch respectively. At each\n      timestep 'cell' will be called with a slice of the Tensors in inputs.\n    seq_lengths: A [batch_size] Tensor of ints encoding the length of each\n      sequence in the batch (sequences can be padded to a common length).\n    num_samples: The number of particles to use in each particle filter.\n    resampling_criterion: The resampling criterion to use for this particle\n      filter. Must accept the number of samples, the effective sample size,\n      and the current timestep and return a boolean Tensor of shape [batch_size]\n      indicating whether each particle filter should resample. See\n      ess_criterion and related functions defined in this file for examples.\n    parallel_iterations: The number of parallel iterations to use for the\n      internal while loop. Note that values greater than 1 can introduce\n      non-determinism even when random_seed is provided.\n    swap_memory: Whether GPU-CPU memory swapping should be enabled for the\n      internal while loop.\n    random_seed: The random seed to pass to the resampling operations in\n      the particle filter. Mainly useful for testing.\n\n  Returns:\n    log_p_hat: A Tensor of shape [batch_size] containing FIVO's estimate of the\n      log marginal probability of the observations.\n    kl: A Tensor of shape [batch_size] containing the sum over time of the kl\n      divergence from q_t(z_t|x) to p_t(z_t), averaged over particles. Note that\n      this includes kl terms from trajectories that are culled during resampling\n      steps.\n    log_weights: A Tensor of shape [max_seq_len, batch_size, num_samples]\n      containing the log weights at each timestep of the particle filter. Note\n      that on timesteps when a resampling operation is performed the log weights\n      are reset to 0. Will not be valid for timesteps past the end of a\n      sequence.\n    log_ess: A Tensor of shape [max_seq_len, batch_size] containing the log\n      effective sample size of each particle filter at each timestep. Will not\n      be valid for timesteps past the end of a sequence.\n    resampled: A Tensor of shape [max_seq_len, batch_size] indicating when the\n      particle filters resampled. Will be 1.0 on timesteps when resampling\n      occurred and 0.0 on timesteps when it did not.\n  \"\"\"\n   batch_size represents the number of particle filters running in parallel.\n  batch_size = tf.shape(seq_lengths)[0]\n  max_seq_len = tf.reduce_max(seq_lengths)\n  seq_mask = tf.transpose(\n      tf.sequence_mask(seq_lengths, maxlen=max_seq_len, dtype=tf.float32),\n      perm=[1, 0])\n\n   Each sequence in the batch will be the input data for a different\n   particle filter. The batch will be laid out as:\n     particle 1 of particle filter 1\n     particle 1 of particle filter 2\n     ...\n     particle 1 of particle filter batch_size\n     particle 2 of particle filter 1\n     ...\n     particle num_samples of particle filter batch_size\n  if num_samples > 1:\n    inputs, seq_mask = nested.tile_tensors([inputs, seq_mask], [1, num_samples])\n  inputs_ta, mask_ta = nested.tas_for_tensors([inputs, seq_mask], max_seq_len)\n\n  t0 = tf.constant(0, tf.int32)\n  init_states = cell.zero_state(batch_size * num_samples, tf.float32)\n  ta_names = ['log_weights', 'log_ess', 'resampled']\n  tas = [tf.TensorArray(tf.float32, max_seq_len, name='%s_ta' % n)\n         for n in ta_names]\n  log_weights_acc = tf.zeros([num_samples, batch_size], dtype=tf.float32)\n  log_p_hat_acc = tf.zeros([batch_size], dtype=tf.float32)\n  kl_acc = tf.zeros([num_samples * batch_size], dtype=tf.float32)\n  accs = (log_weights_acc, log_p_hat_acc, kl_acc)\n\n  def while_predicate(t, *unused_args):\n    return t < max_seq_len\n\n  def while_step(t, rnn_state, tas, accs):\n    \"\"\"Implements one timestep of FIVO computation.\"\"\"\n    log_weights_acc, log_p_hat_acc, kl_acc = accs\n    cur_inputs, cur_mask = nested.read_tas([inputs_ta, mask_ta], t)\n     Run the cell for one step.\n    log_q_z, log_p_z, log_p_x_given_z, kl, new_state = cell(\n        cur_inputs,\n        rnn_state,\n        cur_mask,\n    )\n     Compute the incremental weight and use it to update the current\n     accumulated weight.\n    kl_acc += kl * cur_mask\n    log_alpha = (log_p_x_given_z + log_p_z - log_q_z) * cur_mask\n    log_alpha = tf.reshape(log_alpha, [num_samples, batch_size])\n    log_weights_acc += log_alpha\n     Calculate the effective sample size.\n    ess_num = 2 * tf.reduce_logsumexp(log_weights_acc, axis=0)\n    ess_denom = tf.reduce_logsumexp(2 * log_weights_acc, axis=0)\n    log_ess = ess_num - ess_denom\n     Calculate the ancestor indices via resampling. Because we maintain the\n     log unnormalized weights, we pass the weights in as logits, allowing\n     the distribution object to apply a softmax and normalize them.\n    resampling_dist = tf.contrib.distributions.Categorical(\n        logits=tf.transpose(log_weights_acc, perm=[1, 0]))\n    ancestor_inds = tf.stop_gradient(\n        resampling_dist.sample(sample_shape=num_samples, seed=random_seed))\n     Because the batch is flattened and laid out as discussed\n     above, we must modify ancestor_inds to index the proper samples.\n     The particles in the ith filter are distributed every batch_size rows\n     in the batch, and offset i rows from the top. So, to correct the indices\n     we multiply by the batch_size and add the proper offset. Crucially,\n     when ancestor_inds is flattened the layout of the batch is maintained.\n    offset = tf.expand_dims(tf.range(batch_size), 0)\n    ancestor_inds = tf.reshape(ancestor_inds * batch_size + offset, [-1])\n    noresample_inds = tf.range(num_samples * batch_size)\n     Decide whether or not we should resample; don't resample if we are past\n     the end of a sequence.\n    should_resample = resampling_criterion(num_samples, log_ess, t)\n    should_resample = tf.logical_and(should_resample,\n                                     cur_mask[:batch_size] > 0.)\n    float_should_resample = tf.to_float(should_resample)\n    ancestor_inds = tf.where(\n        tf.tile(should_resample, [num_samples]),\n        ancestor_inds,\n        noresample_inds)\n    new_state = nested.gather_tensors(new_state, ancestor_inds)\n     Update the TensorArrays before we reset the weights so that we capture\n     the incremental weights and not zeros.\n    ta_updates = [log_weights_acc, log_ess, float_should_resample]\n    new_tas = [ta.write(t, x) for ta, x in zip(tas, ta_updates)]\n     For the particle filters that resampled, update log_p_hat and\n     reset weights to zero.\n    log_p_hat_update = tf.reduce_logsumexp(\n        log_weights_acc, axis=0) - tf.log(tf.to_float(num_samples))\n    log_p_hat_acc += log_p_hat_update * float_should_resample\n    log_weights_acc *= (1. - tf.tile(float_should_resample[tf.newaxis, :],\n                                     [num_samples, 1]))\n    new_accs = (log_weights_acc, log_p_hat_acc, kl_acc)\n    return t + 1, new_state, new_tas, new_accs\n\n  _, _, tas, accs = tf.while_loop(\n      while_predicate,\n      while_step,\n      loop_vars=(t0, init_states, tas, accs),\n      parallel_iterations=parallel_iterations,\n      swap_memory=swap_memory)\n\n  log_weights, log_ess, resampled = [x.stack() for x in tas]\n  final_log_weights, log_p_hat, kl = accs\n   Add in the final weight update to log_p_hat.\n  log_p_hat += (tf.reduce_logsumexp(final_log_weights, axis=0) -\n                tf.log(tf.to_float(num_samples)))\n  kl = tf.reduce_mean(tf.reshape(kl, [num_samples, batch_size]), axis=0)\n  log_weights = tf.transpose(log_weights, perm=[0, 2, 1])\n  return log_p_hat, kl, log_weights, log_ess, resampled\n", "comments": "   implementation objectives training stochastic latent variable models   contains implementations importance weighted autoencoder objective (iwae) filtering variational objective (fivo)         future   import absolute import   future   import division   future   import print function  import tensorflow tf  import nested utils nested   def iwae(cell           inputs           seq lengths           num samples 1           parallel iterations 30           swap memory true)       computes iwae lower bound log marginal probability     this method accepts stochastic latent variable model observations   computes stochastic lower bound log marginal probability   observations  the iwae estimator defined averaging multiple importance   weights  for details see  importance weighted autoencoders  burda   et al  https   arxiv org abs 1509 00519     when num samples   1  bound becomes evidence lower bound (elbo)     args      cell  a callable implements one timestep model  see       models vrnn py example      inputs  the inputs model  a potentially nested list tuple       tensors shape  max seq len  batch size        the tensors must       rank least two matching shapes first two       dimensions  represent time batch respectively  at       timestep  cell  called slice tensors inputs      seq lengths  a  batch size  tensor ints encoding length       sequence batch (sequences padded common length)      num samples  the number samples use      parallel iterations  the number parallel iterations use       internal loop      swap memory  whether gpu cpu memory swapping enabled       internal loop     returns      log p hat  a tensor shape  batch size  containing iwae estimate       log marginal probability observations      kl  a tensor shape  batch size  containing kl divergence       q(z x) p(z)  averaged samples      log weights  a tensor shape  max seq len  batch size  num samples        containing log weights timestep  will valid       timesteps past end sequence      log ess  a tensor shape  max seq len  batch size  containing log       effective sample size timestep  will valid timesteps       past end sequence          batch size   tf shape(seq lengths) 0    max seq len   tf reduce max(seq lengths)   seq mask   tf transpose(       tf sequence mask(seq lengths  maxlen max seq len  dtype tf float32)        perm  1  0 )   num samples   1      inputs  seq mask   nested tile tensors( inputs  seq mask    1  num samples )   inputs ta  mask ta   nested tas tensors( inputs  seq mask   max seq len)    t0   tf constant(0  tf int32)   init states   cell zero state(batch size   num samples  tf float32)   ta names     log weights    log ess     tas    tf tensorarray(tf float32  max seq len  name   ta    n)          n ta names    log weights acc   tf zeros( num samples  batch size   dtype tf float32)   kl acc   tf zeros( num samples   batch size   dtype tf float32)   accs   (log weights acc  kl acc)    def predicate(t   unused args)      return   max seq len    def step(t  rnn state  tas  accs)         implements one timestep iwae computation         log weights acc  kl acc   accs     cur inputs  cur mask   nested read tas( inputs ta  mask ta   t)       run cell one step      log q z  log p z  log p x given z  kl  new state   cell(         cur inputs          rnn state          cur mask      )       compute incremental weight use update current       accumulated weight      kl acc    kl   cur mask     log alpha   (log p x given z   log p z   log q z)   cur mask     log alpha   tf reshape(log alpha   num samples  batch size )     log weights acc    log alpha       calculate effective sample size      ess num   2   tf reduce logsumexp(log weights acc  axis 0)     ess denom   tf reduce logsumexp(2   log weights acc  axis 0)     log ess   ess num   ess denom       update  tensorarrays accumulators      ta updates    log weights acc  log ess      new tas    ta write(t  x) ta  x zip(tas  ta updates)      new accs   (log weights acc  kl acc)     return   1  new state  new tas  new accs          tas  accs   tf loop(       predicate        step        loop vars (t0  init states  tas  accs)        parallel iterations parallel iterations        swap memory swap memory)    log weights  log ess    x stack() x tas    final log weights  kl   accs   log p hat   (tf reduce logsumexp(final log weights  axis 0)                  tf log(tf float(num samples)))   kl   tf reduce mean(tf reshape(kl   num samples  batch size )  axis 0)   log weights   tf transpose(log weights  perm  0  2  1 )   return log p hat  kl  log weights  log ess   def ess criterion(num samples  log ess  unused t)       a criterion resamples based effective sample size       return log ess    tf log(num samples   2 0)   def never resample criterion(unused num samples  log ess  unused t)       a criterion never resamples       return tf cast(tf zeros like(log ess)  tf bool)   def always resample criterion(unused num samples  log ess  unused t)       a criterion resamples every timestep       return tf cast(tf ones like(log ess)  tf bool)   def fivo(cell           inputs           seq lengths           num samples 1           resampling criterion ess criterion           parallel iterations 30           swap memory true           random seed none)       computes fivo lower bound log marginal probability     this method accepts stochastic latent variable model observations   computes stochastic lower bound log marginal probability   observations  the lower bound defined particle filter unbiased   estimate marginal probability observations  for details see    filtering variational objectives  maddison et al    https   arxiv org abs 1705 09279     when resampling criterion  never resample   bound becomes iwae     args      cell  a callable implements one timestep model  see       models vrnn py example      inputs  the inputs model  a potentially nested list tuple       tensors shape  max seq len  batch size        the tensors must       rank least two matching shapes first two       dimensions  represent time batch respectively  at       timestep  cell  called slice tensors inputs      seq lengths  a  batch size  tensor ints encoding length       sequence batch (sequences padded common length)      num samples  the number particles use particle filter      resampling criterion  the resampling criterion use particle       filter  must accept number samples  effective sample size        current timestep return boolean tensor shape  batch size        indicating whether particle filter resample  see       ess criterion related functions defined file examples      parallel iterations  the number parallel iterations use       internal loop  note values greater 1 introduce       non determinism even random seed provided      swap memory  whether gpu cpu memory swapping enabled       internal loop      random seed  the random seed pass resampling operations       particle filter  mainly useful testing     returns      log p hat  a tensor shape  batch size  containing fivo estimate       log marginal probability observations      kl  a tensor shape  batch size  containing sum time kl       divergence q t(z x) p t(z t)  averaged particles  note       includes kl terms trajectories culled resampling       steps      log weights  a tensor shape  max seq len  batch size  num samples        containing log weights timestep particle filter  note       timesteps resampling operation performed log weights       reset 0  will valid timesteps past end       sequence      log ess  a tensor shape  max seq len  batch size  containing log       effective sample size particle filter timestep  will       valid timesteps past end sequence      resampled  a tensor shape  max seq len  batch size  indicating       particle filters resampled  will 1 0 timesteps resampling       occurred 0 0 timesteps            batch size represents number particle filters running parallel    batch size   tf shape(seq lengths) 0    max seq len   tf reduce max(seq lengths)   seq mask   tf transpose(       tf sequence mask(seq lengths  maxlen max seq len  dtype tf float32)        perm  1  0 )      each sequence batch input data different     particle filter  the batch laid        particle 1 particle filter 1       particle 1 particle filter 2                 particle 1 particle filter batch size       particle 2 particle filter 1                 particle num samples particle filter batch size   num samples   1      inputs  seq mask   nested tile tensors( inputs  seq mask    1  num samples )   inputs ta  mask ta   nested tas tensors( inputs  seq mask   max seq len)    t0   tf constant(0  tf int32)   init states   cell zero state(batch size   num samples  tf float32)   ta names     log weights    log ess    resampled     tas    tf tensorarray(tf float32  max seq len  name   ta    n)          n ta names    log weights acc   tf zeros( num samples  batch size   dtype tf float32)   log p hat acc   tf zeros( batch size   dtype tf float32)   kl acc   tf zeros( num samples   batch size   dtype tf float32)   accs   (log weights acc  log p hat acc  kl acc)    def predicate(t   unused args)      return   max seq len    def step(t  rnn state  tas  accs)         implements one timestep fivo computation        copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       run cell one step     compute incremental weight use update current    accumulated weight     calculate effective sample size     update  tensorarrays accumulators     batch size represents number particle filters running parallel     each sequence batch input data different    particle filter  the batch laid       particle 1 particle filter 1      particle 1 particle filter 2               particle 1 particle filter batch size      particle 2 particle filter 1               particle num samples particle filter batch size    run cell one step     compute incremental weight use update current    accumulated weight     calculate effective sample size     calculate ancestor indices via resampling  because maintain    log unnormalized weights  pass weights logits  allowing    distribution object apply softmax normalize     because batch flattened laid discussed     must modify ancestor inds index proper samples     the particles ith filter distributed every batch size rows    batch  offset rows top  so  correct indices    multiply batch size add proper offset  crucially     ancestor inds flattened layout batch maintained     decide whether resample  resample past    end sequence     update tensorarrays reset weights capture    incremental weights zeros     for particle filters resampled  update log p hat    reset weights zero     add final weight update log p hat  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Implementation of objectives for training stochastic latent variable models.\n\nContains implementations of the Importance Weighted Autoencoder objective (IWAE)\nand the Filtering Variational objective (FIVO).\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport nested_utils as nested\n\n\ndef iwae(cell,\n         inputs,\n         seq_lengths,\n         num_samples=1,\n         parallel_iterations=30,\n         swap_memory=True):\n  \"\"\"Computes the IWAE lower bound on the log marginal probability.\n\n  This method accepts a stochastic latent variable model and some observations\n  and computes a stochastic lower bound on the log marginal probability of the\n  observations. The IWAE estimator is defined by averaging multiple importance\n  weights. For more details see \"Importance Weighted Autoencoders\" by Burda\n  et al. https://arxiv.org/abs/1509.00519.\n\n  When num_samples = 1, this bound becomes the evidence lower bound (ELBO).\n\n  Args:\n    cell: A callable that implements one timestep of the model. See\n      models/vrnn.py for an example.\n    inputs: The inputs to the model. A potentially nested list or tuple of\n      Tensors each of shape [max_seq_len, batch_size, ...]. The Tensors must\n      have a rank at least two and have matching shapes in the first two\n      dimensions, which represent time and the batch respectively. At each\n      timestep 'cell' will be called with a slice of the Tensors in inputs.\n    seq_lengths: A [batch_size] Tensor of ints encoding the length of each\n      sequence in the batch (sequences can be padded to a common length).\n    num_samples: The number of samples to use.\n    parallel_iterations: The number of parallel iterations to use for the\n      internal while loop.\n    swap_memory: Whether GPU-CPU memory swapping should be enabled for the\n      internal while loop.\n\n  Returns:\n    log_p_hat: A Tensor of shape [batch_size] containing IWAE's estimate of the\n      log marginal probability of the observations.\n    kl: A Tensor of shape [batch_size] containing the kl divergence\n      from q(z|x) to p(z), averaged over samples.\n    log_weights: A Tensor of shape [max_seq_len, batch_size, num_samples]\n      containing the log weights at each timestep. Will not be valid for\n      timesteps past the end of a sequence.\n    log_ess: A Tensor of shape [max_seq_len, batch_size] containing the log\n      effective sample size at each timestep. Will not be valid for timesteps\n      past the end of a sequence.\n  \"\"\"\n  batch_size = tf.shape(seq_lengths)[0]\n  max_seq_len = tf.reduce_max(seq_lengths)\n  seq_mask = tf.transpose(\n      tf.sequence_mask(seq_lengths, maxlen=max_seq_len, dtype=tf.float32),\n      perm=[1, 0])\n  if num_samples > 1:\n    inputs, seq_mask = nested.tile_tensors([inputs, seq_mask], [1, num_samples])\n  inputs_ta, mask_ta = nested.tas_for_tensors([inputs, seq_mask], max_seq_len)\n\n  t0 = tf.constant(0, tf.int32)\n  init_states = cell.zero_state(batch_size * num_samples, tf.float32)\n  ta_names = ['log_weights', 'log_ess']\n  tas = [tf.TensorArray(tf.float32, max_seq_len, name='%s_ta' % n)\n         for n in ta_names]\n  log_weights_acc = tf.zeros([num_samples, batch_size], dtype=tf.float32)\n  kl_acc = tf.zeros([num_samples * batch_size], dtype=tf.float32)\n  accs = (log_weights_acc, kl_acc)\n\n  def while_predicate(t, *unused_args):\n    return t < max_seq_len\n\n  def while_step(t, rnn_state, tas, accs):\n    \"\"\"Implements one timestep of IWAE computation.\"\"\"\n    log_weights_acc, kl_acc = accs\n    cur_inputs, cur_mask = nested.read_tas([inputs_ta, mask_ta], t)\n    # Run the cell for one step.\n    log_q_z, log_p_z, log_p_x_given_z, kl, new_state = cell(\n        cur_inputs,\n        rnn_state,\n        cur_mask,\n    )\n    # Compute the incremental weight and use it to update the current\n    # accumulated weight.\n    kl_acc += kl * cur_mask\n    log_alpha = (log_p_x_given_z + log_p_z - log_q_z) * cur_mask\n    log_alpha = tf.reshape(log_alpha, [num_samples, batch_size])\n    log_weights_acc += log_alpha\n    # Calculate the effective sample size.\n    ess_num = 2 * tf.reduce_logsumexp(log_weights_acc, axis=0)\n    ess_denom = tf.reduce_logsumexp(2 * log_weights_acc, axis=0)\n    log_ess = ess_num - ess_denom\n    # Update the  Tensorarrays and accumulators.\n    ta_updates = [log_weights_acc, log_ess]\n    new_tas = [ta.write(t, x) for ta, x in zip(tas, ta_updates)]\n    new_accs = (log_weights_acc, kl_acc)\n    return t + 1, new_state, new_tas, new_accs\n\n  _, _, tas, accs = tf.while_loop(\n      while_predicate,\n      while_step,\n      loop_vars=(t0, init_states, tas, accs),\n      parallel_iterations=parallel_iterations,\n      swap_memory=swap_memory)\n\n  log_weights, log_ess = [x.stack() for x in tas]\n  final_log_weights, kl = accs\n  log_p_hat = (tf.reduce_logsumexp(final_log_weights, axis=0) -\n               tf.log(tf.to_float(num_samples)))\n  kl = tf.reduce_mean(tf.reshape(kl, [num_samples, batch_size]), axis=0)\n  log_weights = tf.transpose(log_weights, perm=[0, 2, 1])\n  return log_p_hat, kl, log_weights, log_ess\n\n\ndef ess_criterion(num_samples, log_ess, unused_t):\n  \"\"\"A criterion that resamples based on effective sample size.\"\"\"\n  return log_ess <= tf.log(num_samples / 2.0)\n\n\ndef never_resample_criterion(unused_num_samples, log_ess, unused_t):\n  \"\"\"A criterion that never resamples.\"\"\"\n  return tf.cast(tf.zeros_like(log_ess), tf.bool)\n\n\ndef always_resample_criterion(unused_num_samples, log_ess, unused_t):\n  \"\"\"A criterion resamples at every timestep.\"\"\"\n  return tf.cast(tf.ones_like(log_ess), tf.bool)\n\n\ndef fivo(cell,\n         inputs,\n         seq_lengths,\n         num_samples=1,\n         resampling_criterion=ess_criterion,\n         parallel_iterations=30,\n         swap_memory=True,\n         random_seed=None):\n  \"\"\"Computes the FIVO lower bound on the log marginal probability.\n\n  This method accepts a stochastic latent variable model and some observations\n  and computes a stochastic lower bound on the log marginal probability of the\n  observations. The lower bound is defined by a particle filter's unbiased\n  estimate of the marginal probability of the observations. For more details see\n  \"Filtering Variational Objectives\" by Maddison et al.\n  https://arxiv.org/abs/1705.09279.\n\n  When the resampling criterion is \"never resample\", this bound becomes IWAE.\n\n  Args:\n    cell: A callable that implements one timestep of the model. See\n      models/vrnn.py for an example.\n    inputs: The inputs to the model. A potentially nested list or tuple of\n      Tensors each of shape [max_seq_len, batch_size, ...]. The Tensors must\n      have a rank at least two and have matching shapes in the first two\n      dimensions, which represent time and the batch respectively. At each\n      timestep 'cell' will be called with a slice of the Tensors in inputs.\n    seq_lengths: A [batch_size] Tensor of ints encoding the length of each\n      sequence in the batch (sequences can be padded to a common length).\n    num_samples: The number of particles to use in each particle filter.\n    resampling_criterion: The resampling criterion to use for this particle\n      filter. Must accept the number of samples, the effective sample size,\n      and the current timestep and return a boolean Tensor of shape [batch_size]\n      indicating whether each particle filter should resample. See\n      ess_criterion and related functions defined in this file for examples.\n    parallel_iterations: The number of parallel iterations to use for the\n      internal while loop. Note that values greater than 1 can introduce\n      non-determinism even when random_seed is provided.\n    swap_memory: Whether GPU-CPU memory swapping should be enabled for the\n      internal while loop.\n    random_seed: The random seed to pass to the resampling operations in\n      the particle filter. Mainly useful for testing.\n\n  Returns:\n    log_p_hat: A Tensor of shape [batch_size] containing FIVO's estimate of the\n      log marginal probability of the observations.\n    kl: A Tensor of shape [batch_size] containing the sum over time of the kl\n      divergence from q_t(z_t|x) to p_t(z_t), averaged over particles. Note that\n      this includes kl terms from trajectories that are culled during resampling\n      steps.\n    log_weights: A Tensor of shape [max_seq_len, batch_size, num_samples]\n      containing the log weights at each timestep of the particle filter. Note\n      that on timesteps when a resampling operation is performed the log weights\n      are reset to 0. Will not be valid for timesteps past the end of a\n      sequence.\n    log_ess: A Tensor of shape [max_seq_len, batch_size] containing the log\n      effective sample size of each particle filter at each timestep. Will not\n      be valid for timesteps past the end of a sequence.\n    resampled: A Tensor of shape [max_seq_len, batch_size] indicating when the\n      particle filters resampled. Will be 1.0 on timesteps when resampling\n      occurred and 0.0 on timesteps when it did not.\n  \"\"\"\n  # batch_size represents the number of particle filters running in parallel.\n  batch_size = tf.shape(seq_lengths)[0]\n  max_seq_len = tf.reduce_max(seq_lengths)\n  seq_mask = tf.transpose(\n      tf.sequence_mask(seq_lengths, maxlen=max_seq_len, dtype=tf.float32),\n      perm=[1, 0])\n\n  # Each sequence in the batch will be the input data for a different\n  # particle filter. The batch will be laid out as:\n  #   particle 1 of particle filter 1\n  #   particle 1 of particle filter 2\n  #   ...\n  #   particle 1 of particle filter batch_size\n  #   particle 2 of particle filter 1\n  #   ...\n  #   particle num_samples of particle filter batch_size\n  if num_samples > 1:\n    inputs, seq_mask = nested.tile_tensors([inputs, seq_mask], [1, num_samples])\n  inputs_ta, mask_ta = nested.tas_for_tensors([inputs, seq_mask], max_seq_len)\n\n  t0 = tf.constant(0, tf.int32)\n  init_states = cell.zero_state(batch_size * num_samples, tf.float32)\n  ta_names = ['log_weights', 'log_ess', 'resampled']\n  tas = [tf.TensorArray(tf.float32, max_seq_len, name='%s_ta' % n)\n         for n in ta_names]\n  log_weights_acc = tf.zeros([num_samples, batch_size], dtype=tf.float32)\n  log_p_hat_acc = tf.zeros([batch_size], dtype=tf.float32)\n  kl_acc = tf.zeros([num_samples * batch_size], dtype=tf.float32)\n  accs = (log_weights_acc, log_p_hat_acc, kl_acc)\n\n  def while_predicate(t, *unused_args):\n    return t < max_seq_len\n\n  def while_step(t, rnn_state, tas, accs):\n    \"\"\"Implements one timestep of FIVO computation.\"\"\"\n    log_weights_acc, log_p_hat_acc, kl_acc = accs\n    cur_inputs, cur_mask = nested.read_tas([inputs_ta, mask_ta], t)\n    # Run the cell for one step.\n    log_q_z, log_p_z, log_p_x_given_z, kl, new_state = cell(\n        cur_inputs,\n        rnn_state,\n        cur_mask,\n    )\n    # Compute the incremental weight and use it to update the current\n    # accumulated weight.\n    kl_acc += kl * cur_mask\n    log_alpha = (log_p_x_given_z + log_p_z - log_q_z) * cur_mask\n    log_alpha = tf.reshape(log_alpha, [num_samples, batch_size])\n    log_weights_acc += log_alpha\n    # Calculate the effective sample size.\n    ess_num = 2 * tf.reduce_logsumexp(log_weights_acc, axis=0)\n    ess_denom = tf.reduce_logsumexp(2 * log_weights_acc, axis=0)\n    log_ess = ess_num - ess_denom\n    # Calculate the ancestor indices via resampling. Because we maintain the\n    # log unnormalized weights, we pass the weights in as logits, allowing\n    # the distribution object to apply a softmax and normalize them.\n    resampling_dist = tf.contrib.distributions.Categorical(\n        logits=tf.transpose(log_weights_acc, perm=[1, 0]))\n    ancestor_inds = tf.stop_gradient(\n        resampling_dist.sample(sample_shape=num_samples, seed=random_seed))\n    # Because the batch is flattened and laid out as discussed\n    # above, we must modify ancestor_inds to index the proper samples.\n    # The particles in the ith filter are distributed every batch_size rows\n    # in the batch, and offset i rows from the top. So, to correct the indices\n    # we multiply by the batch_size and add the proper offset. Crucially,\n    # when ancestor_inds is flattened the layout of the batch is maintained.\n    offset = tf.expand_dims(tf.range(batch_size), 0)\n    ancestor_inds = tf.reshape(ancestor_inds * batch_size + offset, [-1])\n    noresample_inds = tf.range(num_samples * batch_size)\n    # Decide whether or not we should resample; don't resample if we are past\n    # the end of a sequence.\n    should_resample = resampling_criterion(num_samples, log_ess, t)\n    should_resample = tf.logical_and(should_resample,\n                                     cur_mask[:batch_size] > 0.)\n    float_should_resample = tf.to_float(should_resample)\n    ancestor_inds = tf.where(\n        tf.tile(should_resample, [num_samples]),\n        ancestor_inds,\n        noresample_inds)\n    new_state = nested.gather_tensors(new_state, ancestor_inds)\n    # Update the TensorArrays before we reset the weights so that we capture\n    # the incremental weights and not zeros.\n    ta_updates = [log_weights_acc, log_ess, float_should_resample]\n    new_tas = [ta.write(t, x) for ta, x in zip(tas, ta_updates)]\n    # For the particle filters that resampled, update log_p_hat and\n    # reset weights to zero.\n    log_p_hat_update = tf.reduce_logsumexp(\n        log_weights_acc, axis=0) - tf.log(tf.to_float(num_samples))\n    log_p_hat_acc += log_p_hat_update * float_should_resample\n    log_weights_acc *= (1. - tf.tile(float_should_resample[tf.newaxis, :],\n                                     [num_samples, 1]))\n    new_accs = (log_weights_acc, log_p_hat_acc, kl_acc)\n    return t + 1, new_state, new_tas, new_accs\n\n  _, _, tas, accs = tf.while_loop(\n      while_predicate,\n      while_step,\n      loop_vars=(t0, init_states, tas, accs),\n      parallel_iterations=parallel_iterations,\n      swap_memory=swap_memory)\n\n  log_weights, log_ess, resampled = [x.stack() for x in tas]\n  final_log_weights, log_p_hat, kl = accs\n  # Add in the final weight update to log_p_hat.\n  log_p_hat += (tf.reduce_logsumexp(final_log_weights, axis=0) -\n                tf.log(tf.to_float(num_samples)))\n  kl = tf.reduce_mean(tf.reshape(kl, [num_samples, batch_size]), axis=0)\n  log_weights = tf.transpose(log_weights, perm=[0, 2, 1])\n  return log_p_hat, kl, log_weights, log_ess, resampled\n", "description": "Models and examples built with TensorFlow", "file_name": "bounds.py", "id": "28624c149bf33e803960150a3c24f430", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/fivo/bounds.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}