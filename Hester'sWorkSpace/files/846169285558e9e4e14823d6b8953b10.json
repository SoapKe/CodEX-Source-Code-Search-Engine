{"author": "donnemartin", "code": "\n'''\nThis tutorial requires your machine to have 2 GPUs\n\"/cpu:0\": The CPU of your machine.\n\"/gpu:0\": The first GPU of your machine\n\"/gpu:1\": The second GPU of your machine\n'''\n\nimport numpy as np\nimport tensorflow as tf\nimport datetime\n\n\nlog_device_placement = True\n\n\nn = 10\n\n'''\nExample: compute A^n + B^n on 2 GPUs\nResults on 8 cores with 2 GTX-980:\n * Single GPU computation time: 0:00:11.277449\n * Multi GPU computation time: 0:00:07.131701\n'''\n\nA = np.random.rand(1e4, 1e4).astype('float32')\nB = np.random.rand(1e4, 1e4).astype('float32')\n\n\nc1 = []\nc2 = []\n\ndef matpow(M, n):\n    if n < 1: \n        return M\n    else:\n        return tf.matmul(M, matpow(M, n-1))\n\n'''\nSingle GPU computing\n'''\nwith tf.device('/gpu:0'):\n    a = tf.constant(A)\n    b = tf.constant(B)\n    #compute A^n and B^n and store results in c1\n    c1.append(matpow(a, n))\n    c1.append(matpow(b, n))\n\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n\n\nt1_1 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    \n    sess.run(sum)\nt2_1 = datetime.datetime.now()\n\n\n'''\nMulti GPU computing\n'''\n#GPU:0 computes A^n\nwith tf.device('/gpu:0'):\n    #compute A^n and store result in c2\n    a = tf.constant(A)\n    c2.append(matpow(a, n))\n\n#GPU:1 computes B^n\nwith tf.device('/gpu:1'):\n    #compute B^n and store result in c2\n    b = tf.constant(B)\n    c2.append(matpow(b, n))\n\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n\nt1_2 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    \n    sess.run(sum)\nt2_2 = datetime.datetime.now()\n\n\nprint \"Single GPU computation time: \" + str(t2_1-t1_1)\nprint \"Multi GPU computation time: \" + str(t2_2-t1_2)", "comments": "    this tutorial requires machine 2 gpus   cpu 0   the cpu machine    gpu 0   the first gpu machine   gpu 1   the second gpu machine      import numpy np import tensorflow tf import datetime   processing units logs log device placement   true   num multiplications perform n   10      example  compute a n   b n 2 gpus results 8 cores 2 gtx 980     single gpu computation time  0 00 11 277449    multi gpu computation time  0 00 07 131701      create random large matrix a   np random rand(1e4  1e4) astype( float32 ) b   np random rand(1e4  1e4) astype( float32 )    creates graph store results c1      c2       def matpow(m  n)      n   1   abstract cases n   1         return m     else          return tf matmul(m  matpow(m  n 1))      single gpu computing     tf device(  gpu 0 )        tf constant(a)     b   tf constant(b)      compute a n b n store results c1     c1 append(matpow(a  n))     c1 append(matpow(b  n))  tf device(  cpu 0 )    sum   tf add n(c1)  addition elements c1  e  a n   b n  t1 1   datetime datetime now() tf session(config tf configproto(log device placement log device placement)) sess        runs op      sess run(sum) t2 1   datetime datetime now()       multi gpu computing       multi gpu basic example   processing units logs   num multiplications perform   create random large matrix    creates graph store results   abstract cases n   1   compute a n b n store results c1   addition elements c1  e  a n   b n    runs op    gpu 0 computes a n   compute a n store result c2   gpu 1 computes b n   compute b n store result c2   addition elements c2  e  a n   b n    runs op  ", "content": "#Multi GPU Basic example\n'''\nThis tutorial requires your machine to have 2 GPUs\n\"/cpu:0\": The CPU of your machine.\n\"/gpu:0\": The first GPU of your machine\n\"/gpu:1\": The second GPU of your machine\n'''\n\nimport numpy as np\nimport tensorflow as tf\nimport datetime\n\n#Processing Units logs\nlog_device_placement = True\n\n#num of multiplications to perform\nn = 10\n\n'''\nExample: compute A^n + B^n on 2 GPUs\nResults on 8 cores with 2 GTX-980:\n * Single GPU computation time: 0:00:11.277449\n * Multi GPU computation time: 0:00:07.131701\n'''\n#Create random large matrix\nA = np.random.rand(1e4, 1e4).astype('float32')\nB = np.random.rand(1e4, 1e4).astype('float32')\n\n# Creates a graph to store results\nc1 = []\nc2 = []\n\ndef matpow(M, n):\n    if n < 1: #Abstract cases where n < 1\n        return M\n    else:\n        return tf.matmul(M, matpow(M, n-1))\n\n'''\nSingle GPU computing\n'''\nwith tf.device('/gpu:0'):\n    a = tf.constant(A)\n    b = tf.constant(B)\n    #compute A^n and B^n and store results in c1\n    c1.append(matpow(a, n))\n    c1.append(matpow(b, n))\n\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n\n\nt1_1 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\nt2_1 = datetime.datetime.now()\n\n\n'''\nMulti GPU computing\n'''\n#GPU:0 computes A^n\nwith tf.device('/gpu:0'):\n    #compute A^n and store result in c2\n    a = tf.constant(A)\n    c2.append(matpow(a, n))\n\n#GPU:1 computes B^n\nwith tf.device('/gpu:1'):\n    #compute B^n and store result in c2\n    b = tf.constant(B)\n    c2.append(matpow(b, n))\n\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n\nt1_2 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\nt2_2 = datetime.datetime.now()\n\n\nprint \"Single GPU computation time: \" + str(t2_1-t1_1)\nprint \"Multi GPU computation time: \" + str(t2_2-t1_2)", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "multigpu_basics.py", "id": "846169285558e9e4e14823d6b8953b10", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/deep-learning/tensor-flow-examples/multigpu_basics.py", "save_time": "", "source": "", "update_at": "2018-03-13T23:30:30Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}