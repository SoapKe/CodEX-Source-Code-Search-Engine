{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\nfrom collections import namedtuple\ntry:\n  from queue import Queue   Python 3\nexcept ImportError:\n  from Queue import Queue   Python 2\nimport re\nimport threading\nimport numpy as np\nimport tensorflow as tf\n\nData = namedtuple('Data', ['X', 'Y', 'MultiYs', 'qid'])\n\n\nclass SampleBuilder:\n\n  def __init__(self, config):\n    self.config = config\n\n    self.kb_raw = self.read_kb()\n    self.data_raw = self.read_raw_data()\n\n     dictionary of entities, normal words, and relations\n    self.dict_all = self.gen_dict()\n    self.reverse_dict_all = dict(\n        zip(self.dict_all.values(), self.dict_all.keys()))\n\n    tf.logging.info('size of dict: %d' % len(self.dict_all))\n\n    self.kb = self.build_kb()\n    self.data_all = self.build_samples()\n\n  def read_kb(self):\n    kb_raw = []\n    for line in open(self.config.KB_file):\n      sub, rel, obj = line.strip().split('|')\n      kb_raw.append((sub, rel, obj))\n    tf.logging.info(' of KB records: %d' % len(kb_raw))\n    return kb_raw\n\n  def read_raw_data(self):\n    data = dict()\n    for name in self.config.data_files:\n      raw = []\n      tf.logging.info(\n        'Reading data file {}'.format(self.config.data_files[name]))\n      for line in open(self.config.data_files[name]):\n        question, answers = line.strip().split('\\t')\n        question = question.replace('],', ']')   ignore ',' in the template\n        raw.append((question, answers))\n      data[name] = raw\n    return data\n\n  def build_kb(self):\n    tf.logging.info('Indexing KB...')\n    kb = []\n    for sub, rel, obj in self.kb_raw:\n      kb.append([self.dict_all[sub], self.dict_all[rel], self.dict_all[obj]])\n    return kb\n\n  def gen_dict(self):\n    s = set()\n    for sub, rel, obj in self.kb_raw:\n      s.add(sub)\n      s.add(rel)\n      s.add(obj)\n    for name in self.data_raw:\n      for question, answers in self.data_raw[name]:\n        normal = re.split('\\[[^\\]]+\\]', question)\n        for phrase in normal:\n          for word in phrase.split():\n            s.add(word)\n    s = list(s)\n    d = {s[idx]: idx for idx in range(len(s))}\n    return d\n\n  def build_samples(self):\n\n    def map_entity_idx(text):\n      entities = re.findall('\\[[^\\]]+\\]', text)\n      for entity in entities:\n        entity = entity[1:-1]\n        index = self.dict_all[entity]\n        text = text.replace('[%s]' % entity, '@%d' % index)\n      return text\n\n    data_all = dict()\n\n    for name in self.data_raw:\n      X, Y, MultiYs, qid = [], [], [], []\n      for i, (question, answers) in enumerate(self.data_raw[name]):\n        qdata, labels = [], []\n        question = map_entity_idx(question)\n        for word in question.split():\n          if word[0] == '@':\n            qdata.append(int(word[1:]))\n          else:\n            qdata.append(self.dict_all[word])\n        for answer in answers.split('|'):\n          labels.append(self.dict_all[answer])\n        if len(qdata) > self.config.T_encoder:\n          self.config.T_encoder = len(qdata)\n        for label in labels:\n          X.append(qdata)\n          Y.append(label)\n          MultiYs.append(set(labels))\n          qid.append(i)\n      data_all[name] = Data(X=X, Y=Y, MultiYs=MultiYs, qid=qid)\n\n    return data_all\n\n\ndef _run_prefetch(prefetch_queue, batch_loader, data, shuffle, one_pass,\n                  config):\n  assert len(data.X) == len(data.Y) == len(data.MultiYs) == len(data.qid)\n  num_samples = len(data.X)\n  batch_size = config.batch_size\n\n  n_sample = 0\n  fetch_order = config.rng.permutation(num_samples)\n  while True:\n    sample_ids = fetch_order[n_sample:n_sample + batch_size]\n    batch = batch_loader.load_one_batch(sample_ids)\n    prefetch_queue.put(batch, block=True)\n\n    n_sample += len(sample_ids)\n    if n_sample >= num_samples:\n      if one_pass:\n        prefetch_queue.put(None, block=True)\n      n_sample = 0\n      if shuffle:\n        fetch_order = config.rng.permutation(num_samples)\n\n\nclass DataReader:\n  def __init__(self,\n               config,\n               data,\n               assembler,\n               shuffle=True,\n               one_pass=False,\n               prefetch_num=10):\n    self.config = config\n\n    self.data = data\n    self.assembler = assembler\n    self.batch_loader = BatchLoader(self.config,\n                                    self.data, self.assembler)\n\n    self.shuffle = shuffle\n    self.one_pass = one_pass\n    self.prefetch_queue = Queue(maxsize=prefetch_num)\n    self.prefetch_thread = threading.Thread(target=_run_prefetch,\n                                            args=(self.prefetch_queue,\n                                                  self.batch_loader, self.data,\n                                                  self.shuffle, self.one_pass,\n                                                  self.config))\n    self.prefetch_thread.daemon = True\n    self.prefetch_thread.start()\n\n  def batches(self):\n    while True:\n      if self.prefetch_queue.empty():\n        tf.logging.warning('Waiting for data loading (IO is slow)...')\n      batch = self.prefetch_queue.get(block=True)\n      if batch is None:\n        assert self.one_pass\n        tf.logging.info('One pass finished!')\n        raise StopIteration()\n      yield batch\n\n\nclass BatchLoader:\n  def __init__(self, config,\n               data, assembler):\n    self.config = config\n\n    self.data = data\n    self.assembler = assembler\n\n    self.T_encoder = config.T_encoder\n    self.T_decoder = config.T_decoder\n\n    tf.logging.info('T_encoder: %d' % self.T_encoder)\n    tf.logging.info('T_decoder: %d' % self.T_decoder)\n    tf.logging.info('batch size: %d' % self.config.batch_size)\n\n    self.gt_layout_tokens = config.gt_layout_tokens\n\n  def load_one_batch(self, sample_ids):\n    actual_batch_size = len(sample_ids)\n    input_seq_batch = np.zeros((self.T_encoder, actual_batch_size), np.int32)\n    seq_len_batch = np.zeros(actual_batch_size, np.int32)\n    ans_label_batch = np.zeros(actual_batch_size, np.int32)\n    ans_set_labels_list = [None] * actual_batch_size\n    question_id_list = [None] * actual_batch_size\n    gt_layout_batch = np.zeros((self.T_decoder, actual_batch_size), np.int32)\n\n    for batch_i in range(actual_batch_size):\n      idx = sample_ids[batch_i]\n      seq_len = len(self.data.X[idx])\n      seq_len_batch[batch_i] = seq_len\n      input_seq_batch[:seq_len, batch_i] = self.data.X[idx]\n      ans_label_batch[batch_i] = self.data.Y[idx]\n      ans_set_labels_list[batch_i] = self.data.MultiYs[idx]\n      question_id_list[batch_i] = self.data.qid[idx]\n\n      gt_layout_batch[:, batch_i] = self.assembler.module_list2tokens(\n        self.gt_layout_tokens, self.T_decoder)\n\n    batch = dict(input_seq_batch=input_seq_batch,\n                 seq_len_batch=seq_len_batch,\n                 ans_label_batch=ans_label_batch,\n                 gt_layout_batch=gt_layout_batch,\n                 ans_set_labels_list=ans_set_labels_list,\n                 question_id_list=question_id_list)\n    return batch\n", "comments": "  copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       python 3    python 2    dictionary entities  normal words  relations    ignore     template ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import namedtuple\ntry:\n  from queue import Queue  # Python 3\nexcept ImportError:\n  from Queue import Queue  # Python 2\nimport re\nimport threading\nimport numpy as np\nimport tensorflow as tf\n\nData = namedtuple('Data', ['X', 'Y', 'MultiYs', 'qid'])\n\n\nclass SampleBuilder:\n\n  def __init__(self, config):\n    self.config = config\n\n    self.kb_raw = self.read_kb()\n    self.data_raw = self.read_raw_data()\n\n    # dictionary of entities, normal words, and relations\n    self.dict_all = self.gen_dict()\n    self.reverse_dict_all = dict(\n        zip(self.dict_all.values(), self.dict_all.keys()))\n\n    tf.logging.info('size of dict: %d' % len(self.dict_all))\n\n    self.kb = self.build_kb()\n    self.data_all = self.build_samples()\n\n  def read_kb(self):\n    kb_raw = []\n    for line in open(self.config.KB_file):\n      sub, rel, obj = line.strip().split('|')\n      kb_raw.append((sub, rel, obj))\n    tf.logging.info('# of KB records: %d' % len(kb_raw))\n    return kb_raw\n\n  def read_raw_data(self):\n    data = dict()\n    for name in self.config.data_files:\n      raw = []\n      tf.logging.info(\n        'Reading data file {}'.format(self.config.data_files[name]))\n      for line in open(self.config.data_files[name]):\n        question, answers = line.strip().split('\\t')\n        question = question.replace('],', ']')  # ignore ',' in the template\n        raw.append((question, answers))\n      data[name] = raw\n    return data\n\n  def build_kb(self):\n    tf.logging.info('Indexing KB...')\n    kb = []\n    for sub, rel, obj in self.kb_raw:\n      kb.append([self.dict_all[sub], self.dict_all[rel], self.dict_all[obj]])\n    return kb\n\n  def gen_dict(self):\n    s = set()\n    for sub, rel, obj in self.kb_raw:\n      s.add(sub)\n      s.add(rel)\n      s.add(obj)\n    for name in self.data_raw:\n      for question, answers in self.data_raw[name]:\n        normal = re.split('\\[[^\\]]+\\]', question)\n        for phrase in normal:\n          for word in phrase.split():\n            s.add(word)\n    s = list(s)\n    d = {s[idx]: idx for idx in range(len(s))}\n    return d\n\n  def build_samples(self):\n\n    def map_entity_idx(text):\n      entities = re.findall('\\[[^\\]]+\\]', text)\n      for entity in entities:\n        entity = entity[1:-1]\n        index = self.dict_all[entity]\n        text = text.replace('[%s]' % entity, '@%d' % index)\n      return text\n\n    data_all = dict()\n\n    for name in self.data_raw:\n      X, Y, MultiYs, qid = [], [], [], []\n      for i, (question, answers) in enumerate(self.data_raw[name]):\n        qdata, labels = [], []\n        question = map_entity_idx(question)\n        for word in question.split():\n          if word[0] == '@':\n            qdata.append(int(word[1:]))\n          else:\n            qdata.append(self.dict_all[word])\n        for answer in answers.split('|'):\n          labels.append(self.dict_all[answer])\n        if len(qdata) > self.config.T_encoder:\n          self.config.T_encoder = len(qdata)\n        for label in labels:\n          X.append(qdata)\n          Y.append(label)\n          MultiYs.append(set(labels))\n          qid.append(i)\n      data_all[name] = Data(X=X, Y=Y, MultiYs=MultiYs, qid=qid)\n\n    return data_all\n\n\ndef _run_prefetch(prefetch_queue, batch_loader, data, shuffle, one_pass,\n                  config):\n  assert len(data.X) == len(data.Y) == len(data.MultiYs) == len(data.qid)\n  num_samples = len(data.X)\n  batch_size = config.batch_size\n\n  n_sample = 0\n  fetch_order = config.rng.permutation(num_samples)\n  while True:\n    sample_ids = fetch_order[n_sample:n_sample + batch_size]\n    batch = batch_loader.load_one_batch(sample_ids)\n    prefetch_queue.put(batch, block=True)\n\n    n_sample += len(sample_ids)\n    if n_sample >= num_samples:\n      if one_pass:\n        prefetch_queue.put(None, block=True)\n      n_sample = 0\n      if shuffle:\n        fetch_order = config.rng.permutation(num_samples)\n\n\nclass DataReader:\n  def __init__(self,\n               config,\n               data,\n               assembler,\n               shuffle=True,\n               one_pass=False,\n               prefetch_num=10):\n    self.config = config\n\n    self.data = data\n    self.assembler = assembler\n    self.batch_loader = BatchLoader(self.config,\n                                    self.data, self.assembler)\n\n    self.shuffle = shuffle\n    self.one_pass = one_pass\n    self.prefetch_queue = Queue(maxsize=prefetch_num)\n    self.prefetch_thread = threading.Thread(target=_run_prefetch,\n                                            args=(self.prefetch_queue,\n                                                  self.batch_loader, self.data,\n                                                  self.shuffle, self.one_pass,\n                                                  self.config))\n    self.prefetch_thread.daemon = True\n    self.prefetch_thread.start()\n\n  def batches(self):\n    while True:\n      if self.prefetch_queue.empty():\n        tf.logging.warning('Waiting for data loading (IO is slow)...')\n      batch = self.prefetch_queue.get(block=True)\n      if batch is None:\n        assert self.one_pass\n        tf.logging.info('One pass finished!')\n        raise StopIteration()\n      yield batch\n\n\nclass BatchLoader:\n  def __init__(self, config,\n               data, assembler):\n    self.config = config\n\n    self.data = data\n    self.assembler = assembler\n\n    self.T_encoder = config.T_encoder\n    self.T_decoder = config.T_decoder\n\n    tf.logging.info('T_encoder: %d' % self.T_encoder)\n    tf.logging.info('T_decoder: %d' % self.T_decoder)\n    tf.logging.info('batch size: %d' % self.config.batch_size)\n\n    self.gt_layout_tokens = config.gt_layout_tokens\n\n  def load_one_batch(self, sample_ids):\n    actual_batch_size = len(sample_ids)\n    input_seq_batch = np.zeros((self.T_encoder, actual_batch_size), np.int32)\n    seq_len_batch = np.zeros(actual_batch_size, np.int32)\n    ans_label_batch = np.zeros(actual_batch_size, np.int32)\n    ans_set_labels_list = [None] * actual_batch_size\n    question_id_list = [None] * actual_batch_size\n    gt_layout_batch = np.zeros((self.T_decoder, actual_batch_size), np.int32)\n\n    for batch_i in range(actual_batch_size):\n      idx = sample_ids[batch_i]\n      seq_len = len(self.data.X[idx])\n      seq_len_batch[batch_i] = seq_len\n      input_seq_batch[:seq_len, batch_i] = self.data.X[idx]\n      ans_label_batch[batch_i] = self.data.Y[idx]\n      ans_set_labels_list[batch_i] = self.data.MultiYs[idx]\n      question_id_list[batch_i] = self.data.qid[idx]\n\n      gt_layout_batch[:, batch_i] = self.assembler.module_list2tokens(\n        self.gt_layout_tokens, self.T_decoder)\n\n    batch = dict(input_seq_batch=input_seq_batch,\n                 seq_len_batch=seq_len_batch,\n                 ans_label_batch=ans_label_batch,\n                 gt_layout_batch=gt_layout_batch,\n                 ans_set_labels_list=ans_set_labels_list,\n                 question_id_list=question_id_list)\n    return batch\n", "description": "Models and examples built with TensorFlow", "file_name": "data_reader.py", "id": "cbcb1e3bf1682e6ca6e79e990a742814", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/qa_kg/util/data_reader.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}