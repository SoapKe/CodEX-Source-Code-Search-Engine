{"author": "NVIDIA", "code": "\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\nimport torch.nn as nn\n\n\nclass VGGEncoder(nn.Module):\n    def __init__(self, level):\n        super(VGGEncoder, self).__init__()\n        self.level = level\n        \n        \n        self.conv0 = nn.Conv2d(3, 3, 1, 1, 0)\n        \n        self.pad1_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        \n        self.conv1_1 = nn.Conv2d(3, 64, 3, 1, 0)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        \n        \n        if level < 2: return\n        \n        self.pad1_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv1_2 = nn.Conv2d(64, 64, 3, 1, 0)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        \n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        \n        self.pad2_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv2_1 = nn.Conv2d(64, 128, 3, 1, 0)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        \n        \n        if level < 3: return\n        \n        self.pad2_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv2_2 = nn.Conv2d(128, 128, 3, 1, 0)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        \n        \n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        \n        self.pad3_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_1 = nn.Conv2d(128, 256, 3, 1, 0)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        \n        \n        if level < 4: return\n        \n        self.pad3_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_2 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        \n        \n        self.pad3_3 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_3 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        \n        \n        self.pad3_4 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_4 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_4 = nn.ReLU(inplace=True)\n        \n        \n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        \n        self.pad4_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv4_1 = nn.Conv2d(256, 512, 3, 1, 0)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        \n    \n    def forward(self, x):\n        out = self.conv0(x)\n        \n        out = self.pad1_1(out)\n        out = self.conv1_1(out)\n        out = self.relu1_1(out)\n        \n        if self.level < 2:\n            return out\n        \n        out = self.pad1_2(out)\n        out = self.conv1_2(out)\n        pool1 = self.relu1_2(out)\n        \n        out, pool1_idx = self.maxpool1(pool1)\n        \n        out = self.pad2_1(out)\n        out = self.conv2_1(out)\n        out = self.relu2_1(out)\n        \n        if self.level < 3:\n            return out, pool1_idx, pool1.size()\n        \n        out = self.pad2_2(out)\n        out = self.conv2_2(out)\n        pool2 = self.relu2_2(out)\n        \n        out, pool2_idx = self.maxpool2(pool2)\n        \n        out = self.pad3_1(out)\n        out = self.conv3_1(out)\n        out = self.relu3_1(out)\n        \n        if self.level < 4:\n            return out, pool1_idx, pool1.size(), pool2_idx, pool2.size()\n        \n        out = self.pad3_2(out)\n        out = self.conv3_2(out)\n        out = self.relu3_2(out)\n        \n        out = self.pad3_3(out)\n        out = self.conv3_3(out)\n        out = self.relu3_3(out)\n        \n        out = self.pad3_4(out)\n        out = self.conv3_4(out)\n        pool3 = self.relu3_4(out)\n        out, pool3_idx = self.maxpool3(pool3)\n        \n        out = self.pad4_1(out)\n        out = self.conv4_1(out)\n        out = self.relu4_1(out)\n        \n        return out, pool1_idx, pool1.size(), pool2_idx, pool2.size(), pool3_idx, pool3.size()\n    \n    def forward_multiple(self, x):\n        out = self.conv0(x)\n        \n        out = self.pad1_1(out)\n        out = self.conv1_1(out)\n        out = self.relu1_1(out)\n        \n        if self.level < 2: return out\n        \n        out1 = out\n        \n        out = self.pad1_2(out)\n        out = self.conv1_2(out)\n        pool1 = self.relu1_2(out)\n        \n        out, pool1_idx = self.maxpool1(pool1)\n        \n        out = self.pad2_1(out)\n        out = self.conv2_1(out)\n        out = self.relu2_1(out)\n        \n        if self.level < 3: return out, out1\n        \n        out2 = out\n        \n        out = self.pad2_2(out)\n        out = self.conv2_2(out)\n        pool2 = self.relu2_2(out)\n        \n        out, pool2_idx = self.maxpool2(pool2)\n        \n        out = self.pad3_1(out)\n        out = self.conv3_1(out)\n        out = self.relu3_1(out)\n        \n        if self.level < 4: return out, out2, out1\n        \n        out3 = out\n        \n        out = self.pad3_2(out)\n        out = self.conv3_2(out)\n        out = self.relu3_2(out)\n        \n        out = self.pad3_3(out)\n        out = self.conv3_3(out)\n        out = self.relu3_3(out)\n        \n        out = self.pad3_4(out)\n        out = self.conv3_4(out)\n        pool3 = self.relu3_4(out)\n        out, pool3_idx = self.maxpool3(pool3)\n        \n        out = self.pad4_1(out)\n        out = self.conv4_1(out)\n        out = self.relu4_1(out)\n        \n        return out, out3, out2, out1\n\n\nclass VGGDecoder(nn.Module):\n    def __init__(self, level):\n        super(VGGDecoder, self).__init__()\n        self.level = level\n        \n        if level > 3:\n            self.pad4_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv4_1 = nn.Conv2d(512, 256, 3, 1, 0)\n            self.relu4_1 = nn.ReLU(inplace=True)\n            \n            \n            self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            \n            \n            self.pad3_4 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_4 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_4 = nn.ReLU(inplace=True)\n            \n            \n            self.pad3_3 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_3 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_3 = nn.ReLU(inplace=True)\n            \n            \n            self.pad3_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_2 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_2 = nn.ReLU(inplace=True)\n            \n        \n        if level > 2:\n            self.pad3_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_1 = nn.Conv2d(256, 128, 3, 1, 0)\n            self.relu3_1 = nn.ReLU(inplace=True)\n            \n            \n            self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            \n            \n            self.pad2_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv2_2 = nn.Conv2d(128, 128, 3, 1, 0)\n            self.relu2_2 = nn.ReLU(inplace=True)\n            \n        \n        if level > 1:\n            self.pad2_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv2_1 = nn.Conv2d(128, 64, 3, 1, 0)\n            self.relu2_1 = nn.ReLU(inplace=True)\n            \n            \n            self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            \n            \n            self.pad1_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv1_2 = nn.Conv2d(64, 64, 3, 1, 0)\n            self.relu1_2 = nn.ReLU(inplace=True)\n            \n        \n        if level > 0:\n            self.pad1_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv1_1 = nn.Conv2d(64, 3, 3, 1, 0)\n    \n    def forward(self, x, pool1_idx=None, pool1_size=None, pool2_idx=None, pool2_size=None, pool3_idx=None,\n                pool3_size=None):\n        out = x\n        \n        if self.level > 3:\n            out = self.pad4_1(out)\n            out = self.conv4_1(out)\n            out = self.relu4_1(out)\n            out = self.unpool3(out, pool3_idx, output_size=pool3_size)\n            \n            out = self.pad3_4(out)\n            out = self.conv3_4(out)\n            out = self.relu3_4(out)\n            \n            out = self.pad3_3(out)\n            out = self.conv3_3(out)\n            out = self.relu3_3(out)\n            \n            out = self.pad3_2(out)\n            out = self.conv3_2(out)\n            out = self.relu3_2(out)\n        \n        if self.level > 2:\n            out = self.pad3_1(out)\n            out = self.conv3_1(out)\n            out = self.relu3_1(out)\n            out = self.unpool2(out, pool2_idx, output_size=pool2_size)\n            \n            out = self.pad2_2(out)\n            out = self.conv2_2(out)\n            out = self.relu2_2(out)\n        \n        if self.level > 1:\n            out = self.pad2_1(out)\n            out = self.conv2_1(out)\n            out = self.relu2_1(out)\n            out = self.unpool1(out, pool1_idx, output_size=pool1_size)\n            \n            out = self.pad1_2(out)\n            out = self.conv1_2(out)\n            out = self.relu1_2(out)\n        \n        if self.level > 0:\n            out = self.pad1_1(out)\n            out = self.conv1_1(out)\n        \n        return out\n", "comments": "    copyright (c) 2018 nvidia corporation   all rights reserved  licensed cc by nc sa 4 0 license (https   creativecommons org licenses nc sa 4 0 legalcode)         224 x 224    226 x 226    224 x 224    224 x 224    112 x 112    112 x 112    112 x 112    56 x 56    56 x 56    56 x 56    56 x 56    56 x 56    28 x 28    28 x 28    28 x 28    56 x 56    56 x 56    56 x 56    56 x 56    56 x 56    112 x 112    112 x 112    112 x 112    224 x 224    224 x 224 ", "content": "\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\nimport torch.nn as nn\n\n\nclass VGGEncoder(nn.Module):\n    def __init__(self, level):\n        super(VGGEncoder, self).__init__()\n        self.level = level\n        \n        # 224 x 224\n        self.conv0 = nn.Conv2d(3, 3, 1, 1, 0)\n        \n        self.pad1_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        # 226 x 226\n        self.conv1_1 = nn.Conv2d(3, 64, 3, 1, 0)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        # 224 x 224\n        \n        if level < 2: return\n        \n        self.pad1_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv1_2 = nn.Conv2d(64, 64, 3, 1, 0)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        # 224 x 224\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        # 112 x 112\n        \n        self.pad2_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv2_1 = nn.Conv2d(64, 128, 3, 1, 0)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        # 112 x 112\n        \n        if level < 3: return\n        \n        self.pad2_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv2_2 = nn.Conv2d(128, 128, 3, 1, 0)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        # 112 x 112\n        \n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        # 56 x 56\n        \n        self.pad3_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_1 = nn.Conv2d(128, 256, 3, 1, 0)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        # 56 x 56\n        \n        if level < 4: return\n        \n        self.pad3_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_2 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        # 56 x 56\n        \n        self.pad3_3 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_3 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        # 56 x 56\n        \n        self.pad3_4 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv3_4 = nn.Conv2d(256, 256, 3, 1, 0)\n        self.relu3_4 = nn.ReLU(inplace=True)\n        # 56 x 56\n        \n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        # 28 x 28\n        \n        self.pad4_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n        self.conv4_1 = nn.Conv2d(256, 512, 3, 1, 0)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        # 28 x 28\n    \n    def forward(self, x):\n        out = self.conv0(x)\n        \n        out = self.pad1_1(out)\n        out = self.conv1_1(out)\n        out = self.relu1_1(out)\n        \n        if self.level < 2:\n            return out\n        \n        out = self.pad1_2(out)\n        out = self.conv1_2(out)\n        pool1 = self.relu1_2(out)\n        \n        out, pool1_idx = self.maxpool1(pool1)\n        \n        out = self.pad2_1(out)\n        out = self.conv2_1(out)\n        out = self.relu2_1(out)\n        \n        if self.level < 3:\n            return out, pool1_idx, pool1.size()\n        \n        out = self.pad2_2(out)\n        out = self.conv2_2(out)\n        pool2 = self.relu2_2(out)\n        \n        out, pool2_idx = self.maxpool2(pool2)\n        \n        out = self.pad3_1(out)\n        out = self.conv3_1(out)\n        out = self.relu3_1(out)\n        \n        if self.level < 4:\n            return out, pool1_idx, pool1.size(), pool2_idx, pool2.size()\n        \n        out = self.pad3_2(out)\n        out = self.conv3_2(out)\n        out = self.relu3_2(out)\n        \n        out = self.pad3_3(out)\n        out = self.conv3_3(out)\n        out = self.relu3_3(out)\n        \n        out = self.pad3_4(out)\n        out = self.conv3_4(out)\n        pool3 = self.relu3_4(out)\n        out, pool3_idx = self.maxpool3(pool3)\n        \n        out = self.pad4_1(out)\n        out = self.conv4_1(out)\n        out = self.relu4_1(out)\n        \n        return out, pool1_idx, pool1.size(), pool2_idx, pool2.size(), pool3_idx, pool3.size()\n    \n    def forward_multiple(self, x):\n        out = self.conv0(x)\n        \n        out = self.pad1_1(out)\n        out = self.conv1_1(out)\n        out = self.relu1_1(out)\n        \n        if self.level < 2: return out\n        \n        out1 = out\n        \n        out = self.pad1_2(out)\n        out = self.conv1_2(out)\n        pool1 = self.relu1_2(out)\n        \n        out, pool1_idx = self.maxpool1(pool1)\n        \n        out = self.pad2_1(out)\n        out = self.conv2_1(out)\n        out = self.relu2_1(out)\n        \n        if self.level < 3: return out, out1\n        \n        out2 = out\n        \n        out = self.pad2_2(out)\n        out = self.conv2_2(out)\n        pool2 = self.relu2_2(out)\n        \n        out, pool2_idx = self.maxpool2(pool2)\n        \n        out = self.pad3_1(out)\n        out = self.conv3_1(out)\n        out = self.relu3_1(out)\n        \n        if self.level < 4: return out, out2, out1\n        \n        out3 = out\n        \n        out = self.pad3_2(out)\n        out = self.conv3_2(out)\n        out = self.relu3_2(out)\n        \n        out = self.pad3_3(out)\n        out = self.conv3_3(out)\n        out = self.relu3_3(out)\n        \n        out = self.pad3_4(out)\n        out = self.conv3_4(out)\n        pool3 = self.relu3_4(out)\n        out, pool3_idx = self.maxpool3(pool3)\n        \n        out = self.pad4_1(out)\n        out = self.conv4_1(out)\n        out = self.relu4_1(out)\n        \n        return out, out3, out2, out1\n\n\nclass VGGDecoder(nn.Module):\n    def __init__(self, level):\n        super(VGGDecoder, self).__init__()\n        self.level = level\n        \n        if level > 3:\n            self.pad4_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv4_1 = nn.Conv2d(512, 256, 3, 1, 0)\n            self.relu4_1 = nn.ReLU(inplace=True)\n            # 28 x 28\n            \n            self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            # 56 x 56\n            \n            self.pad3_4 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_4 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_4 = nn.ReLU(inplace=True)\n            # 56 x 56\n            \n            self.pad3_3 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_3 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_3 = nn.ReLU(inplace=True)\n            # 56 x 56\n            \n            self.pad3_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_2 = nn.Conv2d(256, 256, 3, 1, 0)\n            self.relu3_2 = nn.ReLU(inplace=True)\n            # 56 x 56\n        \n        if level > 2:\n            self.pad3_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv3_1 = nn.Conv2d(256, 128, 3, 1, 0)\n            self.relu3_1 = nn.ReLU(inplace=True)\n            # 56 x 56\n            \n            self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            # 112 x 112\n            \n            self.pad2_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv2_2 = nn.Conv2d(128, 128, 3, 1, 0)\n            self.relu2_2 = nn.ReLU(inplace=True)\n            # 112 x 112\n        \n        if level > 1:\n            self.pad2_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv2_1 = nn.Conv2d(128, 64, 3, 1, 0)\n            self.relu2_1 = nn.ReLU(inplace=True)\n            # 112 x 112\n            \n            self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n            # 224 x 224\n            \n            self.pad1_2 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv1_2 = nn.Conv2d(64, 64, 3, 1, 0)\n            self.relu1_2 = nn.ReLU(inplace=True)\n            # 224 x 224\n        \n        if level > 0:\n            self.pad1_1 = nn.ReflectionPad2d((1, 1, 1, 1))\n            self.conv1_1 = nn.Conv2d(64, 3, 3, 1, 0)\n    \n    def forward(self, x, pool1_idx=None, pool1_size=None, pool2_idx=None, pool2_size=None, pool3_idx=None,\n                pool3_size=None):\n        out = x\n        \n        if self.level > 3:\n            out = self.pad4_1(out)\n            out = self.conv4_1(out)\n            out = self.relu4_1(out)\n            out = self.unpool3(out, pool3_idx, output_size=pool3_size)\n            \n            out = self.pad3_4(out)\n            out = self.conv3_4(out)\n            out = self.relu3_4(out)\n            \n            out = self.pad3_3(out)\n            out = self.conv3_3(out)\n            out = self.relu3_3(out)\n            \n            out = self.pad3_2(out)\n            out = self.conv3_2(out)\n            out = self.relu3_2(out)\n        \n        if self.level > 2:\n            out = self.pad3_1(out)\n            out = self.conv3_1(out)\n            out = self.relu3_1(out)\n            out = self.unpool2(out, pool2_idx, output_size=pool2_size)\n            \n            out = self.pad2_2(out)\n            out = self.conv2_2(out)\n            out = self.relu2_2(out)\n        \n        if self.level > 1:\n            out = self.pad2_1(out)\n            out = self.conv2_1(out)\n            out = self.relu2_1(out)\n            out = self.unpool1(out, pool1_idx, output_size=pool1_size)\n            \n            out = self.pad1_2(out)\n            out = self.conv1_2(out)\n            out = self.relu1_2(out)\n        \n        if self.level > 0:\n            out = self.pad1_1(out)\n            out = self.conv1_1(out)\n        \n        return out\n", "description": "Style transfer, deep learning, feature transform", "file_name": "models.py", "id": "5473d0067f1daf7cc43b36d473ff74e2", "language": "Python", "project_name": "FastPhotoStyle", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/NVIDIA-FastPhotoStyle/NVIDIA-FastPhotoStyle-208d4f6/models.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:35:44Z", "url": "https://github.com/NVIDIA/FastPhotoStyle", "wiki": true}