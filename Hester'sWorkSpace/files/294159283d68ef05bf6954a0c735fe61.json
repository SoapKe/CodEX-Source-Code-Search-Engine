{"author": "tensorflow", "code": "\n\n Copyright 2016 Google Inc. All Rights Reserved.\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n\"\"\"Prepare a corpus for processing by swivel.\n\nCreates a sharded word co-occurrence matrix from a text file input corpus.\n\nUsage:\n\n  prep.py --output_dir <output-dir> --input <text-file>\n\nOptions:\n\n  --input <filename>\n      The input text.\n\n  --output_dir <directory>\n      Specifies the output directory where the various Swivel data\n      files should be placed.\n\n  --shard_size <int>\n      Specifies the shard size; default 4096.\n\n  --min_count <int>\n      Specifies the minimum number of times a word should appear\n      to be included in the vocabulary; default 5.\n\n  --max_vocab <int>\n      Specifies the maximum vocabulary size; default shard size\n      times 1024.\n\n  --vocab <filename>\n      Use the specified unigram vocabulary instead of generating\n      it from the corpus.\n\n  --window_size <int>\n      Specifies the window size for computing co-occurrence stats;\n      default 10.\n\n  --bufsz <int>\n      The number of co-occurrences that are buffered; default 16M.\n\n\"\"\"\n\nimport itertools\nimport math\nimport os\nimport struct\nimport sys\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string('input', '', 'The input text.')\nflags.DEFINE_string('output_dir', '/tmp/swivel_data',\n                    'Output directory for Swivel data')\nflags.DEFINE_integer('shard_size', 4096, 'The size for each shard')\nflags.DEFINE_integer('min_count', 5,\n                     'The minimum number of times a word should occur to be '\n                     'included in the vocabulary')\nflags.DEFINE_integer('max_vocab', 4096 * 64, 'The maximum vocabulary size')\nflags.DEFINE_string('vocab', '', 'Vocabulary to use instead of generating one')\nflags.DEFINE_integer('window_size', 10, 'The window size')\nflags.DEFINE_integer('bufsz', 16 * 1024 * 1024,\n                     'The number of co-occurrences to buffer')\n\nFLAGS = flags.FLAGS\n\nshard_cooc_fmt = struct.Struct('iif')\n\n\ndef words(line):\n  \"\"\"Splits a line of text into tokens.\"\"\"\n  return line.strip().split()\n\n\ndef create_vocabulary(lines):\n  \"\"\"Reads text lines and generates a vocabulary.\"\"\"\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  vocab = {}\n  for lineno, line in enumerate(lines, start=1):\n    for word in words(line):\n      vocab.setdefault(word, 0)\n      vocab[word] += 1\n\n    if lineno % 100000 == 0:\n      pos = lines.tell()\n      sys.stdout.write('\\rComputing vocabulary: %0.1f%% (%d/%d)...' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n  sys.stdout.write('\\n')\n\n  vocab = [(tok, n) for tok, n in vocab.iteritems() if n >= FLAGS.min_count]\n  vocab.sort(key=lambda kv: (-kv[1], kv[0]))\n\n  num_words = min(len(vocab), FLAGS.max_vocab)\n  if num_words % FLAGS.shard_size != 0:\n    num_words -= num_words % FLAGS.shard_size\n\n  if not num_words:\n    raise Exception('empty vocabulary')\n\n  print('vocabulary contains %d tokens' % num_words)\n\n  vocab = vocab[:num_words]\n  return [tok for tok, n in vocab]\n\n\ndef write_vocab_and_sums(vocab, sums, vocab_filename, sums_filename):\n  \"\"\"Writes vocabulary and marginal sum files.\"\"\"\n  with open(os.path.join(FLAGS.output_dir, vocab_filename), 'w') as vocab_out:\n    with open(os.path.join(FLAGS.output_dir, sums_filename), 'w') as sums_out:\n      for tok, cnt in itertools.izip(vocab, sums):\n        print >> vocab_out, tok\n        print >> sums_out, cnt\n\n\ndef compute_coocs(lines, vocab):\n  \"\"\"Compute the co-occurrence statistics from the text.\n\n  This generates a temporary file for each shard that contains the intermediate\n  counts from the shard: these counts must be subsequently sorted and collated.\n\n  \"\"\"\n  word_to_id = {tok: idx for idx, tok in enumerate(vocab)}\n\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  shardfiles = {}\n  for row in range(num_shards):\n    for col in range(num_shards):\n      filename = os.path.join(\n          FLAGS.output_dir, 'shard-%03d-%03d.tmp' % (row, col))\n\n      shardfiles[(row, col)] = open(filename, 'w+')\n\n  def flush_coocs():\n    for (row_id, col_id), cnt in coocs.iteritems():\n      row_shard = row_id % num_shards\n      row_off = row_id / num_shards\n      col_shard = col_id % num_shards\n      col_off = col_id / num_shards\n\n       Since we only stored (a, b), we emit both (a, b) and (b, a).\n      shardfiles[(row_shard, col_shard)].write(\n          shard_cooc_fmt.pack(row_off, col_off, cnt))\n\n      shardfiles[(col_shard, row_shard)].write(\n          shard_cooc_fmt.pack(col_off, row_off, cnt))\n\n  coocs = {}\n  sums = [0.0] * len(vocab)\n\n  for lineno, line in enumerate(lines, start=1):\n     Computes the word IDs for each word in the sentence.  This has the effect\n     of \"stretching\" the window past OOV tokens.\n    wids = filter(\n        lambda wid: wid is not None,\n        (word_to_id.get(w) for w in words(line)))\n\n    for pos in xrange(len(wids)):\n      lid = wids[pos]\n      window_extent = min(FLAGS.window_size + 1, len(wids) - pos)\n      for off in xrange(1, window_extent):\n        rid = wids[pos + off]\n        pair = (min(lid, rid), max(lid, rid))\n        count = 1.0 / off\n        sums[lid] += count\n        sums[rid] += count\n        coocs.setdefault(pair, 0.0)\n        coocs[pair] += count\n\n      sums[lid] += 1.0\n      pair = (lid, lid)\n      coocs.setdefault(pair, 0.0)\n      coocs[pair] += 0.5   Only add 1/2 since we output (a, b) and (b, a)\n\n    if lineno % 10000 == 0:\n      pos = lines.tell()\n      sys.stdout.write('\\rComputing co-occurrences: %0.1f%% (%d/%d)...' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n      if len(coocs) > FLAGS.bufsz:\n        flush_coocs()\n        coocs = {}\n\n  flush_coocs()\n  sys.stdout.write('\\n')\n\n  return shardfiles, sums\n\n\ndef write_shards(vocab, shardfiles):\n  \"\"\"Processes the temporary files to generate the final shard data.\n\n  The shard data is stored as a tf.Example protos using a TFRecordWriter. The\n  temporary files are removed from the filesystem once they've been processed.\n\n  \"\"\"\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  ix = 0\n  for (row, col), fh in shardfiles.iteritems():\n    ix += 1\n    sys.stdout.write('\\rwriting shard %d/%d' % (ix, len(shardfiles)))\n    sys.stdout.flush()\n\n     Read the entire binary co-occurrence and unpack it into an array.\n    fh.seek(0)\n    buf = fh.read()\n    os.unlink(fh.name)\n    fh.close()\n\n    coocs = [\n        shard_cooc_fmt.unpack_from(buf, off)\n        for off in range(0, len(buf), shard_cooc_fmt.size)]\n\n     Sort and merge co-occurrences for the same pairs.\n    coocs.sort()\n\n    if coocs:\n      current_pos = 0\n      current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n      for next_pos in range(1, len(coocs)):\n        next_row_col = (coocs[next_pos][0], coocs[next_pos][1])\n        if current_row_col == next_row_col:\n          coocs[current_pos] = (\n              coocs[current_pos][0],\n              coocs[current_pos][1],\n              coocs[current_pos][2] + coocs[next_pos][2])\n        else:\n          current_pos += 1\n          if current_pos < next_pos:\n            coocs[current_pos] = coocs[next_pos]\n\n          current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n\n      coocs = coocs[:(1 + current_pos)]\n\n     Convert to a TF Example proto.\n    def _int64s(xs):\n      return tf.train.Feature(int64_list=tf.train.Int64List(value=list(xs)))\n\n    def _floats(xs):\n      return tf.train.Feature(float_list=tf.train.FloatList(value=list(xs)))\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'global_row': _int64s(\n            row + num_shards * i for i in range(FLAGS.shard_size)),\n        'global_col': _int64s(\n            col + num_shards * i for i in range(FLAGS.shard_size)),\n\n        'sparse_local_row': _int64s(cooc[0] for cooc in coocs),\n        'sparse_local_col': _int64s(cooc[1] for cooc in coocs),\n        'sparse_value': _floats(cooc[2] for cooc in coocs),\n    }))\n\n    filename = os.path.join(FLAGS.output_dir, 'shard-%03d-%03d.pb' % (row, col))\n    with open(filename, 'w') as out:\n      out.write(example.SerializeToString())\n\n  sys.stdout.write('\\n')\n\n\ndef main(_):\n   Create the output directory, if necessary\n  if FLAGS.output_dir and not os.path.isdir(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n   Read the file onces to create the vocabulary.\n  if FLAGS.vocab:\n    with open(FLAGS.vocab, 'r') as lines:\n      vocab = [line.strip() for line in lines]\n  else:\n    with open(FLAGS.input, 'r') as lines:\n      vocab = create_vocabulary(lines)\n\n   Now read the file again to determine the co-occurrence stats.\n  with open(FLAGS.input, 'r') as lines:\n    shardfiles, sums = compute_coocs(lines, vocab)\n\n   Collect individual shards into the shards.recs file.\n  write_shards(vocab, shardfiles)\n\n   Now write the marginals.  They're symmetric for this application.\n  write_vocab_and_sums(vocab, sums, 'row_vocab.txt', 'row_sums.txt')\n  write_vocab_and_sums(vocab, sums, 'col_vocab.txt', 'col_sums.txt')\n\n  print('done!')\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   prepare corpus processing swivel   creates sharded word co occurrence matrix text file input corpus   usage     prep py   output dir  output dir    input  text file   options       input  filename        the input text       output dir  directory        specifies output directory various swivel data       files placed       shard size  int        specifies shard size  default 4096       min count  int        specifies minimum number times word appear       included vocabulary  default 5       max vocab  int        specifies maximum vocabulary size  default shard size       times 1024       vocab  filename        use specified unigram vocabulary instead generating       corpus       window size  int        specifies window size computing co occurrence stats        default 10       bufsz  int        the number co occurrences buffered  default 16m        import itertools import math import os import struct import sys  six moves import xrange import tensorflow tf  flags   tf app flags  flags define string( input        the input text  ) flags define string( output dir     tmp swivel data                        output directory swivel data ) flags define integer( shard size   4096   the size shard ) flags define integer( min count   5                        the minimum number times word occur                         included vocabulary ) flags define integer( max vocab   4096   64   the maximum vocabulary size ) flags define string( vocab        vocabulary use instead generating one ) flags define integer( window size   10   the window size ) flags define integer( bufsz   16   1024   1024                        the number co occurrences buffer )  flags   flags flags  shard cooc fmt   struct struct( iif )   def words(line)       splits line text tokens       return line strip() split()   def create vocabulary(lines)       reads text lines generates vocabulary       lines seek(0  os seek end)   nbytes   lines tell()   lines seek(0  os seek set)    vocab        lineno  line enumerate(lines  start 1)      word words(line)        vocab setdefault(word  0)       vocab word     1      lineno   100000    0        pos   lines tell()       sys stdout write(  rcomputing vocabulary   0 1f   (  d)       (           100 0   pos   nbytes  pos  nbytes))       sys stdout flush()    sys stdout write(  n )    vocab    (tok  n) tok  n vocab iteritems() n    flags min count    vocab sort(key lambda kv  ( kv 1   kv 0 ))    num words   min(len(vocab)  flags max vocab)   num words   flags shard size    0      num words    num words   flags shard size    num words      raise exception( empty vocabulary )    print( vocabulary contains  tokens    num words)    vocab   vocab  num words    return  tok tok  n vocab    def write vocab sums(vocab  sums  vocab filename  sums filename)       writes vocabulary marginal sum files       open(os path join(flags output dir  vocab filename)   w ) vocab      open(os path join(flags output dir  sums filename)   w ) sums        tok  cnt itertools izip(vocab  sums)          print    vocab  tok         print    sums  cnt   def compute coocs(lines  vocab)       compute co occurrence statistics text     this generates temporary file shard contains intermediate   counts shard  counts must subsequently sorted collated           word id    tok  idx idx  tok enumerate(vocab)     lines seek(0  os seek end)   nbytes   lines tell()   lines seek(0  os seek set)    num shards   len(vocab)   flags shard size    shardfiles        row range(num shards)      col range(num shards)        filename   os path join(           flags output dir   shard  03d  03d tmp    (row  col))        shardfiles (row  col)    open(filename   w  )    def flush coocs()      (row id  col id)  cnt coocs iteritems()        row shard   row id   num shards       row   row id   num shards       col shard   col id   num shards       col   col id   num shards          since stored (a  b)  emit (a  b) (b  a)        shardfiles (row shard  col shard)  write(           shard cooc fmt pack(row  col  cnt))        shardfiles (col shard  row shard)  write(           shard cooc fmt pack(col  row  cnt))    coocs        sums    0 0    len(vocab)    lineno  line enumerate(lines  start 1)        computes word ids word sentence   this effect        stretching  window past oov tokens      wids   filter(         lambda wid  wid none          (word id get(w) w words(line)))      pos xrange(len(wids))        lid   wids pos        window extent   min(flags window size   1  len(wids)   pos)       xrange(1  window extent)          rid   wids pos            pair   (min(lid  rid)  max(lid  rid))         count   1 0           sums lid     count         sums rid     count         coocs setdefault(pair  0 0)         coocs pair     count        sums lid     1 0       pair   (lid  lid)       coocs setdefault(pair  0 0)       coocs pair     0 5    only add 1 2 since output (a  b) (b  a)      lineno   10000    0        pos   lines tell()       sys stdout write(  rcomputing co occurrences   0 1f   (  d)       (           100 0   pos   nbytes  pos  nbytes))       sys stdout flush()        len(coocs)   flags bufsz          flush coocs()         coocs         flush coocs()   sys stdout write(  n )    return shardfiles  sums   def write shards(vocab  shardfiles)       processes temporary files generate final shard data     the shard data stored tf example protos using tfrecordwriter  the   temporary files removed filesystem processed             usr bin env python       copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     since stored (a  b)  emit (a  b) (b  a)     computes word ids word sentence   this effect     stretching  window past oov tokens     only add 1 2 since output (a  b) (b  a)    read entire binary co occurrence unpack array     sort merge co occurrences pairs     convert tf example proto     create output directory  necessary    read file onces create vocabulary     now read file determine co occurrence stats     collect individual shards shards recs file     now write marginals   they symmetric application  ", "content": "#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Prepare a corpus for processing by swivel.\n\nCreates a sharded word co-occurrence matrix from a text file input corpus.\n\nUsage:\n\n  prep.py --output_dir <output-dir> --input <text-file>\n\nOptions:\n\n  --input <filename>\n      The input text.\n\n  --output_dir <directory>\n      Specifies the output directory where the various Swivel data\n      files should be placed.\n\n  --shard_size <int>\n      Specifies the shard size; default 4096.\n\n  --min_count <int>\n      Specifies the minimum number of times a word should appear\n      to be included in the vocabulary; default 5.\n\n  --max_vocab <int>\n      Specifies the maximum vocabulary size; default shard size\n      times 1024.\n\n  --vocab <filename>\n      Use the specified unigram vocabulary instead of generating\n      it from the corpus.\n\n  --window_size <int>\n      Specifies the window size for computing co-occurrence stats;\n      default 10.\n\n  --bufsz <int>\n      The number of co-occurrences that are buffered; default 16M.\n\n\"\"\"\n\nimport itertools\nimport math\nimport os\nimport struct\nimport sys\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string('input', '', 'The input text.')\nflags.DEFINE_string('output_dir', '/tmp/swivel_data',\n                    'Output directory for Swivel data')\nflags.DEFINE_integer('shard_size', 4096, 'The size for each shard')\nflags.DEFINE_integer('min_count', 5,\n                     'The minimum number of times a word should occur to be '\n                     'included in the vocabulary')\nflags.DEFINE_integer('max_vocab', 4096 * 64, 'The maximum vocabulary size')\nflags.DEFINE_string('vocab', '', 'Vocabulary to use instead of generating one')\nflags.DEFINE_integer('window_size', 10, 'The window size')\nflags.DEFINE_integer('bufsz', 16 * 1024 * 1024,\n                     'The number of co-occurrences to buffer')\n\nFLAGS = flags.FLAGS\n\nshard_cooc_fmt = struct.Struct('iif')\n\n\ndef words(line):\n  \"\"\"Splits a line of text into tokens.\"\"\"\n  return line.strip().split()\n\n\ndef create_vocabulary(lines):\n  \"\"\"Reads text lines and generates a vocabulary.\"\"\"\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  vocab = {}\n  for lineno, line in enumerate(lines, start=1):\n    for word in words(line):\n      vocab.setdefault(word, 0)\n      vocab[word] += 1\n\n    if lineno % 100000 == 0:\n      pos = lines.tell()\n      sys.stdout.write('\\rComputing vocabulary: %0.1f%% (%d/%d)...' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n  sys.stdout.write('\\n')\n\n  vocab = [(tok, n) for tok, n in vocab.iteritems() if n >= FLAGS.min_count]\n  vocab.sort(key=lambda kv: (-kv[1], kv[0]))\n\n  num_words = min(len(vocab), FLAGS.max_vocab)\n  if num_words % FLAGS.shard_size != 0:\n    num_words -= num_words % FLAGS.shard_size\n\n  if not num_words:\n    raise Exception('empty vocabulary')\n\n  print('vocabulary contains %d tokens' % num_words)\n\n  vocab = vocab[:num_words]\n  return [tok for tok, n in vocab]\n\n\ndef write_vocab_and_sums(vocab, sums, vocab_filename, sums_filename):\n  \"\"\"Writes vocabulary and marginal sum files.\"\"\"\n  with open(os.path.join(FLAGS.output_dir, vocab_filename), 'w') as vocab_out:\n    with open(os.path.join(FLAGS.output_dir, sums_filename), 'w') as sums_out:\n      for tok, cnt in itertools.izip(vocab, sums):\n        print >> vocab_out, tok\n        print >> sums_out, cnt\n\n\ndef compute_coocs(lines, vocab):\n  \"\"\"Compute the co-occurrence statistics from the text.\n\n  This generates a temporary file for each shard that contains the intermediate\n  counts from the shard: these counts must be subsequently sorted and collated.\n\n  \"\"\"\n  word_to_id = {tok: idx for idx, tok in enumerate(vocab)}\n\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  shardfiles = {}\n  for row in range(num_shards):\n    for col in range(num_shards):\n      filename = os.path.join(\n          FLAGS.output_dir, 'shard-%03d-%03d.tmp' % (row, col))\n\n      shardfiles[(row, col)] = open(filename, 'w+')\n\n  def flush_coocs():\n    for (row_id, col_id), cnt in coocs.iteritems():\n      row_shard = row_id % num_shards\n      row_off = row_id / num_shards\n      col_shard = col_id % num_shards\n      col_off = col_id / num_shards\n\n      # Since we only stored (a, b), we emit both (a, b) and (b, a).\n      shardfiles[(row_shard, col_shard)].write(\n          shard_cooc_fmt.pack(row_off, col_off, cnt))\n\n      shardfiles[(col_shard, row_shard)].write(\n          shard_cooc_fmt.pack(col_off, row_off, cnt))\n\n  coocs = {}\n  sums = [0.0] * len(vocab)\n\n  for lineno, line in enumerate(lines, start=1):\n    # Computes the word IDs for each word in the sentence.  This has the effect\n    # of \"stretching\" the window past OOV tokens.\n    wids = filter(\n        lambda wid: wid is not None,\n        (word_to_id.get(w) for w in words(line)))\n\n    for pos in xrange(len(wids)):\n      lid = wids[pos]\n      window_extent = min(FLAGS.window_size + 1, len(wids) - pos)\n      for off in xrange(1, window_extent):\n        rid = wids[pos + off]\n        pair = (min(lid, rid), max(lid, rid))\n        count = 1.0 / off\n        sums[lid] += count\n        sums[rid] += count\n        coocs.setdefault(pair, 0.0)\n        coocs[pair] += count\n\n      sums[lid] += 1.0\n      pair = (lid, lid)\n      coocs.setdefault(pair, 0.0)\n      coocs[pair] += 0.5  # Only add 1/2 since we output (a, b) and (b, a)\n\n    if lineno % 10000 == 0:\n      pos = lines.tell()\n      sys.stdout.write('\\rComputing co-occurrences: %0.1f%% (%d/%d)...' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n      if len(coocs) > FLAGS.bufsz:\n        flush_coocs()\n        coocs = {}\n\n  flush_coocs()\n  sys.stdout.write('\\n')\n\n  return shardfiles, sums\n\n\ndef write_shards(vocab, shardfiles):\n  \"\"\"Processes the temporary files to generate the final shard data.\n\n  The shard data is stored as a tf.Example protos using a TFRecordWriter. The\n  temporary files are removed from the filesystem once they've been processed.\n\n  \"\"\"\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  ix = 0\n  for (row, col), fh in shardfiles.iteritems():\n    ix += 1\n    sys.stdout.write('\\rwriting shard %d/%d' % (ix, len(shardfiles)))\n    sys.stdout.flush()\n\n    # Read the entire binary co-occurrence and unpack it into an array.\n    fh.seek(0)\n    buf = fh.read()\n    os.unlink(fh.name)\n    fh.close()\n\n    coocs = [\n        shard_cooc_fmt.unpack_from(buf, off)\n        for off in range(0, len(buf), shard_cooc_fmt.size)]\n\n    # Sort and merge co-occurrences for the same pairs.\n    coocs.sort()\n\n    if coocs:\n      current_pos = 0\n      current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n      for next_pos in range(1, len(coocs)):\n        next_row_col = (coocs[next_pos][0], coocs[next_pos][1])\n        if current_row_col == next_row_col:\n          coocs[current_pos] = (\n              coocs[current_pos][0],\n              coocs[current_pos][1],\n              coocs[current_pos][2] + coocs[next_pos][2])\n        else:\n          current_pos += 1\n          if current_pos < next_pos:\n            coocs[current_pos] = coocs[next_pos]\n\n          current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n\n      coocs = coocs[:(1 + current_pos)]\n\n    # Convert to a TF Example proto.\n    def _int64s(xs):\n      return tf.train.Feature(int64_list=tf.train.Int64List(value=list(xs)))\n\n    def _floats(xs):\n      return tf.train.Feature(float_list=tf.train.FloatList(value=list(xs)))\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'global_row': _int64s(\n            row + num_shards * i for i in range(FLAGS.shard_size)),\n        'global_col': _int64s(\n            col + num_shards * i for i in range(FLAGS.shard_size)),\n\n        'sparse_local_row': _int64s(cooc[0] for cooc in coocs),\n        'sparse_local_col': _int64s(cooc[1] for cooc in coocs),\n        'sparse_value': _floats(cooc[2] for cooc in coocs),\n    }))\n\n    filename = os.path.join(FLAGS.output_dir, 'shard-%03d-%03d.pb' % (row, col))\n    with open(filename, 'w') as out:\n      out.write(example.SerializeToString())\n\n  sys.stdout.write('\\n')\n\n\ndef main(_):\n  # Create the output directory, if necessary\n  if FLAGS.output_dir and not os.path.isdir(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  # Read the file onces to create the vocabulary.\n  if FLAGS.vocab:\n    with open(FLAGS.vocab, 'r') as lines:\n      vocab = [line.strip() for line in lines]\n  else:\n    with open(FLAGS.input, 'r') as lines:\n      vocab = create_vocabulary(lines)\n\n  # Now read the file again to determine the co-occurrence stats.\n  with open(FLAGS.input, 'r') as lines:\n    shardfiles, sums = compute_coocs(lines, vocab)\n\n  # Collect individual shards into the shards.recs file.\n  write_shards(vocab, shardfiles)\n\n  # Now write the marginals.  They're symmetric for this application.\n  write_vocab_and_sums(vocab, sums, 'row_vocab.txt', 'row_sums.txt')\n  write_vocab_and_sums(vocab, sums, 'col_vocab.txt', 'col_sums.txt')\n\n  print('done!')\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "prep.py", "id": "294159283d68ef05bf6954a0c735fe61", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/swivel/prep.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}