{"author": "aws", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\"). You\n may not use this file except in compliance with the License. A copy of\n the License is located at\n\n     http://aws.amazon.com/apache2.0/\n\n or in the \"license\" file accompanying this file. This file is\n distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n ANY KIND, either express or implied. See the License for the specific\n language governing permissions and limitations under the License.\n\n\n Note that all of these functions can be found in the unit tests.\n The only difference is that these tests use botocore's actual session\n variables to communicate with s3 as these are integration tests.  Therefore,\n only tests that use sessions are included as integration tests.\n\nimport unittest\nimport os\nimport itertools\n\nimport botocore.session\nfrom awscli import EnvironmentVariables\nfrom awscli.customizations.s3.filegenerator import FileGenerator, FileStat\nfrom tests.unit.customizations.s3 import compare_files\nfrom tests.integration.customizations.s3 import make_s3_files, s3_cleanup\n\n\nclass S3FileGeneratorIntTest(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session(EnvironmentVariables)\n         Use the datetime and and blob parsing of the CLI\n        factory = self.session.get_component('response_parser_factory')\n        factory.set_parser_defaults(\n            blob_parser=lambda x: x,\n            timestamp_parser=lambda x: x)\n        self.client = self.session.create_client('s3', region_name='us-west-2')\n        self.bucket = make_s3_files(self.session)\n        self.file1 = self.bucket + '/' + 'text1.txt'\n        self.file2 = self.bucket + '/' + 'another_directory/text2.txt'\n\n    def tearDown(self):\n        s3_cleanup(self.bucket, self.session)\n\n    def test_s3_file(self):\n        \n         Generate a single s3 file\n         Note: Size and last update are not tested because s3 generates them.\n        \n        input_s3_file = {'src': {'path': self.file1, 'type': 's3'},\n                         'dest': {'path': 'text1.txt', 'type': 'local'},\n                         'dir_op': False, 'use_src_name': False}\n        expected_file_size = 15\n        result_list = list(\n            FileGenerator(self.client, '').call(input_s3_file))\n        file_stat = FileStat(src=self.file1, dest='text1.txt',\n                             compare_key='text1.txt',\n                             size=expected_file_size,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n\n        expected_list = [file_stat]\n        self.assertEqual(len(result_list), 1)\n        compare_files(self, result_list[0], expected_list[0])\n\n    def test_s3_directory(self):\n        \n         Generates s3 files under a common prefix. Also it ensures that\n         zero size files are ignored.\n         Note: Size and last update are not tested because s3 generates them.\n        \n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        result_list = list(\n            FileGenerator(self.client, '').call(input_s3_file))\n        file_stat = FileStat(src=self.file2,\n                             dest='another_directory' + os.sep + 'text2.txt',\n                             compare_key='another_directory/text2.txt',\n                             size=21,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n        file_stat2 = FileStat(src=self.file1,\n                              dest='text1.txt',\n                              compare_key='text1.txt',\n                              size=15,\n                              last_update=result_list[1].last_update,\n                              src_type='s3',\n                              dest_type='local', operation_name='')\n\n        expected_result = [file_stat, file_stat2]\n        self.assertEqual(len(result_list), 2)\n        compare_files(self, result_list[0], expected_result[0])\n        compare_files(self, result_list[1], expected_result[1])\n\n    def test_s3_delete_directory(self):\n        \n         Generates s3 files under a common prefix. Also it ensures that\n         the directory itself is included because it is a delete command\n         Note: Size and last update are not tested because s3 generates them.\n        \n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        result_list = list(\n            FileGenerator(self.client, 'delete').call(input_s3_file))\n\n        file_stat1 = FileStat(\n            src=self.bucket + '/another_directory/',\n            dest='another_directory' + os.sep,\n            compare_key='another_directory/',\n            size=0,\n            last_update=result_list[0].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n        file_stat2 = FileStat(\n            src=self.file2,\n            dest='another_directory' + os.sep + 'text2.txt',\n            compare_key='another_directory/text2.txt',\n            size=21,\n            last_update=result_list[1].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n        file_stat3 = FileStat(\n            src=self.file1,\n            dest='text1.txt',\n            compare_key='text1.txt',\n            size=15,\n            last_update=result_list[2].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n\n        expected_list = [file_stat1, file_stat2, file_stat3]\n        self.assertEqual(len(result_list), 3)\n        compare_files(self, result_list[0], expected_list[0])\n        compare_files(self, result_list[1], expected_list[1])\n        compare_files(self, result_list[2], expected_list[2])\n\n    def test_page_size(self):\n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        file_gen = FileGenerator(self.client, '',\n                                 page_size=1).call(input_s3_file)\n        limited_file_gen = itertools.islice(file_gen, 1)\n        result_list = list(limited_file_gen)\n        file_stat = FileStat(src=self.file2,\n                             dest='another_directory' + os.sep + 'text2.txt',\n                             compare_key='another_directory/text2.txt',\n                             size=21,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n         Ensure only one item is returned from ``ListObjects``\n        self.assertEqual(len(result_list), 1)\n        compare_files(self, result_list[0], file_stat)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "comments": "  copyright 2013 amazon com  inc  affiliates  all rights reserved        licensed apache license  version 2 0 (the  license )  you    may use file except compliance license  a copy    license located           http   aws amazon com apache2 0         license  file accompanying file  this file    distributed  as is  basis  without warranties or conditions of    any kind  either express implied  see license specific    language governing permissions limitations license     note functions found unit tests     the difference tests use botocore actual session    variables communicate s3 integration tests   therefore     tests use sessions included integration tests     use datetime blob parsing cli       generate single s3 file    note  size last update tested s3 generates           generates s3 files common prefix  also ensures    zero size files ignored     note  size last update tested s3 generates           generates s3 files common prefix  also ensures    directory included delete command    note  size last update tested s3 generates        ensure one item returned   listobjects   ", "content": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\n# Note that all of these functions can be found in the unit tests.\n# The only difference is that these tests use botocore's actual session\n# variables to communicate with s3 as these are integration tests.  Therefore,\n# only tests that use sessions are included as integration tests.\n\nimport unittest\nimport os\nimport itertools\n\nimport botocore.session\nfrom awscli import EnvironmentVariables\nfrom awscli.customizations.s3.filegenerator import FileGenerator, FileStat\nfrom tests.unit.customizations.s3 import compare_files\nfrom tests.integration.customizations.s3 import make_s3_files, s3_cleanup\n\n\nclass S3FileGeneratorIntTest(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session(EnvironmentVariables)\n        # Use the datetime and and blob parsing of the CLI\n        factory = self.session.get_component('response_parser_factory')\n        factory.set_parser_defaults(\n            blob_parser=lambda x: x,\n            timestamp_parser=lambda x: x)\n        self.client = self.session.create_client('s3', region_name='us-west-2')\n        self.bucket = make_s3_files(self.session)\n        self.file1 = self.bucket + '/' + 'text1.txt'\n        self.file2 = self.bucket + '/' + 'another_directory/text2.txt'\n\n    def tearDown(self):\n        s3_cleanup(self.bucket, self.session)\n\n    def test_s3_file(self):\n        #\n        # Generate a single s3 file\n        # Note: Size and last update are not tested because s3 generates them.\n        #\n        input_s3_file = {'src': {'path': self.file1, 'type': 's3'},\n                         'dest': {'path': 'text1.txt', 'type': 'local'},\n                         'dir_op': False, 'use_src_name': False}\n        expected_file_size = 15\n        result_list = list(\n            FileGenerator(self.client, '').call(input_s3_file))\n        file_stat = FileStat(src=self.file1, dest='text1.txt',\n                             compare_key='text1.txt',\n                             size=expected_file_size,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n\n        expected_list = [file_stat]\n        self.assertEqual(len(result_list), 1)\n        compare_files(self, result_list[0], expected_list[0])\n\n    def test_s3_directory(self):\n        #\n        # Generates s3 files under a common prefix. Also it ensures that\n        # zero size files are ignored.\n        # Note: Size and last update are not tested because s3 generates them.\n        #\n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        result_list = list(\n            FileGenerator(self.client, '').call(input_s3_file))\n        file_stat = FileStat(src=self.file2,\n                             dest='another_directory' + os.sep + 'text2.txt',\n                             compare_key='another_directory/text2.txt',\n                             size=21,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n        file_stat2 = FileStat(src=self.file1,\n                              dest='text1.txt',\n                              compare_key='text1.txt',\n                              size=15,\n                              last_update=result_list[1].last_update,\n                              src_type='s3',\n                              dest_type='local', operation_name='')\n\n        expected_result = [file_stat, file_stat2]\n        self.assertEqual(len(result_list), 2)\n        compare_files(self, result_list[0], expected_result[0])\n        compare_files(self, result_list[1], expected_result[1])\n\n    def test_s3_delete_directory(self):\n        #\n        # Generates s3 files under a common prefix. Also it ensures that\n        # the directory itself is included because it is a delete command\n        # Note: Size and last update are not tested because s3 generates them.\n        #\n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        result_list = list(\n            FileGenerator(self.client, 'delete').call(input_s3_file))\n\n        file_stat1 = FileStat(\n            src=self.bucket + '/another_directory/',\n            dest='another_directory' + os.sep,\n            compare_key='another_directory/',\n            size=0,\n            last_update=result_list[0].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n        file_stat2 = FileStat(\n            src=self.file2,\n            dest='another_directory' + os.sep + 'text2.txt',\n            compare_key='another_directory/text2.txt',\n            size=21,\n            last_update=result_list[1].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n        file_stat3 = FileStat(\n            src=self.file1,\n            dest='text1.txt',\n            compare_key='text1.txt',\n            size=15,\n            last_update=result_list[2].last_update,\n            src_type='s3',\n            dest_type='local', operation_name='delete')\n\n        expected_list = [file_stat1, file_stat2, file_stat3]\n        self.assertEqual(len(result_list), 3)\n        compare_files(self, result_list[0], expected_list[0])\n        compare_files(self, result_list[1], expected_list[1])\n        compare_files(self, result_list[2], expected_list[2])\n\n    def test_page_size(self):\n        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},\n                         'dest': {'path': '', 'type': 'local'},\n                         'dir_op': True, 'use_src_name': True}\n        file_gen = FileGenerator(self.client, '',\n                                 page_size=1).call(input_s3_file)\n        limited_file_gen = itertools.islice(file_gen, 1)\n        result_list = list(limited_file_gen)\n        file_stat = FileStat(src=self.file2,\n                             dest='another_directory' + os.sep + 'text2.txt',\n                             compare_key='another_directory/text2.txt',\n                             size=21,\n                             last_update=result_list[0].last_update,\n                             src_type='s3',\n                             dest_type='local', operation_name='')\n        # Ensure only one item is returned from ``ListObjects``\n        self.assertEqual(len(result_list), 1)\n        compare_files(self, result_list[0], file_stat)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "description": "Universal Command Line Interface for Amazon Web Services", "file_name": "test_filegenerator.py", "id": "c719cd74e26d1600777d3d4ab8b97cf6", "language": "Python", "project_name": "aws-cli", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/aws-aws-cli/aws-aws-cli-d705c60/tests/integration/customizations/s3/test_filegenerator.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:33:26Z", "url": "https://github.com/aws/aws-cli", "wiki": false}