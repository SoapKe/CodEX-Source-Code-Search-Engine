{"author": "tensorflow", "code": "\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python import graph_builder\nfrom dragnn.python import lexicon\nfrom dragnn.python import spec_builder\nfrom dragnn.python import visualization\nfrom syntaxnet import sentence_pb2\n\nimport dragnn.python.load_dragnn_cc_impl\nimport syntaxnet.load_parser_ops\n\ndata_dir = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), 'tutorial_data')\nlexicon_dir = '/tmp/tutorial/lexicon'\ntraining_sentence = os.path.join(data_dir, 'sentence.prototext')\nif not os.path.isdir(lexicon_dir):\n  os.makedirs(lexicon_dir)\n\n\ndef main(argv):\n  del argv  \n  \n  \n  lexicon.build_lexicon(\n      lexicon_dir,\n      training_sentence,\n      training_corpus_format='sentence-prototext')\n\n  \n  \n  tagger = spec_builder.ComponentSpecBuilder('tagger')\n  tagger.set_network_unit(name='FeedForwardNetwork', hidden_layer_sizes='256')\n  tagger.set_transition_system(name='tagger')\n  tagger.add_fixed_feature(name='words', fml='input.word', embedding_dim=64)\n  tagger.add_rnn_link(embedding_dim=-1)\n  tagger.fill_from_resources(lexicon_dir)\n\n  master_spec = spec_pb2.MasterSpec()\n  master_spec.component.extend([tagger.spec])\n\n  hyperparam_config = spec_pb2.GridPoint()\n\n  \n  graph = tf.Graph()\n  with graph.as_default():\n    builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)\n\n    target = spec_pb2.TrainTarget()\n    target.name = 'all'\n    target.unroll_using_oracle.extend([True])\n    dry_run = builder.add_training_from_config(target, trace_only=True)\n\n  \n  sentence = sentence_pb2.Sentence()\n  text_format.Merge(open(training_sentence).read(), sentence)\n  training_set = [sentence.SerializeToString()]\n\n  with tf.Session(graph=graph) as sess:\n    \n    sess.run(tf.initialize_all_variables())\n    traces = sess.run(\n        dry_run['traces'], feed_dict={dry_run['input_batch']: training_set})\n\n  with open('dragnn_tutorial_1.html', 'w') as f:\n    f.write(visualization.trace_html(traces[0], height='300px').encode('utf-8'))\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   first example  rnn pos tagger        unused    constructs lexical resources syntaxnet given resource path     training data     construct componentspec tagging  this simple left right rnn    sequence tagger     build tensorflow graph     read serialized protos training data     make sure initialize underlying state  ", "content": "\"\"\"First example--RNN POS tagger.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python import graph_builder\nfrom dragnn.python import lexicon\nfrom dragnn.python import spec_builder\nfrom dragnn.python import visualization\nfrom syntaxnet import sentence_pb2\n\nimport dragnn.python.load_dragnn_cc_impl\nimport syntaxnet.load_parser_ops\n\ndata_dir = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), 'tutorial_data')\nlexicon_dir = '/tmp/tutorial/lexicon'\ntraining_sentence = os.path.join(data_dir, 'sentence.prototext')\nif not os.path.isdir(lexicon_dir):\n  os.makedirs(lexicon_dir)\n\n\ndef main(argv):\n  del argv  # unused\n  # Constructs lexical resources for SyntaxNet in the given resource path, from\n  # the training data.\n  lexicon.build_lexicon(\n      lexicon_dir,\n      training_sentence,\n      training_corpus_format='sentence-prototext')\n\n  # Construct the ComponentSpec for tagging. This is a simple left-to-right RNN\n  # sequence tagger.\n  tagger = spec_builder.ComponentSpecBuilder('tagger')\n  tagger.set_network_unit(name='FeedForwardNetwork', hidden_layer_sizes='256')\n  tagger.set_transition_system(name='tagger')\n  tagger.add_fixed_feature(name='words', fml='input.word', embedding_dim=64)\n  tagger.add_rnn_link(embedding_dim=-1)\n  tagger.fill_from_resources(lexicon_dir)\n\n  master_spec = spec_pb2.MasterSpec()\n  master_spec.component.extend([tagger.spec])\n\n  hyperparam_config = spec_pb2.GridPoint()\n\n  # Build the TensorFlow graph.\n  graph = tf.Graph()\n  with graph.as_default():\n    builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)\n\n    target = spec_pb2.TrainTarget()\n    target.name = 'all'\n    target.unroll_using_oracle.extend([True])\n    dry_run = builder.add_training_from_config(target, trace_only=True)\n\n  # Read in serialized protos from training data.\n  sentence = sentence_pb2.Sentence()\n  text_format.Merge(open(training_sentence).read(), sentence)\n  training_set = [sentence.SerializeToString()]\n\n  with tf.Session(graph=graph) as sess:\n    # Make sure to re-initialize all underlying state.\n    sess.run(tf.initialize_all_variables())\n    traces = sess.run(\n        dry_run['traces'], feed_dict={dry_run['input_batch']: training_set})\n\n  with open('dragnn_tutorial_1.html', 'w') as f:\n    f.write(visualization.trace_html(traces[0], height='300px').encode('utf-8'))\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "tutorial_1.py", "id": "5b508c6747f30672dfce75d34e74d032", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/syntaxnet/examples/dragnn/tutorial_1.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}