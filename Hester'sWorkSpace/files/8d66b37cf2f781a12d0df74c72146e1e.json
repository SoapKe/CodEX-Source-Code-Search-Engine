{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n internal imports\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import reader\nfrom magenta.models.nsynth import utils\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\"master\",\n                           \"\",\n                           \"BNS name of the TensorFlow master to use.\")\ntf.app.flags.DEFINE_string(\"logdir\", \"/tmp/baseline/train\",\n                           \"Directory where to write event logs.\")\ntf.app.flags.DEFINE_string(\"train_path\",\n                           \"\",\n                           \"Path the nsynth-train.tfrecord.\")\ntf.app.flags.DEFINE_string(\"model\", \"ae\", \"Which model to use in models/\")\ntf.app.flags.DEFINE_string(\"config\",\n                           \"nfft_1024\",\n                           \"Which config to use in models/configs/\")\ntf.app.flags.DEFINE_integer(\"save_summaries_secs\",\n                            15,\n                            \"Frequency at which summaries are saved, in \"\n                            \"seconds.\")\ntf.app.flags.DEFINE_integer(\"save_interval_secs\",\n                            15,\n                            \"Frequency at which the model is saved, in \"\n                            \"seconds.\")\ntf.app.flags.DEFINE_integer(\"ps_tasks\",\n                            0,\n                            \"Number of parameter servers. If 0, parameters \"\n                            \"are handled locally by the worker.\")\ntf.app.flags.DEFINE_integer(\"task\",\n                            0,\n                            \"Task ID. Used when training with multiple \"\n                            \"workers to identify each worker.\")\ntf.app.flags.DEFINE_string(\"log\", \"INFO\",\n                           \"The threshold for what messages will be logged.\"\n                           \"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if not tf.gfile.Exists(FLAGS.logdir):\n    tf.gfile.MakeDirs(FLAGS.logdir)\n\n  with tf.Graph().as_default():\n\n     If ps_tasks is 0, the local device is used. When using multiple\n     (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n     across the different devices.\n    model = utils.get_module(\"baseline.models.%s\" % FLAGS.model)\n    hparams = model.get_hparams(FLAGS.config)\n\n     Run the Reader on the CPU\n    cpu_device = (\"/job:worker/cpu:0\" if FLAGS.ps_tasks else\n                  \"/job:localhost/replica:0/task:0/cpu:0\")\n\n    with tf.device(cpu_device):\n      with tf.name_scope(\"Reader\"):\n        batch = reader.NSynthDataset(\n            FLAGS.train_path, is_training=True).get_baseline_batch(hparams)\n\n    with tf.device(tf.train.replica_device_setter(ps_tasks=FLAGS.ps_tasks)):\n      train_op = model.train_op(batch, hparams, FLAGS.config)\n\n       Run training\n      slim.learning.train(\n          train_op=train_op,\n          logdir=FLAGS.logdir,\n          master=FLAGS.master,\n          is_chief=FLAGS.task == 0,\n          number_of_steps=hparams.max_steps,\n          save_summaries_secs=FLAGS.save_summaries_secs,\n          save_interval_secs=FLAGS.save_interval_secs)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "comments": "   trains model using tf slim        copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     internal imports    if ps tasks 0  local device used  when using multiple    (non local) replicas  replicadevicesetter distributes variables    across different devices     run reader cpu    run training ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Trains model using tf.slim.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# internal imports\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import reader\nfrom magenta.models.nsynth import utils\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\"master\",\n                           \"\",\n                           \"BNS name of the TensorFlow master to use.\")\ntf.app.flags.DEFINE_string(\"logdir\", \"/tmp/baseline/train\",\n                           \"Directory where to write event logs.\")\ntf.app.flags.DEFINE_string(\"train_path\",\n                           \"\",\n                           \"Path the nsynth-train.tfrecord.\")\ntf.app.flags.DEFINE_string(\"model\", \"ae\", \"Which model to use in models/\")\ntf.app.flags.DEFINE_string(\"config\",\n                           \"nfft_1024\",\n                           \"Which config to use in models/configs/\")\ntf.app.flags.DEFINE_integer(\"save_summaries_secs\",\n                            15,\n                            \"Frequency at which summaries are saved, in \"\n                            \"seconds.\")\ntf.app.flags.DEFINE_integer(\"save_interval_secs\",\n                            15,\n                            \"Frequency at which the model is saved, in \"\n                            \"seconds.\")\ntf.app.flags.DEFINE_integer(\"ps_tasks\",\n                            0,\n                            \"Number of parameter servers. If 0, parameters \"\n                            \"are handled locally by the worker.\")\ntf.app.flags.DEFINE_integer(\"task\",\n                            0,\n                            \"Task ID. Used when training with multiple \"\n                            \"workers to identify each worker.\")\ntf.app.flags.DEFINE_string(\"log\", \"INFO\",\n                           \"The threshold for what messages will be logged.\"\n                           \"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if not tf.gfile.Exists(FLAGS.logdir):\n    tf.gfile.MakeDirs(FLAGS.logdir)\n\n  with tf.Graph().as_default():\n\n    # If ps_tasks is 0, the local device is used. When using multiple\n    # (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n    # across the different devices.\n    model = utils.get_module(\"baseline.models.%s\" % FLAGS.model)\n    hparams = model.get_hparams(FLAGS.config)\n\n    # Run the Reader on the CPU\n    cpu_device = (\"/job:worker/cpu:0\" if FLAGS.ps_tasks else\n                  \"/job:localhost/replica:0/task:0/cpu:0\")\n\n    with tf.device(cpu_device):\n      with tf.name_scope(\"Reader\"):\n        batch = reader.NSynthDataset(\n            FLAGS.train_path, is_training=True).get_baseline_batch(hparams)\n\n    with tf.device(tf.train.replica_device_setter(ps_tasks=FLAGS.ps_tasks)):\n      train_op = model.train_op(batch, hparams, FLAGS.config)\n\n      # Run training\n      slim.learning.train(\n          train_op=train_op,\n          logdir=FLAGS.logdir,\n          master=FLAGS.master,\n          is_chief=FLAGS.task == 0,\n          number_of_steps=hparams.max_steps,\n          save_summaries_secs=FLAGS.save_summaries_secs,\n          save_interval_secs=FLAGS.save_interval_secs)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "train.py", "id": "8d66b37cf2f781a12d0df74c72146e1e", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-magenta/tensorflow-magenta-c3eda3d/magenta/models/nsynth/baseline/train.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:52:33Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}