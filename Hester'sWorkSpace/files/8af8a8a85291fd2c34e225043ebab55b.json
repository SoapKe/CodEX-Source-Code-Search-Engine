{"author": "scrapy", "code": "import json\nimport os\nimport time\n\nfrom threading import Thread\nfrom libmproxy import controller, proxy\nfrom netlib import http_auth\nfrom testfixtures import LogCapture\n\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.http import Request\nfrom tests.spiders import SimpleSpider, SingleRequestSpider\nfrom tests.mockserver import MockServer\n\n\nclass HTTPSProxy(controller.Master, Thread):\n\n    def __init__(self, port):\n        password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n        authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n            'keys', 'mitmproxy-ca.pem')\n        server = proxy.ProxyServer(proxy.ProxyConfig(\n            authenticator = authenticator,\n            cacert = cert_path),\n            port)\n        Thread.__init__(self)\n        controller.Master.__init__(self, server)\n\n\nclass ProxyConnectTestCase(TestCase):\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self._oldenv = os.environ.copy()\n        self._proxy = HTTPSProxy(8888)\n        self._proxy.start()\n        \n        time.sleep(1.0)\n        os.environ['http_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n        self._proxy.shutdown()\n        os.environ = self._oldenv\n\n    @defer.inlineCallbacks\n    def test_https_connect_tunnel(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(200, l)\n\n    @defer.inlineCallbacks\n    def test_https_noconnect(self):\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(200, l)\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    @defer.inlineCallbacks\n    def test_https_connect_tunnel_error(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n        self._assert_got_tunnel_error(l)\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_auth_error(self):\n        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        \n        \n        self._assert_got_tunnel_error(l)\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n        request = Request(\"https://localhost:8999/echo\")\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(seed=request)\n        self._assert_got_response_code(200, l)\n        echo = json.loads(crawler.spider.meta['responses'][0].body)\n        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n\n    @defer.inlineCallbacks\n    def test_https_noconnect_auth_error(self):\n        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(407, l)\n\n    def _assert_got_response_code(self, code, log):\n        print(log)\n        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n\n    def _assert_got_tunnel_error(self, log):\n        print(log)\n        self.assertIn('TunnelError', str(log))\n", "comments": "  wait proxy start     the proxy returns 407 error code reach client     sees tunnelerror  ", "content": "import json\nimport os\nimport time\n\nfrom threading import Thread\nfrom libmproxy import controller, proxy\nfrom netlib import http_auth\nfrom testfixtures import LogCapture\n\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.http import Request\nfrom tests.spiders import SimpleSpider, SingleRequestSpider\nfrom tests.mockserver import MockServer\n\n\nclass HTTPSProxy(controller.Master, Thread):\n\n    def __init__(self, port):\n        password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n        authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n            'keys', 'mitmproxy-ca.pem')\n        server = proxy.ProxyServer(proxy.ProxyConfig(\n            authenticator = authenticator,\n            cacert = cert_path),\n            port)\n        Thread.__init__(self)\n        controller.Master.__init__(self, server)\n\n\nclass ProxyConnectTestCase(TestCase):\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self._oldenv = os.environ.copy()\n        self._proxy = HTTPSProxy(8888)\n        self._proxy.start()\n        # Wait for the proxy to start.\n        time.sleep(1.0)\n        os.environ['http_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n        self._proxy.shutdown()\n        os.environ = self._oldenv\n\n    @defer.inlineCallbacks\n    def test_https_connect_tunnel(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(200, l)\n\n    @defer.inlineCallbacks\n    def test_https_noconnect(self):\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(200, l)\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    @defer.inlineCallbacks\n    def test_https_connect_tunnel_error(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n        self._assert_got_tunnel_error(l)\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_auth_error(self):\n        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        # The proxy returns a 407 error code but it does not reach the client;\n        # he just sees a TunnelError.\n        self._assert_got_tunnel_error(l)\n        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n        request = Request(\"https://localhost:8999/echo\")\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(seed=request)\n        self._assert_got_response_code(200, l)\n        echo = json.loads(crawler.spider.meta['responses'][0].body)\n        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n\n    @defer.inlineCallbacks\n    def test_https_noconnect_auth_error(self):\n        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as l:\n            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n        self._assert_got_response_code(407, l)\n\n    def _assert_got_response_code(self, code, log):\n        print(log)\n        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n\n    def _assert_got_tunnel_error(self, log):\n        print(log)\n        self.assertIn('TunnelError', str(log))\n", "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.", "file_name": "test_proxy_connect.py", "id": "8af8a8a85291fd2c34e225043ebab55b", "language": "Python", "project_name": "scrapy", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/scrapy-scrapy/scrapy-scrapy-6a7cdf9/tests/test_proxy_connect.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:39:41Z", "url": "https://github.com/scrapy/scrapy", "wiki": true}