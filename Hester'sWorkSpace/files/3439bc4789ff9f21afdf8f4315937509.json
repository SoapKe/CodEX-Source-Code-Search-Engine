{"author": "HelloZeroNet", "code": "import os\nimport json\nimport logging\nimport hashlib\nimport re\nimport time\nimport random\nimport sys\nimport struct\nimport socket\nimport urllib\nimport urllib2\nimport hashlib\nimport collections\n\nimport gevent\nimport gevent.pool\n\nimport util\nfrom lib import bencode\nfrom lib.subtl.subtl import UdpTrackerClient\nfrom Config import config\nfrom Peer import Peer\nfrom Worker import WorkerManager\nfrom Debug import Debug\nfrom Content import ContentManager\nfrom SiteStorage import SiteStorage\nfrom Crypt import CryptHash\nfrom util import helper\nfrom util import Diff\nfrom Plugin import PluginManager\nimport SiteManager\n\n\n@PluginManager.acceptPlugins\nclass Site(object):\n\n    def __init__(self, address, allow_create=True, settings=None):\n        self.address = re.sub(\"[^A-Za-z0-9]\", \"\", address)  \n        self.address_hash = hashlib.sha256(self.address).digest()\n        self.address_short = \"%s..%s\" % (self.address[:6], self.address[-4:])  \n        self.log = logging.getLogger(\"Site:%s\" % self.address_short)\n        self.addEventListeners()\n\n        self.content = None  \n        self.peers = {}  \n        self.peers_recent = collections.deque(maxlen=100)\n        self.peer_blacklist = SiteManager.peer_blacklist  \n        self.time_announce = 0  \n        self.last_tracker_id = random.randint(0, 10)  \n        self.worker_manager = WorkerManager(self)  \n        self.bad_files = {}  \n        self.content_updated = None  \n        self.notifications = []  \n        self.page_requested = False  \n        self.websockets = []  \n\n        self.connection_server = None\n        self.loadSettings(settings)  \n        self.storage = SiteStorage(self, allow_create=allow_create)  \n        self.content_manager = ContentManager(self)\n        self.content_manager.loadContents()  \n        if \"main\" in sys.modules and \"file_server\" in dir(sys.modules[\"main\"]):  \n            self.connection_server = sys.modules[\"main\"].file_server\n        else:\n            self.connection_server = None\n        if not self.settings.get(\"auth_key\"):  \n            self.settings[\"auth_key\"] = CryptHash.random()\n            self.log.debug(\"New auth key: %s\" % self.settings[\"auth_key\"])\n\n        if not self.settings.get(\"wrapper_key\"):  \n            self.settings[\"wrapper_key\"] = CryptHash.random()\n            self.log.debug(\"New wrapper key: %s\" % self.settings[\"wrapper_key\"])\n\n        if not self.settings.get(\"ajax_key\"):  \n            self.settings[\"ajax_key\"] = CryptHash.random()\n            self.log.debug(\"New ajax key: %s\" % self.settings[\"ajax_key\"])\n\n    def __str__(self):\n        return \"Site %s\" % self.address_short\n\n    def __repr__(self):\n        return \"<%s>\" % self.__str__()\n\n    \n    def loadSettings(self, settings=None):\n        if not settings:\n            settings = json.load(open(\"%s/sites.json\" % config.data_dir)).get(self.address)\n        if settings:\n            self.settings = settings\n            if \"cache\" not in settings:\n                settings[\"cache\"] = {}\n            if \"size_files_optional\" not in settings:\n                settings[\"size_optional\"] = 0\n            if \"optional_downloaded\" not in settings:\n                settings[\"optional_downloaded\"] = 0\n            self.bad_files = settings[\"cache\"].get(\"bad_files\", {})\n            settings[\"cache\"][\"bad_files\"] = {}\n            \n            for inner_path in self.bad_files:\n                self.bad_files[inner_path] = min(self.bad_files[inner_path], 20)\n        else:\n            self.settings = {\n                \"own\": False, \"serving\": True, \"permissions\": [],\n                \"added\": int(time.time()), \"optional_downloaded\": 0, \"size_optional\": 0\n            }  \n            if config.download_optional == \"auto\":\n                self.settings[\"autodownloadoptional\"] = True\n\n        \n        if self.address == config.homepage and \"ADMIN\" not in self.settings[\"permissions\"]:\n            self.settings[\"permissions\"].append(\"ADMIN\")\n\n        return\n\n    \n    def saveSettings(self):\n        if not SiteManager.site_manager.sites:\n            SiteManager.site_manager.sites = {}\n        if not SiteManager.site_manager.sites.get(self.address):\n            SiteManager.site_manager.sites[self.address] = self\n            SiteManager.site_manager.load(False)\n        SiteManager.site_manager.save()\n\n    def getSettingsCache(self):\n        back = {}\n        back[\"bad_files\"] = self.bad_files\n        back[\"hashfield\"] = self.content_manager.hashfield.tostring().encode(\"base64\")\n        return back\n\n    \n    def getSizeLimit(self):\n        return self.settings.get(\"size_limit\", int(config.size_limit))\n\n    \n    def getNextSizeLimit(self):\n        size_limits = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000]\n        size = self.settings.get(\"size\", 0)\n        for size_limit in size_limits:\n            if size * 1.2 < size_limit * 1024 * 1024:\n                return size_limit\n        return 999999\n\n    \n    def downloadContent(self, inner_path, download_files=True, peer=None, check_modifications=False, diffs={}):\n        s = time.time()\n        if config.verbose:\n            self.log.debug(\"Downloading %s...\" % inner_path)\n\n        if not inner_path.endswith(\"content.json\"):\n            return False\n\n        found = self.needFile(inner_path, update=self.bad_files.get(inner_path))\n        content_inner_dir = helper.getDirname(inner_path)\n        if not found:\n            self.log.debug(\"Download %s failed, check_modifications: %s\" % (inner_path, check_modifications))\n            if check_modifications:  \n                self.onFileDone.once(lambda file_name: self.checkModifications(0), \"check_modifications\")\n            return False  \n\n        if config.verbose:\n            self.log.debug(\"Got %s\" % inner_path)\n        changed, deleted = self.content_manager.loadContent(inner_path, load_includes=False)\n\n        if inner_path == \"content.json\":\n            self.saveSettings()\n\n        if peer:  \n            peer.last_content_json_update = self.content_manager.contents[inner_path][\"modified\"]\n\n        \n        file_threads = []\n        if download_files:\n            for file_relative_path in self.content_manager.contents[inner_path].get(\"files\", {}).keys():\n                file_inner_path = content_inner_dir + file_relative_path\n\n                \n                diff_success = False\n                diff_actions = diffs.get(file_relative_path)\n                if diff_actions and self.bad_files.get(file_inner_path):\n                    try:\n                        s = time.time()\n                        new_file = Diff.patch(self.storage.open(file_inner_path, \"rb\"), diff_actions)\n                        new_file.seek(0)\n                        time_diff = time.time() - s\n\n                        s = time.time()\n                        diff_success = self.content_manager.verifyFile(file_inner_path, new_file)\n                        time_verify = time.time() - s\n\n                        if diff_success:\n                            s = time.time()\n                            new_file.seek(0)\n                            self.storage.write(file_inner_path, new_file)\n                            time_write = time.time() - s\n\n                            s = time.time()\n                            self.onFileDone(file_inner_path)\n                            time_on_done = time.time() - s\n\n                            self.log.debug(\n                                \"Patched successfully: %s (diff: %.3fs, verify: %.3fs, write: %.3fs, on_done: %.3fs)\" %\n                                (file_inner_path, time_diff, time_verify, time_write, time_on_done)\n                            )\n                    except Exception, err:\n                        self.log.debug(\"Failed to patch %s: %s\" % (file_inner_path, err))\n                        diff_success = False\n\n                if not diff_success:\n                    \n                    res = self.needFile(file_inner_path, blocking=False, update=self.bad_files.get(file_inner_path), peer=peer)\n                    if res is not True and res is not False:  \n                        file_threads.append(res)  \n\n            \n            if inner_path == \"content.json\":\n                gevent.spawn(self.updateHashfield)\n\n            for file_relative_path in self.content_manager.contents[inner_path].get(\"files_optional\", {}).keys():\n                file_inner_path = content_inner_dir + file_relative_path\n                if file_inner_path not in changed and not self.bad_files.get(file_inner_path):\n                    continue\n                if not self.isDownloadable(file_inner_path):\n                    continue\n                \n                res = self.pooledNeedFile(\n                    file_inner_path, blocking=False, update=self.bad_files.get(file_inner_path), peer=peer\n                )\n                if res is not True and res is not False:  \n                    file_threads.append(res)  \n\n        \n        include_threads = []\n        for file_relative_path in self.content_manager.contents[inner_path].get(\"includes\", {}).keys():\n            file_inner_path = content_inner_dir + file_relative_path\n            include_thread = gevent.spawn(self.downloadContent, file_inner_path, download_files=download_files, peer=peer)\n            include_threads.append(include_thread)\n\n        if config.verbose:\n            self.log.debug(\"%s: Downloading %s includes...\" % (inner_path, len(include_threads)))\n        gevent.joinall(include_threads)\n        if config.verbose:\n            self.log.debug(\"%s: Includes download ended\" % inner_path)\n\n        if check_modifications:  \n            self.checkModifications(0)\n\n        if config.verbose:\n            self.log.debug(\"%s: Downloading %s files, changed: %s...\" % (inner_path, len(file_threads), len(changed)))\n        gevent.joinall(file_threads)\n        if config.verbose:\n            self.log.debug(\"%s: DownloadContent ended in %.3fs\" % (inner_path, time.time() - s))\n\n        if not self.worker_manager.tasks:\n            self.onComplete()  \n\n        return True\n\n    \n    def getReachableBadFiles(self):\n        if not self.bad_files:\n            return False\n        return [bad_file for bad_file, retry in self.bad_files.iteritems() if retry < 3]\n\n    \n    def retryBadFiles(self, force=False):\n        self.checkBadFiles()\n\n        self.log.debug(\"Retry %s bad files\" % len(self.bad_files))\n        content_inner_paths = []\n        file_inner_paths = []\n\n        for bad_file, tries in self.bad_files.items():\n            if force or random.randint(0, min(40, tries)) < 4:  \n                if bad_file.endswith(\"content.json\"):\n                    content_inner_paths.append(bad_file)\n                else:\n                    file_inner_paths.append(bad_file)\n\n        if content_inner_paths:\n            self.pooledDownloadContent(content_inner_paths, only_if_bad=True)\n\n        if file_inner_paths:\n            self.pooledDownloadFile(file_inner_paths, only_if_bad=True)\n\n    def checkBadFiles(self):\n        for bad_file in self.bad_files.keys():\n            file_info = self.content_manager.getFileInfo(bad_file)\n            if bad_file.endswith(\"content.json\"):\n                if file_info is False and bad_file != \"content.json\":\n                    del self.bad_files[bad_file]\n                    self.log.debug(\"No info for file: %s, removing from bad_files\" % bad_file)\n            else:\n                if file_info is False or not file_info.get(\"size\"):\n                    del self.bad_files[bad_file]\n                    self.log.debug(\"No info or size for file: %s, removing from bad_files\" % bad_file)\n\n    \n    @util.Noparallel(blocking=False)\n    def download(self, check_size=False, blind_includes=False):\n        if not self.connection_server:\n            self.log.debug(\"No connection server found, skipping download\")\n            return False\n\n        self.log.debug(\n            \"Start downloading, bad_files: %s, check_size: %s, blind_includes: %s\" %\n            (self.bad_files, check_size, blind_includes)\n        )\n        gevent.spawn(self.announce, force=True)\n        if check_size:  \n            valid = self.downloadContent(\"content.json\", download_files=False)  \n            if not valid:\n                return False  \n\n        \n        valid = self.downloadContent(\"content.json\", check_modifications=blind_includes)\n\n        self.onComplete.once(lambda: self.retryBadFiles(force=True))\n\n        return valid\n\n    def pooledDownloadContent(self, inner_paths, pool_size=100, only_if_bad=False):\n        self.log.debug(\"New downloadContent pool: len: %s\" % len(inner_paths))\n        self.worker_manager.started_task_num += len(inner_paths)\n        pool = gevent.pool.Pool(pool_size)\n        for inner_path in inner_paths:\n            if not only_if_bad or inner_path in self.bad_files:\n                pool.spawn(self.downloadContent, inner_path)\n            self.worker_manager.started_task_num -= 1\n        self.log.debug(\"Ended downloadContent pool len: %s\" % len(inner_paths))\n\n    def pooledDownloadFile(self, inner_paths, pool_size=100, only_if_bad=False):\n        self.log.debug(\"New downloadFile pool: len: %s\" % len(inner_paths))\n        self.worker_manager.started_task_num += len(inner_paths)\n        pool = gevent.pool.Pool(pool_size)\n        for inner_path in inner_paths:\n            if not only_if_bad or inner_path in self.bad_files:\n                pool.spawn(self.needFile, inner_path, update=True)\n            self.worker_manager.started_task_num -= 1\n        self.log.debug(\"Ended downloadFile pool len: %s\" % len(inner_paths))\n\n    \n    def updater(self, peers_try, queried, since):\n        while 1:\n            if not peers_try or len(queried) >= 3:  \n                break\n            peer = peers_try.pop(0)\n            if config.verbose:\n                self.log.debug(\"Try to get updates from: %s Left: %s\" % (peer, peers_try))\n\n            res = None\n            with gevent.Timeout(20, exception=False):\n                res = peer.listModified(since)\n\n            if not res or \"modified_files\" not in res:\n                continue  \n\n            queried.append(peer)\n            modified_contents = []\n            my_modified = self.content_manager.listModified(since)\n            for inner_path, modified in res[\"modified_files\"].iteritems():  \n                has_newer = int(modified) > my_modified.get(inner_path, 0)\n                has_older = int(modified) < my_modified.get(inner_path, 0)\n                if inner_path not in self.bad_files and not self.content_manager.isArchived(inner_path, modified):\n                    if has_newer:\n                        \n                        modified_contents.append(inner_path)\n                        self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1\n                    if has_older:\n                        self.log.debug(\"%s client has older version of %s, publishing there...\" % (peer, inner_path))\n                        gevent.spawn(self.publisher, inner_path, [peer], [], 1)\n            if modified_contents:\n                self.log.debug(\"%s new modified file from %s\" % (len(modified_contents), peer))\n                modified_contents.sort(key=lambda inner_path: 0 - res[\"modified_files\"][inner_path])  \n                gevent.spawn(self.pooledDownloadContent, modified_contents)\n\n    \n    \n    def checkModifications(self, since=None):\n        s = time.time()\n        peers_try = []  \n        queried = []  \n        limit = 5\n\n        \n        if not self.peers:\n            self.announce()\n            for wait in range(10):\n                time.sleep(5 + wait)\n                self.log.debug(\"Waiting for peers...\")\n                if self.peers:\n                    break\n\n        peers_try = self.getConnectedPeers()\n        peers_connected_num = len(peers_try)\n        if peers_connected_num < limit * 2:  \n            peers_try += self.getRecentPeers(limit * 5)\n\n        if since is None:  \n            since = self.settings.get(\"modified\", 60 * 60 * 24) - 60 * 60 * 24\n\n        if config.verbose:\n            self.log.debug(\n                \"Try to get listModifications from peers: %s, connected: %s, since: %s\" %\n                (peers_try, peers_connected_num, since)\n            )\n\n        updaters = []\n        for i in range(3):\n            updaters.append(gevent.spawn(self.updater, peers_try, queried, since))\n\n        gevent.joinall(updaters, timeout=10)  \n\n        if not queried:  \n            peers_try[0:0] = [peer for peer in self.getConnectedPeers() if peer.connection.connected]  \n            for _ in range(10):\n                gevent.joinall(updaters, timeout=10)  \n                if queried:\n                    break\n\n        self.log.debug(\"Queried listModifications from: %s in %.3fs\" % (queried, time.time() - s))\n        time.sleep(0.1)\n        return queried\n\n    \n    \n    @util.Noparallel()\n    def update(self, announce=False, check_files=False, since=None):\n        self.content_manager.loadContent(\"content.json\", load_includes=False)  \n        self.content_updated = None  \n        self.updateWebsocket(updating=True)\n\n        \n        self.checkBadFiles()\n\n        if announce:\n            self.announce()\n\n        \n        if check_files and since == 0:\n            self.bad_files = {}\n\n        queried = self.checkModifications(since)\n\n        if check_files:\n            self.storage.updateBadFiles(quick_check=True)  \n\n        changed, deleted = self.content_manager.loadContent(\"content.json\", load_includes=False)\n\n        if self.bad_files:\n            self.log.debug(\"Bad files: %s\" % self.bad_files)\n            gevent.spawn(self.retryBadFiles, force=True)\n\n        if len(queried) == 0:\n            \n            self.content_updated = False\n            self.bad_files[\"content.json\"] = 1\n        else:\n            self.content_updated = time.time()\n\n        self.updateWebsocket(updated=True)\n\n    \n    def redownloadContents(self):\n        \n        content_threads = []\n        for inner_path in self.content_manager.contents.keys():\n            content_threads.append(self.needFile(inner_path, update=True, blocking=False))\n\n        self.log.debug(\"Waiting %s content.json to finish...\" % len(content_threads))\n        gevent.joinall(content_threads)\n\n    \n    def publisher(self, inner_path, peers, published, limit, diffs={}, event_done=None, cb_progress=None):\n        file_size = self.storage.getSize(inner_path)\n        content_json_modified = self.content_manager.contents[inner_path][\"modified\"]\n        body = self.storage.read(inner_path)\n\n        \n        tor_manager = self.connection_server.tor_manager\n        if tor_manager and tor_manager.enabled and tor_manager.start_onions:\n            my_ip = tor_manager.getOnion(self.address)\n            if my_ip:\n                my_ip += \".onion\"\n            my_port = config.fileserver_port\n        else:\n            my_ip = config.ip_external\n            if self.connection_server.port_opened:\n                my_port = config.fileserver_port\n            else:\n                my_port = 0\n\n        while 1:\n            if not peers or len(published) >= limit:\n                if event_done:\n                    event_done.set(True)\n                break  \n            peer = peers.pop()\n            if peer in published:\n                continue\n            if peer.last_content_json_update == content_json_modified:\n                self.log.debug(\"%s already received this update for %s, skipping\" % (peer, inner_path))\n                continue\n\n            if peer.connection and peer.connection.last_ping_delay:  \n                \n                timeout = 5 + int(file_size / 1024) + peer.connection.last_ping_delay\n            else:  \n                \n                timeout = 10 + int(file_size / 1024)\n            result = {\"exception\": \"Timeout\"}\n\n            for retry in range(2):\n                try:\n                    with gevent.Timeout(timeout, False):\n                        result = peer.request(\"update\", {\n                            \"site\": self.address,\n                            \"inner_path\": inner_path,\n                            \"body\": body,\n                            \"diffs\": diffs\n                        })\n                    if result:\n                        break\n                except Exception, err:\n                    self.log.error(\"Publish error: %s\" % Debug.formatException(err))\n                    result = {\"exception\": Debug.formatException(err)}\n\n            if result and \"ok\" in result:\n                published.append(peer)\n                if cb_progress and len(published) <= limit:\n                    cb_progress(len(published), limit)\n                self.log.info(\"[OK] %s: %s %s/%s\" % (peer.key, result[\"ok\"], len(published), limit))\n            else:\n                if result == {\"exception\": \"Timeout\"}:\n                    peer.onConnectionError(\"Publish timeout\")\n                self.log.info(\"[FAILED] %s: %s\" % (peer.key, result))\n            time.sleep(0.01)\n\n    \n    @util.Noparallel()\n    def publish(self, limit=\"default\", inner_path=\"content.json\", diffs={}, cb_progress=None):\n        published = []  \n        publishers = []  \n\n        if not self.peers:\n            self.announce()\n\n        if limit == \"default\":\n            limit = 5\n        threads = limit\n\n        peers = self.getConnectedPeers()\n        num_connected_peers = len(peers)\n\n        random.shuffle(peers)\n        peers = sorted(peers, key=lambda peer: peer.connection.handshake.get(\"rev\", 0) < config.rev - 100)  \n\n        if len(peers) < limit * 2 and len(self.peers) > len(peers):  \n            peers += self.getRecentPeers(limit * 2)\n\n        peers = set(peers)\n\n        self.log.info(\"Publishing %s to %s/%s peers (connected: %s) diffs: %s (%.2fk)...\" % (\n            inner_path, limit, len(self.peers), num_connected_peers, diffs.keys(), float(len(str(diffs))) / 1024\n        ))\n\n        if not peers:\n            return 0  \n\n        event_done = gevent.event.AsyncResult()\n        for i in range(min(len(peers), limit, threads)):\n            publisher = gevent.spawn(self.publisher, inner_path, peers, published, limit, diffs, event_done, cb_progress)\n            publishers.append(publisher)\n\n        event_done.get()  \n        if len(published) < min(len(self.peers), limit):\n            time.sleep(0.2)  \n        if len(published) == 0:\n            gevent.joinall(publishers)  \n\n        \n        self.log.info(\n            \"Successfuly %s published to %s peers, publishing to %s more peers in the background\" %\n            (inner_path, len(published), limit)\n        )\n\n        for thread in range(2):\n            gevent.spawn(self.publisher, inner_path, peers, published, limit=limit * 2, diffs=diffs)\n\n        \n        gevent.spawn(self.sendMyHashfield, 100)\n\n        return len(published)\n\n    \n    def clone(self, address, privatekey=None, address_index=None, root_inner_path=\"\", overwrite=False):\n        import shutil\n        new_site = SiteManager.site_manager.need(address, all_file=False)\n        default_dirs = []  \n        for dir_name in os.listdir(self.storage.directory):\n            if \"-default\" in dir_name:\n                default_dirs.append(dir_name.replace(\"-default\", \"\"))\n\n        self.log.debug(\"Cloning to %s, ignore dirs: %s, root: %s\" % (address, default_dirs, root_inner_path))\n\n        \n        if not new_site.storage.isFile(\"content.json\") and not overwrite:\n            \n            if \"size_limit\" in self.settings:\n                new_site.settings[\"size_limit\"] = self.settings[\"size_limit\"]\n\n            \n            if self.storage.isFile(root_inner_path + \"/content.json-default\"):\n                content_json = self.storage.loadJson(root_inner_path + \"/content.json-default\")\n            else:\n                content_json = self.storage.loadJson(\"content.json\")\n\n            if \"domain\" in content_json:\n                del content_json[\"domain\"]\n            content_json[\"title\"] = \"my\" + content_json[\"title\"]\n            content_json[\"cloned_from\"] = self.address\n            content_json[\"clone_root\"] = root_inner_path\n            content_json[\"files\"] = {}\n            if address_index:\n                content_json[\"address_index\"] = address_index  \n            new_site.storage.writeJson(\"content.json\", content_json)\n            new_site.content_manager.loadContent(\n                \"content.json\", add_bad_files=False, delete_removed_files=False, load_includes=False\n            )\n\n        \n        for content_inner_path, content in self.content_manager.contents.items():\n            file_relative_paths = content.get(\"files\", {}).keys()\n\n            \n            file_relative_paths.sort()\n            file_relative_paths.sort(key=lambda key: key.replace(\"-default\", \"\").endswith(\"content.json\"))\n\n            for file_relative_path in file_relative_paths:\n                file_inner_path = helper.getDirname(content_inner_path) + file_relative_path  \n                file_inner_path = file_inner_path.strip(\"/\")  \n                if not file_inner_path.startswith(root_inner_path):\n                    self.log.debug(\"[SKIP] %s (not in clone root)\" % file_inner_path)\n                    continue\n                if file_inner_path.split(\"/\")[0] in default_dirs:  \n                    self.log.debug(\"[SKIP] %s (has default alternative)\" % file_inner_path)\n                    continue\n                file_path = self.storage.getPath(file_inner_path)\n\n                \n                if root_inner_path:\n                    file_inner_path_dest = re.sub(\"^%s/\" % re.escape(root_inner_path), \"\", file_inner_path)\n                    file_path_dest = new_site.storage.getPath(file_inner_path_dest)\n                else:\n                    file_inner_path_dest = file_inner_path\n                    file_path_dest = new_site.storage.getPath(file_inner_path)\n\n                self.log.debug(\"[COPY] %s to %s...\" % (file_inner_path, file_path_dest))\n                dest_dir = os.path.dirname(file_path_dest)\n                if not os.path.isdir(dest_dir):\n                    os.makedirs(dest_dir)\n                if file_inner_path_dest.replace(\"-default\", \"\") == \"content.json\":  \n                    continue\n\n                shutil.copy(file_path, file_path_dest)\n\n                \n                if \"-default\" in file_inner_path:\n                    file_path_dest = new_site.storage.getPath(file_inner_path.replace(\"-default\", \"\"))\n                    if new_site.storage.isFile(file_inner_path.replace(\"-default\", \"\")) and not overwrite:\n                        \n                        self.log.debug(\"[SKIP] Default file: %s (already exist)\" % file_inner_path)\n                        continue\n                    self.log.debug(\"[COPY] Default file: %s to %s...\" % (file_inner_path, file_path_dest))\n                    dest_dir = os.path.dirname(file_path_dest)\n                    if not os.path.isdir(dest_dir):\n                        os.makedirs(dest_dir)\n                    shutil.copy(file_path, file_path_dest)\n                    \n                    if file_path_dest.endswith(\"/content.json\"):\n                        new_site.storage.onUpdated(file_inner_path.replace(\"-default\", \"\"))\n                        new_site.content_manager.loadContent(\n                            file_inner_path.replace(\"-default\", \"\"), add_bad_files=False,\n                            delete_removed_files=False, load_includes=False\n                        )\n                        if privatekey:\n                            new_site.content_manager.sign(file_inner_path.replace(\"-default\", \"\"), privatekey)\n                            new_site.content_manager.loadContent(\n                                file_inner_path, add_bad_files=False, delete_removed_files=False, load_includes=False\n                            )\n\n        if privatekey:\n            new_site.content_manager.sign(\"content.json\", privatekey)\n            new_site.content_manager.loadContent(\n                \"content.json\", add_bad_files=False, delete_removed_files=False, load_includes=False\n            )\n\n        \n        if new_site.storage.isFile(\"dbschema.json\"):\n            new_site.storage.closeDb()\n            new_site.storage.rebuildDb()\n\n        return new_site\n\n    @util.Pooled(100)\n    def pooledNeedFile(self, *args, **kwargs):\n        return self.needFile(*args, **kwargs)\n\n    def isFileDownloadAllowed(self, inner_path, file_info):\n        if file_info.get(\"size\", 0) > config.file_size_limit * 1024 * 1024:\n            self.log.debug(\n                \"File size %s too large: %sMB > %sMB, skipping...\" %\n                (inner_path, file_info.get(\"size\", 0) / 1024 / 1024, config.file_size_limit)\n            )\n            return False\n        else:\n            return True\n\n    def needFileInfo(self, inner_path):\n        file_info = self.content_manager.getFileInfo(inner_path)\n        if not file_info:\n            \n            self.log.debug(\"No info for %s, waiting for all content.json\" % inner_path)\n            success = self.downloadContent(\"content.json\", download_files=False)\n            if not success:\n                return False\n            file_info = self.content_manager.getFileInfo(inner_path)\n        return file_info\n\n    \n    def needFile(self, inner_path, update=False, blocking=True, peer=None, priority=0):\n        if self.storage.isFile(inner_path) and not update:  \n            return True\n        elif self.settings[\"serving\"] is False:  \n            return False\n        else:  \n            self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1  \n            if not self.content_manager.contents.get(\"content.json\"):  \n                self.log.debug(\"Need content.json first\")\n                gevent.spawn(self.announce)\n                if inner_path != \"content.json\":  \n                    task = self.worker_manager.addTask(\"content.json\", peer)\n                    task[\"evt\"].get()\n                    self.content_manager.loadContent()\n                    if not self.content_manager.contents.get(\"content.json\"):\n                        return False  \n\n            file_info = None\n            if not inner_path.endswith(\"content.json\"):\n                file_info = self.needFileInfo(inner_path)\n                if not file_info:\n                    return False\n                if \"cert_signers\" in file_info and not file_info[\"content_inner_path\"] in self.content_manager.contents:\n                    self.log.debug(\"Missing content.json for requested user file: %s\" % inner_path)\n                    if self.bad_files.get(file_info[\"content_inner_path\"], 0) > 5:\n                        self.log.debug(\"File %s not reachable: retry %s\" % (\n                            inner_path, self.bad_files.get(file_info[\"content_inner_path\"], 0)\n                        ))\n                        return False\n                    self.downloadContent(file_info[\"content_inner_path\"])\n\n                if not self.isFileDownloadAllowed(inner_path, file_info):\n                    self.log.debug(\"%s: Download not allowed\" % inner_path)\n                    return False\n\n            task = self.worker_manager.addTask(inner_path, peer, priority=priority, file_info=file_info)\n            if blocking:\n                return task[\"evt\"].get()\n            else:\n                return task[\"evt\"]\n\n    \n    \n    def addPeer(self, ip, port, return_peer=False, connection=None, source=\"other\"):\n        if not ip or ip == \"0.0.0.0\":\n            return False\n        key = \"%s:%s\" % (ip, port)\n        peer = self.peers.get(key)\n        if peer:  \n            peer.found(source)\n            if return_peer:  \n                return peer\n            else:\n                return False\n        else:  \n            if (ip, port) in self.peer_blacklist:\n                return False  \n            peer = Peer(ip, port, self)\n            self.peers[key] = peer\n            peer.found(source)\n            return peer\n\n    \n    @util.Noparallel(blocking=False)\n    def announcePex(self, query_num=2, need_num=5):\n        peers = [peer for peer in self.peers.values() if peer.connection and peer.connection.connected]  \n        if len(peers) == 0:  \n            self.log.debug(\"Small number of peers detected...query all of peers using pex\")\n            peers = self.peers.values()\n            need_num = 10\n\n        random.shuffle(peers)\n        done = 0\n        added = 0\n        for peer in peers:\n            res = peer.pex(need_num=need_num)\n            if type(res) == int:  \n                done += 1\n                added += res\n                if res:\n                    self.worker_manager.onPeers()\n                    self.updateWebsocket(peers_added=res)\n            if done == query_num:\n                break\n        self.log.debug(\"Pex result: from %s peers got %s new peers.\" % (done, added))\n\n    \n    \n    def announceTracker(self, tracker_protocol, tracker_address, fileserver_port=0, add_types=[], my_peer_id=\"\", mode=\"start\"):\n        s = time.time()\n        if mode == \"update\":\n            num_want = 10\n        else:\n            num_want = 30\n\n        if \"ip4\" not in add_types:\n            fileserver_port = 0\n\n        if tracker_protocol == \"udp\":  \n            if config.disable_udp:\n                return False  \n            ip, port = tracker_address.split(\":\")\n            tracker = UdpTrackerClient(ip, int(port))\n            tracker.peer_port = fileserver_port\n            try:\n                tracker.connect()\n                tracker.poll_once()\n                tracker.announce(info_hash=hashlib.sha1(self.address).hexdigest(), num_want=num_want, left=431102370)\n                back = tracker.poll_once()\n                if back and type(back) is dict:\n                    peers = back[\"response\"][\"peers\"]\n                else:\n                    raise Exception(\"No response\")\n            except Exception, err:\n                self.log.warning(\"Tracker error: udp://%s:%s (%s)\" % (ip, port, err))\n                return False\n\n        elif tracker_protocol == \"http\":  \n            params = {\n                'info_hash': hashlib.sha1(self.address).digest(),\n                'peer_id': my_peer_id, 'port': fileserver_port,\n                'uploaded': 0, 'downloaded': 0, 'left': 431102370, 'compact': 1, 'numwant': num_want,\n                'event': 'started'\n            }\n            req = None\n            try:\n                url = \"http://\" + tracker_address + \"?\" + urllib.urlencode(params)\n                \n                with gevent.Timeout(30, False):  \n                    req = urllib2.urlopen(url, timeout=25)\n                    response = req.read()\n                    req.fp._sock.recv = None  \n                    req.close()\n                    req = None\n                if not response:\n                    self.log.warning(\"Tracker error: http://%s (No response)\" % tracker_address)\n                    return False\n                \n                peer_data = bencode.decode(response)[\"peers\"]\n                response = None\n                peer_count = len(peer_data) / 6\n                peers = []\n                for peer_offset in xrange(peer_count):\n                    off = 6 * peer_offset\n                    peer = peer_data[off:off + 6]\n                    addr, port = struct.unpack('!LH', peer)\n                    peers.append({\"addr\": socket.inet_ntoa(struct.pack('!L', addr)), \"port\": port})\n            except Exception, err:\n                self.log.warning(\"Tracker error: http://%s (%s)\" % (tracker_address, err))\n                if req:\n                    req.close()\n                    req = None\n                return False\n        else:\n            peers = []\n\n        \n        added = 0\n        for peer in peers:\n            if peer[\"port\"] == 1:  \n                peer[\"port\"] = 0\n            if not peer[\"port\"]:\n                continue  \n            if self.addPeer(peer[\"addr\"], peer[\"port\"], source=\"tracker\"):\n                added += 1\n        if added:\n            self.worker_manager.onPeers()\n            self.updateWebsocket(peers_added=added)\n            self.log.debug(\n                \"Tracker result: %s://%s (found %s peers, new: %s, total: %s)\" %\n                (tracker_protocol, tracker_address, len(peers), added, len(self.peers))\n            )\n        return time.time() - s\n\n    \n    def announce(self, force=False, mode=\"start\", pex=True):\n        if time.time() < self.time_announce + 30 and not force:\n            return  \n        self.time_announce = time.time()\n\n        trackers = config.trackers\n        \n        if config.disable_udp:\n            trackers = [tracker for tracker in trackers if not tracker.startswith(\"udp://\")]\n        if self.connection_server and self.connection_server.tor_manager and not self.connection_server.tor_manager.enabled:\n            trackers = [tracker for tracker in trackers if \".onion\" not in tracker]\n\n        if trackers and (mode == \"update\" or mode == \"more\"):  \n            self.last_tracker_id += 1\n            self.last_tracker_id = self.last_tracker_id % len(trackers)\n            trackers = [trackers[self.last_tracker_id]]  \n\n        errors = []\n        slow = []\n        add_types = []\n        if self.connection_server:\n            my_peer_id = self.connection_server.peer_id\n\n            \n            if self.connection_server.port_opened:\n                add_types.append(\"ip4\")\n            if self.connection_server.tor_manager and self.connection_server.tor_manager.start_onions:\n                add_types.append(\"onion\")\n        else:\n            my_peer_id = \"\"\n\n        s = time.time()\n        announced = 0\n        threads = []\n        fileserver_port = config.fileserver_port\n\n        for tracker in trackers:  \n            tracker_protocol, tracker_address = tracker.split(\"://\")\n            thread = gevent.spawn(\n                self.announceTracker, tracker_protocol, tracker_address, fileserver_port, add_types, my_peer_id, mode\n            )\n            threads.append(thread)\n            thread.tracker_address = tracker_address\n            thread.tracker_protocol = tracker_protocol\n\n        gevent.joinall(threads, timeout=10)  \n\n        for thread in threads:\n            if thread.value:\n                if thread.value > 1:\n                    slow.append(\"%.2fs %s://%s\" % (thread.value, thread.tracker_protocol, thread.tracker_address))\n                announced += 1\n            else:\n                if thread.ready():\n                    errors.append(\"%s://%s\" % (thread.tracker_protocol, thread.tracker_address))\n                else:  \n                    slow.append(\"10s+ %s://%s\" % (thread.tracker_protocol, thread.tracker_address))\n\n        \n        self.settings[\"peers\"] = len(self.peers)\n\n        if len(errors) < len(threads):  \n            if announced == 1:\n                announced_to = trackers[0]\n            else:\n                announced_to = \"%s trackers\" % announced\n            if config.verbose:\n                self.log.debug(\n                    \"Announced types %s in mode %s to %s in %.3fs, errors: %s, slow: %s\" %\n                    (add_types, mode, announced_to, time.time() - s, errors, slow)\n                )\n        else:\n            if mode != \"update\":\n                self.log.error(\"Announce to %s trackers in %.3fs, failed\" % (announced, time.time() - s))\n\n        if pex:\n            if not [peer for peer in self.peers.values() if peer.connection and peer.connection.connected]:\n                \n                gevent.spawn_later(3, self.announcePex, need_num=10)  \n            else:  \n                if mode == \"more\":  \n                    self.announcePex(need_num=10)\n                else:\n                    self.announcePex()\n\n    \n    def needConnections(self, num=6, check_site_on_reconnect=False):\n        need = min(len(self.peers), num, config.connected_limit)  \n\n        connected = len(self.getConnectedPeers())\n\n        connected_before = connected\n\n        self.log.debug(\"Need connections: %s, Current: %s, Total: %s\" % (need, connected, len(self.peers)))\n\n        if connected < need:  \n            for peer in self.peers.values():\n                if not peer.connection or not peer.connection.connected:  \n                    peer.pex()  \n                    if peer.connection and peer.connection.connected:\n                        connected += 1  \n                if connected >= need:\n                    break\n            self.log.debug(\n                \"Connected before: %s, after: %s. Check site: %s.\" %\n                (connected_before, connected, check_site_on_reconnect)\n            )\n\n        if check_site_on_reconnect and connected_before == 0 and connected > 0 and self.connection_server.has_internet:\n            gevent.spawn(self.update, check_files=False)\n\n        return connected\n\n    \n    def getConnectablePeers(self, need_num=5, ignore=[], allow_private=True):\n        peers = self.peers.values()\n        found = []\n        for peer in peers:\n            if peer.key.endswith(\":0\"):\n                continue  \n            if not peer.connection:\n                continue  \n            if peer.key in ignore:\n                continue  \n            if time.time() - peer.connection.last_recv_time > 60 * 60 * 2:  \n                peer.connection = None  \n                continue\n            if not allow_private and helper.isPrivateIp(peer.ip):\n                continue\n            found.append(peer)\n            if len(found) >= need_num:\n                break  \n\n        if len(found) < need_num:  \n            found = [\n                peer for peer in peers\n                if not peer.key.endswith(\":0\") and\n                peer.key not in ignore and\n                (allow_private or not helper.isPrivateIp(peer.ip))\n            ][0:need_num - len(found)]\n\n        return found\n\n    \n    def getRecentPeers(self, need_num):\n        found = list(set(self.peers_recent))\n        self.log.debug(\"Recent peers %s of %s (need: %s)\" % (len(found), len(self.peers_recent), need_num))\n\n        if len(found) >= need_num or len(found) >= len(self.peers):\n            return found[0:need_num]\n\n        \n        need_more = need_num - len(found)\n        found_more = sorted(\n            self.peers.values()[0:need_more * 50],\n            key=lambda peer: peer.time_found + peer.reputation * 60,\n            reverse=True\n        )[0:need_more * 2]\n        random.shuffle(found_more)\n\n        found += found_more\n\n        return found[0:need_num]\n\n    def getConnectedPeers(self):\n        back = []\n        if not self.connection_server:\n            return []\n\n        tor_manager = self.connection_server.tor_manager\n        for connection in self.connection_server.connections:\n            if not connection.connected and time.time() - connection.start_time > 20:  \n                continue\n            peer = self.peers.get(\"%s:%s\" % (connection.ip, connection.port))\n            if peer:\n                if connection.target_onion and tor_manager.start_onions and tor_manager.getOnion(self.address) != connection.target_onion:\n                    continue\n                if not peer.connection:\n                    peer.connect(connection)\n                back.append(peer)\n        return back\n\n    \n    def cleanupPeers(self, peers_protected=[]):\n        peers = self.peers.values()\n        if len(peers) > 20:\n            \n            removed = 0\n            if len(peers) > 1000:\n                ttl = 60 * 60 * 1\n            else:\n                ttl = 60 * 60 * 4\n\n            for peer in peers:\n                if peer.connection and peer.connection.connected:\n                    continue\n                if peer.connection and not peer.connection.connected:\n                    peer.connection = None  \n                if time.time() - peer.time_found > ttl:  \n                    peer.remove(\"Time found expired\")\n                    removed += 1\n                if removed > len(peers) * 0.1:  \n                    break\n\n            if removed:\n                self.log.debug(\"Cleanup peers result: Removed %s, left: %s\" % (removed, len(self.peers)))\n\n        \n        closed = 0\n        connected_peers = [peer for peer in self.getConnectedPeers() if peer.connection.connected]  \n        need_to_close = len(connected_peers) - config.connected_limit\n\n        if closed < need_to_close:\n            \n            for peer in sorted(connected_peers, key=lambda peer: min(peer.connection.sites, 5)):\n                if not peer.connection:\n                    continue\n                if peer.key in peers_protected:\n                    continue\n                if peer.connection.sites > 5:\n                    break\n                peer.connection.close(\"Cleanup peers\")\n                peer.connection = None\n                closed += 1\n                if closed >= need_to_close:\n                    break\n\n        if need_to_close > 0:\n            self.log.debug(\"Connected: %s, Need to close: %s, Closed: %s\" % (len(connected_peers), need_to_close, closed))\n\n    \n    def sendMyHashfield(self, limit=5):\n        if not self.content_manager.hashfield:  \n            return False\n\n        sent = 0\n        connected_peers = self.getConnectedPeers()\n        for peer in connected_peers:\n            if peer.sendMyHashfield():\n                sent += 1\n                if sent >= limit:\n                    break\n        if sent:\n            my_hashfield_changed = self.content_manager.hashfield.time_changed\n            self.log.debug(\"Sent my hashfield (chaged %.3fs ago) to %s peers\" % (time.time() - my_hashfield_changed, sent))\n        return sent\n\n    \n    def updateHashfield(self, limit=5):\n        \n        if not self.content_manager.hashfield and not self.content_manager.contents.get(\"content.json\", {}).get(\"files_optional\"):\n            return False\n\n        queried = 0\n        connected_peers = self.getConnectedPeers()\n        for peer in connected_peers:\n            if peer.time_hashfield:\n                continue\n            if peer.updateHashfield():\n                queried += 1\n            if queried >= limit:\n                break\n        if queried:\n            self.log.debug(\"Queried hashfield from %s peers\" % queried)\n        return queried\n\n    \n    def isDownloadable(self, inner_path):\n        return self.settings.get(\"autodownloadoptional\")\n\n    def delete(self):\n        self.settings[\"serving\"] = False\n        self.saveSettings()\n        self.worker_manager.running = False\n        self.worker_manager.stopWorkers()\n        self.storage.deleteFiles()\n        self.updateWebsocket()\n        self.content_manager.contents.db.deleteSite(self)\n        SiteManager.site_manager.delete(self.address)\n\n    \n\n    \n    def addEventListeners(self):\n        self.onFileStart = util.Event()  \n        self.onFileDone = util.Event()  \n        self.onFileFail = util.Event()  \n        self.onComplete = util.Event()  \n\n        self.onFileStart.append(lambda inner_path: self.fileStarted())  \n        self.onFileDone.append(lambda inner_path: self.fileDone(inner_path))\n        self.onFileFail.append(lambda inner_path: self.fileFailed(inner_path))\n\n    \n    def updateWebsocket(self, **kwargs):\n        if kwargs:\n            param = {\"event\": kwargs.items()[0]}\n        else:\n            param = None\n        for ws in self.websockets:\n            ws.event(\"siteChanged\", self, param)\n\n    def messageWebsocket(self, message, type=\"info\", progress=None):\n        for ws in self.websockets:\n            if progress is None:\n                ws.cmd(\"notification\", [type, message])\n            else:\n                ws.cmd(\"progress\", [type, message, progress])\n\n    \n    @util.Noparallel(blocking=False)\n    def fileStarted(self):\n        time.sleep(0.001)  \n        self.updateWebsocket(file_started=True)\n\n    \n    def fileDone(self, inner_path):\n        \n        if inner_path in self.bad_files:\n            if config.verbose:\n                self.log.debug(\"Bad file solved: %s\" % inner_path)\n            del(self.bad_files[inner_path])\n\n        \n        if inner_path == \"content.json\":\n            self.content_updated = time.time()\n\n        self.updateWebsocket(file_done=inner_path)\n\n    \n    def fileFailed(self, inner_path):\n        if inner_path == \"content.json\":\n            self.content_updated = False\n            self.log.debug(\"Can't update content.json\")\n        if inner_path in self.bad_files and self.connection_server.has_internet:\n            self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1\n\n        self.updateWebsocket(file_failed=inner_path)\n\n        if self.bad_files.get(inner_path, 0) > 30:\n            self.log.debug(\"Giving up on %s\" % inner_path)\n            del self.bad_files[inner_path]  \n", "comments": "make sure correct address short address logging load content.json key: ip:port value: peer.peer ignore peers (eg myself) last announce time tracker last announced tracker id handle site download peers sha check failed files need redownload {\"inner.content\": 1} (key: file value: failed accept) content.js update time pending notifications displayed page load [error|ok|info message timeout] page viewed browser active site websocket connections load settings sites.json save load site files load content.json files use global file server default possible to auth user site (obsolete removed) to auth websocket permissions to auth websocket permissions load site settings data/sites.json give minimum 10 tries restart default add admin permissions homepage save site settings data/sites.json max site size mb next size limit based current size download file content.json download failed check modifications succed later could download content.json update last received update peer prevent re-sending update start download files try diff first start download dont wait finish return event need downloading file allowed append evt optionals files start download dont wait finish return event need downloading file allowed append evt wait includes download check every file up-to-date no task trigger site complete return bad files less 3 retry retry download bad files larger number tries = less likely check every 15min download files site check size first just download content.json files cant download content.jsons size fits download everything update worker try find client supports listmodifications command stop 3 successful query failed query check peer newer files we dont file older download newest first check modified content.json files peers add modified files bad_files return: successfully queried peers [peer peer...] try peers successfully queried peers wait peers add non-connected peers necessary no since defined download last modification time-1day wait 10 sec workers done query modifications start another 3 thread first 3 stuck add connected peers wait another 10 sec none updaters finished update content.json peers download changed files return: none reload content.json reset content updated time remove files longer content.json full update reset bad files quick check mark bad files based file size failed query modifications update site redownload content.json download content.json publish worker find ip port all peers done published engouht peer connected timeout: 5sec + size kb + last_ping peer connected timeout: 10sec + size kb update content.json peers successfully published (peer) publisher threads prefer newer clients add non-connected peers necessary no peers found wait done if less need sleep bit no successful publish wait publisher publish peers backgroup send hashfield every connected peer changed copy site dont copy directories (has -default version) copy root content.json new site: content.json exist yet create new one source site use content.json-default specified site owner's bip32 index copy files sign content.json end make sure every file included relative content.json strip leading / dont copy directories -default postfixed alternative copy file normally keep -default postfixed dir file allow cloning later don't copy root content.json-default if -default path create -default less copy file don't overwrite site files default ones sign content json rebuild db no info file download content.json first check download file exist file exist need anything site serving wait file downloaded mark bad file no content.json download first! prevent double download content.json download failed add update peer site return_peer: always return peer even already present already ip always return peer new peer ignore blacklist (eg myself) gather peer connected peers connected peers small number connected peers site connect we result gather peers tracker return: complete time false error udp tracker no udp supported http tracker load url make sure timeout hacky avoidance memory leak older python versions decode peers adding peers some trackers accept port 0 send port 1 not-connectable dont add peers port 0 add get peers tracker no reannouncing within 30 secs filter trackers based supported networks only announce one tracker increment queried tracker id we going use one type addresses reach start announce threads wait announce finish still running save peers num less errors total tracker nums if connected peer yet wait connections spawn 3 secs later else announce immediately need peers keep connections get updates need 5 peer max total peers need no peer connection disconnected initiate peer exchange successfully connected return: probably peers verified connectable recently not connectable no connection the requester peer last message 2 hours ago cleanup: dead connection found requested number peers return good peers return: recently found peers add random peers still connected 20s cleanup probably dead peers close connection much cleanup old peers dead connection not found tracker via pex last 4 hour don't remove much close peers limit only fully connected peers try keep connections sites send hashfield peers no optional files update hashfield return optional files returns optional file need downloaded - events - add event listeners if workermanager added new task if workermanager successfully downloaded file if workermanager failed download file all file finished no parameters make noparallel batching working send site status update websocket clients file download started wait files adds file downloaded successful file downloaded remove bad files update content.json last downlad time file download failed give 30 tries", "content": "import os\nimport json\nimport logging\nimport hashlib\nimport re\nimport time\nimport random\nimport sys\nimport struct\nimport socket\nimport urllib\nimport urllib2\nimport hashlib\nimport collections\n\nimport gevent\nimport gevent.pool\n\nimport util\nfrom lib import bencode\nfrom lib.subtl.subtl import UdpTrackerClient\nfrom Config import config\nfrom Peer import Peer\nfrom Worker import WorkerManager\nfrom Debug import Debug\nfrom Content import ContentManager\nfrom SiteStorage import SiteStorage\nfrom Crypt import CryptHash\nfrom util import helper\nfrom util import Diff\nfrom Plugin import PluginManager\nimport SiteManager\n\n\n@PluginManager.acceptPlugins\nclass Site(object):\n\n    def __init__(self, address, allow_create=True, settings=None):\n        self.address = re.sub(\"[^A-Za-z0-9]\", \"\", address)  # Make sure its correct address\n        self.address_hash = hashlib.sha256(self.address).digest()\n        self.address_short = \"%s..%s\" % (self.address[:6], self.address[-4:])  # Short address for logging\n        self.log = logging.getLogger(\"Site:%s\" % self.address_short)\n        self.addEventListeners()\n\n        self.content = None  # Load content.json\n        self.peers = {}  # Key: ip:port, Value: Peer.Peer\n        self.peers_recent = collections.deque(maxlen=100)\n        self.peer_blacklist = SiteManager.peer_blacklist  # Ignore this peers (eg. myself)\n        self.time_announce = 0  # Last announce time to tracker\n        self.last_tracker_id = random.randint(0, 10)  # Last announced tracker id\n        self.worker_manager = WorkerManager(self)  # Handle site download from other peers\n        self.bad_files = {}  # SHA check failed files, need to redownload {\"inner.content\": 1} (key: file, value: failed accept)\n        self.content_updated = None  # Content.js update time\n        self.notifications = []  # Pending notifications displayed once on page load [error|ok|info, message, timeout]\n        self.page_requested = False  # Page viewed in browser\n        self.websockets = []  # Active site websocket connections\n\n        self.connection_server = None\n        self.loadSettings(settings)  # Load settings from sites.json\n        self.storage = SiteStorage(self, allow_create=allow_create)  # Save and load site files\n        self.content_manager = ContentManager(self)\n        self.content_manager.loadContents()  # Load content.json files\n        if \"main\" in sys.modules and \"file_server\" in dir(sys.modules[\"main\"]):  # Use global file server by default if possible\n            self.connection_server = sys.modules[\"main\"].file_server\n        else:\n            self.connection_server = None\n        if not self.settings.get(\"auth_key\"):  # To auth user in site (Obsolete, will be removed)\n            self.settings[\"auth_key\"] = CryptHash.random()\n            self.log.debug(\"New auth key: %s\" % self.settings[\"auth_key\"])\n\n        if not self.settings.get(\"wrapper_key\"):  # To auth websocket permissions\n            self.settings[\"wrapper_key\"] = CryptHash.random()\n            self.log.debug(\"New wrapper key: %s\" % self.settings[\"wrapper_key\"])\n\n        if not self.settings.get(\"ajax_key\"):  # To auth websocket permissions\n            self.settings[\"ajax_key\"] = CryptHash.random()\n            self.log.debug(\"New ajax key: %s\" % self.settings[\"ajax_key\"])\n\n    def __str__(self):\n        return \"Site %s\" % self.address_short\n\n    def __repr__(self):\n        return \"<%s>\" % self.__str__()\n\n    # Load site settings from data/sites.json\n    def loadSettings(self, settings=None):\n        if not settings:\n            settings = json.load(open(\"%s/sites.json\" % config.data_dir)).get(self.address)\n        if settings:\n            self.settings = settings\n            if \"cache\" not in settings:\n                settings[\"cache\"] = {}\n            if \"size_files_optional\" not in settings:\n                settings[\"size_optional\"] = 0\n            if \"optional_downloaded\" not in settings:\n                settings[\"optional_downloaded\"] = 0\n            self.bad_files = settings[\"cache\"].get(\"bad_files\", {})\n            settings[\"cache\"][\"bad_files\"] = {}\n            # Give it minimum 10 tries after restart\n            for inner_path in self.bad_files:\n                self.bad_files[inner_path] = min(self.bad_files[inner_path], 20)\n        else:\n            self.settings = {\n                \"own\": False, \"serving\": True, \"permissions\": [],\n                \"added\": int(time.time()), \"optional_downloaded\": 0, \"size_optional\": 0\n            }  # Default\n            if config.download_optional == \"auto\":\n                self.settings[\"autodownloadoptional\"] = True\n\n        # Add admin permissions to homepage\n        if self.address == config.homepage and \"ADMIN\" not in self.settings[\"permissions\"]:\n            self.settings[\"permissions\"].append(\"ADMIN\")\n\n        return\n\n    # Save site settings to data/sites.json\n    def saveSettings(self):\n        if not SiteManager.site_manager.sites:\n            SiteManager.site_manager.sites = {}\n        if not SiteManager.site_manager.sites.get(self.address):\n            SiteManager.site_manager.sites[self.address] = self\n            SiteManager.site_manager.load(False)\n        SiteManager.site_manager.save()\n\n    def getSettingsCache(self):\n        back = {}\n        back[\"bad_files\"] = self.bad_files\n        back[\"hashfield\"] = self.content_manager.hashfield.tostring().encode(\"base64\")\n        return back\n\n    # Max site size in MB\n    def getSizeLimit(self):\n        return self.settings.get(\"size_limit\", int(config.size_limit))\n\n    # Next size limit based on current size\n    def getNextSizeLimit(self):\n        size_limits = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000]\n        size = self.settings.get(\"size\", 0)\n        for size_limit in size_limits:\n            if size * 1.2 < size_limit * 1024 * 1024:\n                return size_limit\n        return 999999\n\n    # Download all file from content.json\n    def downloadContent(self, inner_path, download_files=True, peer=None, check_modifications=False, diffs={}):\n        s = time.time()\n        if config.verbose:\n            self.log.debug(\"Downloading %s...\" % inner_path)\n\n        if not inner_path.endswith(\"content.json\"):\n            return False\n\n        found = self.needFile(inner_path, update=self.bad_files.get(inner_path))\n        content_inner_dir = helper.getDirname(inner_path)\n        if not found:\n            self.log.debug(\"Download %s failed, check_modifications: %s\" % (inner_path, check_modifications))\n            if check_modifications:  # Download failed, but check modifications if its succed later\n                self.onFileDone.once(lambda file_name: self.checkModifications(0), \"check_modifications\")\n            return False  # Could not download content.json\n\n        if config.verbose:\n            self.log.debug(\"Got %s\" % inner_path)\n        changed, deleted = self.content_manager.loadContent(inner_path, load_includes=False)\n\n        if inner_path == \"content.json\":\n            self.saveSettings()\n\n        if peer:  # Update last received update from peer to prevent re-sending the same update to it\n            peer.last_content_json_update = self.content_manager.contents[inner_path][\"modified\"]\n\n        # Start download files\n        file_threads = []\n        if download_files:\n            for file_relative_path in self.content_manager.contents[inner_path].get(\"files\", {}).keys():\n                file_inner_path = content_inner_dir + file_relative_path\n\n                # Try to diff first\n                diff_success = False\n                diff_actions = diffs.get(file_relative_path)\n                if diff_actions and self.bad_files.get(file_inner_path):\n                    try:\n                        s = time.time()\n                        new_file = Diff.patch(self.storage.open(file_inner_path, \"rb\"), diff_actions)\n                        new_file.seek(0)\n                        time_diff = time.time() - s\n\n                        s = time.time()\n                        diff_success = self.content_manager.verifyFile(file_inner_path, new_file)\n                        time_verify = time.time() - s\n\n                        if diff_success:\n                            s = time.time()\n                            new_file.seek(0)\n                            self.storage.write(file_inner_path, new_file)\n                            time_write = time.time() - s\n\n                            s = time.time()\n                            self.onFileDone(file_inner_path)\n                            time_on_done = time.time() - s\n\n                            self.log.debug(\n                                \"Patched successfully: %s (diff: %.3fs, verify: %.3fs, write: %.3fs, on_done: %.3fs)\" %\n                                (file_inner_path, time_diff, time_verify, time_write, time_on_done)\n                            )\n                    except Exception, err:\n                        self.log.debug(\"Failed to patch %s: %s\" % (file_inner_path, err))\n                        diff_success = False\n\n                if not diff_success:\n                    # Start download and dont wait for finish, return the event\n                    res = self.needFile(file_inner_path, blocking=False, update=self.bad_files.get(file_inner_path), peer=peer)\n                    if res is not True and res is not False:  # Need downloading and file is allowed\n                        file_threads.append(res)  # Append evt\n\n            # Optionals files\n            if inner_path == \"content.json\":\n                gevent.spawn(self.updateHashfield)\n\n            for file_relative_path in self.content_manager.contents[inner_path].get(\"files_optional\", {}).keys():\n                file_inner_path = content_inner_dir + file_relative_path\n                if file_inner_path not in changed and not self.bad_files.get(file_inner_path):\n                    continue\n                if not self.isDownloadable(file_inner_path):\n                    continue\n                # Start download and dont wait for finish, return the event\n                res = self.pooledNeedFile(\n                    file_inner_path, blocking=False, update=self.bad_files.get(file_inner_path), peer=peer\n                )\n                if res is not True and res is not False:  # Need downloading and file is allowed\n                    file_threads.append(res)  # Append evt\n\n        # Wait for includes download\n        include_threads = []\n        for file_relative_path in self.content_manager.contents[inner_path].get(\"includes\", {}).keys():\n            file_inner_path = content_inner_dir + file_relative_path\n            include_thread = gevent.spawn(self.downloadContent, file_inner_path, download_files=download_files, peer=peer)\n            include_threads.append(include_thread)\n\n        if config.verbose:\n            self.log.debug(\"%s: Downloading %s includes...\" % (inner_path, len(include_threads)))\n        gevent.joinall(include_threads)\n        if config.verbose:\n            self.log.debug(\"%s: Includes download ended\" % inner_path)\n\n        if check_modifications:  # Check if every file is up-to-date\n            self.checkModifications(0)\n\n        if config.verbose:\n            self.log.debug(\"%s: Downloading %s files, changed: %s...\" % (inner_path, len(file_threads), len(changed)))\n        gevent.joinall(file_threads)\n        if config.verbose:\n            self.log.debug(\"%s: DownloadContent ended in %.3fs\" % (inner_path, time.time() - s))\n\n        if not self.worker_manager.tasks:\n            self.onComplete()  # No more task trigger site complete\n\n        return True\n\n    # Return bad files with less than 3 retry\n    def getReachableBadFiles(self):\n        if not self.bad_files:\n            return False\n        return [bad_file for bad_file, retry in self.bad_files.iteritems() if retry < 3]\n\n    # Retry download bad files\n    def retryBadFiles(self, force=False):\n        self.checkBadFiles()\n\n        self.log.debug(\"Retry %s bad files\" % len(self.bad_files))\n        content_inner_paths = []\n        file_inner_paths = []\n\n        for bad_file, tries in self.bad_files.items():\n            if force or random.randint(0, min(40, tries)) < 4:  # Larger number tries = less likely to check every 15min\n                if bad_file.endswith(\"content.json\"):\n                    content_inner_paths.append(bad_file)\n                else:\n                    file_inner_paths.append(bad_file)\n\n        if content_inner_paths:\n            self.pooledDownloadContent(content_inner_paths, only_if_bad=True)\n\n        if file_inner_paths:\n            self.pooledDownloadFile(file_inner_paths, only_if_bad=True)\n\n    def checkBadFiles(self):\n        for bad_file in self.bad_files.keys():\n            file_info = self.content_manager.getFileInfo(bad_file)\n            if bad_file.endswith(\"content.json\"):\n                if file_info is False and bad_file != \"content.json\":\n                    del self.bad_files[bad_file]\n                    self.log.debug(\"No info for file: %s, removing from bad_files\" % bad_file)\n            else:\n                if file_info is False or not file_info.get(\"size\"):\n                    del self.bad_files[bad_file]\n                    self.log.debug(\"No info or size for file: %s, removing from bad_files\" % bad_file)\n\n    # Download all files of the site\n    @util.Noparallel(blocking=False)\n    def download(self, check_size=False, blind_includes=False):\n        if not self.connection_server:\n            self.log.debug(\"No connection server found, skipping download\")\n            return False\n\n        self.log.debug(\n            \"Start downloading, bad_files: %s, check_size: %s, blind_includes: %s\" %\n            (self.bad_files, check_size, blind_includes)\n        )\n        gevent.spawn(self.announce, force=True)\n        if check_size:  # Check the size first\n            valid = self.downloadContent(\"content.json\", download_files=False)  # Just download content.json files\n            if not valid:\n                return False  # Cant download content.jsons or size is not fits\n\n        # Download everything\n        valid = self.downloadContent(\"content.json\", check_modifications=blind_includes)\n\n        self.onComplete.once(lambda: self.retryBadFiles(force=True))\n\n        return valid\n\n    def pooledDownloadContent(self, inner_paths, pool_size=100, only_if_bad=False):\n        self.log.debug(\"New downloadContent pool: len: %s\" % len(inner_paths))\n        self.worker_manager.started_task_num += len(inner_paths)\n        pool = gevent.pool.Pool(pool_size)\n        for inner_path in inner_paths:\n            if not only_if_bad or inner_path in self.bad_files:\n                pool.spawn(self.downloadContent, inner_path)\n            self.worker_manager.started_task_num -= 1\n        self.log.debug(\"Ended downloadContent pool len: %s\" % len(inner_paths))\n\n    def pooledDownloadFile(self, inner_paths, pool_size=100, only_if_bad=False):\n        self.log.debug(\"New downloadFile pool: len: %s\" % len(inner_paths))\n        self.worker_manager.started_task_num += len(inner_paths)\n        pool = gevent.pool.Pool(pool_size)\n        for inner_path in inner_paths:\n            if not only_if_bad or inner_path in self.bad_files:\n                pool.spawn(self.needFile, inner_path, update=True)\n            self.worker_manager.started_task_num -= 1\n        self.log.debug(\"Ended downloadFile pool len: %s\" % len(inner_paths))\n\n    # Update worker, try to find client that supports listModifications command\n    def updater(self, peers_try, queried, since):\n        while 1:\n            if not peers_try or len(queried) >= 3:  # Stop after 3 successful query\n                break\n            peer = peers_try.pop(0)\n            if config.verbose:\n                self.log.debug(\"Try to get updates from: %s Left: %s\" % (peer, peers_try))\n\n            res = None\n            with gevent.Timeout(20, exception=False):\n                res = peer.listModified(since)\n\n            if not res or \"modified_files\" not in res:\n                continue  # Failed query\n\n            queried.append(peer)\n            modified_contents = []\n            my_modified = self.content_manager.listModified(since)\n            for inner_path, modified in res[\"modified_files\"].iteritems():  # Check if the peer has newer files than we\n                has_newer = int(modified) > my_modified.get(inner_path, 0)\n                has_older = int(modified) < my_modified.get(inner_path, 0)\n                if inner_path not in self.bad_files and not self.content_manager.isArchived(inner_path, modified):\n                    if has_newer:\n                        # We dont have this file or we have older\n                        modified_contents.append(inner_path)\n                        self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1\n                    if has_older:\n                        self.log.debug(\"%s client has older version of %s, publishing there...\" % (peer, inner_path))\n                        gevent.spawn(self.publisher, inner_path, [peer], [], 1)\n            if modified_contents:\n                self.log.debug(\"%s new modified file from %s\" % (len(modified_contents), peer))\n                modified_contents.sort(key=lambda inner_path: 0 - res[\"modified_files\"][inner_path])  # Download newest first\n                gevent.spawn(self.pooledDownloadContent, modified_contents)\n\n    # Check modified content.json files from peers and add modified files to bad_files\n    # Return: Successfully queried peers [Peer, Peer...]\n    def checkModifications(self, since=None):\n        s = time.time()\n        peers_try = []  # Try these peers\n        queried = []  # Successfully queried from these peers\n        limit = 5\n\n        # Wait for peers\n        if not self.peers:\n            self.announce()\n            for wait in range(10):\n                time.sleep(5 + wait)\n                self.log.debug(\"Waiting for peers...\")\n                if self.peers:\n                    break\n\n        peers_try = self.getConnectedPeers()\n        peers_connected_num = len(peers_try)\n        if peers_connected_num < limit * 2:  # Add more, non-connected peers if necessary\n            peers_try += self.getRecentPeers(limit * 5)\n\n        if since is None:  # No since defined, download from last modification time-1day\n            since = self.settings.get(\"modified\", 60 * 60 * 24) - 60 * 60 * 24\n\n        if config.verbose:\n            self.log.debug(\n                \"Try to get listModifications from peers: %s, connected: %s, since: %s\" %\n                (peers_try, peers_connected_num, since)\n            )\n\n        updaters = []\n        for i in range(3):\n            updaters.append(gevent.spawn(self.updater, peers_try, queried, since))\n\n        gevent.joinall(updaters, timeout=10)  # Wait 10 sec to workers done query modifications\n\n        if not queried:  # Start another 3 thread if first 3 is stuck\n            peers_try[0:0] = [peer for peer in self.getConnectedPeers() if peer.connection.connected]  # Add connected peers\n            for _ in range(10):\n                gevent.joinall(updaters, timeout=10)  # Wait another 10 sec if none of updaters finished\n                if queried:\n                    break\n\n        self.log.debug(\"Queried listModifications from: %s in %.3fs\" % (queried, time.time() - s))\n        time.sleep(0.1)\n        return queried\n\n    # Update content.json from peers and download changed files\n    # Return: None\n    @util.Noparallel()\n    def update(self, announce=False, check_files=False, since=None):\n        self.content_manager.loadContent(\"content.json\", load_includes=False)  # Reload content.json\n        self.content_updated = None  # Reset content updated time\n        self.updateWebsocket(updating=True)\n\n        # Remove files that no longer in content.json\n        self.checkBadFiles()\n\n        if announce:\n            self.announce()\n\n        # Full update, we can reset bad files\n        if check_files and since == 0:\n            self.bad_files = {}\n\n        queried = self.checkModifications(since)\n\n        if check_files:\n            self.storage.updateBadFiles(quick_check=True)  # Quick check and mark bad files based on file size\n\n        changed, deleted = self.content_manager.loadContent(\"content.json\", load_includes=False)\n\n        if self.bad_files:\n            self.log.debug(\"Bad files: %s\" % self.bad_files)\n            gevent.spawn(self.retryBadFiles, force=True)\n\n        if len(queried) == 0:\n            # Failed to query modifications\n            self.content_updated = False\n            self.bad_files[\"content.json\"] = 1\n        else:\n            self.content_updated = time.time()\n\n        self.updateWebsocket(updated=True)\n\n    # Update site by redownload all content.json\n    def redownloadContents(self):\n        # Download all content.json again\n        content_threads = []\n        for inner_path in self.content_manager.contents.keys():\n            content_threads.append(self.needFile(inner_path, update=True, blocking=False))\n\n        self.log.debug(\"Waiting %s content.json to finish...\" % len(content_threads))\n        gevent.joinall(content_threads)\n\n    # Publish worker\n    def publisher(self, inner_path, peers, published, limit, diffs={}, event_done=None, cb_progress=None):\n        file_size = self.storage.getSize(inner_path)\n        content_json_modified = self.content_manager.contents[inner_path][\"modified\"]\n        body = self.storage.read(inner_path)\n\n        # Find out my ip and port\n        tor_manager = self.connection_server.tor_manager\n        if tor_manager and tor_manager.enabled and tor_manager.start_onions:\n            my_ip = tor_manager.getOnion(self.address)\n            if my_ip:\n                my_ip += \".onion\"\n            my_port = config.fileserver_port\n        else:\n            my_ip = config.ip_external\n            if self.connection_server.port_opened:\n                my_port = config.fileserver_port\n            else:\n                my_port = 0\n\n        while 1:\n            if not peers or len(published) >= limit:\n                if event_done:\n                    event_done.set(True)\n                break  # All peers done, or published engouht\n            peer = peers.pop()\n            if peer in published:\n                continue\n            if peer.last_content_json_update == content_json_modified:\n                self.log.debug(\"%s already received this update for %s, skipping\" % (peer, inner_path))\n                continue\n\n            if peer.connection and peer.connection.last_ping_delay:  # Peer connected\n                # Timeout: 5sec + size in kb + last_ping\n                timeout = 5 + int(file_size / 1024) + peer.connection.last_ping_delay\n            else:  # Peer not connected\n                # Timeout: 10sec + size in kb\n                timeout = 10 + int(file_size / 1024)\n            result = {\"exception\": \"Timeout\"}\n\n            for retry in range(2):\n                try:\n                    with gevent.Timeout(timeout, False):\n                        result = peer.request(\"update\", {\n                            \"site\": self.address,\n                            \"inner_path\": inner_path,\n                            \"body\": body,\n                            \"diffs\": diffs\n                        })\n                    if result:\n                        break\n                except Exception, err:\n                    self.log.error(\"Publish error: %s\" % Debug.formatException(err))\n                    result = {\"exception\": Debug.formatException(err)}\n\n            if result and \"ok\" in result:\n                published.append(peer)\n                if cb_progress and len(published) <= limit:\n                    cb_progress(len(published), limit)\n                self.log.info(\"[OK] %s: %s %s/%s\" % (peer.key, result[\"ok\"], len(published), limit))\n            else:\n                if result == {\"exception\": \"Timeout\"}:\n                    peer.onConnectionError(\"Publish timeout\")\n                self.log.info(\"[FAILED] %s: %s\" % (peer.key, result))\n            time.sleep(0.01)\n\n    # Update content.json on peers\n    @util.Noparallel()\n    def publish(self, limit=\"default\", inner_path=\"content.json\", diffs={}, cb_progress=None):\n        published = []  # Successfully published (Peer)\n        publishers = []  # Publisher threads\n\n        if not self.peers:\n            self.announce()\n\n        if limit == \"default\":\n            limit = 5\n        threads = limit\n\n        peers = self.getConnectedPeers()\n        num_connected_peers = len(peers)\n\n        random.shuffle(peers)\n        peers = sorted(peers, key=lambda peer: peer.connection.handshake.get(\"rev\", 0) < config.rev - 100)  # Prefer newer clients\n\n        if len(peers) < limit * 2 and len(self.peers) > len(peers):  # Add more, non-connected peers if necessary\n            peers += self.getRecentPeers(limit * 2)\n\n        peers = set(peers)\n\n        self.log.info(\"Publishing %s to %s/%s peers (connected: %s) diffs: %s (%.2fk)...\" % (\n            inner_path, limit, len(self.peers), num_connected_peers, diffs.keys(), float(len(str(diffs))) / 1024\n        ))\n\n        if not peers:\n            return 0  # No peers found\n\n        event_done = gevent.event.AsyncResult()\n        for i in range(min(len(peers), limit, threads)):\n            publisher = gevent.spawn(self.publisher, inner_path, peers, published, limit, diffs, event_done, cb_progress)\n            publishers.append(publisher)\n\n        event_done.get()  # Wait for done\n        if len(published) < min(len(self.peers), limit):\n            time.sleep(0.2)  # If less than we need sleep a bit\n        if len(published) == 0:\n            gevent.joinall(publishers)  # No successful publish, wait for all publisher\n\n        # Publish more peers in the backgroup\n        self.log.info(\n            \"Successfuly %s published to %s peers, publishing to %s more peers in the background\" %\n            (inner_path, len(published), limit)\n        )\n\n        for thread in range(2):\n            gevent.spawn(self.publisher, inner_path, peers, published, limit=limit * 2, diffs=diffs)\n\n        # Send my hashfield to every connected peer if changed\n        gevent.spawn(self.sendMyHashfield, 100)\n\n        return len(published)\n\n    # Copy this site\n    def clone(self, address, privatekey=None, address_index=None, root_inner_path=\"\", overwrite=False):\n        import shutil\n        new_site = SiteManager.site_manager.need(address, all_file=False)\n        default_dirs = []  # Dont copy these directories (has -default version)\n        for dir_name in os.listdir(self.storage.directory):\n            if \"-default\" in dir_name:\n                default_dirs.append(dir_name.replace(\"-default\", \"\"))\n\n        self.log.debug(\"Cloning to %s, ignore dirs: %s, root: %s\" % (address, default_dirs, root_inner_path))\n\n        # Copy root content.json\n        if not new_site.storage.isFile(\"content.json\") and not overwrite:\n            # New site: Content.json not exist yet, create a new one from source site\n            if \"size_limit\" in self.settings:\n                new_site.settings[\"size_limit\"] = self.settings[\"size_limit\"]\n\n            # Use content.json-default is specified\n            if self.storage.isFile(root_inner_path + \"/content.json-default\"):\n                content_json = self.storage.loadJson(root_inner_path + \"/content.json-default\")\n            else:\n                content_json = self.storage.loadJson(\"content.json\")\n\n            if \"domain\" in content_json:\n                del content_json[\"domain\"]\n            content_json[\"title\"] = \"my\" + content_json[\"title\"]\n            content_json[\"cloned_from\"] = self.address\n            content_json[\"clone_root\"] = root_inner_path\n            content_json[\"files\"] = {}\n            if address_index:\n                content_json[\"address_index\"] = address_index  # Site owner's BIP32 index\n            new_site.storage.writeJson(\"content.json\", content_json)\n            new_site.content_manager.loadContent(\n                \"content.json\", add_bad_files=False, delete_removed_files=False, load_includes=False\n            )\n\n        # Copy files\n        for content_inner_path, content in self.content_manager.contents.items():\n            file_relative_paths = content.get(\"files\", {}).keys()\n\n            # Sign content.json at the end to make sure every file is included\n            file_relative_paths.sort()\n            file_relative_paths.sort(key=lambda key: key.replace(\"-default\", \"\").endswith(\"content.json\"))\n\n            for file_relative_path in file_relative_paths:\n                file_inner_path = helper.getDirname(content_inner_path) + file_relative_path  # Relative to content.json\n                file_inner_path = file_inner_path.strip(\"/\")  # Strip leading /\n                if not file_inner_path.startswith(root_inner_path):\n                    self.log.debug(\"[SKIP] %s (not in clone root)\" % file_inner_path)\n                    continue\n                if file_inner_path.split(\"/\")[0] in default_dirs:  # Dont copy directories that has -default postfixed alternative\n                    self.log.debug(\"[SKIP] %s (has default alternative)\" % file_inner_path)\n                    continue\n                file_path = self.storage.getPath(file_inner_path)\n\n                # Copy the file normally to keep the -default postfixed dir and file to allow cloning later\n                if root_inner_path:\n                    file_inner_path_dest = re.sub(\"^%s/\" % re.escape(root_inner_path), \"\", file_inner_path)\n                    file_path_dest = new_site.storage.getPath(file_inner_path_dest)\n                else:\n                    file_inner_path_dest = file_inner_path\n                    file_path_dest = new_site.storage.getPath(file_inner_path)\n\n                self.log.debug(\"[COPY] %s to %s...\" % (file_inner_path, file_path_dest))\n                dest_dir = os.path.dirname(file_path_dest)\n                if not os.path.isdir(dest_dir):\n                    os.makedirs(dest_dir)\n                if file_inner_path_dest.replace(\"-default\", \"\") == \"content.json\":  # Don't copy root content.json-default\n                    continue\n\n                shutil.copy(file_path, file_path_dest)\n\n                # If -default in path, create a -default less copy of the file\n                if \"-default\" in file_inner_path:\n                    file_path_dest = new_site.storage.getPath(file_inner_path.replace(\"-default\", \"\"))\n                    if new_site.storage.isFile(file_inner_path.replace(\"-default\", \"\")) and not overwrite:\n                        # Don't overwrite site files with default ones\n                        self.log.debug(\"[SKIP] Default file: %s (already exist)\" % file_inner_path)\n                        continue\n                    self.log.debug(\"[COPY] Default file: %s to %s...\" % (file_inner_path, file_path_dest))\n                    dest_dir = os.path.dirname(file_path_dest)\n                    if not os.path.isdir(dest_dir):\n                        os.makedirs(dest_dir)\n                    shutil.copy(file_path, file_path_dest)\n                    # Sign if content json\n                    if file_path_dest.endswith(\"/content.json\"):\n                        new_site.storage.onUpdated(file_inner_path.replace(\"-default\", \"\"))\n                        new_site.content_manager.loadContent(\n                            file_inner_path.replace(\"-default\", \"\"), add_bad_files=False,\n                            delete_removed_files=False, load_includes=False\n                        )\n                        if privatekey:\n                            new_site.content_manager.sign(file_inner_path.replace(\"-default\", \"\"), privatekey)\n                            new_site.content_manager.loadContent(\n                                file_inner_path, add_bad_files=False, delete_removed_files=False, load_includes=False\n                            )\n\n        if privatekey:\n            new_site.content_manager.sign(\"content.json\", privatekey)\n            new_site.content_manager.loadContent(\n                \"content.json\", add_bad_files=False, delete_removed_files=False, load_includes=False\n            )\n\n        # Rebuild DB\n        if new_site.storage.isFile(\"dbschema.json\"):\n            new_site.storage.closeDb()\n            new_site.storage.rebuildDb()\n\n        return new_site\n\n    @util.Pooled(100)\n    def pooledNeedFile(self, *args, **kwargs):\n        return self.needFile(*args, **kwargs)\n\n    def isFileDownloadAllowed(self, inner_path, file_info):\n        if file_info.get(\"size\", 0) > config.file_size_limit * 1024 * 1024:\n            self.log.debug(\n                \"File size %s too large: %sMB > %sMB, skipping...\" %\n                (inner_path, file_info.get(\"size\", 0) / 1024 / 1024, config.file_size_limit)\n            )\n            return False\n        else:\n            return True\n\n    def needFileInfo(self, inner_path):\n        file_info = self.content_manager.getFileInfo(inner_path)\n        if not file_info:\n            # No info for file, download all content.json first\n            self.log.debug(\"No info for %s, waiting for all content.json\" % inner_path)\n            success = self.downloadContent(\"content.json\", download_files=False)\n            if not success:\n                return False\n            file_info = self.content_manager.getFileInfo(inner_path)\n        return file_info\n\n    # Check and download if file not exist\n    def needFile(self, inner_path, update=False, blocking=True, peer=None, priority=0):\n        if self.storage.isFile(inner_path) and not update:  # File exist, no need to do anything\n            return True\n        elif self.settings[\"serving\"] is False:  # Site not serving\n            return False\n        else:  # Wait until file downloaded\n            self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1  # Mark as bad file\n            if not self.content_manager.contents.get(\"content.json\"):  # No content.json, download it first!\n                self.log.debug(\"Need content.json first\")\n                gevent.spawn(self.announce)\n                if inner_path != \"content.json\":  # Prevent double download\n                    task = self.worker_manager.addTask(\"content.json\", peer)\n                    task[\"evt\"].get()\n                    self.content_manager.loadContent()\n                    if not self.content_manager.contents.get(\"content.json\"):\n                        return False  # Content.json download failed\n\n            file_info = None\n            if not inner_path.endswith(\"content.json\"):\n                file_info = self.needFileInfo(inner_path)\n                if not file_info:\n                    return False\n                if \"cert_signers\" in file_info and not file_info[\"content_inner_path\"] in self.content_manager.contents:\n                    self.log.debug(\"Missing content.json for requested user file: %s\" % inner_path)\n                    if self.bad_files.get(file_info[\"content_inner_path\"], 0) > 5:\n                        self.log.debug(\"File %s not reachable: retry %s\" % (\n                            inner_path, self.bad_files.get(file_info[\"content_inner_path\"], 0)\n                        ))\n                        return False\n                    self.downloadContent(file_info[\"content_inner_path\"])\n\n                if not self.isFileDownloadAllowed(inner_path, file_info):\n                    self.log.debug(\"%s: Download not allowed\" % inner_path)\n                    return False\n\n            task = self.worker_manager.addTask(inner_path, peer, priority=priority, file_info=file_info)\n            if blocking:\n                return task[\"evt\"].get()\n            else:\n                return task[\"evt\"]\n\n    # Add or update a peer to site\n    # return_peer: Always return the peer even if it was already present\n    def addPeer(self, ip, port, return_peer=False, connection=None, source=\"other\"):\n        if not ip or ip == \"0.0.0.0\":\n            return False\n        key = \"%s:%s\" % (ip, port)\n        peer = self.peers.get(key)\n        if peer:  # Already has this ip\n            peer.found(source)\n            if return_peer:  # Always return peer\n                return peer\n            else:\n                return False\n        else:  # New peer\n            if (ip, port) in self.peer_blacklist:\n                return False  # Ignore blacklist (eg. myself)\n            peer = Peer(ip, port, self)\n            self.peers[key] = peer\n            peer.found(source)\n            return peer\n\n    # Gather peer from connected peers\n    @util.Noparallel(blocking=False)\n    def announcePex(self, query_num=2, need_num=5):\n        peers = [peer for peer in self.peers.values() if peer.connection and peer.connection.connected]  # Connected peers\n        if len(peers) == 0:  # Small number of connected peers for this site, connect to any\n            self.log.debug(\"Small number of peers detected...query all of peers using pex\")\n            peers = self.peers.values()\n            need_num = 10\n\n        random.shuffle(peers)\n        done = 0\n        added = 0\n        for peer in peers:\n            res = peer.pex(need_num=need_num)\n            if type(res) == int:  # We have result\n                done += 1\n                added += res\n                if res:\n                    self.worker_manager.onPeers()\n                    self.updateWebsocket(peers_added=res)\n            if done == query_num:\n                break\n        self.log.debug(\"Pex result: from %s peers got %s new peers.\" % (done, added))\n\n    # Gather peers from tracker\n    # Return: Complete time or False on error\n    def announceTracker(self, tracker_protocol, tracker_address, fileserver_port=0, add_types=[], my_peer_id=\"\", mode=\"start\"):\n        s = time.time()\n        if mode == \"update\":\n            num_want = 10\n        else:\n            num_want = 30\n\n        if \"ip4\" not in add_types:\n            fileserver_port = 0\n\n        if tracker_protocol == \"udp\":  # Udp tracker\n            if config.disable_udp:\n                return False  # No udp supported\n            ip, port = tracker_address.split(\":\")\n            tracker = UdpTrackerClient(ip, int(port))\n            tracker.peer_port = fileserver_port\n            try:\n                tracker.connect()\n                tracker.poll_once()\n                tracker.announce(info_hash=hashlib.sha1(self.address).hexdigest(), num_want=num_want, left=431102370)\n                back = tracker.poll_once()\n                if back and type(back) is dict:\n                    peers = back[\"response\"][\"peers\"]\n                else:\n                    raise Exception(\"No response\")\n            except Exception, err:\n                self.log.warning(\"Tracker error: udp://%s:%s (%s)\" % (ip, port, err))\n                return False\n\n        elif tracker_protocol == \"http\":  # Http tracker\n            params = {\n                'info_hash': hashlib.sha1(self.address).digest(),\n                'peer_id': my_peer_id, 'port': fileserver_port,\n                'uploaded': 0, 'downloaded': 0, 'left': 431102370, 'compact': 1, 'numwant': num_want,\n                'event': 'started'\n            }\n            req = None\n            try:\n                url = \"http://\" + tracker_address + \"?\" + urllib.urlencode(params)\n                # Load url\n                with gevent.Timeout(30, False):  # Make sure of timeout\n                    req = urllib2.urlopen(url, timeout=25)\n                    response = req.read()\n                    req.fp._sock.recv = None  # Hacky avoidance of memory leak for older python versions\n                    req.close()\n                    req = None\n                if not response:\n                    self.log.warning(\"Tracker error: http://%s (No response)\" % tracker_address)\n                    return False\n                # Decode peers\n                peer_data = bencode.decode(response)[\"peers\"]\n                response = None\n                peer_count = len(peer_data) / 6\n                peers = []\n                for peer_offset in xrange(peer_count):\n                    off = 6 * peer_offset\n                    peer = peer_data[off:off + 6]\n                    addr, port = struct.unpack('!LH', peer)\n                    peers.append({\"addr\": socket.inet_ntoa(struct.pack('!L', addr)), \"port\": port})\n            except Exception, err:\n                self.log.warning(\"Tracker error: http://%s (%s)\" % (tracker_address, err))\n                if req:\n                    req.close()\n                    req = None\n                return False\n        else:\n            peers = []\n\n        # Adding peers\n        added = 0\n        for peer in peers:\n            if peer[\"port\"] == 1:  # Some trackers does not accept port 0, so we send port 1 as not-connectable\n                peer[\"port\"] = 0\n            if not peer[\"port\"]:\n                continue  # Dont add peers with port 0\n            if self.addPeer(peer[\"addr\"], peer[\"port\"], source=\"tracker\"):\n                added += 1\n        if added:\n            self.worker_manager.onPeers()\n            self.updateWebsocket(peers_added=added)\n            self.log.debug(\n                \"Tracker result: %s://%s (found %s peers, new: %s, total: %s)\" %\n                (tracker_protocol, tracker_address, len(peers), added, len(self.peers))\n            )\n        return time.time() - s\n\n    # Add myself and get other peers from tracker\n    def announce(self, force=False, mode=\"start\", pex=True):\n        if time.time() < self.time_announce + 30 and not force:\n            return  # No reannouncing within 30 secs\n        self.time_announce = time.time()\n\n        trackers = config.trackers\n        # Filter trackers based on supported networks\n        if config.disable_udp:\n            trackers = [tracker for tracker in trackers if not tracker.startswith(\"udp://\")]\n        if self.connection_server and self.connection_server.tor_manager and not self.connection_server.tor_manager.enabled:\n            trackers = [tracker for tracker in trackers if \".onion\" not in tracker]\n\n        if trackers and (mode == \"update\" or mode == \"more\"):  # Only announce on one tracker, increment the queried tracker id\n            self.last_tracker_id += 1\n            self.last_tracker_id = self.last_tracker_id % len(trackers)\n            trackers = [trackers[self.last_tracker_id]]  # We only going to use this one\n\n        errors = []\n        slow = []\n        add_types = []\n        if self.connection_server:\n            my_peer_id = self.connection_server.peer_id\n\n            # Type of addresses they can reach me\n            if self.connection_server.port_opened:\n                add_types.append(\"ip4\")\n            if self.connection_server.tor_manager and self.connection_server.tor_manager.start_onions:\n                add_types.append(\"onion\")\n        else:\n            my_peer_id = \"\"\n\n        s = time.time()\n        announced = 0\n        threads = []\n        fileserver_port = config.fileserver_port\n\n        for tracker in trackers:  # Start announce threads\n            tracker_protocol, tracker_address = tracker.split(\"://\")\n            thread = gevent.spawn(\n                self.announceTracker, tracker_protocol, tracker_address, fileserver_port, add_types, my_peer_id, mode\n            )\n            threads.append(thread)\n            thread.tracker_address = tracker_address\n            thread.tracker_protocol = tracker_protocol\n\n        gevent.joinall(threads, timeout=10)  # Wait for announce finish\n\n        for thread in threads:\n            if thread.value:\n                if thread.value > 1:\n                    slow.append(\"%.2fs %s://%s\" % (thread.value, thread.tracker_protocol, thread.tracker_address))\n                announced += 1\n            else:\n                if thread.ready():\n                    errors.append(\"%s://%s\" % (thread.tracker_protocol, thread.tracker_address))\n                else:  # Still running\n                    slow.append(\"10s+ %s://%s\" % (thread.tracker_protocol, thread.tracker_address))\n\n        # Save peers num\n        self.settings[\"peers\"] = len(self.peers)\n\n        if len(errors) < len(threads):  # Less errors than total tracker nums\n            if announced == 1:\n                announced_to = trackers[0]\n            else:\n                announced_to = \"%s trackers\" % announced\n            if config.verbose:\n                self.log.debug(\n                    \"Announced types %s in mode %s to %s in %.3fs, errors: %s, slow: %s\" %\n                    (add_types, mode, announced_to, time.time() - s, errors, slow)\n                )\n        else:\n            if mode != \"update\":\n                self.log.error(\"Announce to %s trackers in %.3fs, failed\" % (announced, time.time() - s))\n\n        if pex:\n            if not [peer for peer in self.peers.values() if peer.connection and peer.connection.connected]:\n                # If no connected peer yet then wait for connections\n                gevent.spawn_later(3, self.announcePex, need_num=10)  # Spawn 3 secs later\n            else:  # Else announce immediately\n                if mode == \"more\":  # Need more peers\n                    self.announcePex(need_num=10)\n                else:\n                    self.announcePex()\n\n    # Keep connections to get the updates\n    def needConnections(self, num=6, check_site_on_reconnect=False):\n        need = min(len(self.peers), num, config.connected_limit)  # Need 5 peer, but max total peers\n\n        connected = len(self.getConnectedPeers())\n\n        connected_before = connected\n\n        self.log.debug(\"Need connections: %s, Current: %s, Total: %s\" % (need, connected, len(self.peers)))\n\n        if connected < need:  # Need more than we have\n            for peer in self.peers.values():\n                if not peer.connection or not peer.connection.connected:  # No peer connection or disconnected\n                    peer.pex()  # Initiate peer exchange\n                    if peer.connection and peer.connection.connected:\n                        connected += 1  # Successfully connected\n                if connected >= need:\n                    break\n            self.log.debug(\n                \"Connected before: %s, after: %s. Check site: %s.\" %\n                (connected_before, connected, check_site_on_reconnect)\n            )\n\n        if check_site_on_reconnect and connected_before == 0 and connected > 0 and self.connection_server.has_internet:\n            gevent.spawn(self.update, check_files=False)\n\n        return connected\n\n    # Return: Probably peers verified to be connectable recently\n    def getConnectablePeers(self, need_num=5, ignore=[], allow_private=True):\n        peers = self.peers.values()\n        found = []\n        for peer in peers:\n            if peer.key.endswith(\":0\"):\n                continue  # Not connectable\n            if not peer.connection:\n                continue  # No connection\n            if peer.key in ignore:\n                continue  # The requester has this peer\n            if time.time() - peer.connection.last_recv_time > 60 * 60 * 2:  # Last message more than 2 hours ago\n                peer.connection = None  # Cleanup: Dead connection\n                continue\n            if not allow_private and helper.isPrivateIp(peer.ip):\n                continue\n            found.append(peer)\n            if len(found) >= need_num:\n                break  # Found requested number of peers\n\n        if len(found) < need_num:  # Return not that good peers\n            found = [\n                peer for peer in peers\n                if not peer.key.endswith(\":0\") and\n                peer.key not in ignore and\n                (allow_private or not helper.isPrivateIp(peer.ip))\n            ][0:need_num - len(found)]\n\n        return found\n\n    # Return: Recently found peers\n    def getRecentPeers(self, need_num):\n        found = list(set(self.peers_recent))\n        self.log.debug(\"Recent peers %s of %s (need: %s)\" % (len(found), len(self.peers_recent), need_num))\n\n        if len(found) >= need_num or len(found) >= len(self.peers):\n            return found[0:need_num]\n\n        # Add random peers\n        need_more = need_num - len(found)\n        found_more = sorted(\n            self.peers.values()[0:need_more * 50],\n            key=lambda peer: peer.time_found + peer.reputation * 60,\n            reverse=True\n        )[0:need_more * 2]\n        random.shuffle(found_more)\n\n        found += found_more\n\n        return found[0:need_num]\n\n    def getConnectedPeers(self):\n        back = []\n        if not self.connection_server:\n            return []\n\n        tor_manager = self.connection_server.tor_manager\n        for connection in self.connection_server.connections:\n            if not connection.connected and time.time() - connection.start_time > 20:  # Still not connected after 20s\n                continue\n            peer = self.peers.get(\"%s:%s\" % (connection.ip, connection.port))\n            if peer:\n                if connection.target_onion and tor_manager.start_onions and tor_manager.getOnion(self.address) != connection.target_onion:\n                    continue\n                if not peer.connection:\n                    peer.connect(connection)\n                back.append(peer)\n        return back\n\n    # Cleanup probably dead peers and close connection if too much\n    def cleanupPeers(self, peers_protected=[]):\n        peers = self.peers.values()\n        if len(peers) > 20:\n            # Cleanup old peers\n            removed = 0\n            if len(peers) > 1000:\n                ttl = 60 * 60 * 1\n            else:\n                ttl = 60 * 60 * 4\n\n            for peer in peers:\n                if peer.connection and peer.connection.connected:\n                    continue\n                if peer.connection and not peer.connection.connected:\n                    peer.connection = None  # Dead connection\n                if time.time() - peer.time_found > ttl:  # Not found on tracker or via pex in last 4 hour\n                    peer.remove(\"Time found expired\")\n                    removed += 1\n                if removed > len(peers) * 0.1:  # Don't remove too much at once\n                    break\n\n            if removed:\n                self.log.debug(\"Cleanup peers result: Removed %s, left: %s\" % (removed, len(self.peers)))\n\n        # Close peers over the limit\n        closed = 0\n        connected_peers = [peer for peer in self.getConnectedPeers() if peer.connection.connected]  # Only fully connected peers\n        need_to_close = len(connected_peers) - config.connected_limit\n\n        if closed < need_to_close:\n            # Try to keep connections with more sites\n            for peer in sorted(connected_peers, key=lambda peer: min(peer.connection.sites, 5)):\n                if not peer.connection:\n                    continue\n                if peer.key in peers_protected:\n                    continue\n                if peer.connection.sites > 5:\n                    break\n                peer.connection.close(\"Cleanup peers\")\n                peer.connection = None\n                closed += 1\n                if closed >= need_to_close:\n                    break\n\n        if need_to_close > 0:\n            self.log.debug(\"Connected: %s, Need to close: %s, Closed: %s\" % (len(connected_peers), need_to_close, closed))\n\n    # Send hashfield to peers\n    def sendMyHashfield(self, limit=5):\n        if not self.content_manager.hashfield:  # No optional files\n            return False\n\n        sent = 0\n        connected_peers = self.getConnectedPeers()\n        for peer in connected_peers:\n            if peer.sendMyHashfield():\n                sent += 1\n                if sent >= limit:\n                    break\n        if sent:\n            my_hashfield_changed = self.content_manager.hashfield.time_changed\n            self.log.debug(\"Sent my hashfield (chaged %.3fs ago) to %s peers\" % (time.time() - my_hashfield_changed, sent))\n        return sent\n\n    # Update hashfield\n    def updateHashfield(self, limit=5):\n        # Return if no optional files\n        if not self.content_manager.hashfield and not self.content_manager.contents.get(\"content.json\", {}).get(\"files_optional\"):\n            return False\n\n        queried = 0\n        connected_peers = self.getConnectedPeers()\n        for peer in connected_peers:\n            if peer.time_hashfield:\n                continue\n            if peer.updateHashfield():\n                queried += 1\n            if queried >= limit:\n                break\n        if queried:\n            self.log.debug(\"Queried hashfield from %s peers\" % queried)\n        return queried\n\n    # Returns if the optional file is need to be downloaded or not\n    def isDownloadable(self, inner_path):\n        return self.settings.get(\"autodownloadoptional\")\n\n    def delete(self):\n        self.settings[\"serving\"] = False\n        self.saveSettings()\n        self.worker_manager.running = False\n        self.worker_manager.stopWorkers()\n        self.storage.deleteFiles()\n        self.updateWebsocket()\n        self.content_manager.contents.db.deleteSite(self)\n        SiteManager.site_manager.delete(self.address)\n\n    # - Events -\n\n    # Add event listeners\n    def addEventListeners(self):\n        self.onFileStart = util.Event()  # If WorkerManager added new task\n        self.onFileDone = util.Event()  # If WorkerManager successfully downloaded a file\n        self.onFileFail = util.Event()  # If WorkerManager failed to download a file\n        self.onComplete = util.Event()  # All file finished\n\n        self.onFileStart.append(lambda inner_path: self.fileStarted())  # No parameters to make Noparallel batching working\n        self.onFileDone.append(lambda inner_path: self.fileDone(inner_path))\n        self.onFileFail.append(lambda inner_path: self.fileFailed(inner_path))\n\n    # Send site status update to websocket clients\n    def updateWebsocket(self, **kwargs):\n        if kwargs:\n            param = {\"event\": kwargs.items()[0]}\n        else:\n            param = None\n        for ws in self.websockets:\n            ws.event(\"siteChanged\", self, param)\n\n    def messageWebsocket(self, message, type=\"info\", progress=None):\n        for ws in self.websockets:\n            if progress is None:\n                ws.cmd(\"notification\", [type, message])\n            else:\n                ws.cmd(\"progress\", [type, message, progress])\n\n    # File download started\n    @util.Noparallel(blocking=False)\n    def fileStarted(self):\n        time.sleep(0.001)  # Wait for other files adds\n        self.updateWebsocket(file_started=True)\n\n    # File downloaded successful\n    def fileDone(self, inner_path):\n        # File downloaded, remove it from bad files\n        if inner_path in self.bad_files:\n            if config.verbose:\n                self.log.debug(\"Bad file solved: %s\" % inner_path)\n            del(self.bad_files[inner_path])\n\n        # Update content.json last downlad time\n        if inner_path == \"content.json\":\n            self.content_updated = time.time()\n\n        self.updateWebsocket(file_done=inner_path)\n\n    # File download failed\n    def fileFailed(self, inner_path):\n        if inner_path == \"content.json\":\n            self.content_updated = False\n            self.log.debug(\"Can't update content.json\")\n        if inner_path in self.bad_files and self.connection_server.has_internet:\n            self.bad_files[inner_path] = self.bad_files.get(inner_path, 0) + 1\n\n        self.updateWebsocket(file_failed=inner_path)\n\n        if self.bad_files.get(inner_path, 0) > 30:\n            self.log.debug(\"Giving up on %s\" % inner_path)\n            del self.bad_files[inner_path]  # Give up after 30 tries\n", "description": "ZeroNet - Decentralized websites using Bitcoin crypto and BitTorrent network", "file_name": "Site.py", "id": "3439bc4789ff9f21afdf8f4315937509", "language": "Python", "project_name": "ZeroNet", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/HelloZeroNet-ZeroNet/HelloZeroNet-ZeroNet-3bdb6a2/src/Site/Site.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:21:08Z", "url": "https://github.com/HelloZeroNet/ZeroNet", "wiki": true}