{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\n\n\n\nimport base64\nimport os\nimport os.path\nimport random\nimport time\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom google.protobuf import text_format\n\nfrom syntaxnet.ops import gen_parser_ops\nfrom syntaxnet import task_spec_pb2\nfrom syntaxnet import sentence_pb2\n\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python.sentence_io import ConllSentenceReader\n\nfrom dragnn.python import evaluation\nfrom dragnn.python import graph_builder\nfrom dragnn.python import lexicon\nfrom dragnn.python import spec_builder\nfrom dragnn.python import trainer_lib\n\nfrom syntaxnet.util import check\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('tf_master', '',\n                    'TensorFlow execution engine to connect to.')\nflags.DEFINE_string('dragnn_spec', '', 'Path to the spec defining the model.')\nflags.DEFINE_string('resource_path', '', 'Path to constructed resources.')\nflags.DEFINE_string('hyperparams',\n                    'adam_beta1:0.9 adam_beta2:0.9 adam_eps:0.00001 '\n                    'decay_steps:128000 dropout_rate:0.8 gradient_clip_norm:1 '\n                    'learning_method:\"adam\" learning_rate:0.0005 seed:1 '\n                    'use_moving_average:true',\n                    'Hyperparameters of the model to train, either in ProtoBuf'\n                    'text format or base64-encoded ProtoBuf text format.')\nflags.DEFINE_string('tensorboard_dir', '',\n                    'Directory for TensorBoard logs output.')\nflags.DEFINE_string('checkpoint_filename', '',\n                    'Filename to save the best checkpoint to.')\n\nflags.DEFINE_string('training_corpus_path', '', 'Path to training data.')\nflags.DEFINE_string('tune_corpus_path', '', 'Path to tuning set data.')\n\nflags.DEFINE_bool('compute_lexicon', False, '')\nflags.DEFINE_bool('projectivize_training_set', True, '')\n\nflags.DEFINE_integer('batch_size', 4, 'Batch size.')\nflags.DEFINE_integer('report_every', 200,\n                     'Report cost and training accuracy every this many steps.')\nflags.DEFINE_integer('job_id', 0, 'The trainer will clear checkpoints if the '\n                     'saved job id is less than the id this flag. If you want '\n                     'training to start over, increment this id.')\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  check.IsTrue(FLAGS.checkpoint_filename)\n  check.IsTrue(FLAGS.tensorboard_dir)\n  check.IsTrue(FLAGS.resource_path)\n\n  if not gfile.IsDirectory(FLAGS.resource_path):\n    gfile.MakeDirs(FLAGS.resource_path)\n\n  training_corpus_path = gfile.Glob(FLAGS.training_corpus_path)[0]\n  tune_corpus_path = gfile.Glob(FLAGS.tune_corpus_path)[0]\n\n   SummaryWriter for TensorBoard\n  tf.logging.info('TensorBoard directory: \"%s\"', FLAGS.tensorboard_dir)\n  tf.logging.info('Deleting prior data if exists...')\n\n  stats_file = '%s.stats' % FLAGS.checkpoint_filename\n  try:\n    stats = gfile.GFile(stats_file, 'r').readlines()[0].split(',')\n    stats = [int(x) for x in stats]\n  except errors.OpError:\n    stats = [-1, 0, 0]\n\n  tf.logging.info('Read ckpt stats: %s', str(stats))\n  do_restore = True\n  if stats[0] < FLAGS.job_id:\n    do_restore = False\n    tf.logging.info('Deleting last job: %d', stats[0])\n    try:\n      gfile.DeleteRecursively(FLAGS.tensorboard_dir)\n      gfile.Remove(FLAGS.checkpoint_filename)\n    except errors.OpError as err:\n      tf.logging.error('Unable to delete prior files: %s', err)\n    stats = [FLAGS.job_id, 0, 0]\n\n  tf.logging.info('Creating the directory again...')\n  gfile.MakeDirs(FLAGS.tensorboard_dir)\n  tf.logging.info('Created! Instatiating SummaryWriter...')\n  summary_writer = trainer_lib.get_summary_writer(FLAGS.tensorboard_dir)\n  tf.logging.info('Creating TensorFlow checkpoint dir...')\n  gfile.MakeDirs(os.path.dirname(FLAGS.checkpoint_filename))\n\n   Constructs lexical resources for SyntaxNet in the given resource path, from\n   the training data.\n  if FLAGS.compute_lexicon:\n    logging.info('Computing lexicon...')\n    lexicon.build_lexicon(\n        FLAGS.resource_path, training_corpus_path, morph_to_pos=True)\n\n  tf.logging.info('Loading MasterSpec...')\n  master_spec = spec_pb2.MasterSpec()\n  with gfile.FastGFile(FLAGS.dragnn_spec, 'r') as fin:\n    text_format.Parse(fin.read(), master_spec)\n  spec_builder.complete_master_spec(master_spec, None, FLAGS.resource_path)\n  logging.info('Constructed master spec: %s', str(master_spec))\n  hyperparam_config = spec_pb2.GridPoint()\n\n   Build the TensorFlow graph.\n  tf.logging.info('Building Graph...')\n  hyperparam_config = spec_pb2.GridPoint()\n  try:\n    text_format.Parse(FLAGS.hyperparams, hyperparam_config)\n  except text_format.ParseError:\n    text_format.Parse(base64.b64decode(FLAGS.hyperparams), hyperparam_config)\n  g = tf.Graph()\n  with g.as_default():\n    builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)\n    component_targets = [\n        spec_pb2.TrainTarget(\n            name=component.name,\n            max_index=idx + 1,\n            unroll_using_oracle=[False] * idx + [True])\n        for idx, component in enumerate(master_spec.component)\n        if 'shift-only' not in component.transition_system.registered_name\n    ]\n    trainers = [\n        builder.add_training_from_config(target) for target in component_targets\n    ]\n    annotator = builder.add_annotation()\n    builder.add_saver()\n\n   Read in serialized protos from training data.\n  training_set = ConllSentenceReader(\n      training_corpus_path,\n      projectivize=FLAGS.projectivize_training_set,\n      morph_to_pos=True).corpus()\n  tune_set = ConllSentenceReader(\n      tune_corpus_path, projectivize=False, morph_to_pos=True).corpus()\n\n   Ready to train!\n  logging.info('Training on %d sentences.', len(training_set))\n  logging.info('Tuning on %d sentences.', len(tune_set))\n\n  pretrain_steps = [10000, 0]\n  tagger_steps = 100000\n  train_steps = [tagger_steps, 8 * tagger_steps]\n\n  with tf.Session(FLAGS.tf_master, graph=g) as sess:\n     Make sure to re-initialize all underlying state.\n    sess.run(tf.global_variables_initializer())\n\n    if do_restore:\n      tf.logging.info('Restoring from checkpoint...')\n      builder.saver.restore(sess, FLAGS.checkpoint_filename)\n\n      prev_tagger_steps = stats[1]\n      prev_parser_steps = stats[2]\n      tf.logging.info('adjusting schedule from steps: %d, %d',\n                      prev_tagger_steps, prev_parser_steps)\n      pretrain_steps[0] = max(pretrain_steps[0] - prev_tagger_steps, 0)\n      tf.logging.info('new pretrain steps: %d', pretrain_steps[0])\n\n    trainer_lib.run_training(\n        sess, trainers, annotator, evaluation.parser_summaries, pretrain_steps,\n        train_steps, training_set, tune_set, tune_set, FLAGS.batch_size,\n        summary_writer, FLAGS.report_every, builder.saver,\n        FLAGS.checkpoint_filename, stats)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   a program train tensorflow neural net parser conll file        copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       summarywriter tensorboard    constructs lexical resources syntaxnet given resource path     training data     build tensorflow graph     read serialized protos training data     ready train     make sure initialize underlying state  ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A program to train a tensorflow neural net parser from a conll file.\"\"\"\n\n\n\n\nimport base64\nimport os\nimport os.path\nimport random\nimport time\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom google.protobuf import text_format\n\nfrom syntaxnet.ops import gen_parser_ops\nfrom syntaxnet import task_spec_pb2\nfrom syntaxnet import sentence_pb2\n\nfrom dragnn.protos import spec_pb2\nfrom dragnn.python.sentence_io import ConllSentenceReader\n\nfrom dragnn.python import evaluation\nfrom dragnn.python import graph_builder\nfrom dragnn.python import lexicon\nfrom dragnn.python import spec_builder\nfrom dragnn.python import trainer_lib\n\nfrom syntaxnet.util import check\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('tf_master', '',\n                    'TensorFlow execution engine to connect to.')\nflags.DEFINE_string('dragnn_spec', '', 'Path to the spec defining the model.')\nflags.DEFINE_string('resource_path', '', 'Path to constructed resources.')\nflags.DEFINE_string('hyperparams',\n                    'adam_beta1:0.9 adam_beta2:0.9 adam_eps:0.00001 '\n                    'decay_steps:128000 dropout_rate:0.8 gradient_clip_norm:1 '\n                    'learning_method:\"adam\" learning_rate:0.0005 seed:1 '\n                    'use_moving_average:true',\n                    'Hyperparameters of the model to train, either in ProtoBuf'\n                    'text format or base64-encoded ProtoBuf text format.')\nflags.DEFINE_string('tensorboard_dir', '',\n                    'Directory for TensorBoard logs output.')\nflags.DEFINE_string('checkpoint_filename', '',\n                    'Filename to save the best checkpoint to.')\n\nflags.DEFINE_string('training_corpus_path', '', 'Path to training data.')\nflags.DEFINE_string('tune_corpus_path', '', 'Path to tuning set data.')\n\nflags.DEFINE_bool('compute_lexicon', False, '')\nflags.DEFINE_bool('projectivize_training_set', True, '')\n\nflags.DEFINE_integer('batch_size', 4, 'Batch size.')\nflags.DEFINE_integer('report_every', 200,\n                     'Report cost and training accuracy every this many steps.')\nflags.DEFINE_integer('job_id', 0, 'The trainer will clear checkpoints if the '\n                     'saved job id is less than the id this flag. If you want '\n                     'training to start over, increment this id.')\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  check.IsTrue(FLAGS.checkpoint_filename)\n  check.IsTrue(FLAGS.tensorboard_dir)\n  check.IsTrue(FLAGS.resource_path)\n\n  if not gfile.IsDirectory(FLAGS.resource_path):\n    gfile.MakeDirs(FLAGS.resource_path)\n\n  training_corpus_path = gfile.Glob(FLAGS.training_corpus_path)[0]\n  tune_corpus_path = gfile.Glob(FLAGS.tune_corpus_path)[0]\n\n  # SummaryWriter for TensorBoard\n  tf.logging.info('TensorBoard directory: \"%s\"', FLAGS.tensorboard_dir)\n  tf.logging.info('Deleting prior data if exists...')\n\n  stats_file = '%s.stats' % FLAGS.checkpoint_filename\n  try:\n    stats = gfile.GFile(stats_file, 'r').readlines()[0].split(',')\n    stats = [int(x) for x in stats]\n  except errors.OpError:\n    stats = [-1, 0, 0]\n\n  tf.logging.info('Read ckpt stats: %s', str(stats))\n  do_restore = True\n  if stats[0] < FLAGS.job_id:\n    do_restore = False\n    tf.logging.info('Deleting last job: %d', stats[0])\n    try:\n      gfile.DeleteRecursively(FLAGS.tensorboard_dir)\n      gfile.Remove(FLAGS.checkpoint_filename)\n    except errors.OpError as err:\n      tf.logging.error('Unable to delete prior files: %s', err)\n    stats = [FLAGS.job_id, 0, 0]\n\n  tf.logging.info('Creating the directory again...')\n  gfile.MakeDirs(FLAGS.tensorboard_dir)\n  tf.logging.info('Created! Instatiating SummaryWriter...')\n  summary_writer = trainer_lib.get_summary_writer(FLAGS.tensorboard_dir)\n  tf.logging.info('Creating TensorFlow checkpoint dir...')\n  gfile.MakeDirs(os.path.dirname(FLAGS.checkpoint_filename))\n\n  # Constructs lexical resources for SyntaxNet in the given resource path, from\n  # the training data.\n  if FLAGS.compute_lexicon:\n    logging.info('Computing lexicon...')\n    lexicon.build_lexicon(\n        FLAGS.resource_path, training_corpus_path, morph_to_pos=True)\n\n  tf.logging.info('Loading MasterSpec...')\n  master_spec = spec_pb2.MasterSpec()\n  with gfile.FastGFile(FLAGS.dragnn_spec, 'r') as fin:\n    text_format.Parse(fin.read(), master_spec)\n  spec_builder.complete_master_spec(master_spec, None, FLAGS.resource_path)\n  logging.info('Constructed master spec: %s', str(master_spec))\n  hyperparam_config = spec_pb2.GridPoint()\n\n  # Build the TensorFlow graph.\n  tf.logging.info('Building Graph...')\n  hyperparam_config = spec_pb2.GridPoint()\n  try:\n    text_format.Parse(FLAGS.hyperparams, hyperparam_config)\n  except text_format.ParseError:\n    text_format.Parse(base64.b64decode(FLAGS.hyperparams), hyperparam_config)\n  g = tf.Graph()\n  with g.as_default():\n    builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)\n    component_targets = [\n        spec_pb2.TrainTarget(\n            name=component.name,\n            max_index=idx + 1,\n            unroll_using_oracle=[False] * idx + [True])\n        for idx, component in enumerate(master_spec.component)\n        if 'shift-only' not in component.transition_system.registered_name\n    ]\n    trainers = [\n        builder.add_training_from_config(target) for target in component_targets\n    ]\n    annotator = builder.add_annotation()\n    builder.add_saver()\n\n  # Read in serialized protos from training data.\n  training_set = ConllSentenceReader(\n      training_corpus_path,\n      projectivize=FLAGS.projectivize_training_set,\n      morph_to_pos=True).corpus()\n  tune_set = ConllSentenceReader(\n      tune_corpus_path, projectivize=False, morph_to_pos=True).corpus()\n\n  # Ready to train!\n  logging.info('Training on %d sentences.', len(training_set))\n  logging.info('Tuning on %d sentences.', len(tune_set))\n\n  pretrain_steps = [10000, 0]\n  tagger_steps = 100000\n  train_steps = [tagger_steps, 8 * tagger_steps]\n\n  with tf.Session(FLAGS.tf_master, graph=g) as sess:\n    # Make sure to re-initialize all underlying state.\n    sess.run(tf.global_variables_initializer())\n\n    if do_restore:\n      tf.logging.info('Restoring from checkpoint...')\n      builder.saver.restore(sess, FLAGS.checkpoint_filename)\n\n      prev_tagger_steps = stats[1]\n      prev_parser_steps = stats[2]\n      tf.logging.info('adjusting schedule from steps: %d, %d',\n                      prev_tagger_steps, prev_parser_steps)\n      pretrain_steps[0] = max(pretrain_steps[0] - prev_tagger_steps, 0)\n      tf.logging.info('new pretrain steps: %d', pretrain_steps[0])\n\n    trainer_lib.run_training(\n        sess, trainers, annotator, evaluation.parser_summaries, pretrain_steps,\n        train_steps, training_set, tune_set, tune_set, FLAGS.batch_size,\n        summary_writer, FLAGS.report_every, builder.saver,\n        FLAGS.checkpoint_filename, stats)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "trainer.py", "id": "84072300a803f7d2cb31ff7fe2b4ea2b", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/syntaxnet/dragnn/tools/trainer.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}