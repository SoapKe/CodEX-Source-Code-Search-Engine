{"author": "localstack", "code": "import re\nimport os\nimport json\nimport logging\nimport socket\nimport tempfile\nfrom localstack.utils.common import (short_uid, parallelize, is_port_open,\n    rm_rf, unzip, download, clean_cache, mktime, load_file, mkdir, run, md5)\nfrom localstack.utils.aws.aws_models import (ElasticSearch, S3Notification,\n    EventSource, DynamoDB, DynamoDBStream, FirehoseStream, S3Bucket, SqsQueue,\n    KinesisShard, KinesisStream, LambdaFunction)\nfrom localstack.utils.aws import aws_stack\nfrom localstack.utils.common import to_str\nfrom localstack.constants import REGION_LOCAL, DEFAULT_REGION\nfrom six import iteritems\n\n\nAWS_CACHE_TIMEOUT = 5  \nAWS_LAMBDA_CODE_CACHE_TIMEOUT = 5 * 60  \nMOCK_OBJ = False\nTMP_DOWNLOAD_FILE_PATTERN = os.path.join(tempfile.gettempdir(), 'tmpfile.*')\nTMP_DOWNLOAD_CACHE_MAX_AGE = 30 * 60\nlast_cache_cleanup_time = {'time': 0}\n\n\nKINESIS_RECENT_EVENTS_TIME_DIFF_SECS = 60\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef run_cached(cmd, cache_duration_secs=None):\n    if cache_duration_secs is None:\n        cache_duration_secs = AWS_CACHE_TIMEOUT\n    env_vars = os.environ.copy()\n    env_vars.update({\n        'AWS_ACCESS_KEY_ID': os.environ.get('AWS_ACCESS_KEY_ID') or 'foobar',\n        'AWS_SECRET_ACCESS_KEY': os.environ.get('AWS_SECRET_ACCESS_KEY') or 'foobar',\n        'AWS_DEFAULT_REGION': os.environ.get('AWS_DEFAULT_REGION') or DEFAULT_REGION,\n        'PYTHONWARNINGS': 'ignore:Unverified HTTPS request'\n    })\n    return run(cmd, cache_duration_secs=cache_duration_secs, env_vars=env_vars)\n\n\ndef run_aws_cmd(service, cmd_params, env=None, cache_duration_secs=None):\n    cmd = '%s %s' % (aws_cmd(service, env), cmd_params)\n    return run_cached(cmd, cache_duration_secs=cache_duration_secs)\n\n\ndef cmd_s3api(cmd_params, env):\n    return run_aws_cmd('s3api', cmd_params, env)\n\n\ndef cmd_es(cmd_params, env):\n    return run_aws_cmd('es', cmd_params, env)\n\n\ndef cmd_kinesis(cmd_params, env, cache_duration_secs=None):\n    return run_aws_cmd('kinesis', cmd_params, env,\n        cache_duration_secs=cache_duration_secs)\n\n\ndef cmd_dynamodb(cmd_params, env):\n    return run_aws_cmd('dynamodb', cmd_params, env)\n\n\ndef cmd_firehose(cmd_params, env):\n    return run_aws_cmd('firehose', cmd_params, env)\n\n\ndef cmd_sqs(cmd_params, env):\n    return run_aws_cmd('sqs', cmd_params, env)\n\n\ndef cmd_lambda(cmd_params, env, cache_duration_secs=None):\n    return run_aws_cmd('lambda', cmd_params, env,\n        cache_duration_secs=cache_duration_secs)\n\n\ndef aws_cmd(service, env):\n    \n\n    cmd = '{ test `which aws` || . .venv/bin/activate; }; aws'\n    endpoint_url = None\n    env = aws_stack.get_environment(env)\n    if env.region == REGION_LOCAL:\n        endpoint_url = aws_stack.get_local_service_url(service)\n    if endpoint_url:\n        if endpoint_url.startswith('https://'):\n            cmd += ' --no-verify-ssl'\n        cmd = '%s --endpoint-url=\"%s\"' % (cmd, endpoint_url)\n        if not is_port_open(endpoint_url):\n            raise socket.error()\n    cmd = '%s %s' % (cmd, service)\n    return cmd\n\n\ndef get_kinesis_streams(filter='.*', pool={}, env=None):\n    if MOCK_OBJ:\n        return []\n    result = []\n    try:\n        out = cmd_kinesis('list-streams', env)\n        out = json.loads(out)\n        for name in out['StreamNames']:\n            if re.match(filter, name):\n                details = cmd_kinesis('describe-stream --stream-name %s' % name, env=env)\n                details = json.loads(details)\n                arn = details['StreamDescription']['StreamARN']\n                stream = KinesisStream(arn)\n                pool[arn] = stream\n                stream.shards = get_kinesis_shards(stream_details=details, env=env)\n                result.append(stream)\n    except socket.error:\n        pass\n    return result\n\n\ndef get_kinesis_shards(stream_name=None, stream_details=None, env=None):\n    if not stream_details:\n        out = cmd_kinesis('describe-stream --stream-name %s' % stream_name, env)\n        stream_details = json.loads(out)\n    shards = stream_details['StreamDescription']['Shards']\n    result = []\n    for s in shards:\n        shard = KinesisShard(s['ShardId'])\n        shard.start_key = s['HashKeyRange']['StartingHashKey']\n        shard.end_key = s['HashKeyRange']['EndingHashKey']\n        result.append(shard)\n    return result\n\n\ndef get_sqs_queues(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_sqs('list-queues', env)\n        if not out.strip():\n            return result\n        queues = json.loads(out)['QueueUrls']\n        for q in queues:\n            name = q.split('/')[-1]\n            account = q.split('/')[-2]\n            arn = 'arn:aws:sqs:%s:%s:%s' % (DEFAULT_REGION, account, name)\n            if re.match(filter, name):\n                queue = SqsQueue(arn)\n                result.append(queue)\n    except socket.error:\n        pass\n    return result\n\n\n\ndef resolve_string_or_variable(string, code_map):\n    if re.match(r'^[\"\\'].*[\"\\']$', string):\n        return string.replace('\"', '').replace(\"'\", '')\n    LOG.warning('Variable resolution not implemented')\n    return None\n\n\n\ndef extract_endpoints(code_map, pool={}):\n    result = []\n    identifiers = []\n    for key, code in iteritems(code_map):\n        \n        pattern = r'[\\'\"](.*\\.es\\.amazonaws\\.com)[\\'\"]'\n        for es in re.findall(pattern, code):\n            if es not in identifiers:\n                identifiers.append(es)\n                es = EventSource.get(es, pool=pool, type=ElasticSearch)\n                if es:\n                    result.append(es)\n        \n        pattern = r'\\.put_record_batch\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for firehose in re.findall(pattern, code):\n            if firehose not in identifiers:\n                identifiers.append(firehose)\n                firehose = EventSource.get(firehose, pool=pool, type=FirehoseStream)\n                if firehose:\n                    result.append(firehose)\n        \n        \n        pattern = r'\\.(insert|get)_document\\s*\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for (op, dynamo) in re.findall(pattern, code):\n            dynamo = resolve_string_or_variable(dynamo, code_map)\n            if dynamo not in identifiers:\n                identifiers.append(dynamo)\n                dynamo = EventSource.get(dynamo, pool=pool, type=DynamoDB)\n                if dynamo:\n                    result.append(dynamo)\n        \n        pattern = r'\\.upload_file\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for s3 in re.findall(pattern, code):\n            s3 = resolve_string_or_variable(s3, code_map)\n            if s3 not in identifiers:\n                identifiers.append(s3)\n                s3 = EventSource.get(s3, pool=pool, type=S3Bucket)\n                if s3:\n                    result.append(s3)\n    return result\n\n\ndef get_lambda_functions(filter='.*', details=False, pool={}, env=None):\n    if MOCK_OBJ:\n        return []\n\n    result = []\n\n    def handle(func):\n        func_name = func['FunctionName']\n        if re.match(filter, func_name):\n            arn = func['FunctionArn']\n            f = LambdaFunction(arn)\n            pool[arn] = f\n            result.append(f)\n            if details:\n                sources = get_lambda_event_sources(f.name(), env=env)\n                for src in sources:\n                    arn = src['EventSourceArn']\n                    f.event_sources.append(EventSource.get(arn, pool=pool))\n                try:\n                    code_map = get_lambda_code(func_name, env=env)\n                    f.targets = extract_endpoints(code_map, pool)\n                except Exception:\n                    LOG.warning(\"Unable to get code for lambda '%s'\" % func_name)\n\n    try:\n        out = cmd_lambda('list-functions', env)\n        out = json.loads(out)\n        parallelize(handle, out['Functions'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_lambda_event_sources(func_name=None, env=None):\n    if MOCK_OBJ:\n        return {}\n\n    cmd = 'list-event-source-mappings'\n    if func_name:\n        cmd = '%s --function-name %s' % (cmd, func_name)\n    out = cmd_lambda(cmd, env=env)\n    out = json.loads(out)\n    result = out['EventSourceMappings']\n    return result\n\n\ndef get_lambda_code(func_name, retries=1, cache_time=None, env=None):\n    if MOCK_OBJ:\n        return ''\n    env = aws_stack.get_environment(env)\n    if cache_time is None and env.region != REGION_LOCAL:\n        cache_time = AWS_LAMBDA_CODE_CACHE_TIMEOUT\n    out = cmd_lambda('get-function --function-name %s' % func_name, env, cache_time)\n    out = json.loads(out)\n    loc = out['Code']['Location']\n    hash = md5(loc)\n    folder = TMP_DOWNLOAD_FILE_PATTERN.replace('*', hash)\n    filename = 'archive.zip'\n    archive = '%s/%s' % (folder, filename)\n    try:\n        mkdir(folder)\n        if not os.path.isfile(archive):\n            download(loc, archive, verify_ssl=False)\n        if len(os.listdir(folder)) <= 1:\n            zip_path = os.path.join(folder, filename)\n            unzip(zip_path, folder)\n    except Exception as e:\n        print('WARN: %s' % e)\n        rm_rf(archive)\n        if retries > 0:\n            return get_lambda_code(func_name, retries=retries - 1, cache_time=1, env=env)\n        else:\n            print('WARNING: Unable to retrieve lambda code: %s' % e)\n\n    \n    result = {}\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            prefix = root.split(folder)[-1]\n            key = '%s/%s' % (prefix, file)\n            if re.match(r'.+\\.py$', key) or re.match(r'.+\\.js$', key):\n                codefile = '%s/%s' % (root, file)\n                result[key] = load_file(codefile)\n\n    \n    clean_cache(file_pattern=TMP_DOWNLOAD_FILE_PATTERN,\n        last_clean_time=last_cache_cleanup_time,\n        max_age=TMP_DOWNLOAD_CACHE_MAX_AGE)\n    \n    rm_rf(folder)\n\n    return result\n\n\ndef get_elasticsearch_domains(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_es('list-domain-names', env)\n        out = json.loads(out)\n\n        def handle(domain):\n            domain = domain['DomainName']\n            if re.match(filter, domain):\n                details = cmd_es('describe-elasticsearch-domain --domain-name %s' % domain, env)\n                details = json.loads(details)['DomainStatus']\n                arn = details['ARN']\n                es = ElasticSearch(arn)\n                es.endpoint = details.get('Endpoint', 'n/a')\n                result.append(es)\n                pool[arn] = es\n        parallelize(handle, out['DomainNames'])\n    except socket.error:\n        pass\n\n    return result\n\n\ndef get_dynamo_dbs(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_dynamodb('list-tables', env)\n        out = json.loads(out)\n\n        def handle(table):\n            if re.match(filter, table):\n                details = cmd_dynamodb('describe-table --table-name %s' % table, env)\n                details = json.loads(details)['Table']\n                arn = details['TableArn']\n                db = DynamoDB(arn)\n                db.count = details['ItemCount']\n                db.bytes = details['TableSizeBytes']\n                db.created_at = details['CreationDateTime']\n                result.append(db)\n                pool[arn] = db\n        parallelize(handle, out['TableNames'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_s3_buckets(filter='.*', pool={}, details=False, env=None):\n    result = []\n\n    def handle(bucket):\n        bucket_name = bucket['Name']\n        if re.match(filter, bucket_name):\n            arn = 'arn:aws:s3:::%s' % bucket_name\n            bucket = S3Bucket(arn)\n            result.append(bucket)\n            pool[arn] = bucket\n            if details:\n                try:\n                    out = cmd_s3api('get-bucket-notification-configuration --bucket %s' % bucket_name, env=env)\n                    if out:\n                        out = json.loads(out)\n                        if 'CloudFunctionConfiguration' in out:\n                            func = out['CloudFunctionConfiguration']['CloudFunction']\n                            func = EventSource.get(func, pool=pool)\n                            n = S3Notification(func.id)\n                            n.target = func\n                            bucket.notifications.append(n)\n                except Exception as e:\n                    print('WARNING: Unable to get details for bucket: %s' % e)\n\n    try:\n        out = cmd_s3api('list-buckets', env)\n        out = json.loads(out)\n        parallelize(handle, out['Buckets'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_firehose_streams(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_firehose('list-delivery-streams', env)\n        out = json.loads(out)\n        for stream_name in out['DeliveryStreamNames']:\n            if re.match(filter, stream_name):\n                details = cmd_firehose(\n                    'describe-delivery-stream --delivery-stream-name %s' % stream_name, env)\n                details = json.loads(details)['DeliveryStreamDescription']\n                arn = details['DeliveryStreamARN']\n                s = FirehoseStream(arn)\n                for dest in details['Destinations']:\n                    dest_s3 = dest['S3DestinationDescription']['BucketARN']\n                    bucket = EventSource.get(dest_s3, pool=pool)\n                    s.destinations.append(bucket)\n                result.append(s)\n    except socket.error:\n        pass\n    return result\n\n\ndef read_kinesis_iterator(shard_iterator, max_results=10, env=None):\n    data = cmd_kinesis('get-records --shard-iterator %s --limit %s' %\n        (shard_iterator, max_results), env, cache_duration_secs=0)\n    data = json.loads(to_str(data))\n    result = data\n    return result\n\n\ndef get_kinesis_events(stream_name, shard_id, max_results=10, env=None):\n    env = aws_stack.get_environment(env)\n    records = aws_stack.kinesis_get_latest_records(stream_name, shard_id, count=max_results, env=env)\n    for r in records:\n        r['ApproximateArrivalTimestamp'] = mktime(r['ApproximateArrivalTimestamp'])\n    result = {\n        'events': records\n    }\n    return result\n\n\ndef get_graph(name_filter='.*', env=None):\n    result = {\n        'nodes': [],\n        'edges': []\n    }\n\n    pool = {}\n\n    if True:\n        result = {\n            'nodes': [],\n            'edges': []\n        }\n        node_ids = {}\n        \n        # (ES,DynamoDB,S3) -> (Kinesis,Lambda)\n        domains = get_elasticsearch_domains(name_filter, pool=pool, env=env)\n        dbs = get_dynamo_dbs(name_filter, pool=pool, env=env)\n        buckets = get_s3_buckets(name_filter, details=True, pool=pool, env=env)\n        streams = get_kinesis_streams(name_filter, pool=pool, env=env)\n        firehoses = get_firehose_streams(name_filter, pool=pool, env=env)\n        lambdas = get_lambda_functions(name_filter, details=True, pool=pool, env=env)\n        queues = get_sqs_queues(name_filter, pool=pool, env=env)\n\n        for es in domains:\n            uid = short_uid()\n            node_ids[es.id] = uid\n            result['nodes'].append({'id': uid, 'arn': es.id, 'name': es.name(), 'type': 'es'})\n        for b in buckets:\n            uid = short_uid()\n            node_ids[b.id] = uid\n            result['nodes'].append({'id': uid, 'arn': b.id, 'name': b.name(), 'type': 's3'})\n        for db in dbs:\n            uid = short_uid()\n            node_ids[db.id] = uid\n            result['nodes'].append({'id': uid, 'arn': db.id, 'name': db.name(), 'type': 'dynamodb'})\n        for s in streams:\n            uid = short_uid()\n            node_ids[s.id] = uid\n            result['nodes'].append({'id': uid, 'arn': s.id, 'name': s.name(), 'type': 'kinesis'})\n            for shard in s.shards:\n                uid1 = short_uid()\n                name = re.sub(r'shardId-0*', '', shard.id) or '0'\n                result['nodes'].append({'id': uid1, 'arn': shard.id, 'name': name,\n                    'type': 'kinesis_shard', 'streamName': s.name(), 'parent': uid})\n        for f in firehoses:\n            uid = short_uid()\n            node_ids[f.id] = uid\n            result['nodes'].append({'id': uid, 'arn': f.id, 'name': f.name(), 'type': 'firehose'})\n            for d in f.destinations:\n                result['edges'].append({'source': uid, 'target': node_ids[d.id]})\n        for q in queues:\n            uid = short_uid()\n            node_ids[q.id] = uid\n            result['nodes'].append({'id': uid, 'arn': q.id, 'name': q.name(), 'type': 'sqs'})\n        for l in lambdas:\n            uid = short_uid()\n            node_ids[l.id] = uid\n            result['nodes'].append({'id': uid, 'arn': l.id, 'name': l.name(), 'type': 'lambda'})\n            for s in l.event_sources:\n                lookup_id = s.id\n                if isinstance(s, DynamoDBStream):\n                    lookup_id = s.table.id\n                result['edges'].append({'source': node_ids.get(lookup_id), 'target': uid})\n            for t in l.targets:\n                lookup_id = t.id\n                result['edges'].append({'source': uid, 'target': node_ids.get(lookup_id)})\n        for b in buckets:\n            for n in b.notifications:\n                src_uid = node_ids[b.id]\n                tgt_uid = node_ids[n.target.id]\n                result['edges'].append({'source': src_uid, 'target': tgt_uid})\n\n    return result\n", "comments": "  5 seconds    5 minutes    time delta recent kinesis events    logger    todo  use boto3 instead running aws cli commands     todo move util    todo move util    elasticsearch references    elasticsearch references    dynamodb references    todo fix pattern generic    s3 references    traverse subdirectories get script sources    cleanup cache    todo  delete cache time    make sure load components right order     (es dynamodb s3)    (kinesis lambda) ", "content": "import re\nimport os\nimport json\nimport logging\nimport socket\nimport tempfile\nfrom localstack.utils.common import (short_uid, parallelize, is_port_open,\n    rm_rf, unzip, download, clean_cache, mktime, load_file, mkdir, run, md5)\nfrom localstack.utils.aws.aws_models import (ElasticSearch, S3Notification,\n    EventSource, DynamoDB, DynamoDBStream, FirehoseStream, S3Bucket, SqsQueue,\n    KinesisShard, KinesisStream, LambdaFunction)\nfrom localstack.utils.aws import aws_stack\nfrom localstack.utils.common import to_str\nfrom localstack.constants import REGION_LOCAL, DEFAULT_REGION\nfrom six import iteritems\n\n\nAWS_CACHE_TIMEOUT = 5  # 5 seconds\nAWS_LAMBDA_CODE_CACHE_TIMEOUT = 5 * 60  # 5 minutes\nMOCK_OBJ = False\nTMP_DOWNLOAD_FILE_PATTERN = os.path.join(tempfile.gettempdir(), 'tmpfile.*')\nTMP_DOWNLOAD_CACHE_MAX_AGE = 30 * 60\nlast_cache_cleanup_time = {'time': 0}\n\n# time delta for recent Kinesis events\nKINESIS_RECENT_EVENTS_TIME_DIFF_SECS = 60\n\n# logger\nLOG = logging.getLogger(__name__)\n\n\ndef run_cached(cmd, cache_duration_secs=None):\n    if cache_duration_secs is None:\n        cache_duration_secs = AWS_CACHE_TIMEOUT\n    env_vars = os.environ.copy()\n    env_vars.update({\n        'AWS_ACCESS_KEY_ID': os.environ.get('AWS_ACCESS_KEY_ID') or 'foobar',\n        'AWS_SECRET_ACCESS_KEY': os.environ.get('AWS_SECRET_ACCESS_KEY') or 'foobar',\n        'AWS_DEFAULT_REGION': os.environ.get('AWS_DEFAULT_REGION') or DEFAULT_REGION,\n        'PYTHONWARNINGS': 'ignore:Unverified HTTPS request'\n    })\n    return run(cmd, cache_duration_secs=cache_duration_secs, env_vars=env_vars)\n\n\ndef run_aws_cmd(service, cmd_params, env=None, cache_duration_secs=None):\n    cmd = '%s %s' % (aws_cmd(service, env), cmd_params)\n    return run_cached(cmd, cache_duration_secs=cache_duration_secs)\n\n\ndef cmd_s3api(cmd_params, env):\n    return run_aws_cmd('s3api', cmd_params, env)\n\n\ndef cmd_es(cmd_params, env):\n    return run_aws_cmd('es', cmd_params, env)\n\n\ndef cmd_kinesis(cmd_params, env, cache_duration_secs=None):\n    return run_aws_cmd('kinesis', cmd_params, env,\n        cache_duration_secs=cache_duration_secs)\n\n\ndef cmd_dynamodb(cmd_params, env):\n    return run_aws_cmd('dynamodb', cmd_params, env)\n\n\ndef cmd_firehose(cmd_params, env):\n    return run_aws_cmd('firehose', cmd_params, env)\n\n\ndef cmd_sqs(cmd_params, env):\n    return run_aws_cmd('sqs', cmd_params, env)\n\n\ndef cmd_lambda(cmd_params, env, cache_duration_secs=None):\n    return run_aws_cmd('lambda', cmd_params, env,\n        cache_duration_secs=cache_duration_secs)\n\n\ndef aws_cmd(service, env):\n    # TODO: use boto3 instead of running aws-cli commands here!\n\n    cmd = '{ test `which aws` || . .venv/bin/activate; }; aws'\n    endpoint_url = None\n    env = aws_stack.get_environment(env)\n    if env.region == REGION_LOCAL:\n        endpoint_url = aws_stack.get_local_service_url(service)\n    if endpoint_url:\n        if endpoint_url.startswith('https://'):\n            cmd += ' --no-verify-ssl'\n        cmd = '%s --endpoint-url=\"%s\"' % (cmd, endpoint_url)\n        if not is_port_open(endpoint_url):\n            raise socket.error()\n    cmd = '%s %s' % (cmd, service)\n    return cmd\n\n\ndef get_kinesis_streams(filter='.*', pool={}, env=None):\n    if MOCK_OBJ:\n        return []\n    result = []\n    try:\n        out = cmd_kinesis('list-streams', env)\n        out = json.loads(out)\n        for name in out['StreamNames']:\n            if re.match(filter, name):\n                details = cmd_kinesis('describe-stream --stream-name %s' % name, env=env)\n                details = json.loads(details)\n                arn = details['StreamDescription']['StreamARN']\n                stream = KinesisStream(arn)\n                pool[arn] = stream\n                stream.shards = get_kinesis_shards(stream_details=details, env=env)\n                result.append(stream)\n    except socket.error:\n        pass\n    return result\n\n\ndef get_kinesis_shards(stream_name=None, stream_details=None, env=None):\n    if not stream_details:\n        out = cmd_kinesis('describe-stream --stream-name %s' % stream_name, env)\n        stream_details = json.loads(out)\n    shards = stream_details['StreamDescription']['Shards']\n    result = []\n    for s in shards:\n        shard = KinesisShard(s['ShardId'])\n        shard.start_key = s['HashKeyRange']['StartingHashKey']\n        shard.end_key = s['HashKeyRange']['EndingHashKey']\n        result.append(shard)\n    return result\n\n\ndef get_sqs_queues(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_sqs('list-queues', env)\n        if not out.strip():\n            return result\n        queues = json.loads(out)['QueueUrls']\n        for q in queues:\n            name = q.split('/')[-1]\n            account = q.split('/')[-2]\n            arn = 'arn:aws:sqs:%s:%s:%s' % (DEFAULT_REGION, account, name)\n            if re.match(filter, name):\n                queue = SqsQueue(arn)\n                result.append(queue)\n    except socket.error:\n        pass\n    return result\n\n\n# TODO move to util\ndef resolve_string_or_variable(string, code_map):\n    if re.match(r'^[\"\\'].*[\"\\']$', string):\n        return string.replace('\"', '').replace(\"'\", '')\n    LOG.warning('Variable resolution not implemented')\n    return None\n\n\n# TODO move to util\ndef extract_endpoints(code_map, pool={}):\n    result = []\n    identifiers = []\n    for key, code in iteritems(code_map):\n        # Elasticsearch references\n        pattern = r'[\\'\"](.*\\.es\\.amazonaws\\.com)[\\'\"]'\n        for es in re.findall(pattern, code):\n            if es not in identifiers:\n                identifiers.append(es)\n                es = EventSource.get(es, pool=pool, type=ElasticSearch)\n                if es:\n                    result.append(es)\n        # Elasticsearch references\n        pattern = r'\\.put_record_batch\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for firehose in re.findall(pattern, code):\n            if firehose not in identifiers:\n                identifiers.append(firehose)\n                firehose = EventSource.get(firehose, pool=pool, type=FirehoseStream)\n                if firehose:\n                    result.append(firehose)\n        # DynamoDB references\n        # TODO fix pattern to be generic\n        pattern = r'\\.(insert|get)_document\\s*\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for (op, dynamo) in re.findall(pattern, code):\n            dynamo = resolve_string_or_variable(dynamo, code_map)\n            if dynamo not in identifiers:\n                identifiers.append(dynamo)\n                dynamo = EventSource.get(dynamo, pool=pool, type=DynamoDB)\n                if dynamo:\n                    result.append(dynamo)\n        # S3 references\n        pattern = r'\\.upload_file\\([^,]+,\\s*([^,\\s]+)\\s*,'\n        for s3 in re.findall(pattern, code):\n            s3 = resolve_string_or_variable(s3, code_map)\n            if s3 not in identifiers:\n                identifiers.append(s3)\n                s3 = EventSource.get(s3, pool=pool, type=S3Bucket)\n                if s3:\n                    result.append(s3)\n    return result\n\n\ndef get_lambda_functions(filter='.*', details=False, pool={}, env=None):\n    if MOCK_OBJ:\n        return []\n\n    result = []\n\n    def handle(func):\n        func_name = func['FunctionName']\n        if re.match(filter, func_name):\n            arn = func['FunctionArn']\n            f = LambdaFunction(arn)\n            pool[arn] = f\n            result.append(f)\n            if details:\n                sources = get_lambda_event_sources(f.name(), env=env)\n                for src in sources:\n                    arn = src['EventSourceArn']\n                    f.event_sources.append(EventSource.get(arn, pool=pool))\n                try:\n                    code_map = get_lambda_code(func_name, env=env)\n                    f.targets = extract_endpoints(code_map, pool)\n                except Exception:\n                    LOG.warning(\"Unable to get code for lambda '%s'\" % func_name)\n\n    try:\n        out = cmd_lambda('list-functions', env)\n        out = json.loads(out)\n        parallelize(handle, out['Functions'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_lambda_event_sources(func_name=None, env=None):\n    if MOCK_OBJ:\n        return {}\n\n    cmd = 'list-event-source-mappings'\n    if func_name:\n        cmd = '%s --function-name %s' % (cmd, func_name)\n    out = cmd_lambda(cmd, env=env)\n    out = json.loads(out)\n    result = out['EventSourceMappings']\n    return result\n\n\ndef get_lambda_code(func_name, retries=1, cache_time=None, env=None):\n    if MOCK_OBJ:\n        return ''\n    env = aws_stack.get_environment(env)\n    if cache_time is None and env.region != REGION_LOCAL:\n        cache_time = AWS_LAMBDA_CODE_CACHE_TIMEOUT\n    out = cmd_lambda('get-function --function-name %s' % func_name, env, cache_time)\n    out = json.loads(out)\n    loc = out['Code']['Location']\n    hash = md5(loc)\n    folder = TMP_DOWNLOAD_FILE_PATTERN.replace('*', hash)\n    filename = 'archive.zip'\n    archive = '%s/%s' % (folder, filename)\n    try:\n        mkdir(folder)\n        if not os.path.isfile(archive):\n            download(loc, archive, verify_ssl=False)\n        if len(os.listdir(folder)) <= 1:\n            zip_path = os.path.join(folder, filename)\n            unzip(zip_path, folder)\n    except Exception as e:\n        print('WARN: %s' % e)\n        rm_rf(archive)\n        if retries > 0:\n            return get_lambda_code(func_name, retries=retries - 1, cache_time=1, env=env)\n        else:\n            print('WARNING: Unable to retrieve lambda code: %s' % e)\n\n    # traverse subdirectories and get script sources\n    result = {}\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            prefix = root.split(folder)[-1]\n            key = '%s/%s' % (prefix, file)\n            if re.match(r'.+\\.py$', key) or re.match(r'.+\\.js$', key):\n                codefile = '%s/%s' % (root, file)\n                result[key] = load_file(codefile)\n\n    # cleanup cache\n    clean_cache(file_pattern=TMP_DOWNLOAD_FILE_PATTERN,\n        last_clean_time=last_cache_cleanup_time,\n        max_age=TMP_DOWNLOAD_CACHE_MAX_AGE)\n    # TODO: delete only if cache_time is over\n    rm_rf(folder)\n\n    return result\n\n\ndef get_elasticsearch_domains(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_es('list-domain-names', env)\n        out = json.loads(out)\n\n        def handle(domain):\n            domain = domain['DomainName']\n            if re.match(filter, domain):\n                details = cmd_es('describe-elasticsearch-domain --domain-name %s' % domain, env)\n                details = json.loads(details)['DomainStatus']\n                arn = details['ARN']\n                es = ElasticSearch(arn)\n                es.endpoint = details.get('Endpoint', 'n/a')\n                result.append(es)\n                pool[arn] = es\n        parallelize(handle, out['DomainNames'])\n    except socket.error:\n        pass\n\n    return result\n\n\ndef get_dynamo_dbs(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_dynamodb('list-tables', env)\n        out = json.loads(out)\n\n        def handle(table):\n            if re.match(filter, table):\n                details = cmd_dynamodb('describe-table --table-name %s' % table, env)\n                details = json.loads(details)['Table']\n                arn = details['TableArn']\n                db = DynamoDB(arn)\n                db.count = details['ItemCount']\n                db.bytes = details['TableSizeBytes']\n                db.created_at = details['CreationDateTime']\n                result.append(db)\n                pool[arn] = db\n        parallelize(handle, out['TableNames'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_s3_buckets(filter='.*', pool={}, details=False, env=None):\n    result = []\n\n    def handle(bucket):\n        bucket_name = bucket['Name']\n        if re.match(filter, bucket_name):\n            arn = 'arn:aws:s3:::%s' % bucket_name\n            bucket = S3Bucket(arn)\n            result.append(bucket)\n            pool[arn] = bucket\n            if details:\n                try:\n                    out = cmd_s3api('get-bucket-notification-configuration --bucket %s' % bucket_name, env=env)\n                    if out:\n                        out = json.loads(out)\n                        if 'CloudFunctionConfiguration' in out:\n                            func = out['CloudFunctionConfiguration']['CloudFunction']\n                            func = EventSource.get(func, pool=pool)\n                            n = S3Notification(func.id)\n                            n.target = func\n                            bucket.notifications.append(n)\n                except Exception as e:\n                    print('WARNING: Unable to get details for bucket: %s' % e)\n\n    try:\n        out = cmd_s3api('list-buckets', env)\n        out = json.loads(out)\n        parallelize(handle, out['Buckets'])\n    except socket.error:\n        pass\n    return result\n\n\ndef get_firehose_streams(filter='.*', pool={}, env=None):\n    result = []\n    try:\n        out = cmd_firehose('list-delivery-streams', env)\n        out = json.loads(out)\n        for stream_name in out['DeliveryStreamNames']:\n            if re.match(filter, stream_name):\n                details = cmd_firehose(\n                    'describe-delivery-stream --delivery-stream-name %s' % stream_name, env)\n                details = json.loads(details)['DeliveryStreamDescription']\n                arn = details['DeliveryStreamARN']\n                s = FirehoseStream(arn)\n                for dest in details['Destinations']:\n                    dest_s3 = dest['S3DestinationDescription']['BucketARN']\n                    bucket = EventSource.get(dest_s3, pool=pool)\n                    s.destinations.append(bucket)\n                result.append(s)\n    except socket.error:\n        pass\n    return result\n\n\ndef read_kinesis_iterator(shard_iterator, max_results=10, env=None):\n    data = cmd_kinesis('get-records --shard-iterator %s --limit %s' %\n        (shard_iterator, max_results), env, cache_duration_secs=0)\n    data = json.loads(to_str(data))\n    result = data\n    return result\n\n\ndef get_kinesis_events(stream_name, shard_id, max_results=10, env=None):\n    env = aws_stack.get_environment(env)\n    records = aws_stack.kinesis_get_latest_records(stream_name, shard_id, count=max_results, env=env)\n    for r in records:\n        r['ApproximateArrivalTimestamp'] = mktime(r['ApproximateArrivalTimestamp'])\n    result = {\n        'events': records\n    }\n    return result\n\n\ndef get_graph(name_filter='.*', env=None):\n    result = {\n        'nodes': [],\n        'edges': []\n    }\n\n    pool = {}\n\n    if True:\n        result = {\n            'nodes': [],\n            'edges': []\n        }\n        node_ids = {}\n        # Make sure we load components in the right order:\n        # (ES,DynamoDB,S3) -> (Kinesis,Lambda)\n        domains = get_elasticsearch_domains(name_filter, pool=pool, env=env)\n        dbs = get_dynamo_dbs(name_filter, pool=pool, env=env)\n        buckets = get_s3_buckets(name_filter, details=True, pool=pool, env=env)\n        streams = get_kinesis_streams(name_filter, pool=pool, env=env)\n        firehoses = get_firehose_streams(name_filter, pool=pool, env=env)\n        lambdas = get_lambda_functions(name_filter, details=True, pool=pool, env=env)\n        queues = get_sqs_queues(name_filter, pool=pool, env=env)\n\n        for es in domains:\n            uid = short_uid()\n            node_ids[es.id] = uid\n            result['nodes'].append({'id': uid, 'arn': es.id, 'name': es.name(), 'type': 'es'})\n        for b in buckets:\n            uid = short_uid()\n            node_ids[b.id] = uid\n            result['nodes'].append({'id': uid, 'arn': b.id, 'name': b.name(), 'type': 's3'})\n        for db in dbs:\n            uid = short_uid()\n            node_ids[db.id] = uid\n            result['nodes'].append({'id': uid, 'arn': db.id, 'name': db.name(), 'type': 'dynamodb'})\n        for s in streams:\n            uid = short_uid()\n            node_ids[s.id] = uid\n            result['nodes'].append({'id': uid, 'arn': s.id, 'name': s.name(), 'type': 'kinesis'})\n            for shard in s.shards:\n                uid1 = short_uid()\n                name = re.sub(r'shardId-0*', '', shard.id) or '0'\n                result['nodes'].append({'id': uid1, 'arn': shard.id, 'name': name,\n                    'type': 'kinesis_shard', 'streamName': s.name(), 'parent': uid})\n        for f in firehoses:\n            uid = short_uid()\n            node_ids[f.id] = uid\n            result['nodes'].append({'id': uid, 'arn': f.id, 'name': f.name(), 'type': 'firehose'})\n            for d in f.destinations:\n                result['edges'].append({'source': uid, 'target': node_ids[d.id]})\n        for q in queues:\n            uid = short_uid()\n            node_ids[q.id] = uid\n            result['nodes'].append({'id': uid, 'arn': q.id, 'name': q.name(), 'type': 'sqs'})\n        for l in lambdas:\n            uid = short_uid()\n            node_ids[l.id] = uid\n            result['nodes'].append({'id': uid, 'arn': l.id, 'name': l.name(), 'type': 'lambda'})\n            for s in l.event_sources:\n                lookup_id = s.id\n                if isinstance(s, DynamoDBStream):\n                    lookup_id = s.table.id\n                result['edges'].append({'source': node_ids.get(lookup_id), 'target': uid})\n            for t in l.targets:\n                lookup_id = t.id\n                result['edges'].append({'source': uid, 'target': node_ids.get(lookup_id)})\n        for b in buckets:\n            for n in b.notifications:\n                src_uid = node_ids[b.id]\n                tgt_uid = node_ids[n.target.id]\n                result['edges'].append({'source': src_uid, 'target': tgt_uid})\n\n    return result\n", "description": "\ud83d\udcbb  A fully functional local AWS cloud stack. Develop and test your cloud apps offline!", "file_name": "infra.py", "id": "d9fc0d18f7ac5405ddf65c70a2605666", "language": "Python", "project_name": "localstack", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/localstack-localstack/localstack-localstack-fcc848e/localstack/dashboard/infra.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:04:08Z", "url": "https://github.com/localstack/localstack", "wiki": true}