{"author": "tflearn", "code": "# -*- coding: utf-8 -*-\n\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tflearn\nimport tflearn.activations as activations\n\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom tflearn.activations import relu\nfrom tflearn.data_utils import shuffle, to_categorical\nfrom tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\nfrom tflearn.layers.core import dropout, flatten, fully_connected, input_data\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.normalization import batch_normalization\nfrom tflearn.utils import repeat\n\n\ndef block35(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\ndef block17(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\n\ndef block8(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\nX, Y = oxflower17.load_data(one_hot=True, resize_pics=(299, 299))\n\nnum_classes = 17\ndropout_keep_prob = 0.8\n\nnetwork = input_data(shape=[None, 299, 299, 3])\nconv1a_3_3 = relu(batch_normalization(conv_2d(network, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\nconv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\nconv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\nmaxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\nconv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\nconv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\nmaxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n\ntower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n\ntower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n\ntower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\ntower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\ntower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n\ntower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\ntower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n\ntower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n\nnet = repeat(tower_5b_out, 10, block35, scale=0.17)\n\ntower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\ntower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\ntower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\ntower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\nnet = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\nnet = repeat(net, 20, block17, scale=0.1)\n\ntower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\ntower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n\ntower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n\ntower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\ntower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\ntower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n\ntower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\nnet = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n\nnet = repeat(net, 9, block8, scale=0.2)\nnet = block8(net, activation=None)\n\nnet = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\nnet = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\nnet = flatten(net)\nnet = dropout(net, dropout_keep_prob)\nloss = fully_connected(net, num_classes,activation='softmax')\n\n\nnetwork = tflearn.regression(loss, optimizer='RMSprop',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.0001)\nmodel = tflearn.DNN(network, checkpoint_path='inception_resnet_v2',\n                    max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\nmodel.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n          show_metric=True, batch_size=32, snapshot_step=2000,\n          snapshot_epoch=False, run_id='inception_resnet_v2_17flowers')\n", "comments": "    inception resnet v2   applying  inception resnet v2  oxford 17 category flower dataset classification task   references      inception v4  inception resnet impact residual connections     learning   christian szegedy  sergey ioffe  vincent vanhoucke  alex alemi   links      http   arxiv org abs 1602 07261             coding  utf 8        data loading preprocessing ", "content": "# -*- coding: utf-8 -*-\n\n\"\"\" inception_resnet_v2.\n\nApplying 'inception_resnet_v2' to Oxford's 17 Category Flower Dataset classification task.\n\nReferences:\n    Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi.\n\nLinks:\n    http://arxiv.org/abs/1602.07261\n\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tflearn\nimport tflearn.activations as activations\n# Data loading and preprocessing\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom tflearn.activations import relu\nfrom tflearn.data_utils import shuffle, to_categorical\nfrom tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\nfrom tflearn.layers.core import dropout, flatten, fully_connected, input_data\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.normalization import batch_normalization\nfrom tflearn.utils import repeat\n\n\ndef block35(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\ndef block17(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\n\ndef block8(net, scale=1.0, activation=\"relu\"):\n    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n    net += scale * tower_out\n    if activation:\n        if isinstance(activation, str):\n            net = activations.get(activation)(net)\n        elif hasattr(activation, '__call__'):\n            net = activation(net)\n        else:\n            raise ValueError(\"Invalid Activation.\")\n    return net\n\nX, Y = oxflower17.load_data(one_hot=True, resize_pics=(299, 299))\n\nnum_classes = 17\ndropout_keep_prob = 0.8\n\nnetwork = input_data(shape=[None, 299, 299, 3])\nconv1a_3_3 = relu(batch_normalization(conv_2d(network, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\nconv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\nconv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\nmaxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\nconv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\nconv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\nmaxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n\ntower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n\ntower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n\ntower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\ntower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\ntower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n\ntower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\ntower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n\ntower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n\nnet = repeat(tower_5b_out, 10, block35, scale=0.17)\n\ntower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\ntower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\ntower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\ntower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\nnet = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\nnet = repeat(net, 20, block17, scale=0.1)\n\ntower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\ntower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n\ntower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\ntower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n\ntower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\ntower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\ntower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n\ntower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\nnet = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n\nnet = repeat(net, 9, block8, scale=0.2)\nnet = block8(net, activation=None)\n\nnet = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\nnet = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\nnet = flatten(net)\nnet = dropout(net, dropout_keep_prob)\nloss = fully_connected(net, num_classes,activation='softmax')\n\n\nnetwork = tflearn.regression(loss, optimizer='RMSprop',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.0001)\nmodel = tflearn.DNN(network, checkpoint_path='inception_resnet_v2',\n                    max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\nmodel.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n          show_metric=True, batch_size=32, snapshot_step=2000,\n          snapshot_epoch=False, run_id='inception_resnet_v2_17flowers')\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "inception_resnet_v2.py", "id": "eccf0d439f4bc67501367d730e0c073e", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/images/inception_resnet_v2.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}