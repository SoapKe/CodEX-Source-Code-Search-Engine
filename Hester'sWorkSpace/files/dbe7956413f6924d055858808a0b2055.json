{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Eval pre-trained 1 billion word language model.\n\"\"\"\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nimport data_utils\n\nFLAGS = tf.flags.FLAGS\n General flags.\ntf.flags.DEFINE_string('mode', 'eval',\n                       'One of [sample, eval, dump_emb, dump_lstm_emb]. '\n                       '\"sample\" mode samples future word predictions, using '\n                       'FLAGS.prefix as prefix (prefix could be left empty). '\n                       '\"eval\" mode calculates perplexity of the '\n                       'FLAGS.input_data. '\n                       '\"dump_emb\" mode dumps word and softmax embeddings to '\n                       'FLAGS.save_dir. embeddings are dumped in the same '\n                       'order as words in vocabulary. All words in vocabulary '\n                       'are dumped.'\n                       'dump_lstm_emb dumps lstm embeddings of FLAGS.sentence '\n                       'to FLAGS.save_dir.')\ntf.flags.DEFINE_string('pbtxt', '',\n                       'GraphDef proto text file used to construct model '\n                       'structure.')\ntf.flags.DEFINE_string('ckpt', '',\n                       'Checkpoint directory used to fill model values.')\ntf.flags.DEFINE_string('vocab_file', '', 'Vocabulary file.')\ntf.flags.DEFINE_string('save_dir', '',\n                       'Used for \"dump_emb\" mode to save word embeddings.')\n sample mode flags.\ntf.flags.DEFINE_string('prefix', '',\n                       'Used for \"sample\" mode to predict next words.')\ntf.flags.DEFINE_integer('max_sample_words', 100,\n                        'Sampling stops either when </S> is met or this number '\n                        'of steps has passed.')\ntf.flags.DEFINE_integer('num_samples', 3,\n                        'Number of samples to generate for the prefix.')\n dump_lstm_emb mode flags.\ntf.flags.DEFINE_string('sentence', '',\n                       'Used as input for \"dump_lstm_emb\" mode.')\n eval mode flags.\ntf.flags.DEFINE_string('input_data', '',\n                       'Input data files for eval model.')\ntf.flags.DEFINE_integer('max_eval_steps', 1000000,\n                        'Maximum mumber of steps to run \"eval\" mode.')\n\n\n For saving demo resources, use batch size 1 and step 1.\nBATCH_SIZE = 1\nNUM_TIMESTEPS = 1\nMAX_WORD_LEN = 50\n\n\ndef _LoadModel(gd_file, ckpt_file):\n  \"\"\"Load the model from GraphDef and Checkpoint.\n\n  Args:\n    gd_file: GraphDef proto text file.\n    ckpt_file: TensorFlow Checkpoint file.\n\n  Returns:\n    TensorFlow session and tensors dict.\n  \"\"\"\n  with tf.Graph().as_default():\n    sys.stderr.write('Recovering graph.\\n')\n    with tf.gfile.FastGFile(gd_file, 'r') as f:\n      s = f.read().decode()\n      gd = tf.GraphDef()\n      text_format.Merge(s, gd)\n\n    tf.logging.info('Recovering Graph %s', gd_file)\n    t = {}\n    [t['states_init'], t['lstm/lstm_0/control_dependency'],\n     t['lstm/lstm_1/control_dependency'], t['softmax_out'], t['class_ids_out'],\n     t['class_weights_out'], t['log_perplexity_out'], t['inputs_in'],\n     t['targets_in'], t['target_weights_in'], t['char_inputs_in'],\n     t['all_embs'], t['softmax_weights'], t['global_step']\n    ] = tf.import_graph_def(gd, {}, ['states_init',\n                                     'lstm/lstm_0/control_dependency:0',\n                                     'lstm/lstm_1/control_dependency:0',\n                                     'softmax_out:0',\n                                     'class_ids_out:0',\n                                     'class_weights_out:0',\n                                     'log_perplexity_out:0',\n                                     'inputs_in:0',\n                                     'targets_in:0',\n                                     'target_weights_in:0',\n                                     'char_inputs_in:0',\n                                     'all_embs_out:0',\n                                     'Reshape_3:0',\n                                     'global_step:0'], name='')\n\n    sys.stderr.write('Recovering checkpoint %s\\n' % ckpt_file)\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    sess.run('save/restore_all', {'save/Const:0': ckpt_file})\n    sess.run(t['states_init'])\n\n  return sess, t\n\n\ndef _EvalModel(dataset):\n  \"\"\"Evaluate model perplexity using provided dataset.\n\n  Args:\n    dataset: LM1BDataset object.\n  \"\"\"\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  current_step = t['global_step'].eval(session=sess)\n  sys.stderr.write('Loaded step %d.\\n' % current_step)\n\n  data_gen = dataset.get_batch(BATCH_SIZE, NUM_TIMESTEPS, forever=False)\n  sum_num = 0.0\n  sum_den = 0.0\n  perplexity = 0.0\n  for i, (inputs, char_inputs, _, targets, weights) in enumerate(data_gen):\n    input_dict = {t['inputs_in']: inputs,\n                  t['targets_in']: targets,\n                  t['target_weights_in']: weights}\n    if 'char_inputs_in' in t:\n      input_dict[t['char_inputs_in']] = char_inputs\n    log_perp = sess.run(t['log_perplexity_out'], feed_dict=input_dict)\n\n    if np.isnan(log_perp):\n      sys.stderr.error('log_perplexity is Nan.\\n')\n    else:\n      sum_num += log_perp * weights.mean()\n      sum_den += weights.mean()\n    if sum_den > 0:\n      perplexity = np.exp(sum_num / sum_den)\n\n    sys.stderr.write('Eval Step: %d, Average Perplexity: %f.\\n' %\n                     (i, perplexity))\n\n    if i > FLAGS.max_eval_steps:\n      break\n\n\ndef _SampleSoftmax(softmax):\n  return min(np.sum(np.cumsum(softmax) < np.random.rand()), len(softmax) - 1)\n\n\ndef _SampleModel(prefix_words, vocab):\n  \"\"\"Predict next words using the given prefix words.\n\n  Args:\n    prefix_words: Prefix words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  \"\"\"\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if prefix_words.find('<S>') != 0:\n    prefix_words = '<S> ' + prefix_words\n\n  prefix = [vocab.word_to_id(w) for w in prefix_words.split()]\n  prefix_char_ids = [vocab.word_to_char_ids(w) for w in prefix_words.split()]\n  for _ in xrange(FLAGS.num_samples):\n    inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n    char_ids_inputs = np.zeros(\n        [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n    samples = prefix[:]\n    char_ids_samples = prefix_char_ids[:]\n    sent = ''\n    while True:\n      inputs[0, 0] = samples[0]\n      char_ids_inputs[0, 0, :] = char_ids_samples[0]\n      samples = samples[1:]\n      char_ids_samples = char_ids_samples[1:]\n\n      softmax = sess.run(t['softmax_out'],\n                         feed_dict={t['char_inputs_in']: char_ids_inputs,\n                                    t['inputs_in']: inputs,\n                                    t['targets_in']: targets,\n                                    t['target_weights_in']: weights})\n\n      sample = _SampleSoftmax(softmax[0])\n      sample_char_ids = vocab.word_to_char_ids(vocab.id_to_word(sample))\n\n      if not samples:\n        samples = [sample]\n        char_ids_samples = [sample_char_ids]\n      sent += vocab.id_to_word(samples[0]) + ' '\n      sys.stderr.write('%s\\n' % sent)\n\n      if (vocab.id_to_word(samples[0]) == '</S>' or\n          len(sent) > FLAGS.max_sample_words):\n        break\n\n\ndef _DumpEmb(vocab):\n  \"\"\"Dump the softmax weights and word embeddings to files.\n\n  Args:\n    vocab: Vocabulary. Contains vocabulary size and converts word to ids.\n  \"\"\"\n  assert FLAGS.save_dir, 'Must specify FLAGS.save_dir for dump_emb.'\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  softmax_weights = sess.run(t['softmax_weights'])\n  fname = FLAGS.save_dir + '/embeddings_softmax.npy'\n  with tf.gfile.Open(fname, mode='w') as f:\n    np.save(f, softmax_weights)\n  sys.stderr.write('Finished softmax weights\\n')\n\n  all_embs = np.zeros([vocab.size, 1024])\n  for i in xrange(vocab.size):\n    input_dict = {t['inputs_in']: inputs,\n                  t['targets_in']: targets,\n                  t['target_weights_in']: weights}\n    if 'char_inputs_in' in t:\n      input_dict[t['char_inputs_in']] = (\n          vocab.word_char_ids[i].reshape([-1, 1, MAX_WORD_LEN]))\n    embs = sess.run(t['all_embs'], input_dict)\n    all_embs[i, :] = embs\n    sys.stderr.write('Finished word embedding %d/%d\\n' % (i, vocab.size))\n\n  fname = FLAGS.save_dir + '/embeddings_char_cnn.npy'\n  with tf.gfile.Open(fname, mode='w') as f:\n    np.save(f, all_embs)\n  sys.stderr.write('Embedding file saved\\n')\n\n\ndef _DumpSentenceEmbedding(sentence, vocab):\n  \"\"\"Predict next words using the given prefix words.\n\n  Args:\n    sentence: Sentence words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  \"\"\"\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if sentence.find('<S>') != 0:\n    sentence = '<S> ' + sentence\n\n  word_ids = [vocab.word_to_id(w) for w in sentence.split()]\n  char_ids = [vocab.word_to_char_ids(w) for w in sentence.split()]\n\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  char_ids_inputs = np.zeros(\n      [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n  for i in xrange(len(word_ids)):\n    inputs[0, 0] = word_ids[i]\n    char_ids_inputs[0, 0, :] = char_ids[i]\n\n     Add 'lstm/lstm_0/control_dependency' if you want to dump previous layer\n     LSTM.\n    lstm_emb = sess.run(t['lstm/lstm_1/control_dependency'],\n                        feed_dict={t['char_inputs_in']: char_ids_inputs,\n                                   t['inputs_in']: inputs,\n                                   t['targets_in']: targets,\n                                   t['target_weights_in']: weights})\n\n    fname = os.path.join(FLAGS.save_dir, 'lstm_emb_step_%d.npy' % i)\n    with tf.gfile.Open(fname, mode='w') as f:\n      np.save(f, lstm_emb)\n    sys.stderr.write('LSTM embedding step %d file saved\\n' % i)\n\n\ndef main(unused_argv):\n  vocab = data_utils.CharsVocabulary(FLAGS.vocab_file, MAX_WORD_LEN)\n\n  if FLAGS.mode == 'eval':\n    dataset = data_utils.LM1BDataset(FLAGS.input_data, vocab)\n    _EvalModel(dataset)\n  elif FLAGS.mode == 'sample':\n    _SampleModel(FLAGS.prefix, vocab)\n  elif FLAGS.mode == 'dump_emb':\n    _DumpEmb(vocab)\n  elif FLAGS.mode == 'dump_lstm_emb':\n    _DumpSentenceEmbedding(FLAGS.sentence, vocab)\n  else:\n    raise Exception('Mode not supported.')\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   eval pre trained 1 billion word language model      import os import sys  import numpy np six moves import xrange import tensorflow tf  google protobuf import text format import data utils  flags   tf flags flags   general flags  tf flags define string( mode    eval                           one  sample  eval  dump emb  dump lstm emb                              sample  mode samples future word predictions  using                           flags prefix prefix (prefix could left empty)                             eval  mode calculates perplexity                           flags input data                             dump emb  mode dumps word softmax embeddings                           flags save dir  embeddings dumped                           order words vocabulary  all words vocabulary                           dumped                           dump lstm emb dumps lstm embeddings flags sentence                           flags save dir  ) tf flags define string( pbtxt                               graphdef proto text file used construct model                           structure  ) tf flags define string( ckpt                               checkpoint directory used fill model values  ) tf flags define string( vocab file        vocabulary file  ) tf flags define string( save dir                               used  dump emb  mode save word embeddings  )   sample mode flags  tf flags define string( prefix                               used  sample  mode predict next words  ) tf flags define integer( max sample words   100                           sampling stops either   s  met number                            steps passed  ) tf flags define integer( num samples   3                           number samples generate prefix  )   dump lstm emb mode flags  tf flags define string( sentence                               used input  dump lstm emb  mode  )   eval mode flags  tf flags define string( input data                               input data files eval model  ) tf flags define integer( max eval steps   1000000                           maximum mumber steps run  eval  mode  )     for saving demo resources  use batch size 1 step 1  batch size   1 num timesteps   1 max word len   50   def  loadmodel(gd file  ckpt file)       load model graphdef checkpoint     args      gd file  graphdef proto text file      ckpt file  tensorflow checkpoint file     returns      tensorflow session tensors dict          tf graph() default()      sys stderr write( recovering graph  n )     tf gfile fastgfile(gd file   r ) f          f read() decode()       gd   tf graphdef()       text format merge(s  gd)      tf logging info( recovering graph    gd file)                states init     lstm lstm 0 control dependency          lstm lstm 1 control dependency     softmax     class ids          class weights     log perplexity     inputs          targets     target weights     char inputs          embs     softmax weights     global step           tf import graph def(gd        states init                                         lstm lstm 0 control dependency 0                                         lstm lstm 1 control dependency 0                                         softmax 0                                         class ids 0                                         class weights 0                                         log perplexity 0                                         inputs 0                                         targets 0                                         target weights 0                                         char inputs 0                                         embs 0                                         reshape 3 0                                         global step 0    name   )      sys stderr write( recovering checkpoint  n    ckpt file)     sess   tf session(config tf configproto(allow soft placement true))     sess run( save restore     save const 0   ckpt file )     sess run(t  states init  )    return sess    def  evalmodel(dataset)       evaluate model perplexity using provided dataset     args      dataset  lm1bdataset object          sess     loadmodel(flags pbtxt  flags ckpt)    current step    global step   eval(session sess)   sys stderr write( loaded step   n    current step)    data gen   dataset get batch(batch size  num timesteps  forever false)   sum num   0 0   sum den   0 0   perplexity   0 0    (inputs  char inputs     targets  weights) enumerate(data gen)      input dict     inputs    inputs                     targets    targets                     target weights    weights       char inputs         input dict  char inputs      char inputs     log perp   sess run(t  log perplexity    feed dict input dict)      np isnan(log perp)        sys stderr error( log perplexity nan  n )     else        sum num    log perp   weights mean()       sum den    weights mean()     sum den   0        perplexity   np exp(sum num   sum den)      sys stderr write( eval step    average perplexity   f  n                         (i  perplexity))        flags max eval steps        break   def  samplesoftmax(softmax)    return min(np sum(np cumsum(softmax)   np random rand())  len(softmax)   1)   def  samplemodel(prefix words  vocab)       predict next words using given prefix words     args      prefix words  prefix words      vocab  vocabulary  contains max word chard id length converts         words ids          targets   np zeros( batch size  num timesteps   np int32)   weights   np ones( batch size  num timesteps   np float32)    sess     loadmodel(flags pbtxt  flags ckpt)    prefix words find(  s  )    0      prefix words     s      prefix words    prefix    vocab word id(w) w prefix words split()    prefix char ids    vocab word char ids(w) w prefix words split()      xrange(flags num samples)      inputs   np zeros( batch size  num timesteps   np int32)     char ids inputs   np zeros(          batch size  num timesteps  vocab max word length   np int32)     samples   prefix        char ids samples   prefix char ids        sent          true        inputs 0  0    samples 0        char ids inputs 0  0       char ids samples 0        samples   samples 1         char ids samples   char ids samples 1          softmax   sess run(t  softmax                             feed dict   char inputs    char ids inputs                                       inputs    inputs                                       targets    targets                                       target weights    weights )        sample    samplesoftmax(softmax 0 )       sample char ids   vocab word char ids(vocab id word(sample))        samples          samples    sample          char ids samples    sample char ids        sent    vocab id word(samples 0 )             sys stderr write(  n    sent)        (vocab id word(samples 0 )       s             len(sent)   flags max sample words)          break   def  dumpemb(vocab)       dump softmax weights word embeddings files     args      vocab  vocabulary  contains vocabulary size converts word ids          assert flags save dir   must specify flags save dir dump emb     inputs   np zeros( batch size  num timesteps   np int32)   targets   np zeros( batch size  num timesteps   np int32)   weights   np ones( batch size  num timesteps   np float32)    sess     loadmodel(flags pbtxt  flags ckpt)    softmax weights   sess run(t  softmax weights  )   fname   flags save dir     embeddings softmax npy    tf gfile open(fname  mode  w ) f      np save(f  softmax weights)   sys stderr write( finished softmax weights n )    embs   np zeros( vocab size  1024 )   xrange(vocab size)      input dict     inputs    inputs                     targets    targets                     target weights    weights       char inputs         input dict  char inputs      (           vocab word char ids  reshape(  1  1  max word len ))     embs   sess run(t  embs    input dict)     embs       embs     sys stderr write( finished word embedding   n    (i  vocab size))    fname   flags save dir     embeddings char cnn npy    tf gfile open(fname  mode  w ) f      np save(f  embs)   sys stderr write( embedding file saved n )   def  dumpsentenceembedding(sentence  vocab)       predict next words using given prefix words     args      sentence  sentence words      vocab  vocabulary  contains max word chard id length converts         words ids           copyright 2016 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       general flags     sample mode flags     dump lstm emb mode flags     eval mode flags     for saving demo resources  use batch size 1 step 1     add  lstm lstm 0 control dependency  want dump previous layer    lstm  ", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Eval pre-trained 1 billion word language model.\n\"\"\"\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nimport data_utils\n\nFLAGS = tf.flags.FLAGS\n# General flags.\ntf.flags.DEFINE_string('mode', 'eval',\n                       'One of [sample, eval, dump_emb, dump_lstm_emb]. '\n                       '\"sample\" mode samples future word predictions, using '\n                       'FLAGS.prefix as prefix (prefix could be left empty). '\n                       '\"eval\" mode calculates perplexity of the '\n                       'FLAGS.input_data. '\n                       '\"dump_emb\" mode dumps word and softmax embeddings to '\n                       'FLAGS.save_dir. embeddings are dumped in the same '\n                       'order as words in vocabulary. All words in vocabulary '\n                       'are dumped.'\n                       'dump_lstm_emb dumps lstm embeddings of FLAGS.sentence '\n                       'to FLAGS.save_dir.')\ntf.flags.DEFINE_string('pbtxt', '',\n                       'GraphDef proto text file used to construct model '\n                       'structure.')\ntf.flags.DEFINE_string('ckpt', '',\n                       'Checkpoint directory used to fill model values.')\ntf.flags.DEFINE_string('vocab_file', '', 'Vocabulary file.')\ntf.flags.DEFINE_string('save_dir', '',\n                       'Used for \"dump_emb\" mode to save word embeddings.')\n# sample mode flags.\ntf.flags.DEFINE_string('prefix', '',\n                       'Used for \"sample\" mode to predict next words.')\ntf.flags.DEFINE_integer('max_sample_words', 100,\n                        'Sampling stops either when </S> is met or this number '\n                        'of steps has passed.')\ntf.flags.DEFINE_integer('num_samples', 3,\n                        'Number of samples to generate for the prefix.')\n# dump_lstm_emb mode flags.\ntf.flags.DEFINE_string('sentence', '',\n                       'Used as input for \"dump_lstm_emb\" mode.')\n# eval mode flags.\ntf.flags.DEFINE_string('input_data', '',\n                       'Input data files for eval model.')\ntf.flags.DEFINE_integer('max_eval_steps', 1000000,\n                        'Maximum mumber of steps to run \"eval\" mode.')\n\n\n# For saving demo resources, use batch size 1 and step 1.\nBATCH_SIZE = 1\nNUM_TIMESTEPS = 1\nMAX_WORD_LEN = 50\n\n\ndef _LoadModel(gd_file, ckpt_file):\n  \"\"\"Load the model from GraphDef and Checkpoint.\n\n  Args:\n    gd_file: GraphDef proto text file.\n    ckpt_file: TensorFlow Checkpoint file.\n\n  Returns:\n    TensorFlow session and tensors dict.\n  \"\"\"\n  with tf.Graph().as_default():\n    sys.stderr.write('Recovering graph.\\n')\n    with tf.gfile.FastGFile(gd_file, 'r') as f:\n      s = f.read().decode()\n      gd = tf.GraphDef()\n      text_format.Merge(s, gd)\n\n    tf.logging.info('Recovering Graph %s', gd_file)\n    t = {}\n    [t['states_init'], t['lstm/lstm_0/control_dependency'],\n     t['lstm/lstm_1/control_dependency'], t['softmax_out'], t['class_ids_out'],\n     t['class_weights_out'], t['log_perplexity_out'], t['inputs_in'],\n     t['targets_in'], t['target_weights_in'], t['char_inputs_in'],\n     t['all_embs'], t['softmax_weights'], t['global_step']\n    ] = tf.import_graph_def(gd, {}, ['states_init',\n                                     'lstm/lstm_0/control_dependency:0',\n                                     'lstm/lstm_1/control_dependency:0',\n                                     'softmax_out:0',\n                                     'class_ids_out:0',\n                                     'class_weights_out:0',\n                                     'log_perplexity_out:0',\n                                     'inputs_in:0',\n                                     'targets_in:0',\n                                     'target_weights_in:0',\n                                     'char_inputs_in:0',\n                                     'all_embs_out:0',\n                                     'Reshape_3:0',\n                                     'global_step:0'], name='')\n\n    sys.stderr.write('Recovering checkpoint %s\\n' % ckpt_file)\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    sess.run('save/restore_all', {'save/Const:0': ckpt_file})\n    sess.run(t['states_init'])\n\n  return sess, t\n\n\ndef _EvalModel(dataset):\n  \"\"\"Evaluate model perplexity using provided dataset.\n\n  Args:\n    dataset: LM1BDataset object.\n  \"\"\"\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  current_step = t['global_step'].eval(session=sess)\n  sys.stderr.write('Loaded step %d.\\n' % current_step)\n\n  data_gen = dataset.get_batch(BATCH_SIZE, NUM_TIMESTEPS, forever=False)\n  sum_num = 0.0\n  sum_den = 0.0\n  perplexity = 0.0\n  for i, (inputs, char_inputs, _, targets, weights) in enumerate(data_gen):\n    input_dict = {t['inputs_in']: inputs,\n                  t['targets_in']: targets,\n                  t['target_weights_in']: weights}\n    if 'char_inputs_in' in t:\n      input_dict[t['char_inputs_in']] = char_inputs\n    log_perp = sess.run(t['log_perplexity_out'], feed_dict=input_dict)\n\n    if np.isnan(log_perp):\n      sys.stderr.error('log_perplexity is Nan.\\n')\n    else:\n      sum_num += log_perp * weights.mean()\n      sum_den += weights.mean()\n    if sum_den > 0:\n      perplexity = np.exp(sum_num / sum_den)\n\n    sys.stderr.write('Eval Step: %d, Average Perplexity: %f.\\n' %\n                     (i, perplexity))\n\n    if i > FLAGS.max_eval_steps:\n      break\n\n\ndef _SampleSoftmax(softmax):\n  return min(np.sum(np.cumsum(softmax) < np.random.rand()), len(softmax) - 1)\n\n\ndef _SampleModel(prefix_words, vocab):\n  \"\"\"Predict next words using the given prefix words.\n\n  Args:\n    prefix_words: Prefix words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  \"\"\"\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if prefix_words.find('<S>') != 0:\n    prefix_words = '<S> ' + prefix_words\n\n  prefix = [vocab.word_to_id(w) for w in prefix_words.split()]\n  prefix_char_ids = [vocab.word_to_char_ids(w) for w in prefix_words.split()]\n  for _ in xrange(FLAGS.num_samples):\n    inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n    char_ids_inputs = np.zeros(\n        [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n    samples = prefix[:]\n    char_ids_samples = prefix_char_ids[:]\n    sent = ''\n    while True:\n      inputs[0, 0] = samples[0]\n      char_ids_inputs[0, 0, :] = char_ids_samples[0]\n      samples = samples[1:]\n      char_ids_samples = char_ids_samples[1:]\n\n      softmax = sess.run(t['softmax_out'],\n                         feed_dict={t['char_inputs_in']: char_ids_inputs,\n                                    t['inputs_in']: inputs,\n                                    t['targets_in']: targets,\n                                    t['target_weights_in']: weights})\n\n      sample = _SampleSoftmax(softmax[0])\n      sample_char_ids = vocab.word_to_char_ids(vocab.id_to_word(sample))\n\n      if not samples:\n        samples = [sample]\n        char_ids_samples = [sample_char_ids]\n      sent += vocab.id_to_word(samples[0]) + ' '\n      sys.stderr.write('%s\\n' % sent)\n\n      if (vocab.id_to_word(samples[0]) == '</S>' or\n          len(sent) > FLAGS.max_sample_words):\n        break\n\n\ndef _DumpEmb(vocab):\n  \"\"\"Dump the softmax weights and word embeddings to files.\n\n  Args:\n    vocab: Vocabulary. Contains vocabulary size and converts word to ids.\n  \"\"\"\n  assert FLAGS.save_dir, 'Must specify FLAGS.save_dir for dump_emb.'\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  softmax_weights = sess.run(t['softmax_weights'])\n  fname = FLAGS.save_dir + '/embeddings_softmax.npy'\n  with tf.gfile.Open(fname, mode='w') as f:\n    np.save(f, softmax_weights)\n  sys.stderr.write('Finished softmax weights\\n')\n\n  all_embs = np.zeros([vocab.size, 1024])\n  for i in xrange(vocab.size):\n    input_dict = {t['inputs_in']: inputs,\n                  t['targets_in']: targets,\n                  t['target_weights_in']: weights}\n    if 'char_inputs_in' in t:\n      input_dict[t['char_inputs_in']] = (\n          vocab.word_char_ids[i].reshape([-1, 1, MAX_WORD_LEN]))\n    embs = sess.run(t['all_embs'], input_dict)\n    all_embs[i, :] = embs\n    sys.stderr.write('Finished word embedding %d/%d\\n' % (i, vocab.size))\n\n  fname = FLAGS.save_dir + '/embeddings_char_cnn.npy'\n  with tf.gfile.Open(fname, mode='w') as f:\n    np.save(f, all_embs)\n  sys.stderr.write('Embedding file saved\\n')\n\n\ndef _DumpSentenceEmbedding(sentence, vocab):\n  \"\"\"Predict next words using the given prefix words.\n\n  Args:\n    sentence: Sentence words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  \"\"\"\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if sentence.find('<S>') != 0:\n    sentence = '<S> ' + sentence\n\n  word_ids = [vocab.word_to_id(w) for w in sentence.split()]\n  char_ids = [vocab.word_to_char_ids(w) for w in sentence.split()]\n\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  char_ids_inputs = np.zeros(\n      [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n  for i in xrange(len(word_ids)):\n    inputs[0, 0] = word_ids[i]\n    char_ids_inputs[0, 0, :] = char_ids[i]\n\n    # Add 'lstm/lstm_0/control_dependency' if you want to dump previous layer\n    # LSTM.\n    lstm_emb = sess.run(t['lstm/lstm_1/control_dependency'],\n                        feed_dict={t['char_inputs_in']: char_ids_inputs,\n                                   t['inputs_in']: inputs,\n                                   t['targets_in']: targets,\n                                   t['target_weights_in']: weights})\n\n    fname = os.path.join(FLAGS.save_dir, 'lstm_emb_step_%d.npy' % i)\n    with tf.gfile.Open(fname, mode='w') as f:\n      np.save(f, lstm_emb)\n    sys.stderr.write('LSTM embedding step %d file saved\\n' % i)\n\n\ndef main(unused_argv):\n  vocab = data_utils.CharsVocabulary(FLAGS.vocab_file, MAX_WORD_LEN)\n\n  if FLAGS.mode == 'eval':\n    dataset = data_utils.LM1BDataset(FLAGS.input_data, vocab)\n    _EvalModel(dataset)\n  elif FLAGS.mode == 'sample':\n    _SampleModel(FLAGS.prefix, vocab)\n  elif FLAGS.mode == 'dump_emb':\n    _DumpEmb(vocab)\n  elif FLAGS.mode == 'dump_lstm_emb':\n    _DumpSentenceEmbedding(FLAGS.sentence, vocab)\n  else:\n    raise Exception('Mode not supported.')\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "lm_1b_eval.py", "id": "dbe7956413f6924d055858808a0b2055", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/lm_1b/lm_1b_eval.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}