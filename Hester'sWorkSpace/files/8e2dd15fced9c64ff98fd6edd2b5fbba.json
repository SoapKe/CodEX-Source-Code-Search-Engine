{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"A binary to train CIFAR-10 using multiple GPUs with synchronous updates.\n\nAccuracy:\ncifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\nepochs of data) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  ||| ~86% at 60K steps  (5 hours)\n1 Tesla K40m  || ~86% at 100K steps (4 hours)\n2 Tesla K20m  || ~84% at 30K steps  (2.5 hours)\n3 Tesla K20m  |||| ~84% at 30K steps\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport re\nimport time\n\nimport numpy as np\nfrom six.moves import xrange   pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\")\ntf.app.flags.DEFINE_integer('max_steps', 1000000,\n                            \"\"\"Number of batches to run.\"\"\")\ntf.app.flags.DEFINE_integer('num_gpus', 1,\n                            \"\"\"How many GPUs to use.\"\"\")\ntf.app.flags.DEFINE_boolean('log_device_placement', False,\n                            \"\"\"Whether to log device placement.\"\"\")\n\n\ndef tower_loss(scope, images, labels):\n  \"\"\"Calculate the total loss on a single tower running the CIFAR model.\n\n  Args:\n    scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0'\n    images: Images. 4D tensor of shape [batch_size, height, width, 3].\n    labels: Labels. 1D tensor of shape [batch_size].\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  \"\"\"\n\n   Build inference Graph.\n  logits = cifar10.inference(images)\n\n   Build the portion of the Graph calculating the losses. Note that we will\n   assemble the total_loss using a custom function below.\n  _ = cifar10.loss(logits, labels)\n\n   Assemble all of the losses for the current tower only.\n  losses = tf.get_collection('losses', scope)\n\n   Calculate the total loss for the current tower.\n  total_loss = tf.add_n(losses, name='total_loss')\n\n   Attach a scalar summary to all individual losses and the total loss; do the\n   same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n     Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n     session. This helps the clarity of presentation on tensorboard.\n    loss_name = re.sub('%s_[0-9]*/' % cifar10.TOWER_NAME, '', l.op.name)\n    tf.summary.scalar(loss_name, l)\n\n  return total_loss\n\n\ndef average_gradients(tower_grads):\n  \"\"\"Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  \"\"\"\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n     Note that each grad_and_vars looks like the following:\n       ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n       Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n       Append on a 'tower' dimension which we will average over below.\n      grads.append(expanded_g)\n\n     Average over the 'tower' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n     Keep in mind that the Variables are redundant because they are shared\n     across towers. So .. we will just return the first tower's pointer to\n     the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train():\n  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n  with tf.Graph().as_default(), tf.device('/cpu:0'):\n     Create a variable to count the number of train() calls. This equals the\n     number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        'global_step', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n     Calculate the learning rate schedule.\n    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n                             FLAGS.batch_size)\n    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n\n     Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n                                    global_step,\n                                    decay_steps,\n                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n                                    staircase=True)\n\n     Create an optimizer that performs gradient descent.\n    opt = tf.train.GradientDescentOptimizer(lr)\n\n     Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * FLAGS.num_gpus)\n     Calculate the gradients for each model tower.\n    tower_grads = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device('/gpu:%d' % i):\n          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n             Dequeues one batch for the GPU\n            image_batch, label_batch = batch_queue.dequeue()\n             Calculate the loss for one tower of the CIFAR model. This function\n             constructs the entire CIFAR model but shares the variables across\n             all towers.\n            loss = tower_loss(scope, image_batch, label_batch)\n\n             Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n\n             Retain the summaries from the final tower.\n            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n             Calculate the gradients for the batch of data on this CIFAR tower.\n            grads = opt.compute_gradients(loss)\n\n             Keep track of the gradients across all towers.\n            tower_grads.append(grads)\n\n     We must calculate the mean of each gradient. Note that this is the\n     synchronization point across all towers.\n    grads = average_gradients(tower_grads)\n\n     Add a summary to track the learning rate.\n    summaries.append(tf.summary.scalar('learning_rate', lr))\n\n     Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\n\n     Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n     Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.summary.histogram(var.op.name, var))\n\n     Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n     Group all updates to into a single train op.\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n     Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n\n     Build the summary operation from the last tower summaries.\n    summary_op = tf.summary.merge(summaries)\n\n     Build an initialization operation to run below.\n    init = tf.global_variables_initializer()\n\n     Start running operations on the Graph. allow_soft_placement must be set to\n     True to build towers on GPU, as some of the ops do not have GPU\n     implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n     Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n\n      if step % 10 == 0:\n        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = duration / FLAGS.num_gpus\n\n        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n                      'sec/batch)')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n       Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n        saver.save(sess, checkpoint_path, global_step=step)\n\n\ndef main(argv=None):   pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   a binary train cifar 10 using multiple gpus synchronous updates   accuracy  cifar10 multi gpu train py achieves  86  accuracy 100k steps (256 epochs data) judged cifar10 eval py   speed  with batch size 128   system          step time (sec batch)        accuracy                                                                      1 tesla k20m    0 35 0 60                 86  60k steps  (5 hours) 1 tesla k40m    0 25 0 35                 86  100k steps (4 hours) 2 tesla k20m    0 13 0 20                 84  30k steps  (2 5 hours) 3 tesla k20m    0 13 0 18                 84  30k steps 4 tesla k20m     0 10                     84  30k steps  usage  please see tutorial website download cifar 10 data set  compile program train model   http   tensorflow org tutorials deep cnn        future   import absolute import   future   import division   future   import print function  datetime import datetime import os path import import time  import numpy np six moves import xrange    pylint  disable redefined builtin import tensorflow tf import cifar10  flags   tf app flags flags  tf app flags define string( train dir     tmp cifar10 train                                 directory write event logs                                   checkpoint    ) tf app flags define integer( max steps   1000000                                 number batches run    ) tf app flags define integer( num gpus   1                                 how many gpus use    ) tf app flags define boolean( log device placement   false                                 whether log device placement    )   def tower loss(scope  images  labels)       calculate total loss single tower running cifar model     args      scope  unique prefix string identifying cifar tower  e g   tower 0      images  images  4d tensor shape  batch size  height  width  3       labels  labels  1d tensor shape  batch size      returns       tensor shape    containing total loss batch data            build inference graph    logits   cifar10 inference(images)      build portion graph calculating losses  note     assemble total loss using custom function        cifar10 loss(logits  labels)      assemble losses current tower    losses   tf get collection( losses   scope)      calculate total loss current tower    total loss   tf add n(losses  name  total loss )      attach scalar summary individual losses total loss      averaged version losses    l losses    total loss         remove  tower  0 9    name case multi gpu training       session  this helps clarity presentation tensorboard      loss name   sub(   0 9       cifar10 tower name      l op name)     tf summary scalar(loss name  l)    return total loss   def average gradients(tower grads)       calculate average gradient shared variable across towers     note function provides synchronization point across towers     args      tower grads  list lists (gradient  variable) tuples  the outer list       individual gradients  the inner list gradient       calculation tower    returns       list pairs (gradient  variable) gradient averaged      across towers          average grads        grad vars zip( tower grads)        note grad vars looks like following          ((grad0 gpu0  var0 gpu0)        (grad0 gpun  var0 gpun))     grads          g    grad vars          add 0 dimension gradients represent tower        expanded g   tf expand dims(g  0)          append  tower  dimension average        grads append(expanded g)        average  tower  dimension      grad   tf concat(axis 0  values grads)     grad   tf reduce mean(grad  0)        keep mind variables redundant shared       across towers  so    return first tower pointer       variable      v   grad vars 0  1      grad var   (grad  v)     average grads append(grad var)   return average grads   def train()       train cifar 10 number steps        copyright 2015 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       pylint  disable redefined builtin    build inference graph     build portion graph calculating losses  note    assemble total loss using custom function     assemble losses current tower     calculate total loss current tower     attach scalar summary individual losses total loss     averaged version losses     remove  tower  0 9    name case multi gpu training    session  this helps clarity presentation tensorboard     note grad vars looks like following       ((grad0 gpu0  var0 gpu0)        (grad0 gpun  var0 gpun))    add 0 dimension gradients represent tower     append  tower  dimension average     average  tower  dimension     keep mind variables redundant shared    across towers  so    return first tower pointer    variable     create variable count number train() calls  this equals    number batches processed   flags num gpus     calculate learning rate schedule     decay learning rate exponentially based number steps     create optimizer performs gradient descent     get images labels cifar 10     calculate gradients model tower     dequeues one batch gpu    calculate loss one tower cifar model  this function    constructs entire cifar model shares variables across    towers     reuse variables next tower     retain summaries final tower     calculate gradients batch data cifar tower     keep track gradients across towers     we must calculate mean gradient  note    synchronization point across towers     add summary track learning rate     add histograms gradients     apply gradients adjust shared variables     add histograms trainable variables     track moving averages trainable variables     group updates single train op     create saver     build summary operation last tower summaries     build initialization operation run     start running operations graph  allow soft placement must set    true build towers gpu  ops gpu    implementations     start queue runners     save model checkpoint periodically     pylint  disable unused argument ", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"A binary to train CIFAR-10 using multiple GPUs with synchronous updates.\n\nAccuracy:\ncifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\nepochs of data) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n--------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n2 Tesla K20m  | 0.13-0.20              | ~84% at 30K steps  (2.5 hours)\n3 Tesla K20m  | 0.13-0.18              | ~84% at 30K steps\n4 Tesla K20m  | ~0.10                  | ~84% at 30K steps\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport re\nimport time\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\")\ntf.app.flags.DEFINE_integer('max_steps', 1000000,\n                            \"\"\"Number of batches to run.\"\"\")\ntf.app.flags.DEFINE_integer('num_gpus', 1,\n                            \"\"\"How many GPUs to use.\"\"\")\ntf.app.flags.DEFINE_boolean('log_device_placement', False,\n                            \"\"\"Whether to log device placement.\"\"\")\n\n\ndef tower_loss(scope, images, labels):\n  \"\"\"Calculate the total loss on a single tower running the CIFAR model.\n\n  Args:\n    scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0'\n    images: Images. 4D tensor of shape [batch_size, height, width, 3].\n    labels: Labels. 1D tensor of shape [batch_size].\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  \"\"\"\n\n  # Build inference Graph.\n  logits = cifar10.inference(images)\n\n  # Build the portion of the Graph calculating the losses. Note that we will\n  # assemble the total_loss using a custom function below.\n  _ = cifar10.loss(logits, labels)\n\n  # Assemble all of the losses for the current tower only.\n  losses = tf.get_collection('losses', scope)\n\n  # Calculate the total loss for the current tower.\n  total_loss = tf.add_n(losses, name='total_loss')\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    loss_name = re.sub('%s_[0-9]*/' % cifar10.TOWER_NAME, '', l.op.name)\n    tf.summary.scalar(loss_name, l)\n\n  return total_loss\n\n\ndef average_gradients(tower_grads):\n  \"\"\"Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  \"\"\"\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a 'tower' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the 'tower' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower's pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train():\n  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n  with tf.Graph().as_default(), tf.device('/cpu:0'):\n    # Create a variable to count the number of train() calls. This equals the\n    # number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        'global_step', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n    # Calculate the learning rate schedule.\n    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n                             FLAGS.batch_size)\n    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n\n    # Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n                                    global_step,\n                                    decay_steps,\n                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n                                    staircase=True)\n\n    # Create an optimizer that performs gradient descent.\n    opt = tf.train.GradientDescentOptimizer(lr)\n\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * FLAGS.num_gpus)\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device('/gpu:%d' % i):\n          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n            # Dequeues one batch for the GPU\n            image_batch, label_batch = batch_queue.dequeue()\n            # Calculate the loss for one tower of the CIFAR model. This function\n            # constructs the entire CIFAR model but shares the variables across\n            # all towers.\n            loss = tower_loss(scope, image_batch, label_batch)\n\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n\n            # Retain the summaries from the final tower.\n            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n            # Calculate the gradients for the batch of data on this CIFAR tower.\n            grads = opt.compute_gradients(loss)\n\n            # Keep track of the gradients across all towers.\n            tower_grads.append(grads)\n\n    # We must calculate the mean of each gradient. Note that this is the\n    # synchronization point across all towers.\n    grads = average_gradients(tower_grads)\n\n    # Add a summary to track the learning rate.\n    summaries.append(tf.summary.scalar('learning_rate', lr))\n\n    # Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\n\n    # Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.summary.histogram(var.op.name, var))\n\n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    # Group all updates to into a single train op.\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n\n    # Build the summary operation from the last tower summaries.\n    summary_op = tf.summary.merge(summaries)\n\n    # Build an initialization operation to run below.\n    init = tf.global_variables_initializer()\n\n    # Start running operations on the Graph. allow_soft_placement must be set to\n    # True to build towers on GPU, as some of the ops do not have GPU\n    # implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n\n      if step % 10 == 0:\n        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = duration / FLAGS.num_gpus\n\n        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n                      'sec/batch)')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n        saver.save(sess, checkpoint_path, global_step=step)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "cifar10_multi_gpu_train.py", "id": "8e2dd15fced9c64ff98fd6edd2b5fbba", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/tutorials/image/cifar10/cifar10_multi_gpu_train.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}