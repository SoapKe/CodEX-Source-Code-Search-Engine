{"author": "donnemartin", "code": "from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        \n        num_workers = 2       \n        downsampling = 1e-3   \n        \n        \n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        \n        \n        embedding_model.init_sims(replace=True)\n        \n        ()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    \n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__=='__main__':\n    import data_helpers\n    print(\"Loading data...\")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n", "comments": "        trains  saves  loads word2vec model     returns initial weights embedding layer          inputs      sentence matrix   int matrix  num sentences x max sentence len     vocabulary inv    dict  str int      num features      word vector dimensionality                           min word count    minimum word count                             context           context window size             set values various parameters    number threads run parallel    downsample setting frequent words    initialize train model    if plan train model  calling     init sims make model much memory efficient     saving model later use  you load later using word2vec load()     add unknown words ", "content": "from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \"\"\"\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    \"\"\"\n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 2       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don't plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__=='__main__':\n    import data_helpers\n    print(\"Loading data...\")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "w2v.py", "id": "9ed9f1cfb02912883ea27fb5b663f206", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/deep-learning/keras-tutorial/w2v.py", "save_time": "", "source": "", "update_at": "2018-03-18T12:16:56Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}