{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\"\"\"Run a pretrained autoencoder model on an entire dataset, saving encodings.\"\"\"\n\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import reader\nfrom magenta.models.nsynth import utils\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\"master\", \"\",\n                           \"BNS name of the TensorFlow master to use.\")\ntf.app.flags.DEFINE_string(\"model\", \"ae\", \"Which model to use in models/\")\ntf.app.flags.DEFINE_string(\"config\", \"nfft_1024\",\n                           \"Which model to use in configs/\")\ntf.app.flags.DEFINE_string(\"expdir\", \"\",\n                           \"The log directory for this experiment. Required \"\n                           \"if`checkpoint_path` is not given.\")\ntf.app.flags.DEFINE_string(\"checkpoint_path\", \"\",\n                           \"A path to the checkpoint. If not given, the latest \"\n                           \"checkpoint in `expdir` will be used.\")\ntf.app.flags.DEFINE_string(\"tfrecord_path\", \"\",\n                           \"Path to nsynth-{train, valid, test}.tfrecord.\")\ntf.app.flags.DEFINE_string(\"savedir\", \"\", \"Where to save the embeddings.\")\ntf.app.flags.DEFINE_string(\"log\", \"INFO\",\n                           \"The threshold for what messages will be logged.\"\n                           \"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n\n\ndef save_arrays(savedir, hparams, z_val):\n  \"\"\"Save arrays as npy files.\n\n  Args:\n    savedir: Directory where arrays are saved.\n    hparams: Hyperparameters.\n    z_val: Array to save.\n  \"\"\"\n  z_save_val = np.array(z_val).reshape(-1, hparams.num_latent)\n\n  name = FLAGS.tfrecord_path.split(\"/\")[-1].split(\".tfrecord\")[0]\n  save_name = os.path.join(savedir, \"{}_%s.npy\".format(name))\n  with tf.gfile.Open(save_name % \"z\", \"w\") as f:\n    np.save(f, z_save_val)\n\n  tf.logging.info(\"Z_Save:{}\".format(z_save_val.shape))\n  tf.logging.info(\"Successfully saved to {}\".format(save_name % \"\"))\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if FLAGS.checkpoint_path:\n    checkpoint_path = FLAGS.checkpoint_path\n  else:\n    expdir = FLAGS.expdir\n    tf.logging.info(\"Will load latest checkpoint from %s.\", expdir)\n    while not tf.gfile.Exists(expdir):\n      tf.logging.fatal(\"\\tExperiment save dir '%s' does not exist!\", expdir)\n      sys.exit(1)\n\n    try:\n      checkpoint_path = tf.train.latest_checkpoint(expdir)\n    except tf.errors.NotFoundError:\n      tf.logging.fatal(\"There was a problem determining the latest checkpoint.\")\n      sys.exit(1)\n\n  if not tf.train.checkpoint_exists(checkpoint_path):\n    tf.logging.fatal(\"Invalid checkpoint path: %s\", checkpoint_path)\n    sys.exit(1)\n\n  savedir = FLAGS.savedir\n  if not tf.gfile.Exists(savedir):\n    tf.gfile.MakeDirs(savedir)\n\n   Make the graph\n  with tf.Graph().as_default():\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n      model = utils.get_module(\"baseline.models.%s\" % FLAGS.model)\n      hparams = model.get_hparams(FLAGS.config)\n\n       Load the trained model with is_training=False\n      with tf.name_scope(\"Reader\"):\n        batch = reader.NSynthDataset(\n            FLAGS.tfrecord_path,\n            is_training=False).get_baseline_batch(hparams)\n\n      _ = model.train_op(batch, hparams, FLAGS.config)\n      z = tf.get_collection(\"z\")[0]\n\n      init_op = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n      sess.run(init_op)\n\n       Add ops to save and restore all the variables.\n       Restore variables from disk.\n      saver = tf.train.Saver()\n      saver.restore(sess, checkpoint_path)\n      tf.logging.info(\"Model restored.\")\n\n       Start up some threads\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n      i = 0\n      z_val = []\n      try:\n        while True:\n          if coord.should_stop():\n            break\n          res_val = sess.run([z])\n          z_val.append(res_val[0])\n          tf.logging.info(\"Iter: %d\" % i)\n          tf.logging.info(\"Z:{}\".format(res_val[0].shape))\n          i += 1\n          if i + 1 % 1 == 0:\n            save_arrays(savedir, hparams, z_val)\n       Report all exceptions to the coordinator, pylint: disable=broad-except\n      except Exception as e:\n        coord.request_stop(e)\n       pylint: enable=broad-except\n      finally:\n        save_arrays(savedir, hparams, z_val)\n         Terminate as usual.  It is innocuous to request stop twice.\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "comments": "   run pretrained autoencoder model entire dataset  saving encodings      import os import sys  import numpy np import tensorflow tf  magenta models nsynth import reader magenta models nsynth import utils  slim   tf contrib slim flags   tf app flags flags  tf app flags define string( master                                   bns name tensorflow master use  ) tf app flags define string( model    ae    which model use models  ) tf app flags define string( config    nfft 1024                               which model use configs  ) tf app flags define string( expdir                                   the log directory experiment  required                               checkpoint path  given  ) tf app flags define string( checkpoint path                                   a path checkpoint  if given  latest                               checkpoint  expdir  used  ) tf app flags define string( tfrecord path                                   path nsynth  train  valid  test  tfrecord  ) tf app flags define string( savedir        where save embeddings  ) tf app flags define string( log    info                               the threshold messages logged                               debug  info  warn  error  fatal  )   def save arrays(savedir  hparams  z val)       save arrays npy files     args      savedir  directory arrays saved      hparams  hyperparameters      z val  array save           copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     make graph    load trained model training false    add ops save restore variables     restore variables disk     start threads    report exceptions coordinator  pylint  disable broad except    pylint  enable broad except    terminate usual   it innocuous request stop twice  ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Run a pretrained autoencoder model on an entire dataset, saving encodings.\"\"\"\n\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom magenta.models.nsynth import reader\nfrom magenta.models.nsynth import utils\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\"master\", \"\",\n                           \"BNS name of the TensorFlow master to use.\")\ntf.app.flags.DEFINE_string(\"model\", \"ae\", \"Which model to use in models/\")\ntf.app.flags.DEFINE_string(\"config\", \"nfft_1024\",\n                           \"Which model to use in configs/\")\ntf.app.flags.DEFINE_string(\"expdir\", \"\",\n                           \"The log directory for this experiment. Required \"\n                           \"if`checkpoint_path` is not given.\")\ntf.app.flags.DEFINE_string(\"checkpoint_path\", \"\",\n                           \"A path to the checkpoint. If not given, the latest \"\n                           \"checkpoint in `expdir` will be used.\")\ntf.app.flags.DEFINE_string(\"tfrecord_path\", \"\",\n                           \"Path to nsynth-{train, valid, test}.tfrecord.\")\ntf.app.flags.DEFINE_string(\"savedir\", \"\", \"Where to save the embeddings.\")\ntf.app.flags.DEFINE_string(\"log\", \"INFO\",\n                           \"The threshold for what messages will be logged.\"\n                           \"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n\n\ndef save_arrays(savedir, hparams, z_val):\n  \"\"\"Save arrays as npy files.\n\n  Args:\n    savedir: Directory where arrays are saved.\n    hparams: Hyperparameters.\n    z_val: Array to save.\n  \"\"\"\n  z_save_val = np.array(z_val).reshape(-1, hparams.num_latent)\n\n  name = FLAGS.tfrecord_path.split(\"/\")[-1].split(\".tfrecord\")[0]\n  save_name = os.path.join(savedir, \"{}_%s.npy\".format(name))\n  with tf.gfile.Open(save_name % \"z\", \"w\") as f:\n    np.save(f, z_save_val)\n\n  tf.logging.info(\"Z_Save:{}\".format(z_save_val.shape))\n  tf.logging.info(\"Successfully saved to {}\".format(save_name % \"\"))\n\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(FLAGS.log)\n\n  if FLAGS.checkpoint_path:\n    checkpoint_path = FLAGS.checkpoint_path\n  else:\n    expdir = FLAGS.expdir\n    tf.logging.info(\"Will load latest checkpoint from %s.\", expdir)\n    while not tf.gfile.Exists(expdir):\n      tf.logging.fatal(\"\\tExperiment save dir '%s' does not exist!\", expdir)\n      sys.exit(1)\n\n    try:\n      checkpoint_path = tf.train.latest_checkpoint(expdir)\n    except tf.errors.NotFoundError:\n      tf.logging.fatal(\"There was a problem determining the latest checkpoint.\")\n      sys.exit(1)\n\n  if not tf.train.checkpoint_exists(checkpoint_path):\n    tf.logging.fatal(\"Invalid checkpoint path: %s\", checkpoint_path)\n    sys.exit(1)\n\n  savedir = FLAGS.savedir\n  if not tf.gfile.Exists(savedir):\n    tf.gfile.MakeDirs(savedir)\n\n  # Make the graph\n  with tf.Graph().as_default():\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n      model = utils.get_module(\"baseline.models.%s\" % FLAGS.model)\n      hparams = model.get_hparams(FLAGS.config)\n\n      # Load the trained model with is_training=False\n      with tf.name_scope(\"Reader\"):\n        batch = reader.NSynthDataset(\n            FLAGS.tfrecord_path,\n            is_training=False).get_baseline_batch(hparams)\n\n      _ = model.train_op(batch, hparams, FLAGS.config)\n      z = tf.get_collection(\"z\")[0]\n\n      init_op = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n      sess.run(init_op)\n\n      # Add ops to save and restore all the variables.\n      # Restore variables from disk.\n      saver = tf.train.Saver()\n      saver.restore(sess, checkpoint_path)\n      tf.logging.info(\"Model restored.\")\n\n      # Start up some threads\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n      i = 0\n      z_val = []\n      try:\n        while True:\n          if coord.should_stop():\n            break\n          res_val = sess.run([z])\n          z_val.append(res_val[0])\n          tf.logging.info(\"Iter: %d\" % i)\n          tf.logging.info(\"Z:{}\".format(res_val[0].shape))\n          i += 1\n          if i + 1 % 1 == 0:\n            save_arrays(savedir, hparams, z_val)\n      # Report all exceptions to the coordinator, pylint: disable=broad-except\n      except Exception as e:\n        coord.request_stop(e)\n      # pylint: enable=broad-except\n      finally:\n        save_arrays(savedir, hparams, z_val)\n        # Terminate as usual.  It is innocuous to request stop twice.\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "save_embeddings.py", "id": "bd3fa93dbe03106bbbdb27c7f5942f60", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-magenta/tensorflow-magenta-c3eda3d/magenta/models/nsynth/baseline/save_embeddings.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:52:33Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}