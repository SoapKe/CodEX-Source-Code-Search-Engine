{"author": "tflearn", "code": "# -*- coding: utf-8 -*-\n\"\"\"\nExample on how to use Dask with TFLearn. Dask is a simple task scheduling\nsystem that uses directed acyclic graphs (DAGs) of tasks to break up large\ncomputations into many small ones. It can handle large dataset that could\nnot fit totally in ram memory. Note that this example just give a quick\ncompatibility demonstration. In practice, there is no so much need to use\nDask for small dataset such as CIFAR-10.\n\"\"\"\n\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.data_utils import *\nfrom tflearn.layers.estimator import *\n\n\nfrom tflearn.datasets import cifar10\n(X, Y), (X_test, Y_test) = cifar10.load_data()\nY = to_categorical(Y)\nY_test = to_categorical(Y_test)\n\n\n# (Note that it can work with HDF5 Dataset too)\nimport dask.array as da\nX = da.from_array(np.asarray(X), chunks=(1000, 1000, 1000, 1000))\nY = da.from_array(np.asarray(Y), chunks=(1000, 1000, 1000, 1000))\nX_test = da.from_array(np.asarray(X_test), chunks=(1000, 1000, 1000, 1000))\nY_test = da.from_array(np.asarray(Y_test), chunks=(1000, 1000, 1000, 1000))\n\n\nnetwork = input_data(shape=[None, 32, 32, 3])\nnetwork = conv_2d(network, 32, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = dropout(network, 0.75)\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 512, activation='relu')\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 10, activation='softmax')\nnetwork = regression(network, optimizer='adam',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.001)\n\n\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),\n          show_metric=True, batch_size=96, run_id='cifar10_cnn')\n", "comments": "    example use dask tflearn  dask simple task scheduling system uses directed acyclic graphs (dags) tasks break large computations many small ones  it handle large dataset could fit totally ram memory  note example give quick compatibility demonstration  in practice  much need use dask small dataset cifar 10             coding  utf 8        load cifar 10 dataset    create dask array using numpy arrays    (note work hdf5 dataset too)    build network    training ", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nExample on how to use Dask with TFLearn. Dask is a simple task scheduling\nsystem that uses directed acyclic graphs (DAGs) of tasks to break up large\ncomputations into many small ones. It can handle large dataset that could\nnot fit totally in ram memory. Note that this example just give a quick\ncompatibility demonstration. In practice, there is no so much need to use\nDask for small dataset such as CIFAR-10.\n\"\"\"\n\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.data_utils import *\nfrom tflearn.layers.estimator import *\n\n# Load CIFAR-10 Dataset\nfrom tflearn.datasets import cifar10\n(X, Y), (X_test, Y_test) = cifar10.load_data()\nY = to_categorical(Y)\nY_test = to_categorical(Y_test)\n\n# Create DASK array using numpy arrays\n# (Note that it can work with HDF5 Dataset too)\nimport dask.array as da\nX = da.from_array(np.asarray(X), chunks=(1000, 1000, 1000, 1000))\nY = da.from_array(np.asarray(Y), chunks=(1000, 1000, 1000, 1000))\nX_test = da.from_array(np.asarray(X_test), chunks=(1000, 1000, 1000, 1000))\nY_test = da.from_array(np.asarray(Y_test), chunks=(1000, 1000, 1000, 1000))\n\n# Build network\nnetwork = input_data(shape=[None, 32, 32, 3])\nnetwork = conv_2d(network, 32, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = dropout(network, 0.75)\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 512, activation='relu')\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 10, activation='softmax')\nnetwork = regression(network, optimizer='adam',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.001)\n\n# Training\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),\n          show_metric=True, batch_size=96, run_id='cifar10_cnn')\n", "description": "Deep learning library featuring a higher-level API for TensorFlow.", "file_name": "use_dask.py", "id": "681fd69e23c44b988c2dc0c178e4a43f", "language": "Python", "project_name": "tflearn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tflearn-tflearn/tflearn-tflearn-70fb38a/examples/basics/use_dask.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:15:41Z", "url": "https://github.com/tflearn/tflearn", "wiki": true}