{"author": "openai", "code": "import numpy as np\n\nfrom gym import utils\nfrom gym.envs.robotics import hand_env\nfrom gym.envs.robotics.utils import robot_get_obs\n\n\nFINGERTIP_SITE_NAMES = [\n    'robot0:S_fftip',\n    'robot0:S_mftip',\n    'robot0:S_rftip',\n    'robot0:S_lftip',\n    'robot0:S_thtip',\n]\n\n\nDEFAULT_INITIAL_QPOS = {\n    'robot0:WRJ1': -0.16514339750464327,\n    'robot0:WRJ0': -0.31973286565062153,\n    'robot0:FFJ3': 0.14340512546557435,\n    'robot0:FFJ2': 0.32028208333591573,\n    'robot0:FFJ1': 0.7126053607727917,\n    'robot0:FFJ0': 0.6705281001412586,\n    'robot0:MFJ3': 0.000246444303701037,\n    'robot0:MFJ2': 0.3152655251085491,\n    'robot0:MFJ1': 0.7659800313729842,\n    'robot0:MFJ0': 0.7323156897425923,\n    'robot0:RFJ3': 0.00038520700007378114,\n    'robot0:RFJ2': 0.36743546201985233,\n    'robot0:RFJ1': 0.7119514095008576,\n    'robot0:RFJ0': 0.6699446327514138,\n    'robot0:LFJ4': 0.0525442258033891,\n    'robot0:LFJ3': -0.13615534724474673,\n    'robot0:LFJ2': 0.39872030433433003,\n    'robot0:LFJ1': 0.7415570009679252,\n    'robot0:LFJ0': 0.704096378652974,\n    'robot0:THJ4': 0.003673823825070126,\n    'robot0:THJ3': 0.5506291436028695,\n    'robot0:THJ2': -0.014515151997119306,\n    'robot0:THJ1': -0.0015229223564485414,\n    'robot0:THJ0': -0.7894883021600622,\n}\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass HandReachEnv(hand_env.HandEnv, utils.EzPickle):\n    def __init__(\n        self, distance_threshold=0.01, n_substeps=20, relative_control=False,\n        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',\n    ):\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        hand_env.HandEnv.__init__(\n            self, 'hand/reach.xml', n_substeps=n_substeps, initial_qpos=initial_qpos,\n            relative_control=relative_control)\n        utils.EzPickle.__init__(self)\n\n    def _get_achieved_goal(self):\n        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]\n        return np.array(goal).flatten()\n\n    \n    \n\n    def compute_reward(self, achieved_goal, goal, info):\n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    \n    \n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        self.sim.forward()\n\n        self.initial_goal = self._get_achieved_goal().copy()\n        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()\n\n    def _get_obs(self):\n        robot_qpos, robot_qvel = robot_get_obs(self.sim)\n        achieved_goal = self._get_achieved_goal().ravel()\n        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])\n        return {\n            'observation': observation.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _sample_goal(self):\n        thumb_name = 'robot0:S_thtip'\n        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]\n        finger_name = self.np_random.choice(finger_names)\n\n        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)\n        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)\n        assert thumb_idx != finger_idx\n\n        \n        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])\n        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)\n\n        \n        \n        goal = self.initial_goal.copy().reshape(-1, 3)\n        for idx in [thumb_idx, finger_idx]:\n            offset_direction = (meeting_pos - goal[idx])\n            offset_direction /= np.linalg.norm(offset_direction)\n            goal[idx] = meeting_pos - 0.005 * offset_direction\n\n        if self.np_random.uniform() < 0.1:\n            \n            \n            goal = self.initial_goal.copy()\n        return goal.flatten()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _render_callback(self):\n        \n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        goal = self.goal.reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'target{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]\n\n        \n        achieved_goal = self._get_achieved_goal().reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'finger{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]\n        self.sim.forward()\n", "comments": "  goalenv methods                                    robotenv methods                                    pick meeting point hand     slightly move meeting goal towards respective finger avoid    overlap     with probability  ask fingers move back origin     this avoids thumb constantly stays near goal position already     visualize targets     visualize finger positions  ", "content": "import numpy as np\n\nfrom gym import utils\nfrom gym.envs.robotics import hand_env\nfrom gym.envs.robotics.utils import robot_get_obs\n\n\nFINGERTIP_SITE_NAMES = [\n    'robot0:S_fftip',\n    'robot0:S_mftip',\n    'robot0:S_rftip',\n    'robot0:S_lftip',\n    'robot0:S_thtip',\n]\n\n\nDEFAULT_INITIAL_QPOS = {\n    'robot0:WRJ1': -0.16514339750464327,\n    'robot0:WRJ0': -0.31973286565062153,\n    'robot0:FFJ3': 0.14340512546557435,\n    'robot0:FFJ2': 0.32028208333591573,\n    'robot0:FFJ1': 0.7126053607727917,\n    'robot0:FFJ0': 0.6705281001412586,\n    'robot0:MFJ3': 0.000246444303701037,\n    'robot0:MFJ2': 0.3152655251085491,\n    'robot0:MFJ1': 0.7659800313729842,\n    'robot0:MFJ0': 0.7323156897425923,\n    'robot0:RFJ3': 0.00038520700007378114,\n    'robot0:RFJ2': 0.36743546201985233,\n    'robot0:RFJ1': 0.7119514095008576,\n    'robot0:RFJ0': 0.6699446327514138,\n    'robot0:LFJ4': 0.0525442258033891,\n    'robot0:LFJ3': -0.13615534724474673,\n    'robot0:LFJ2': 0.39872030433433003,\n    'robot0:LFJ1': 0.7415570009679252,\n    'robot0:LFJ0': 0.704096378652974,\n    'robot0:THJ4': 0.003673823825070126,\n    'robot0:THJ3': 0.5506291436028695,\n    'robot0:THJ2': -0.014515151997119306,\n    'robot0:THJ1': -0.0015229223564485414,\n    'robot0:THJ0': -0.7894883021600622,\n}\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass HandReachEnv(hand_env.HandEnv, utils.EzPickle):\n    def __init__(\n        self, distance_threshold=0.01, n_substeps=20, relative_control=False,\n        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',\n    ):\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        hand_env.HandEnv.__init__(\n            self, 'hand/reach.xml', n_substeps=n_substeps, initial_qpos=initial_qpos,\n            relative_control=relative_control)\n        utils.EzPickle.__init__(self)\n\n    def _get_achieved_goal(self):\n        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]\n        return np.array(goal).flatten()\n\n    # GoalEnv methods\n    # ----------------------------\n\n    def compute_reward(self, achieved_goal, goal, info):\n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        self.sim.forward()\n\n        self.initial_goal = self._get_achieved_goal().copy()\n        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()\n\n    def _get_obs(self):\n        robot_qpos, robot_qvel = robot_get_obs(self.sim)\n        achieved_goal = self._get_achieved_goal().ravel()\n        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])\n        return {\n            'observation': observation.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _sample_goal(self):\n        thumb_name = 'robot0:S_thtip'\n        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]\n        finger_name = self.np_random.choice(finger_names)\n\n        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)\n        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)\n        assert thumb_idx != finger_idx\n\n        # Pick a meeting point above the hand.\n        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])\n        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)\n\n        # Slightly move meeting goal towards the respective finger to avoid that they\n        # overlap.\n        goal = self.initial_goal.copy().reshape(-1, 3)\n        for idx in [thumb_idx, finger_idx]:\n            offset_direction = (meeting_pos - goal[idx])\n            offset_direction /= np.linalg.norm(offset_direction)\n            goal[idx] = meeting_pos - 0.005 * offset_direction\n\n        if self.np_random.uniform() < 0.1:\n            # With some probability, ask all fingers to move back to the origin.\n            # This avoids that the thumb constantly stays near the goal position already.\n            goal = self.initial_goal.copy()\n        return goal.flatten()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _render_callback(self):\n        # Visualize targets.\n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        goal = self.goal.reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'target{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]\n\n        # Visualize finger positions.\n        achieved_goal = self._get_achieved_goal().reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'finger{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]\n        self.sim.forward()\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "reach.py", "id": "85893aabf577036fa225b2c4691f53e6", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/robotics/hand/reach.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}