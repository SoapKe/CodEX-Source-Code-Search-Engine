{"author": "scikit-learn", "code": "\n\n\n\nimport os\ntry:\n    \n    from urllib2 import Request, build_opener\nexcept ImportError:\n    \n    from urllib.request import Request, build_opener\n\nimport lxml.html\nfrom lxml.etree import ElementTree\nimport numpy as np\n\nimport codecs\n\npages = {\n    u'ar': u'http://ar.wikipedia.org/wiki/%D9%88%D9%8A%D9%83%D9%8A%D8%A8%D9%8A%D8%AF%D9%8A%D8%A7',\n    u'de': u'http://de.wikipedia.org/wiki/Wikipedia',\n    u'en': u'https://en.wikipedia.org/wiki/Wikipedia',\n    u'es': u'http://es.wikipedia.org/wiki/Wikipedia',\n    u'fr': u'http://fr.wikipedia.org/wiki/Wikip%C3%A9dia',\n    u'it': u'http://it.wikipedia.org/wiki/Wikipedia',\n    u'ja': u'http://ja.wikipedia.org/wiki/Wikipedia',\n    u'nl': u'http://nl.wikipedia.org/wiki/Wikipedia',\n    u'pl': u'http://pl.wikipedia.org/wiki/Wikipedia',\n    u'pt': u'http://pt.wikipedia.org/wiki/Wikip%C3%A9dia',\n    u'ru': u'http://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F',\n\n}\n\nhtml_folder = u'html'\ntext_folder = u'paragraphs'\nshort_text_folder = u'short_paragraphs'\nn_words_per_short_text = 5\n\n\nif not os.path.exists(html_folder):\n    os.makedirs(html_folder)\n\nfor lang, page in pages.items():\n\n    text_lang_folder = os.path.join(text_folder, lang)\n    if not os.path.exists(text_lang_folder):\n        os.makedirs(text_lang_folder)\n\n    short_text_lang_folder = os.path.join(short_text_folder, lang)\n    if not os.path.exists(short_text_lang_folder):\n        os.makedirs(short_text_lang_folder)\n\n    opener = build_opener()\n    html_filename = os.path.join(html_folder, lang + '.html')\n    if not os.path.exists(html_filename):\n        print(\"Downloading %s\" % page)\n        request = Request(page)\n        \n        \n        request.add_header('User-Agent', 'OpenAnything/1.0')\n        html_content = opener.open(request).read()\n        open(html_filename, 'wb').write(html_content)\n\n    \n    \n    with codecs.open(html_filename,'r','utf-8') as html_file:\n        html_content = html_file.read()\n    tree = ElementTree(lxml.html.document_fromstring(html_content))\n    i = 0\n    j = 0\n    for p in tree.findall('//p'):\n        content = p.text_content()\n        if len(content) < 100:\n            \n            \n            continue\n\n        text_filename = os.path.join(text_lang_folder,\n                                     '%s_%04d.txt' % (lang, i))\n        print(\"Writing %s\" % text_filename)\n        open(text_filename, 'wb').write(content.encode('utf-8', 'ignore'))\n        i += 1\n\n        \n        \n        if lang in ('zh', 'ja'):\n        \n            continue\n        words = content.split()\n        n_groups = len(words) / n_words_per_short_text\n        if n_groups < 1:\n            continue\n        groups = np.array_split(words, n_groups)\n\n        for group in groups:\n            small_content = u\" \".join(group)\n\n            short_text_filename = os.path.join(short_text_lang_folder,\n                                               '%s_%04d.txt' % (lang, j))\n            print(\"Writing %s\" % short_text_filename)\n            open(short_text_filename, 'wb').write(\n                small_content.encode('utf-8', 'ignore'))\n            j += 1\n            if j >= 1000:\n                break\n\n", "comments": "  simple python script collect text paragraphs various languages    topic namely wikipedia encyclopedia    python 2 compat    python 3       u zh   u http   zh wikipedia org wiki wikipedia      change user agent avoid blocked wikipedia    downloading couple articles considered abusive    decode payload explicitly utf 8 since lxml confused    reason    skip paragraphs short   probably noisy    representative actual language    split paragraph fake smaller paragraphs make    problem harder e g  similar tweets    fixme  whitespace tokenizing work chinese japanese ", "content": "\n# simple python script to collect text paragraphs from various languages on the\n# same topic namely the Wikipedia encyclopedia itself\n\nimport os\ntry:\n    # Python 2 compat\n    from urllib2 import Request, build_opener\nexcept ImportError:\n    # Python 3\n    from urllib.request import Request, build_opener\n\nimport lxml.html\nfrom lxml.etree import ElementTree\nimport numpy as np\n\nimport codecs\n\npages = {\n    u'ar': u'http://ar.wikipedia.org/wiki/%D9%88%D9%8A%D9%83%D9%8A%D8%A8%D9%8A%D8%AF%D9%8A%D8%A7',\n    u'de': u'http://de.wikipedia.org/wiki/Wikipedia',\n    u'en': u'https://en.wikipedia.org/wiki/Wikipedia',\n    u'es': u'http://es.wikipedia.org/wiki/Wikipedia',\n    u'fr': u'http://fr.wikipedia.org/wiki/Wikip%C3%A9dia',\n    u'it': u'http://it.wikipedia.org/wiki/Wikipedia',\n    u'ja': u'http://ja.wikipedia.org/wiki/Wikipedia',\n    u'nl': u'http://nl.wikipedia.org/wiki/Wikipedia',\n    u'pl': u'http://pl.wikipedia.org/wiki/Wikipedia',\n    u'pt': u'http://pt.wikipedia.org/wiki/Wikip%C3%A9dia',\n    u'ru': u'http://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F',\n#    u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',\n}\n\nhtml_folder = u'html'\ntext_folder = u'paragraphs'\nshort_text_folder = u'short_paragraphs'\nn_words_per_short_text = 5\n\n\nif not os.path.exists(html_folder):\n    os.makedirs(html_folder)\n\nfor lang, page in pages.items():\n\n    text_lang_folder = os.path.join(text_folder, lang)\n    if not os.path.exists(text_lang_folder):\n        os.makedirs(text_lang_folder)\n\n    short_text_lang_folder = os.path.join(short_text_folder, lang)\n    if not os.path.exists(short_text_lang_folder):\n        os.makedirs(short_text_lang_folder)\n\n    opener = build_opener()\n    html_filename = os.path.join(html_folder, lang + '.html')\n    if not os.path.exists(html_filename):\n        print(\"Downloading %s\" % page)\n        request = Request(page)\n        # change the User Agent to avoid being blocked by Wikipedia\n        # downloading a couple of articles should not be considered abusive\n        request.add_header('User-Agent', 'OpenAnything/1.0')\n        html_content = opener.open(request).read()\n        open(html_filename, 'wb').write(html_content)\n\n    # decode the payload explicitly as UTF-8 since lxml is confused for some\n    # reason\n    with codecs.open(html_filename,'r','utf-8') as html_file:\n        html_content = html_file.read()\n    tree = ElementTree(lxml.html.document_fromstring(html_content))\n    i = 0\n    j = 0\n    for p in tree.findall('//p'):\n        content = p.text_content()\n        if len(content) < 100:\n            # skip paragraphs that are too short - probably too noisy and not\n            # representative of the actual language\n            continue\n\n        text_filename = os.path.join(text_lang_folder,\n                                     '%s_%04d.txt' % (lang, i))\n        print(\"Writing %s\" % text_filename)\n        open(text_filename, 'wb').write(content.encode('utf-8', 'ignore'))\n        i += 1\n\n        # split the paragraph into fake smaller paragraphs to make the\n        # problem harder e.g. more similar to tweets\n        if lang in ('zh', 'ja'):\n        # FIXME: whitespace tokenizing does not work on chinese and japanese\n            continue\n        words = content.split()\n        n_groups = len(words) / n_words_per_short_text\n        if n_groups < 1:\n            continue\n        groups = np.array_split(words, n_groups)\n\n        for group in groups:\n            small_content = u\" \".join(group)\n\n            short_text_filename = os.path.join(short_text_lang_folder,\n                                               '%s_%04d.txt' % (lang, j))\n            print(\"Writing %s\" % short_text_filename)\n            open(short_text_filename, 'wb').write(\n                small_content.encode('utf-8', 'ignore'))\n            j += 1\n            if j >= 1000:\n                break\n\n", "description": "scikit-learn: machine learning in Python", "file_name": "fetch_data.py", "id": "da726c0db469f81c148b788303cdbad9", "language": "Python", "project_name": "scikit-learn", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/scikit-learn-scikit-learn/scikit-learn-scikit-learn-94ed5a8/doc/tutorial/text_analytics/data/languages/fetch_data.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:58:59Z", "url": "https://github.com/scikit-learn/scikit-learn", "wiki": true}