{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\nimport os\nimport sys\nsys.path.append(os.path.abspath(os.path.join(__file__, '../../')))\nimport numpy as np\nimport tensorflow as tf\nfrom config import get_config\nfrom model_n2nmn.assembler import Assembler\nfrom model_n2nmn.model import Model\nfrom util.data_reader import DataReader\nfrom util.data_reader import SampleBuilder\nfrom util.misc import prepare_dirs_and_logger\nfrom util.misc import save_config\nfrom util.misc import show_all_variables\n\n\ndef main(_):\n  config = prepare_dirs_and_logger(config_raw)\n  save_config(config)\n\n  rng = np.random.RandomState(config.random_seed)\n  tf.set_random_seed(config.random_seed)\n  config.rng = rng\n\n  config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n  config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n  assembler = Assembler(config)\n\n  sample_builder = SampleBuilder(config)\n  config = sample_builder.config   update T_encoder according to data\n  data_train = sample_builder.data_all['train']\n  data_reader_train = DataReader(\n      config, data_train, assembler, shuffle=True, one_pass=False)\n\n  num_vocab_txt = len(sample_builder.dict_all)\n  num_vocab_nmn = len(assembler.module_names)\n  num_choices = len(sample_builder.dict_all)\n\n   Network inputs\n  text_seq_batch = tf.placeholder(tf.int32, [None, None])\n  seq_len_batch = tf.placeholder(tf.int32, [None])\n  ans_label_batch = tf.placeholder(tf.int32, [None])\n  use_gt_layout = tf.constant(True, dtype=tf.bool)\n  gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n\n   The model for training\n  model = Model(\n      config,\n      sample_builder.kb,\n      text_seq_batch,\n      seq_len_batch,\n      num_vocab_txt=num_vocab_txt,\n      num_vocab_nmn=num_vocab_nmn,\n      EOS_idx=assembler.EOS_idx,\n      num_choices=num_choices,\n      decoder_sampling=True,\n      use_gt_layout=use_gt_layout,\n      gt_layout_batch=gt_layout_batch)\n  compiler = model.compiler\n  scores = model.scores\n  log_seq_prob = model.log_seq_prob\n\n   Loss function\n  softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=scores, labels=ans_label_batch)\n   The final per-sample loss, which is loss for valid expr\n   and invalid_expr_loss for invalid expr\n  final_loss_per_sample = softmax_loss_per_sample   All exprs are valid\n\n  avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n  seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n\n  total_training_loss = seq_likelihood_loss + avg_sample_loss\n  total_loss = total_training_loss + config.weight_decay * model.l2_reg\n\n   Train with Adam optimizer\n  solver = tf.train.AdamOptimizer()\n  gradients = solver.compute_gradients(total_loss)\n\n   Clip gradient by L2 norm\n  gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v)\n               for g, v in gradients]\n  solver_op = solver.apply_gradients(gradients)\n\n   Training operation\n  with tf.control_dependencies([solver_op]):\n    train_step = tf.constant(0)\n\n   Write summary to TensorBoard\n  log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n\n  loss_ph = tf.placeholder(tf.float32, [])\n  entropy_ph = tf.placeholder(tf.float32, [])\n  accuracy_ph = tf.placeholder(tf.float32, [])\n  summary_train = [\n      tf.summary.scalar('avg_sample_loss', loss_ph),\n      tf.summary.scalar('entropy', entropy_ph),\n      tf.summary.scalar('avg_accuracy', accuracy_ph)\n  ]\n  log_step_train = tf.summary.merge(summary_train)\n\n   Training\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  snapshot_saver = tf.train.Saver(max_to_keep=None)   keep all snapshots\n  show_all_variables()\n\n  avg_accuracy = 0\n  accuracy_decay = 0.99\n  for n_iter, batch in enumerate(data_reader_train.batches()):\n    if n_iter >= config.max_iter:\n      break\n\n     set up input and output tensors\n    h = sess.partial_run_setup(\n        fetches=[\n            model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss,\n            train_step\n        ],\n        feeds=[\n            text_seq_batch, seq_len_batch, gt_layout_batch,\n            compiler.loom_input_tensor, ans_label_batch\n        ])\n\n     Part 1: Generate module layout\n    tokens, entropy_reg_val = sess.partial_run(\n        h,\n        fetches=(model.predicted_tokens, model.entropy_reg),\n        feed_dict={\n            text_seq_batch: batch['input_seq_batch'],\n            seq_len_batch: batch['seq_len_batch'],\n            gt_layout_batch: batch['gt_layout_batch']\n        })\n     Assemble the layout tokens into network structure\n    expr_list, expr_validity_array = assembler.assemble(tokens)\n     all exprs should be valid (since they are ground-truth)\n    assert np.all(expr_validity_array)\n    labels = batch['ans_label_batch']\n     Build TensorFlow Fold input for NMN\n    expr_feed = compiler.build_feed_dict(expr_list)\n    expr_feed[ans_label_batch] = labels\n\n     Part 2: Run NMN and learning steps\n    scores_val, avg_sample_loss_val, _ = sess.partial_run(\n        h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n\n     Compute accuracy\n    predictions = np.argmax(scores_val, axis=1)\n    accuracy = np.mean(\n        np.logical_and(expr_validity_array, predictions == labels))\n    avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n\n     Add to TensorBoard summary\n    if (n_iter + 1) % config.log_interval == 0:\n      tf.logging.info('iter = %d\\n\\t'\n                      'loss = %f, accuracy (cur) = %f, '\n                      'accuracy (avg) = %f, entropy = %f' %\n                      (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy,\n                       -entropy_reg_val))\n      summary = sess.run(\n          fetches=log_step_train,\n          feed_dict={\n              loss_ph: avg_sample_loss_val,\n              entropy_ph: -entropy_reg_val,\n              accuracy_ph: avg_accuracy\n          })\n      log_writer.add_summary(summary, n_iter + 1)\n\n     Save snapshot\n    if (n_iter + 1) % config.snapshot_interval == 0:\n      snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n      snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n      tf.logging.info('Snapshot saved to %s' % snapshot_file)\n\n  tf.logging.info('Run finished.')\n\n\nif __name__ == '__main__':\n  config_raw, unparsed = get_config()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "comments": "  copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       update t encoder according data    network inputs    the model training    loss function    the final per sample loss  loss valid expr    invalid expr loss invalid expr    all exprs valid    train adam optimizer    clip gradient l2 norm    training operation    write summary tensorboard    training    keep snapshots    set input output tensors    part 1  generate module layout    assemble layout tokens network structure    exprs valid (since ground truth)    build tensorflow fold input nmn    part 2  run nmn learning steps    compute accuracy    add tensorboard summary    save snapshot ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nsys.path.append(os.path.abspath(os.path.join(__file__, '../../')))\nimport numpy as np\nimport tensorflow as tf\nfrom config import get_config\nfrom model_n2nmn.assembler import Assembler\nfrom model_n2nmn.model import Model\nfrom util.data_reader import DataReader\nfrom util.data_reader import SampleBuilder\nfrom util.misc import prepare_dirs_and_logger\nfrom util.misc import save_config\nfrom util.misc import show_all_variables\n\n\ndef main(_):\n  config = prepare_dirs_and_logger(config_raw)\n  save_config(config)\n\n  rng = np.random.RandomState(config.random_seed)\n  tf.set_random_seed(config.random_seed)\n  config.rng = rng\n\n  config.module_names = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n  config.gt_layout_tokens = ['_key_find', '_key_filter', '_val_desc', '<eos>']\n  assembler = Assembler(config)\n\n  sample_builder = SampleBuilder(config)\n  config = sample_builder.config  # update T_encoder according to data\n  data_train = sample_builder.data_all['train']\n  data_reader_train = DataReader(\n      config, data_train, assembler, shuffle=True, one_pass=False)\n\n  num_vocab_txt = len(sample_builder.dict_all)\n  num_vocab_nmn = len(assembler.module_names)\n  num_choices = len(sample_builder.dict_all)\n\n  # Network inputs\n  text_seq_batch = tf.placeholder(tf.int32, [None, None])\n  seq_len_batch = tf.placeholder(tf.int32, [None])\n  ans_label_batch = tf.placeholder(tf.int32, [None])\n  use_gt_layout = tf.constant(True, dtype=tf.bool)\n  gt_layout_batch = tf.placeholder(tf.int32, [None, None])\n\n  # The model for training\n  model = Model(\n      config,\n      sample_builder.kb,\n      text_seq_batch,\n      seq_len_batch,\n      num_vocab_txt=num_vocab_txt,\n      num_vocab_nmn=num_vocab_nmn,\n      EOS_idx=assembler.EOS_idx,\n      num_choices=num_choices,\n      decoder_sampling=True,\n      use_gt_layout=use_gt_layout,\n      gt_layout_batch=gt_layout_batch)\n  compiler = model.compiler\n  scores = model.scores\n  log_seq_prob = model.log_seq_prob\n\n  # Loss function\n  softmax_loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=scores, labels=ans_label_batch)\n  # The final per-sample loss, which is loss for valid expr\n  # and invalid_expr_loss for invalid expr\n  final_loss_per_sample = softmax_loss_per_sample  # All exprs are valid\n\n  avg_sample_loss = tf.reduce_mean(final_loss_per_sample)\n  seq_likelihood_loss = tf.reduce_mean(-log_seq_prob)\n\n  total_training_loss = seq_likelihood_loss + avg_sample_loss\n  total_loss = total_training_loss + config.weight_decay * model.l2_reg\n\n  # Train with Adam optimizer\n  solver = tf.train.AdamOptimizer()\n  gradients = solver.compute_gradients(total_loss)\n\n  # Clip gradient by L2 norm\n  gradients = [(tf.clip_by_norm(g, config.max_grad_norm), v)\n               for g, v in gradients]\n  solver_op = solver.apply_gradients(gradients)\n\n  # Training operation\n  with tf.control_dependencies([solver_op]):\n    train_step = tf.constant(0)\n\n  # Write summary to TensorBoard\n  log_writer = tf.summary.FileWriter(config.log_dir, tf.get_default_graph())\n\n  loss_ph = tf.placeholder(tf.float32, [])\n  entropy_ph = tf.placeholder(tf.float32, [])\n  accuracy_ph = tf.placeholder(tf.float32, [])\n  summary_train = [\n      tf.summary.scalar('avg_sample_loss', loss_ph),\n      tf.summary.scalar('entropy', entropy_ph),\n      tf.summary.scalar('avg_accuracy', accuracy_ph)\n  ]\n  log_step_train = tf.summary.merge(summary_train)\n\n  # Training\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  snapshot_saver = tf.train.Saver(max_to_keep=None)  # keep all snapshots\n  show_all_variables()\n\n  avg_accuracy = 0\n  accuracy_decay = 0.99\n  for n_iter, batch in enumerate(data_reader_train.batches()):\n    if n_iter >= config.max_iter:\n      break\n\n    # set up input and output tensors\n    h = sess.partial_run_setup(\n        fetches=[\n            model.predicted_tokens, model.entropy_reg, scores, avg_sample_loss,\n            train_step\n        ],\n        feeds=[\n            text_seq_batch, seq_len_batch, gt_layout_batch,\n            compiler.loom_input_tensor, ans_label_batch\n        ])\n\n    # Part 1: Generate module layout\n    tokens, entropy_reg_val = sess.partial_run(\n        h,\n        fetches=(model.predicted_tokens, model.entropy_reg),\n        feed_dict={\n            text_seq_batch: batch['input_seq_batch'],\n            seq_len_batch: batch['seq_len_batch'],\n            gt_layout_batch: batch['gt_layout_batch']\n        })\n    # Assemble the layout tokens into network structure\n    expr_list, expr_validity_array = assembler.assemble(tokens)\n    # all exprs should be valid (since they are ground-truth)\n    assert np.all(expr_validity_array)\n    labels = batch['ans_label_batch']\n    # Build TensorFlow Fold input for NMN\n    expr_feed = compiler.build_feed_dict(expr_list)\n    expr_feed[ans_label_batch] = labels\n\n    # Part 2: Run NMN and learning steps\n    scores_val, avg_sample_loss_val, _ = sess.partial_run(\n        h, fetches=(scores, avg_sample_loss, train_step), feed_dict=expr_feed)\n\n    # Compute accuracy\n    predictions = np.argmax(scores_val, axis=1)\n    accuracy = np.mean(\n        np.logical_and(expr_validity_array, predictions == labels))\n    avg_accuracy += (1 - accuracy_decay) * (accuracy - avg_accuracy)\n\n    # Add to TensorBoard summary\n    if (n_iter + 1) % config.log_interval == 0:\n      tf.logging.info('iter = %d\\n\\t'\n                      'loss = %f, accuracy (cur) = %f, '\n                      'accuracy (avg) = %f, entropy = %f' %\n                      (n_iter + 1, avg_sample_loss_val, accuracy, avg_accuracy,\n                       -entropy_reg_val))\n      summary = sess.run(\n          fetches=log_step_train,\n          feed_dict={\n              loss_ph: avg_sample_loss_val,\n              entropy_ph: -entropy_reg_val,\n              accuracy_ph: avg_accuracy\n          })\n      log_writer.add_summary(summary, n_iter + 1)\n\n    # Save snapshot\n    if (n_iter + 1) % config.snapshot_interval == 0:\n      snapshot_file = os.path.join(config.model_dir, '%08d' % (n_iter + 1))\n      snapshot_saver.save(sess, snapshot_file, write_meta_graph=False)\n      tf.logging.info('Snapshot saved to %s' % snapshot_file)\n\n  tf.logging.info('Run finished.')\n\n\nif __name__ == '__main__':\n  config_raw, unparsed = get_config()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "description": "Models and examples built with TensorFlow", "file_name": "train_gt_layout.py", "id": "a0b6cee65712e87a5bd12d5866ade663", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/qa_kg/exp_1_hop/train_gt_layout.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}