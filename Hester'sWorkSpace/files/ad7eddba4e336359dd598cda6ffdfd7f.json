{"author": "donnemartin", "code": "# -*- coding: utf-8 -*-\n\nclass PagesDataStore(object):\n\n    def __init__(self, db):\n        self.db = db\n        pass\n\n    def add_link_to_crawl(self, url):\n        \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n        pass\n\n    def remove_link_to_crawl(self, url):\n        \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n        pass\n\n    def reduce_priority_link_to_crawl(self, url):\n        \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n        pass\n\n    def extract_max_priority_page(self):\n        \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n        pass\n\n    def insert_crawled_link(self, url, signature):\n        \"\"\"Add the given link to `crawled_links`.\"\"\"\n        pass\n\n    def crawled_similar(self, signature):\n        \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n        pass\n\n\nclass Page(object):\n\n    def __init__(self, url, contents, child_urls):\n        self.url = url\n        self.contents = contents\n        self.child_urls = child_urls\n        self.signature = self.create_signature()\n\n    def create_signature(self):\n        \n        pass\n\n\nclass Crawler(object):\n\n    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n        self.pages = pages\n        self.data_store = data_store\n        self.reverse_index_queue = reverse_index_queue\n        self.doc_index_queue = doc_index_queue\n\n    def crawl_page(self, page):\n        for url in page.child_urls:\n            self.data_store.add_link_to_crawl(url)\n        self.reverse_index_queue.generate(page)\n        self.doc_index_queue.generate(page)\n        self.data_store.remove_link_to_crawl(page.url)\n        self.data_store.insert_crawled_link(page.url, page.signature)\n\n    def crawl(self):\n        while True:\n            page = self.data_store.extract_max_priority_page()\n            if page is None:\n                break\n            if self.data_store.crawled_similar(page.signature):\n                self.data_store.reduce_priority_link_to_crawl(page.url)\n            else:\n                self.crawl_page(page)\n            page = self.data_store.extract_max_priority_page()\n", "comments": "   add given link  links crawl              pass      def remove link crawl(self  url)             remove given link  links crawl              pass      def reduce priority link crawl(self  url)             reduce priority link  links crawl  avoid cycles             pass      def extract max priority page(self)             return highest priority link  links crawl              pass      def insert crawled link(self  url  signature)             add given link  crawled links              pass      def crawled similar(self  signature)             determine already crawled page matching given signature           coding  utf 8        create signature based url contents ", "content": "# -*- coding: utf-8 -*-\n\nclass PagesDataStore(object):\n\n    def __init__(self, db):\n        self.db = db\n        pass\n\n    def add_link_to_crawl(self, url):\n        \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n        pass\n\n    def remove_link_to_crawl(self, url):\n        \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n        pass\n\n    def reduce_priority_link_to_crawl(self, url):\n        \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n        pass\n\n    def extract_max_priority_page(self):\n        \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n        pass\n\n    def insert_crawled_link(self, url, signature):\n        \"\"\"Add the given link to `crawled_links`.\"\"\"\n        pass\n\n    def crawled_similar(self, signature):\n        \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n        pass\n\n\nclass Page(object):\n\n    def __init__(self, url, contents, child_urls):\n        self.url = url\n        self.contents = contents\n        self.child_urls = child_urls\n        self.signature = self.create_signature()\n\n    def create_signature(self):\n        # Create signature based on url and contents\n        pass\n\n\nclass Crawler(object):\n\n    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n        self.pages = pages\n        self.data_store = data_store\n        self.reverse_index_queue = reverse_index_queue\n        self.doc_index_queue = doc_index_queue\n\n    def crawl_page(self, page):\n        for url in page.child_urls:\n            self.data_store.add_link_to_crawl(url)\n        self.reverse_index_queue.generate(page)\n        self.doc_index_queue.generate(page)\n        self.data_store.remove_link_to_crawl(page.url)\n        self.data_store.insert_crawled_link(page.url, page.signature)\n\n    def crawl(self):\n        while True:\n            page = self.data_store.extract_max_priority_page()\n            if page is None:\n                break\n            if self.data_store.crawled_similar(page.signature):\n                self.data_store.reduce_priority_link_to_crawl(page.url)\n            else:\n                self.crawl_page(page)\n            page = self.data_store.extract_max_priority_page()\n", "description": "Learn how to design large-scale systems. Prep for the system design interview.  Includes Anki flashcards.", "file_name": "web_crawler_snippets.py", "id": "ad7eddba4e336359dd598cda6ffdfd7f", "language": "Python", "project_name": "system-design-primer", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/donnemartin-system-design-primer/donnemartin-system-design-primer-38a93dc/solutions/system_design/web_crawler/web_crawler_snippets.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:58:48Z", "url": "https://github.com/donnemartin/system-design-primer", "wiki": true}