{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Generate samples from the MaskGAN.\n\nLaunch command:\n  python generate_samples.py\n  --data_dir=/tmp/data/imdb  --data_set=imdb\n  --batch_size=256 --sequence_length=20 --base_directory=/tmp/imdb\n  --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,\n  gen_vd_keep_prob=1.0\" --generator_model=seq2seq_vd\n  --discriminator_model=seq2seq_vd --is_present_rate=0.5\n  --maskgan_ckpt=/tmp/model.ckpt-45494\n  --seq2seq_share_embedding=True --dis_share_embedding=True\n  --attention_option=luong --mask_strategy=contiguous --baseline_method=critic\n  --number_epochs=4\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\nimport os\n Dependency imports\n\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\n\nimport train_mask_gan\nfrom data import imdb_loader\nfrom data import ptb_loader\n\n Data.\nfrom model_utils import helper\nfrom model_utils import model_utils\n\nSAMPLE_TRAIN = 'TRAIN'\nSAMPLE_VALIDATION = 'VALIDATION'\n\n Sample Generation.\n Binary and setup FLAGS.\ntf.app.flags.DEFINE_enum('sample_mode', 'TRAIN',\n                         [SAMPLE_TRAIN, SAMPLE_VALIDATION],\n                         'Dataset to sample from.')\ntf.app.flags.DEFINE_string('output_path', '/tmp', 'Model output directory.')\ntf.app.flags.DEFINE_boolean(\n    'output_masked_logs', False,\n    'Whether to display for human evaluation (show masking).')\ntf.app.flags.DEFINE_integer('number_epochs', 1,\n                            'The number of epochs to produce.')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef get_iterator(data):\n  \"\"\"Return the data iterator.\"\"\"\n  if FLAGS.data_set == 'ptb':\n    iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,\n                                       FLAGS.sequence_length,\n                                       FLAGS.epoch_size_override)\n  elif FLAGS.data_set == 'imdb':\n    iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,\n                                         FLAGS.sequence_length)\n  return iterator\n\n\ndef convert_to_human_readable(id_to_word, arr, p, max_num_to_print):\n  \"\"\"Convert a np.array of indices into words using id_to_word dictionary.\n  Return max_num_to_print results.\n  \"\"\"\n\n  assert arr.ndim == 2\n\n  samples = []\n  for sequence_id in xrange(min(len(arr), max_num_to_print)):\n    sample = []\n    for i, index in enumerate(arr[sequence_id, :]):\n      if p[sequence_id, i] == 1:\n        sample.append(str(id_to_word[index]))\n      else:\n        sample.append('*' + str(id_to_word[index]))\n    buffer_str = ' '.join(sample)\n    samples.append(buffer_str)\n  return samples\n\n\ndef write_unmasked_log(log, id_to_word, sequence_eval):\n  \"\"\"Helper function for logging evaluated sequences without mask.\"\"\"\n  indices_arr = np.asarray(sequence_eval)\n  samples = helper.convert_to_human_readable(id_to_word, indices_arr,\n                                             FLAGS.batch_size)\n  for sample in samples:\n    log.write(sample + '\\n')\n  log.flush()\n  return samples\n\n\ndef write_masked_log(log, id_to_word, sequence_eval, present_eval):\n  indices_arr = np.asarray(sequence_eval)\n  samples = convert_to_human_readable(id_to_word, indices_arr, present_eval,\n                                      FLAGS.batch_size)\n  for sample in samples:\n    log.write(sample + '\\n')\n  log.flush()\n  return samples\n\n\ndef generate_logs(sess, model, log, id_to_word, feed):\n  \"\"\"Impute Sequences using the model for a particular feed and send it to\n  logs.\n  \"\"\"\n   Impute Sequences.\n  [p, inputs_eval, sequence_eval] = sess.run(\n      [model.present, model.inputs, model.fake_sequence], feed_dict=feed)\n\n   Add the 0th time-step for coherence.\n  first_token = np.expand_dims(inputs_eval[:, 0], axis=1)\n  sequence_eval = np.concatenate((first_token, sequence_eval), axis=1)\n\n   0th token always present.\n  p = np.concatenate((np.ones((FLAGS.batch_size, 1)), p), axis=1)\n\n  if FLAGS.output_masked_logs:\n    samples = write_masked_log(log, id_to_word, sequence_eval, p)\n  else:\n    samples = write_unmasked_log(log, id_to_word, sequence_eval)\n  return samples\n\n\ndef generate_samples(hparams, data, id_to_word, log_dir, output_file):\n  \"\"\"\"Generate samples.\n\n    Args:\n      hparams:  Hyperparameters for the MaskGAN.\n      data: Data to evaluate.\n      id_to_word: Dictionary of indices to words.\n      log_dir: Log directory.\n      output_file:  Output file for the samples.\n  \"\"\"\n   Boolean indicating operational mode.\n  is_training = False\n\n   Set a random seed to keep fixed mask.\n  np.random.seed(0)\n\n  with tf.Graph().as_default():\n     Construct the model.\n    model = train_mask_gan.create_MaskGAN(hparams, is_training)\n\n     Retrieve the initial savers.\n    init_savers = model_utils.retrieve_init_savers(hparams)\n\n     Initial saver function to supervisor.\n    init_fn = partial(model_utils.init_fn, init_savers)\n\n    is_chief = FLAGS.task == 0\n\n     Create the supervisor.  It will take care of initialization, summaries,\n     checkpoints, and recovery.\n    sv = tf.Supervisor(\n        logdir=log_dir,\n        is_chief=is_chief,\n        saver=model.saver,\n        global_step=model.global_step,\n        recovery_wait_secs=30,\n        summary_op=None,\n        init_fn=init_fn)\n\n     Get an initialized, and possibly recovered session.  Launch the\n     services: Checkpointing, Summaries, step counting.\n    \n     When multiple replicas of this program are running the services are\n     only launched by the 'chief' replica.\n    with sv.managed_session(\n        FLAGS.master, start_standard_services=False) as sess:\n\n       Generator statefulness over the epoch.\n      [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(\n          [model.eval_initial_state, model.fake_gen_initial_state])\n\n      for n in xrange(FLAGS.number_epochs):\n        print('Epoch number: %d' % n)\n         print('Percent done: %.2f' % float(n) / float(FLAGS.number_epochs))\n        iterator = get_iterator(data)\n        for x, y, _ in iterator:\n          if FLAGS.eval_language_model:\n            is_present_rate = 0.\n          else:\n            is_present_rate = FLAGS.is_present_rate\n          tf.logging.info(\n              'Evaluating on is_present_rate=%.3f.' % is_present_rate)\n\n          model_utils.assign_percent_real(sess, model.percent_real_update,\n                                          model.new_rate, is_present_rate)\n\n           Randomly mask out tokens.\n          p = model_utils.generate_mask()\n\n          eval_feed = {model.inputs: x, model.targets: y, model.present: p}\n\n          if FLAGS.data_set == 'ptb':\n             Statefulness for *evaluation* Generator.\n            for i, (c, h) in enumerate(model.eval_initial_state):\n              eval_feed[c] = gen_initial_state_eval[i].c\n              eval_feed[h] = gen_initial_state_eval[i].h\n\n             Statefulness for the Generator.\n            for i, (c, h) in enumerate(model.fake_gen_initial_state):\n              eval_feed[c] = fake_gen_initial_state_eval[i].c\n              eval_feed[h] = fake_gen_initial_state_eval[i].h\n\n          [gen_initial_state_eval, fake_gen_initial_state_eval, _] = sess.run(\n              [\n                  model.eval_final_state, model.fake_gen_final_state,\n                  model.global_step\n              ],\n              feed_dict=eval_feed)\n\n          generate_logs(sess, model, output_file, id_to_word, eval_feed)\n      output_file.close()\n      print('Closing output_file.')\n      return\n\n\ndef main(_):\n  hparams = train_mask_gan.create_hparams()\n  log_dir = FLAGS.base_directory\n\n  tf.gfile.MakeDirs(FLAGS.output_path)\n  output_file = tf.gfile.GFile(\n      os.path.join(FLAGS.output_path, 'reviews.txt'), mode='w')\n\n   Load data set.\n  if FLAGS.data_set == 'ptb':\n    raw_data = ptb_loader.ptb_raw_data(FLAGS.data_dir)\n    train_data, valid_data, _, _ = raw_data\n  elif FLAGS.data_set == 'imdb':\n    raw_data = imdb_loader.imdb_raw_data(FLAGS.data_dir)\n    train_data, valid_data = raw_data\n  else:\n    raise NotImplementedError\n\n   Generating more data on train set.\n  if FLAGS.sample_mode == SAMPLE_TRAIN:\n    data_set = train_data\n  elif FLAGS.sample_mode == SAMPLE_VALIDATION:\n    data_set = valid_data\n  else:\n    raise NotImplementedError\n\n   Dictionary and reverse dictionry.\n  if FLAGS.data_set == 'ptb':\n    word_to_id = ptb_loader.build_vocab(\n        os.path.join(FLAGS.data_dir, 'ptb.train.txt'))\n  elif FLAGS.data_set == 'imdb':\n    word_to_id = imdb_loader.build_vocab(\n        os.path.join(FLAGS.data_dir, 'vocab.txt'))\n  id_to_word = {v: k for k, v in word_to_id.iteritems()}\n\n  FLAGS.vocab_size = len(id_to_word)\n  print('Vocab size: %d' % FLAGS.vocab_size)\n\n  generate_samples(hparams, data_set, id_to_word, log_dir, output_file)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   generate samples maskgan   launch command    python generate samples py     data dir  tmp data imdb    data set imdb     batch size 256   sequence length 20   base directory  tmp imdb     hparams  gen rnn size 650 dis rnn size 650 gen num layers 2    gen vd keep prob 1 0    generator model seq2seq vd     discriminator model seq2seq vd   present rate 0 5     maskgan ckpt  tmp model ckpt 45494     seq2seq share embedding true   dis share embedding true     attention option luong   mask strategy contiguous   baseline method critic     number epochs 4        future   import absolute import   future   import division   future   import print function  functools import partial import os   dependency imports  import numpy np six moves import xrange import tensorflow tf  import train mask gan data import imdb loader data import ptb loader    data  model utils import helper model utils import model utils  sample train    train  sample validation    validation      sample generation     binary setup flags  tf app flags define enum( sample mode    train                             sample train  sample validation                             dataset sample  ) tf app flags define string( output path     tmp    model output directory  ) tf app flags define boolean(      output masked logs   false       whether display human evaluation (show masking)  ) tf app flags define integer( number epochs   1                               the number epochs produce  )  flags   tf app flags flags   def get iterator(data)       return data iterator       flags data set     ptb       iterator   ptb loader ptb iterator(data  flags batch size                                         flags sequence length                                         flags epoch size override)   elif flags data set     imdb       iterator   imdb loader imdb iterator(data  flags batch size                                           flags sequence length)   return iterator   def convert human readable(id word  arr  p  max num print)       convert np array indices words using id word dictionary    return max num print results           assert arr ndim    2    samples        sequence id xrange(min(len(arr)  max num print))      sample           index enumerate(arr sequence id    )        p sequence id      1          sample append(str(id word index ))       else          sample append(      str(id word index ))     buffer str       join(sample)     samples append(buffer str)   return samples   def write unmasked log(log  id word  sequence eval)       helper function logging evaluated sequences without mask       indices arr   np asarray(sequence eval)   samples   helper convert human readable(id word  indices arr                                               flags batch size)   sample samples      log write(sample     n )   log flush()   return samples   def write masked log(log  id word  sequence eval  present eval)    indices arr   np asarray(sequence eval)   samples   convert human readable(id word  indices arr  present eval                                        flags batch size)   sample samples      log write(sample     n )   log flush()   return samples   def generate logs(sess  model  log  id word  feed)       impute sequences using model particular feed send   logs            impute sequences     p  inputs eval  sequence eval    sess run(        model present  model inputs  model fake sequence   feed dict feed)      add 0th time step coherence    first token   np expand dims(inputs eval    0   axis 1)   sequence eval   np concatenate((first token  sequence eval)  axis 1)      0th token always present    p   np concatenate((np ones((flags batch size  1))  p)  axis 1)    flags output masked logs      samples   write masked log(log  id word  sequence eval  p)   else      samples   write unmasked log(log  id word  sequence eval)   return samples   def generate samples(hparams  data  id word  log dir  output file)        generate samples       args        hparams   hyperparameters maskgan        data  data evaluate        id word  dictionary indices words        log dir  log directory        output file   output file samples           copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       dependency imports    data      sample generation      binary setup flags     impute sequences     add 0th time step coherence     0th token always present     boolean indicating operational mode     set random seed keep fixed mask     construct model      retrieve initial savers      initial saver function supervisor     create supervisor   it take care initialization  summaries     checkpoints  recovery     get initialized  possibly recovered session   launch    services  checkpointing  summaries  step counting        when multiple replicas program running services    launched  chief  replica     generator statefulness epoch     print( percent done    2f    float(n)   float(flags number epochs))    randomly mask tokens     statefulness  evaluation  generator     statefulness generator     load data set     generating data train set     dictionary reverse dictionry  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Generate samples from the MaskGAN.\n\nLaunch command:\n  python generate_samples.py\n  --data_dir=/tmp/data/imdb  --data_set=imdb\n  --batch_size=256 --sequence_length=20 --base_directory=/tmp/imdb\n  --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,\n  gen_vd_keep_prob=1.0\" --generator_model=seq2seq_vd\n  --discriminator_model=seq2seq_vd --is_present_rate=0.5\n  --maskgan_ckpt=/tmp/model.ckpt-45494\n  --seq2seq_share_embedding=True --dis_share_embedding=True\n  --attention_option=luong --mask_strategy=contiguous --baseline_method=critic\n  --number_epochs=4\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\nimport os\n# Dependency imports\n\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\n\nimport train_mask_gan\nfrom data import imdb_loader\nfrom data import ptb_loader\n\n# Data.\nfrom model_utils import helper\nfrom model_utils import model_utils\n\nSAMPLE_TRAIN = 'TRAIN'\nSAMPLE_VALIDATION = 'VALIDATION'\n\n## Sample Generation.\n## Binary and setup FLAGS.\ntf.app.flags.DEFINE_enum('sample_mode', 'TRAIN',\n                         [SAMPLE_TRAIN, SAMPLE_VALIDATION],\n                         'Dataset to sample from.')\ntf.app.flags.DEFINE_string('output_path', '/tmp', 'Model output directory.')\ntf.app.flags.DEFINE_boolean(\n    'output_masked_logs', False,\n    'Whether to display for human evaluation (show masking).')\ntf.app.flags.DEFINE_integer('number_epochs', 1,\n                            'The number of epochs to produce.')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef get_iterator(data):\n  \"\"\"Return the data iterator.\"\"\"\n  if FLAGS.data_set == 'ptb':\n    iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,\n                                       FLAGS.sequence_length,\n                                       FLAGS.epoch_size_override)\n  elif FLAGS.data_set == 'imdb':\n    iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,\n                                         FLAGS.sequence_length)\n  return iterator\n\n\ndef convert_to_human_readable(id_to_word, arr, p, max_num_to_print):\n  \"\"\"Convert a np.array of indices into words using id_to_word dictionary.\n  Return max_num_to_print results.\n  \"\"\"\n\n  assert arr.ndim == 2\n\n  samples = []\n  for sequence_id in xrange(min(len(arr), max_num_to_print)):\n    sample = []\n    for i, index in enumerate(arr[sequence_id, :]):\n      if p[sequence_id, i] == 1:\n        sample.append(str(id_to_word[index]))\n      else:\n        sample.append('*' + str(id_to_word[index]))\n    buffer_str = ' '.join(sample)\n    samples.append(buffer_str)\n  return samples\n\n\ndef write_unmasked_log(log, id_to_word, sequence_eval):\n  \"\"\"Helper function for logging evaluated sequences without mask.\"\"\"\n  indices_arr = np.asarray(sequence_eval)\n  samples = helper.convert_to_human_readable(id_to_word, indices_arr,\n                                             FLAGS.batch_size)\n  for sample in samples:\n    log.write(sample + '\\n')\n  log.flush()\n  return samples\n\n\ndef write_masked_log(log, id_to_word, sequence_eval, present_eval):\n  indices_arr = np.asarray(sequence_eval)\n  samples = convert_to_human_readable(id_to_word, indices_arr, present_eval,\n                                      FLAGS.batch_size)\n  for sample in samples:\n    log.write(sample + '\\n')\n  log.flush()\n  return samples\n\n\ndef generate_logs(sess, model, log, id_to_word, feed):\n  \"\"\"Impute Sequences using the model for a particular feed and send it to\n  logs.\n  \"\"\"\n  # Impute Sequences.\n  [p, inputs_eval, sequence_eval] = sess.run(\n      [model.present, model.inputs, model.fake_sequence], feed_dict=feed)\n\n  # Add the 0th time-step for coherence.\n  first_token = np.expand_dims(inputs_eval[:, 0], axis=1)\n  sequence_eval = np.concatenate((first_token, sequence_eval), axis=1)\n\n  # 0th token always present.\n  p = np.concatenate((np.ones((FLAGS.batch_size, 1)), p), axis=1)\n\n  if FLAGS.output_masked_logs:\n    samples = write_masked_log(log, id_to_word, sequence_eval, p)\n  else:\n    samples = write_unmasked_log(log, id_to_word, sequence_eval)\n  return samples\n\n\ndef generate_samples(hparams, data, id_to_word, log_dir, output_file):\n  \"\"\"\"Generate samples.\n\n    Args:\n      hparams:  Hyperparameters for the MaskGAN.\n      data: Data to evaluate.\n      id_to_word: Dictionary of indices to words.\n      log_dir: Log directory.\n      output_file:  Output file for the samples.\n  \"\"\"\n  # Boolean indicating operational mode.\n  is_training = False\n\n  # Set a random seed to keep fixed mask.\n  np.random.seed(0)\n\n  with tf.Graph().as_default():\n    # Construct the model.\n    model = train_mask_gan.create_MaskGAN(hparams, is_training)\n\n    ## Retrieve the initial savers.\n    init_savers = model_utils.retrieve_init_savers(hparams)\n\n    ## Initial saver function to supervisor.\n    init_fn = partial(model_utils.init_fn, init_savers)\n\n    is_chief = FLAGS.task == 0\n\n    # Create the supervisor.  It will take care of initialization, summaries,\n    # checkpoints, and recovery.\n    sv = tf.Supervisor(\n        logdir=log_dir,\n        is_chief=is_chief,\n        saver=model.saver,\n        global_step=model.global_step,\n        recovery_wait_secs=30,\n        summary_op=None,\n        init_fn=init_fn)\n\n    # Get an initialized, and possibly recovered session.  Launch the\n    # services: Checkpointing, Summaries, step counting.\n    #\n    # When multiple replicas of this program are running the services are\n    # only launched by the 'chief' replica.\n    with sv.managed_session(\n        FLAGS.master, start_standard_services=False) as sess:\n\n      # Generator statefulness over the epoch.\n      [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(\n          [model.eval_initial_state, model.fake_gen_initial_state])\n\n      for n in xrange(FLAGS.number_epochs):\n        print('Epoch number: %d' % n)\n        # print('Percent done: %.2f' % float(n) / float(FLAGS.number_epochs))\n        iterator = get_iterator(data)\n        for x, y, _ in iterator:\n          if FLAGS.eval_language_model:\n            is_present_rate = 0.\n          else:\n            is_present_rate = FLAGS.is_present_rate\n          tf.logging.info(\n              'Evaluating on is_present_rate=%.3f.' % is_present_rate)\n\n          model_utils.assign_percent_real(sess, model.percent_real_update,\n                                          model.new_rate, is_present_rate)\n\n          # Randomly mask out tokens.\n          p = model_utils.generate_mask()\n\n          eval_feed = {model.inputs: x, model.targets: y, model.present: p}\n\n          if FLAGS.data_set == 'ptb':\n            # Statefulness for *evaluation* Generator.\n            for i, (c, h) in enumerate(model.eval_initial_state):\n              eval_feed[c] = gen_initial_state_eval[i].c\n              eval_feed[h] = gen_initial_state_eval[i].h\n\n            # Statefulness for the Generator.\n            for i, (c, h) in enumerate(model.fake_gen_initial_state):\n              eval_feed[c] = fake_gen_initial_state_eval[i].c\n              eval_feed[h] = fake_gen_initial_state_eval[i].h\n\n          [gen_initial_state_eval, fake_gen_initial_state_eval, _] = sess.run(\n              [\n                  model.eval_final_state, model.fake_gen_final_state,\n                  model.global_step\n              ],\n              feed_dict=eval_feed)\n\n          generate_logs(sess, model, output_file, id_to_word, eval_feed)\n      output_file.close()\n      print('Closing output_file.')\n      return\n\n\ndef main(_):\n  hparams = train_mask_gan.create_hparams()\n  log_dir = FLAGS.base_directory\n\n  tf.gfile.MakeDirs(FLAGS.output_path)\n  output_file = tf.gfile.GFile(\n      os.path.join(FLAGS.output_path, 'reviews.txt'), mode='w')\n\n  # Load data set.\n  if FLAGS.data_set == 'ptb':\n    raw_data = ptb_loader.ptb_raw_data(FLAGS.data_dir)\n    train_data, valid_data, _, _ = raw_data\n  elif FLAGS.data_set == 'imdb':\n    raw_data = imdb_loader.imdb_raw_data(FLAGS.data_dir)\n    train_data, valid_data = raw_data\n  else:\n    raise NotImplementedError\n\n  # Generating more data on train set.\n  if FLAGS.sample_mode == SAMPLE_TRAIN:\n    data_set = train_data\n  elif FLAGS.sample_mode == SAMPLE_VALIDATION:\n    data_set = valid_data\n  else:\n    raise NotImplementedError\n\n  # Dictionary and reverse dictionry.\n  if FLAGS.data_set == 'ptb':\n    word_to_id = ptb_loader.build_vocab(\n        os.path.join(FLAGS.data_dir, 'ptb.train.txt'))\n  elif FLAGS.data_set == 'imdb':\n    word_to_id = imdb_loader.build_vocab(\n        os.path.join(FLAGS.data_dir, 'vocab.txt'))\n  id_to_word = {v: k for k, v in word_to_id.iteritems()}\n\n  FLAGS.vocab_size = len(id_to_word)\n  print('Vocab size: %d' % FLAGS.vocab_size)\n\n  generate_samples(hparams, data_set, id_to_word, log_dir, output_file)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "generate_samples.py", "id": "e7807d9de1ecc711ef239a81aa95c78c", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/maskgan/generate_samples.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}