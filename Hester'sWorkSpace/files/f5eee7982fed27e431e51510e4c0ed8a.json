{"author": "rushter", "code": "from sklearn.metrics import roc_auc_score\n\nfrom mla.ensemble import RandomForestClassifier\nfrom mla.ensemble.gbm import GradientBoostingClassifier\nfrom mla.knn import KNNClassifier\nfrom mla.linear_models import LogisticRegression\nfrom mla.metrics import accuracy\nfrom mla.naive_bayes import NaiveBayesClassifier\nfrom mla.neuralnet import NeuralNet\nfrom mla.neuralnet.constraints import MaxNorm\nfrom mla.neuralnet.layers import Activation, Dense, Dropout\nfrom mla.neuralnet.optimizers import Adadelta\nfrom mla.neuralnet.parameters import Parameters\nfrom mla.neuralnet.regularizers import L2\nfrom mla.svm.kernerls import RBF, Linear\nfrom mla.svm.svm import SVM\nfrom mla.utils import one_hot\n\ntry:\n    from sklearn.model_selection import train_test_split\nexcept ImportError:\n    from sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import make_classification\n\n\nX, y = make_classification(n_samples=750, n_features=10,\n                           n_informative=8, random_state=1111,\n                           n_classes=2, class_sep=2.5, n_redundant=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12,\n                                                    random_state=1111)\n\n\n\n\ndef test_linear_model():\n    model = LogisticRegression(lr=0.01, max_iters=500, penalty='l1', C=0.01)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_random_forest():\n    model = RandomForestClassifier(n_estimators=10, max_depth=4)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)[:, 1]\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_svm_classification():\n    y_signed_train = (y_train * 2) - 1\n    y_signed_test = (y_test * 2) - 1\n\n    for kernel in [RBF(gamma=0.1), Linear()]:\n        model = SVM(max_iter=250, kernel=kernel)\n        model.fit(X_train, y_signed_train)\n        predictions = model.predict(X_test)\n        assert accuracy(y_signed_test, predictions) >= 0.8\n\n\ndef test_mlp():\n    y_train_onehot = one_hot(y_train)\n    y_test_onehot = one_hot(y_test)\n\n    model = NeuralNet(\n        layers=[\n            Dense(256, Parameters(init='uniform', regularizers={'W': L2(0.05)})),\n            Activation('relu'),\n            Dropout(0.5),\n            Dense(128, Parameters(init='normal', constraints={'W': MaxNorm()})),\n            Activation('relu'),\n            Dense(2),\n            Activation('softmax'),\n        ],\n        loss='categorical_crossentropy',\n        optimizer=Adadelta(),\n        metric='accuracy',\n        batch_size=64,\n        max_epochs=25,\n\n    )\n    model.fit(X_train, y_train_onehot)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test_onehot[:, 0], predictions[:, 0]) >= 0.95\n\n\ndef test_gbm():\n    model = GradientBoostingClassifier(n_estimators=25, max_depth=3,\n                                       max_features=5, learning_rate=0.1)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_naive_bayes():\n    model = NaiveBayesClassifier()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)[:, 1]\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_knn():\n    clf = KNNClassifier(k=5)\n\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    assert accuracy(y_test, predictions) >= 0.95\n", "comments": "  generate random regression problem    all classifiers except convnet  rnn  lstm  ", "content": "from sklearn.metrics import roc_auc_score\n\nfrom mla.ensemble import RandomForestClassifier\nfrom mla.ensemble.gbm import GradientBoostingClassifier\nfrom mla.knn import KNNClassifier\nfrom mla.linear_models import LogisticRegression\nfrom mla.metrics import accuracy\nfrom mla.naive_bayes import NaiveBayesClassifier\nfrom mla.neuralnet import NeuralNet\nfrom mla.neuralnet.constraints import MaxNorm\nfrom mla.neuralnet.layers import Activation, Dense, Dropout\nfrom mla.neuralnet.optimizers import Adadelta\nfrom mla.neuralnet.parameters import Parameters\nfrom mla.neuralnet.regularizers import L2\nfrom mla.svm.kernerls import RBF, Linear\nfrom mla.svm.svm import SVM\nfrom mla.utils import one_hot\n\ntry:\n    from sklearn.model_selection import train_test_split\nexcept ImportError:\n    from sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import make_classification\n\n# Generate a random regression problem\nX, y = make_classification(n_samples=750, n_features=10,\n                           n_informative=8, random_state=1111,\n                           n_classes=2, class_sep=2.5, n_redundant=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12,\n                                                    random_state=1111)\n\n\n# All classifiers except convnet, RNN, LSTM.\n\ndef test_linear_model():\n    model = LogisticRegression(lr=0.01, max_iters=500, penalty='l1', C=0.01)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_random_forest():\n    model = RandomForestClassifier(n_estimators=10, max_depth=4)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)[:, 1]\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_svm_classification():\n    y_signed_train = (y_train * 2) - 1\n    y_signed_test = (y_test * 2) - 1\n\n    for kernel in [RBF(gamma=0.1), Linear()]:\n        model = SVM(max_iter=250, kernel=kernel)\n        model.fit(X_train, y_signed_train)\n        predictions = model.predict(X_test)\n        assert accuracy(y_signed_test, predictions) >= 0.8\n\n\ndef test_mlp():\n    y_train_onehot = one_hot(y_train)\n    y_test_onehot = one_hot(y_test)\n\n    model = NeuralNet(\n        layers=[\n            Dense(256, Parameters(init='uniform', regularizers={'W': L2(0.05)})),\n            Activation('relu'),\n            Dropout(0.5),\n            Dense(128, Parameters(init='normal', constraints={'W': MaxNorm()})),\n            Activation('relu'),\n            Dense(2),\n            Activation('softmax'),\n        ],\n        loss='categorical_crossentropy',\n        optimizer=Adadelta(),\n        metric='accuracy',\n        batch_size=64,\n        max_epochs=25,\n\n    )\n    model.fit(X_train, y_train_onehot)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test_onehot[:, 0], predictions[:, 0]) >= 0.95\n\n\ndef test_gbm():\n    model = GradientBoostingClassifier(n_estimators=25, max_depth=3,\n                                       max_features=5, learning_rate=0.1)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_naive_bayes():\n    model = NaiveBayesClassifier()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)[:, 1]\n    assert roc_auc_score(y_test, predictions) >= 0.95\n\n\ndef test_knn():\n    clf = KNNClassifier(k=5)\n\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    assert accuracy(y_test, predictions) >= 0.95\n", "description": "Minimal and clean examples of machine learning algorithms", "file_name": "test_classification_accuracy.py", "id": "f5eee7982fed27e431e51510e4c0ed8a", "language": "Python", "project_name": "MLAlgorithms", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/rushter-MLAlgorithms/rushter-MLAlgorithms-d398777/mla/tests/test_classification_accuracy.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:25:48Z", "url": "https://github.com/rushter/MLAlgorithms", "wiki": false}