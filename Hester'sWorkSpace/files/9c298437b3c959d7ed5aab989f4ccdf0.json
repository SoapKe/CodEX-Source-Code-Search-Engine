{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Runs a ResNet model on the CIFAR-10 dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom official.resnet import resnet\n\n_HEIGHT = 32\n_WIDTH = 32\n_NUM_CHANNELS = 3\n_DEFAULT_IMAGE_BYTES = _HEIGHT * _WIDTH * _NUM_CHANNELS\n The record is the image plus a one-byte label\n_RECORD_BYTES = _DEFAULT_IMAGE_BYTES + 1\n_NUM_CLASSES = 10\n_NUM_DATA_FILES = 5\n\n_NUM_IMAGES = {\n    'train': 50000,\n    'validation': 10000,\n}\n\n\n\n Data processing\n\ndef get_filenames(is_training, data_dir):\n  \"\"\"Returns a list of filenames.\"\"\"\n  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n\n  assert os.path.exists(data_dir), (\n      'Run cifar10_download_and_extract.py first to download and extract the '\n      'CIFAR-10 data.')\n\n  if is_training:\n    return [\n        os.path.join(data_dir, 'data_batch_%d.bin' % i)\n        for i in range(1, _NUM_DATA_FILES + 1)\n    ]\n  else:\n    return [os.path.join(data_dir, 'test_batch.bin')]\n\n\ndef parse_record(raw_record, is_training):\n  \"\"\"Parse CIFAR-10 image and label from a raw record.\"\"\"\n   Convert bytes to a vector of uint8 that is record_bytes long.\n  record_vector = tf.decode_raw(raw_record, tf.uint8)\n\n   The first byte represents the label, which we convert from uint8 to int32\n   and then to one-hot.\n  label = tf.cast(record_vector[0], tf.int32)\n  label = tf.one_hot(label, _NUM_CLASSES)\n\n   The remaining bytes after the label represent the image, which we reshape\n   from [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(record_vector[1:_RECORD_BYTES],\n                           [_NUM_CHANNELS, _HEIGHT, _WIDTH])\n\n   Convert from [depth, height, width] to [height, width, depth], and cast as\n   float32.\n  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n  image = preprocess_image(image, is_training)\n\n  return image, label\n\n\ndef preprocess_image(image, is_training):\n  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n  if is_training:\n     Resize the image to add four extra pixels on each side.\n    image = tf.image.resize_image_with_crop_or_pad(\n        image, _HEIGHT + 8, _WIDTH + 8)\n\n     Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _NUM_CHANNELS])\n\n     Randomly flip the image horizontally.\n    image = tf.image.random_flip_left_right(image)\n\n   Subtract off the mean and divide by the variance of the pixels.\n  image = tf.image.per_image_standardization(image)\n  return image\n\n\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1,\n             num_parallel_calls=1, multi_gpu=False):\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n    num_parallel_calls: The number of records that are processed in parallel.\n      This can be optimized per data set but for generally homogeneous data\n      sets, should be approximately the number of available CPU cores.\n    multi_gpu: Whether this is run multi-GPU. Note that this is only required\n      currently to handle the batch leftovers, and can be removed\n      when that is handled directly by Estimator.\n\n  Returns:\n    A dataset that can be used for iteration.\n  \"\"\"\n  filenames = get_filenames(is_training, data_dir)\n  dataset = tf.data.FixedLengthRecordDataset(filenames, _RECORD_BYTES)\n\n  num_images = is_training and _NUM_IMAGES['train'] or _NUM_IMAGES['validation']\n\n  return resnet.process_record_dataset(dataset, is_training, batch_size,\n      _NUM_IMAGES['train'], parse_record, num_epochs, num_parallel_calls,\n      examples_per_epoch=num_images, multi_gpu=multi_gpu)\n\n\ndef get_synth_input_fn():\n  return resnet.get_synth_input_fn(_HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)\n\n\n\n Running the model\n\nclass Cifar10Model(resnet.Model):\n\n  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,\n      version=resnet.DEFAULT_VERSION):\n    \"\"\"These are the parameters that work for CIFAR-10 data.\n\n    Args:\n      resnet_size: The number of convolutional layers needed in the model.\n      data_format: Either 'channels_first' or 'channels_last', specifying which\n        data format to use when setting up the model.\n      num_classes: The number of output classes needed from the model. This\n        enables users to extend the same model to their own datasets.\n      version: Integer representing which version of the ResNet network to use.\n        See README for details. Valid values: [1, 2]\n    \"\"\"\n    if resnet_size % 6 != 2:\n      raise ValueError('resnet_size must be 6n + 2:', resnet_size)\n\n    num_blocks = (resnet_size - 2) // 6\n\n    super(Cifar10Model, self).__init__(\n        resnet_size=resnet_size,\n        bottleneck=False,\n        num_classes=num_classes,\n        num_filters=16,\n        kernel_size=3,\n        conv_stride=1,\n        first_pool_size=None,\n        first_pool_stride=None,\n        second_pool_size=8,\n        second_pool_stride=1,\n        block_sizes=[num_blocks] * 3,\n        block_strides=[1, 2, 2],\n        final_size=64,\n        version=version,\n        data_format=data_format)\n\n\ndef cifar10_model_fn(features, labels, mode, params):\n  \"\"\"Model function for CIFAR-10.\"\"\"\n  features = tf.reshape(features, [-1, _HEIGHT, _WIDTH, _NUM_CHANNELS])\n\n  learning_rate_fn = resnet.learning_rate_with_decay(\n      batch_size=params['batch_size'], batch_denom=128,\n      num_images=_NUM_IMAGES['train'], boundary_epochs=[100, 150, 200],\n      decay_rates=[1, 0.1, 0.01, 0.001])\n\n   We use a weight decay of 0.0002, which performs better\n   than the 0.0001 that was originally suggested.\n  weight_decay = 2e-4\n\n   Empirical testing showed that including batch_normalization variables\n   in the calculation of regularized loss helped validation accuracy\n   for the CIFAR-10 dataset, perhaps because the regularization prevents\n   overfitting on the small data set. We therefore include all vars when\n   regularizing and computing loss during training.\n  def loss_filter_fn(name):\n    return True\n\n  return resnet.resnet_model_fn(features, labels, mode, Cifar10Model,\n                                resnet_size=params['resnet_size'],\n                                weight_decay=weight_decay,\n                                learning_rate_fn=learning_rate_fn,\n                                momentum=0.9,\n                                data_format=params['data_format'],\n                                version=params['version'],\n                                loss_filter_fn=loss_filter_fn,\n                                multi_gpu=params['multi_gpu'])\n\n\ndef main(unused_argv):\n  input_function = FLAGS.use_synthetic_data and get_synth_input_fn() or input_fn\n  resnet.resnet_main(FLAGS, cifar10_model_fn, input_function)\n\n\nif __name__ == '__main__':\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  parser = resnet.ResnetArgParser()\n   Set defaults that are reasonable for this model.\n  parser.set_defaults(data_dir='/tmp/cifar10_data',\n                      model_dir='/tmp/cifar10_model',\n                      resnet_size=32,\n                      train_epochs=250,\n                      epochs_per_eval=10,\n                      batch_size=128)\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n", "comments": "   runs resnet model cifar 10 dataset        future   import absolute import   future   import division   future   import print function  import os import sys  import tensorflow tf  official resnet import resnet   height   32  width   32  num channels   3  default image bytes    height    width    num channels   the record image plus one byte label  record bytes    default image bytes   1  num classes   10  num data files   5   num images          train   50000       validation   10000                                                                                        data processing                                                                                 def get filenames(is training  data dir)       returns list filenames       data dir   os path join(data dir   cifar 10 batches bin )    assert os path exists(data dir)  (        run cifar10 download extract py first download extract          cifar 10 data  )    training      return           os path join(data dir   data batch  bin    i)         range(1   num data files   1)         else      return  os path join(data dir   test batch bin )    def parse record(raw record  training)       parse cifar 10 image label raw record         convert bytes vector uint8 record bytes long    record vector   tf decode raw(raw record  tf uint8)      the first byte represents label  convert uint8 int32     one hot    label   tf cast(record vector 0   tf int32)   label   tf one hot(label   num classes)      the remaining bytes label represent image  reshape      depth   height   width   depth  height  width     depth major   tf reshape(record vector 1  record bytes                                num channels   height   width )      convert  depth  height  width   height  width  depth   cast     float32    image   tf cast(tf transpose(depth major   1  2  0 )  tf float32)    image   preprocess image(image  training)    return image  label   def preprocess image(image  training)       preprocess single image layout  height  width  depth        training        resize image add four extra pixels side      image   tf image resize image crop pad(         image   height   8   width   8)        randomly crop   height   width  section image      image   tf random crop(image    height   width   num channels )        randomly flip image horizontally      image   tf image random flip left right(image)      subtract mean divide variance pixels    image   tf image per image standardization(image)   return image   def input fn(is training  data dir  batch size  num epochs 1               num parallel calls 1  multi gpu false)       input fn using tf data input pipeline cifar 10 dataset     args      training  a boolean denoting whether input training      data dir  the directory containing input data      batch size  the number samples per batch      num epochs  the number epochs repeat dataset      num parallel calls  the number records processed parallel        this optimized per data set generally homogeneous data       sets  approximately number available cpu cores      multi gpu  whether run multi gpu  note required       currently handle batch leftovers  removed       handled directly estimator     returns      a dataset used iteration          filenames   get filenames(is training  data dir)   dataset   tf data fixedlengthrecorddataset(filenames   record bytes)    num images   training  num images  train    num images  validation      return resnet process record dataset(dataset  training  batch size         num images  train    parse record  num epochs  num parallel calls        examples per epoch num images  multi gpu multi gpu)   def get synth input fn()    return resnet get synth input fn( height   width   num channels   num classes)                                                                                     running model                                                                                 class cifar10model(resnet model)     def   init  (self  resnet size  data format none  num classes  num classes        version resnet default version)         these parameters work cifar 10 data       args        resnet size  the number convolutional layers needed model        data format  either  channels first   channels last   specifying         data format use setting model        num classes  the number output classes needed model  this         enables users extend model datasets        version  integer representing version resnet network use          see readme details  valid values   1  2              resnet size   6    2        raise valueerror( resnet size must 6n   2    resnet size)      num blocks   (resnet size   2)    6      super(cifar10model  self)   init  (         resnet size resnet size          bottleneck false          num classes num classes          num filters 16          kernel size 3          conv stride 1          first pool size none          first pool stride none          second pool size 8          second pool stride 1          block sizes  num blocks    3          block strides  1  2  2           final size 64          version version          data format data format)   def cifar10 model fn(features  labels  mode  params)       model function cifar 10        copyright 2017 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       the record image plus one byte label                                                                                     data processing                                                                                     convert bytes vector uint8 record bytes long     the first byte represents label  convert uint8 int32    one hot     the remaining bytes label represent image  reshape     depth   height   width   depth  height  width      convert  depth  height  width   height  width  depth   cast    float32     resize image add four extra pixels side     randomly crop   height   width  section image     randomly flip image horizontally     subtract mean divide variance pixels                                                                                      running model                                                                                     we use weight decay 0 0002  performs better    0 0001 originally suggested     empirical testing showed including batch normalization variables    calculation regularized loss helped validation accuracy    cifar 10 dataset  perhaps regularization prevents    overfitting small data set  we therefore include vars    regularizing computing loss training     set defaults reasonable model  ", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Runs a ResNet model on the CIFAR-10 dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom official.resnet import resnet\n\n_HEIGHT = 32\n_WIDTH = 32\n_NUM_CHANNELS = 3\n_DEFAULT_IMAGE_BYTES = _HEIGHT * _WIDTH * _NUM_CHANNELS\n# The record is the image plus a one-byte label\n_RECORD_BYTES = _DEFAULT_IMAGE_BYTES + 1\n_NUM_CLASSES = 10\n_NUM_DATA_FILES = 5\n\n_NUM_IMAGES = {\n    'train': 50000,\n    'validation': 10000,\n}\n\n\n###############################################################################\n# Data processing\n###############################################################################\ndef get_filenames(is_training, data_dir):\n  \"\"\"Returns a list of filenames.\"\"\"\n  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n\n  assert os.path.exists(data_dir), (\n      'Run cifar10_download_and_extract.py first to download and extract the '\n      'CIFAR-10 data.')\n\n  if is_training:\n    return [\n        os.path.join(data_dir, 'data_batch_%d.bin' % i)\n        for i in range(1, _NUM_DATA_FILES + 1)\n    ]\n  else:\n    return [os.path.join(data_dir, 'test_batch.bin')]\n\n\ndef parse_record(raw_record, is_training):\n  \"\"\"Parse CIFAR-10 image and label from a raw record.\"\"\"\n  # Convert bytes to a vector of uint8 that is record_bytes long.\n  record_vector = tf.decode_raw(raw_record, tf.uint8)\n\n  # The first byte represents the label, which we convert from uint8 to int32\n  # and then to one-hot.\n  label = tf.cast(record_vector[0], tf.int32)\n  label = tf.one_hot(label, _NUM_CLASSES)\n\n  # The remaining bytes after the label represent the image, which we reshape\n  # from [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(record_vector[1:_RECORD_BYTES],\n                           [_NUM_CHANNELS, _HEIGHT, _WIDTH])\n\n  # Convert from [depth, height, width] to [height, width, depth], and cast as\n  # float32.\n  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n  image = preprocess_image(image, is_training)\n\n  return image, label\n\n\ndef preprocess_image(image, is_training):\n  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n  if is_training:\n    # Resize the image to add four extra pixels on each side.\n    image = tf.image.resize_image_with_crop_or_pad(\n        image, _HEIGHT + 8, _WIDTH + 8)\n\n    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _NUM_CHANNELS])\n\n    # Randomly flip the image horizontally.\n    image = tf.image.random_flip_left_right(image)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  image = tf.image.per_image_standardization(image)\n  return image\n\n\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1,\n             num_parallel_calls=1, multi_gpu=False):\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n    num_parallel_calls: The number of records that are processed in parallel.\n      This can be optimized per data set but for generally homogeneous data\n      sets, should be approximately the number of available CPU cores.\n    multi_gpu: Whether this is run multi-GPU. Note that this is only required\n      currently to handle the batch leftovers, and can be removed\n      when that is handled directly by Estimator.\n\n  Returns:\n    A dataset that can be used for iteration.\n  \"\"\"\n  filenames = get_filenames(is_training, data_dir)\n  dataset = tf.data.FixedLengthRecordDataset(filenames, _RECORD_BYTES)\n\n  num_images = is_training and _NUM_IMAGES['train'] or _NUM_IMAGES['validation']\n\n  return resnet.process_record_dataset(dataset, is_training, batch_size,\n      _NUM_IMAGES['train'], parse_record, num_epochs, num_parallel_calls,\n      examples_per_epoch=num_images, multi_gpu=multi_gpu)\n\n\ndef get_synth_input_fn():\n  return resnet.get_synth_input_fn(_HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)\n\n\n###############################################################################\n# Running the model\n###############################################################################\nclass Cifar10Model(resnet.Model):\n\n  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,\n      version=resnet.DEFAULT_VERSION):\n    \"\"\"These are the parameters that work for CIFAR-10 data.\n\n    Args:\n      resnet_size: The number of convolutional layers needed in the model.\n      data_format: Either 'channels_first' or 'channels_last', specifying which\n        data format to use when setting up the model.\n      num_classes: The number of output classes needed from the model. This\n        enables users to extend the same model to their own datasets.\n      version: Integer representing which version of the ResNet network to use.\n        See README for details. Valid values: [1, 2]\n    \"\"\"\n    if resnet_size % 6 != 2:\n      raise ValueError('resnet_size must be 6n + 2:', resnet_size)\n\n    num_blocks = (resnet_size - 2) // 6\n\n    super(Cifar10Model, self).__init__(\n        resnet_size=resnet_size,\n        bottleneck=False,\n        num_classes=num_classes,\n        num_filters=16,\n        kernel_size=3,\n        conv_stride=1,\n        first_pool_size=None,\n        first_pool_stride=None,\n        second_pool_size=8,\n        second_pool_stride=1,\n        block_sizes=[num_blocks] * 3,\n        block_strides=[1, 2, 2],\n        final_size=64,\n        version=version,\n        data_format=data_format)\n\n\ndef cifar10_model_fn(features, labels, mode, params):\n  \"\"\"Model function for CIFAR-10.\"\"\"\n  features = tf.reshape(features, [-1, _HEIGHT, _WIDTH, _NUM_CHANNELS])\n\n  learning_rate_fn = resnet.learning_rate_with_decay(\n      batch_size=params['batch_size'], batch_denom=128,\n      num_images=_NUM_IMAGES['train'], boundary_epochs=[100, 150, 200],\n      decay_rates=[1, 0.1, 0.01, 0.001])\n\n  # We use a weight decay of 0.0002, which performs better\n  # than the 0.0001 that was originally suggested.\n  weight_decay = 2e-4\n\n  # Empirical testing showed that including batch_normalization variables\n  # in the calculation of regularized loss helped validation accuracy\n  # for the CIFAR-10 dataset, perhaps because the regularization prevents\n  # overfitting on the small data set. We therefore include all vars when\n  # regularizing and computing loss during training.\n  def loss_filter_fn(name):\n    return True\n\n  return resnet.resnet_model_fn(features, labels, mode, Cifar10Model,\n                                resnet_size=params['resnet_size'],\n                                weight_decay=weight_decay,\n                                learning_rate_fn=learning_rate_fn,\n                                momentum=0.9,\n                                data_format=params['data_format'],\n                                version=params['version'],\n                                loss_filter_fn=loss_filter_fn,\n                                multi_gpu=params['multi_gpu'])\n\n\ndef main(unused_argv):\n  input_function = FLAGS.use_synthetic_data and get_synth_input_fn() or input_fn\n  resnet.resnet_main(FLAGS, cifar10_model_fn, input_function)\n\n\nif __name__ == '__main__':\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  parser = resnet.ResnetArgParser()\n  # Set defaults that are reasonable for this model.\n  parser.set_defaults(data_dir='/tmp/cifar10_data',\n                      model_dir='/tmp/cifar10_model',\n                      resnet_size=32,\n                      train_epochs=250,\n                      epochs_per_eval=10,\n                      batch_size=128)\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n", "description": "Models and examples built with TensorFlow", "file_name": "cifar10_main.py", "id": "9c298437b3c959d7ed5aab989f4ccdf0", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/official/resnet/cifar10_main.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}