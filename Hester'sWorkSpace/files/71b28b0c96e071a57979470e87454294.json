{"author": "openai", "code": "\nimport argparse\nimport logging\nimport time\n\nimport gym\nimport numpy as np\nimport universe\nfrom universe import pyprofile, wrappers, spaces\nfrom gym import wrappers as gym_wrappers\n\n# if not os.getenv(\"PYPROFILE_FREQUENCY\"):\n\nfrom universe import vectorized\n\nlogger = logging.getLogger()\n\nCHROME_X_OFFSET = 18\nCHROME_Y_OFFSET = 84\n\nclass NoopSpace(gym.Space):\n    \"\"\" Null action space \"\"\"\n    def sample(self, seed=0):\n        return []\n    def contains(self, x):\n        return x == []\n\nclass ForwardSpace(gym.Space):\n    \"\"\" Only move forward action space \"\"\"\n    def __init__(self, key='w'):\n        self.key = [spaces.KeyEvent.by_name(key, down=True)]\n    def sample(self, seed=0):\n        return self.key\n    def contains(self, x):\n        return x == self.key\n\n\nclass RandomAgent(object):\n    \"\"\"\n    Example usage:\n\n        bin/random_agent.py -e gym-core.Pong-v3 --remote localhost:5900+15900\n\n    \"\"\"\n    def __init__(self, action_space, n, vectorized):\n        self.action_space = action_space\n        self.n = n\n        self.vectorized = vectorized\n\n    def __call__(self, observation, reward, done):\n        if self.vectorized:\n            return [self.action_space.sample() for _ in range(self.n)]\n        else:\n            return self.action_space.sample()\n\nif __name__ == '__main__':\n    \n    \n    \n    logger.setLevel(logging.INFO)\n    universe.configure_logging()\n\n    \n    action_choices = ['random', 'noop', 'forward', 'click']\n\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument('-e', '--env_id', default='gym-core.Pong-v3', help='Which environment to run on.')\n    parser.add_argument('-m', '--monitor', action='store_true', help='Whether to activate the monitor.')\n    parser.add_argument('-r', '--remote', default=None, help='The number of environments to create (e.g. -r 20), or the address of pre-existing VNC servers and rewarders to use (e.g. -r vnc://localhost:5900+15900,localhost:5901+15901)')\n    parser.add_argument('-c', '--client-id', default='0', help='Set client id.')\n    parser.add_argument('-v', '--verbose', action='count', dest='verbosity', default=0, help='Set verbosity.')\n    parser.add_argument('-R', '--no-render', action='store_true', help='Do not render the environment locally.')\n    parser.add_argument('-A', '--actions', choices=action_choices, default='random', help='How to sample actions to send to remote environment')\n    parser.add_argument('-d', '--docker-image', help='Force a version of the docker_image used with --remote <int>. e.g --docker-image quay.io/openai/universe.gym-core:0.3')\n    parser.add_argument('-s', '--reuse', default=False, action='store_true', help='Reuse existing Docker container if present, and leave this one running after (only for \"-r n\")')\n    parser.add_argument('-f', '--fps', default=60., type=float, help='Desired frames per second')\n    parser.add_argument('-N', '--max-steps', type=int, default=10**7, help='Maximum number of steps to take')\n    parser.add_argument('-E', '--max-episodes', type=int, default=10**7, help='Maximum number of episodes')\n    parser.add_argument('-T', '--start-timeout', type=int, default=None, help='Rewarder session connection timeout (seconds)')\n    args = parser.parse_args()\n\n    logging.getLogger('gym').setLevel(logging.NOTSET)\n    logging.getLogger('universe').setLevel(logging.NOTSET)\n    if args.verbosity == 0:\n        logger.setLevel(logging.INFO)\n    elif args.verbosity >= 1:\n        logger.setLevel(logging.DEBUG)\n\n    if args.env_id is not None:\n        env = gym.make(args.env_id)\n    else:\n        env = wrappers.WrappedVNCEnv()\n    # env = wrappers.BlockingReset(env)\n    if not isinstance(env, wrappers.GymCoreAction):\n        \n        \n        \n        \n        \n        env = wrappers.experimental.SafeActionSpace(env)\n    else:\n        \n        env.seed([0])\n    env = wrappers.Logger(env)\n\n    if args.monitor:\n        env = wrappers.Monitor(env, '/tmp/vnc_random_agent', force=True)\n\n    if args.actions == 'random':\n        action_space = env.action_space\n    elif args.actions == 'noop':\n        action_space = NoopSpace()\n    elif args.actions == 'forward':\n        action_space = ForwardSpace()\n    elif args.actions == 'click':\n        spec = universe.runtime_spec('flashgames').server_registry[args.env_id]\n        height = spec[\"height\"]\n        width = spec[\"width\"]\n        noclick_regions = [r['coordinates'] for r in spec['regions'] if r['type'] == 'noclick'] if spec.get('regions') else []\n        active_region = (CHROME_X_OFFSET, CHROME_Y_OFFSET, CHROME_X_OFFSET + width, CHROME_Y_OFFSET + height)\n        env = wrappers.SoftmaxClickMouse(env, active_region=active_region, noclick_regions=noclick_regions)\n        action_space = env.action_space\n    else:\n        logger.error(\"Invalid action choice: {}\".format(args.actions))\n        exit(1)\n\n    env.configure(\n        fps=args.fps,\n        \n        \n        remotes=args.remote,\n        client_id=args.client_id,\n        start_timeout=args.start_timeout,\n\n        \n        \n        \n        \n\n        vnc_driver='go', vnc_kwargs={\n            \n            'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 50, 'subsample_level': 0, 'quality_level': 5,\n        },\n    )\n\n    agent = RandomAgent(action_space, n=env.n, vectorized=env.metadata['runtime.vectorized'])\n\n    render = not args.no_render\n    observation_n = env.reset()\n    target = time.time()\n    reward_n = [0] * env.n\n    done_n = [False] * env.n\n\n    observation_count = np.zeros(env.n)\n    episode_length = np.zeros(env.n)\n    episode_score = np.zeros(env.n)\n\n    episodes_completed = 0\n\n    for i in range(args.max_steps):\n        # print(observation_n)\n        ()\n\n        if render:\n            \n            \n            # open the render() window before `reset()`, but that's\n            \n            \n            env.render()\n\n        action_n = agent(observation_n, reward_n, done_n)\n\n        \n        with pyprofile.push('env.step'):\n            observation_n, reward_n, done_n, info = env.step(action_n)\n\n        episode_length += 1\n        if not all(r is None for r in reward_n): \n            episode_score += np.array(reward_n)\n        for i, ob in enumerate(observation_n):\n            if ob is not None and (not isinstance(ob, dict) or ob['vision'] is not None):\n                observation_count[i] += 1\n\n        scores = {}\n        lengths = {}\n        observations = {}\n        for i, done in enumerate(done_n):\n            if not done:\n                continue\n            scores[i] = episode_score[i]\n            lengths[i] = episode_length[i]\n            observations[i] = observation_count[i]\n\n            episode_score[i] = 0\n            episode_length[i] = 0\n            observation_count[i] = 0\n        if len(scores) > 0:\n            logger.info('Total for completed episodes: reward=%s length=%s observations=%s', scores, lengths, observations)\n\n        errored = [i for i, info_i in enumerate(info['n']) if 'error' in info_i]\n        if errored:\n            logger.info('had errored indexes: %s: %s', errored, info)\n\n        episodes_completed += len([d for d in done_n if d])\n        if episodes_completed >= args.max_episodes:\n            break\n\n        # if info.get('n') and info['n'][0].get('env_status.instruction'):\n        #     logger.info('received instruction = %s', info['n'][0]['env_status.instruction'])\n\n        # if observation_n[0].get('text'):\n        #     logger.info('message_n=%s', [observation['text'] for observation in observation_n])\n\n        # if any(done_n) or any(r != 0.0 and r is not None for r in reward_n):\n        #     logger.info('reward_n=%s done_n=%s info=%s', reward_n, done_n, info)\n\n    \n    env.close()\n", "comments": "    null action space         def sample(self  seed 0)          return        def contains(self  x)          return x        class forwardspace(gym space)          only move forward action space         def   init  (self  key  w )          self key    spaces keyevent name(key  true)      def sample(self  seed 0)          return self key     def contains(self  x)          return x    self key    the world simplest agent  class randomagent(object)              example usage           bin random agent py  e gym core pong v3   remote localhost 5900 15900              usr bin env python    os getenv( pyprofile frequency )         pyprofile profile print frequency   5    the world simplest agent     you optionally set logger  also fine set level    logging debug logging warn want change    amount output     actions agent take   random  default    env   wrappers blockingreset(env)    the gymcoresyncenv try mimic core counterparts     thus came pre wrapped wth action space    translator  everything else probably wants safeactionspace    wrapper shield random agent clicking around    everywhere     only gym core seedable    print frequency none     ignore clock skew true     remotes remote  docker image args docker image  reuse args reuse  ignore clock skew true     vnc session driver  go   vnc session kwargs           compress level   0            encoding    tight    compress level   0   fine quality level   50   subsample level   2     print(observation n)    user input handle events()    note first time call render  relatively    slow aggregated rewards  we could    open render() window  reset()      confusing since pops black window    duration reset     take action    checks connected rewarder    info get( n ) info  n   0  get( env status instruction )         logger info( received instruction      info  n   0   env status instruction  )    observation n 0  get( text )         logger info( message n     observation  text   observation observation n )    any(done n) any(r    0 0 r none r reward n)         logger info( reward n  done n  info    reward n  done n  info)    we done  clean ", "content": "#!/usr/bin/env python\nimport argparse\nimport logging\nimport time\n\nimport gym\nimport numpy as np\nimport universe\nfrom universe import pyprofile, wrappers, spaces\nfrom gym import wrappers as gym_wrappers\n\n# if not os.getenv(\"PYPROFILE_FREQUENCY\"):\n#     pyprofile.profile.print_frequency = 5\nfrom universe import vectorized\n\nlogger = logging.getLogger()\n\nCHROME_X_OFFSET = 18\nCHROME_Y_OFFSET = 84\n\nclass NoopSpace(gym.Space):\n    \"\"\" Null action space \"\"\"\n    def sample(self, seed=0):\n        return []\n    def contains(self, x):\n        return x == []\n\nclass ForwardSpace(gym.Space):\n    \"\"\" Only move forward action space \"\"\"\n    def __init__(self, key='w'):\n        self.key = [spaces.KeyEvent.by_name(key, down=True)]\n    def sample(self, seed=0):\n        return self.key\n    def contains(self, x):\n        return x == self.key\n\n# The world's simplest agent!\nclass RandomAgent(object):\n    \"\"\"\n    Example usage:\n\n        bin/random_agent.py -e gym-core.Pong-v3 --remote localhost:5900+15900\n\n    \"\"\"\n    def __init__(self, action_space, n, vectorized):\n        self.action_space = action_space\n        self.n = n\n        self.vectorized = vectorized\n\n    def __call__(self, observation, reward, done):\n        if self.vectorized:\n            return [self.action_space.sample() for _ in range(self.n)]\n        else:\n            return self.action_space.sample()\n\nif __name__ == '__main__':\n    # You can optionally set up the logger. Also fine to set the level\n    # to logging.DEBUG or logging.WARN if you want to change the\n    # amount of output.\n    logger.setLevel(logging.INFO)\n    universe.configure_logging()\n\n    # Actions this agent will take, 'random' is the default\n    action_choices = ['random', 'noop', 'forward', 'click']\n\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument('-e', '--env_id', default='gym-core.Pong-v3', help='Which environment to run on.')\n    parser.add_argument('-m', '--monitor', action='store_true', help='Whether to activate the monitor.')\n    parser.add_argument('-r', '--remote', default=None, help='The number of environments to create (e.g. -r 20), or the address of pre-existing VNC servers and rewarders to use (e.g. -r vnc://localhost:5900+15900,localhost:5901+15901)')\n    parser.add_argument('-c', '--client-id', default='0', help='Set client id.')\n    parser.add_argument('-v', '--verbose', action='count', dest='verbosity', default=0, help='Set verbosity.')\n    parser.add_argument('-R', '--no-render', action='store_true', help='Do not render the environment locally.')\n    parser.add_argument('-A', '--actions', choices=action_choices, default='random', help='How to sample actions to send to remote environment')\n    parser.add_argument('-d', '--docker-image', help='Force a version of the docker_image used with --remote <int>. e.g --docker-image quay.io/openai/universe.gym-core:0.3')\n    parser.add_argument('-s', '--reuse', default=False, action='store_true', help='Reuse existing Docker container if present, and leave this one running after (only for \"-r n\")')\n    parser.add_argument('-f', '--fps', default=60., type=float, help='Desired frames per second')\n    parser.add_argument('-N', '--max-steps', type=int, default=10**7, help='Maximum number of steps to take')\n    parser.add_argument('-E', '--max-episodes', type=int, default=10**7, help='Maximum number of episodes')\n    parser.add_argument('-T', '--start-timeout', type=int, default=None, help='Rewarder session connection timeout (seconds)')\n    args = parser.parse_args()\n\n    logging.getLogger('gym').setLevel(logging.NOTSET)\n    logging.getLogger('universe').setLevel(logging.NOTSET)\n    if args.verbosity == 0:\n        logger.setLevel(logging.INFO)\n    elif args.verbosity >= 1:\n        logger.setLevel(logging.DEBUG)\n\n    if args.env_id is not None:\n        env = gym.make(args.env_id)\n    else:\n        env = wrappers.WrappedVNCEnv()\n    # env = wrappers.BlockingReset(env)\n    if not isinstance(env, wrappers.GymCoreAction):\n        # The GymCoreSyncEnv's try to mimic their core counterparts,\n        # and thus came pre-wrapped wth an action space\n        # translator. Everything else probably wants a SafeActionSpace\n        # wrapper to shield them from random-agent clicking around\n        # everywhere.\n        env = wrappers.experimental.SafeActionSpace(env)\n    else:\n        # Only gym-core are seedable\n        env.seed([0])\n    env = wrappers.Logger(env)\n\n    if args.monitor:\n        env = wrappers.Monitor(env, '/tmp/vnc_random_agent', force=True)\n\n    if args.actions == 'random':\n        action_space = env.action_space\n    elif args.actions == 'noop':\n        action_space = NoopSpace()\n    elif args.actions == 'forward':\n        action_space = ForwardSpace()\n    elif args.actions == 'click':\n        spec = universe.runtime_spec('flashgames').server_registry[args.env_id]\n        height = spec[\"height\"]\n        width = spec[\"width\"]\n        noclick_regions = [r['coordinates'] for r in spec['regions'] if r['type'] == 'noclick'] if spec.get('regions') else []\n        active_region = (CHROME_X_OFFSET, CHROME_Y_OFFSET, CHROME_X_OFFSET + width, CHROME_Y_OFFSET + height)\n        env = wrappers.SoftmaxClickMouse(env, active_region=active_region, noclick_regions=noclick_regions)\n        action_space = env.action_space\n    else:\n        logger.error(\"Invalid action choice: {}\".format(args.actions))\n        exit(1)\n\n    env.configure(\n        fps=args.fps,\n        # print_frequency=None,\n        # ignore_clock_skew=True,\n        remotes=args.remote,\n        client_id=args.client_id,\n        start_timeout=args.start_timeout,\n\n        # remotes=remote, docker_image=args.docker_image, reuse=args.reuse, ignore_clock_skew=True,\n        # vnc_session_driver='go', vnc_session_kwargs={\n        #     'compress_level': 0,\n        # },\n\n        vnc_driver='go', vnc_kwargs={\n            # 'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 50, 'subsample_level': 2,\n            'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 50, 'subsample_level': 0, 'quality_level': 5,\n        },\n    )\n\n    agent = RandomAgent(action_space, n=env.n, vectorized=env.metadata['runtime.vectorized'])\n\n    render = not args.no_render\n    observation_n = env.reset()\n    target = time.time()\n    reward_n = [0] * env.n\n    done_n = [False] * env.n\n\n    observation_count = np.zeros(env.n)\n    episode_length = np.zeros(env.n)\n    episode_score = np.zeros(env.n)\n\n    episodes_completed = 0\n\n    for i in range(args.max_steps):\n        # print(observation_n)\n        # user_input.handle_events()\n\n        if render:\n            # Note the first time you call render, it'll be relatively\n            # slow and you'll have some aggregated rewards. We could\n            # open the render() window before `reset()`, but that's\n            # confusing since it pops up a black window for the\n            # duration of the reset.\n            env.render()\n\n        action_n = agent(observation_n, reward_n, done_n)\n\n        # Take an action\n        with pyprofile.push('env.step'):\n            observation_n, reward_n, done_n, info = env.step(action_n)\n\n        episode_length += 1\n        if not all(r is None for r in reward_n): # checks if we connected the rewarder\n            episode_score += np.array(reward_n)\n        for i, ob in enumerate(observation_n):\n            if ob is not None and (not isinstance(ob, dict) or ob['vision'] is not None):\n                observation_count[i] += 1\n\n        scores = {}\n        lengths = {}\n        observations = {}\n        for i, done in enumerate(done_n):\n            if not done:\n                continue\n            scores[i] = episode_score[i]\n            lengths[i] = episode_length[i]\n            observations[i] = observation_count[i]\n\n            episode_score[i] = 0\n            episode_length[i] = 0\n            observation_count[i] = 0\n        if len(scores) > 0:\n            logger.info('Total for completed episodes: reward=%s length=%s observations=%s', scores, lengths, observations)\n\n        errored = [i for i, info_i in enumerate(info['n']) if 'error' in info_i]\n        if errored:\n            logger.info('had errored indexes: %s: %s', errored, info)\n\n        episodes_completed += len([d for d in done_n if d])\n        if episodes_completed >= args.max_episodes:\n            break\n\n        # if info.get('n') and info['n'][0].get('env_status.instruction'):\n        #     logger.info('received instruction = %s', info['n'][0]['env_status.instruction'])\n\n        # if observation_n[0].get('text'):\n        #     logger.info('message_n=%s', [observation['text'] for observation in observation_n])\n\n        # if any(done_n) or any(r != 0.0 and r is not None for r in reward_n):\n        #     logger.info('reward_n=%s done_n=%s info=%s', reward_n, done_n, info)\n\n    # We're done! clean up\n    env.close()\n", "description": "Universe: a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.", "file_name": "diagnostic-agent.py", "id": "71b28b0c96e071a57979470e87454294", "language": "Python", "project_name": "universe", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-universe/openai-universe-f95a5fe/example/diagnostic-agent/diagnostic-agent.py", "save_time": "", "source": "", "update_at": "2018-03-18T08:39:09Z", "url": "https://github.com/openai/universe", "wiki": true}