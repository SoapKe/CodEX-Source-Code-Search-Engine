{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\"\"\"Compound TensorFlow operations for style transfer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops as framework_ops\nfrom tensorflow.python.ops import variable_scope\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef conditional_instance_norm(inputs,\n                              labels,\n                              num_categories,\n                              center=True,\n                              scale=True,\n                              activation_fn=None,\n                              reuse=None,\n                              variables_collections=None,\n                              outputs_collections=None,\n                              trainable=True,\n                              scope=None):\n  \"\"\"Conditional instance normalization from TODO(vdumoulin): add link.\n\n    \"A Learned Representation for Artistic Style\"\n\n    Vincent Dumoulin, Jon Shlens, Manjunath Kudlur\n\n  Can be used as a normalizer function for conv2d.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    labels: tensor, style labels to condition on.\n    num_categories: int, total number of styles being modeled.\n    center: If True, subtract `beta`. If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    activation_fn: Optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: optional collections for the variables.\n    outputs_collections: collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with tf.variable_scope(scope, 'InstanceNorm', [inputs],\n                         reuse=reuse) as sc:\n    inputs = tf.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' % (\n          inputs.name, params_shape))\n\n    def _label_conditioned_variable(name, initializer, labels, num_categories):\n      \"\"\"Label conditioning.\"\"\"\n      shape = tf.TensorShape([num_categories]).concatenate(params_shape)\n      var_collections = slim.utils.get_variable_collections(\n          variables_collections, name)\n      var = slim.model_variable(name,\n                                shape=shape,\n                                dtype=dtype,\n                                initializer=initializer,\n                                collections=var_collections,\n                                trainable=trainable)\n      conditioned_var = tf.gather(var, labels)\n      conditioned_var = tf.expand_dims(tf.expand_dims(conditioned_var, 1), 1)\n      return conditioned_var\n\n     Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = _label_conditioned_variable(\n          'beta', tf.zeros_initializer(), labels, num_categories)\n    if scale:\n      gamma = _label_conditioned_variable(\n          'gamma', tf.ones_initializer(), labels, num_categories)\n     Calculate the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n     Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            outputs)\n\n\n@slim.add_arg_scope\ndef weighted_instance_norm(inputs,\n                           weights,\n                           num_categories,\n                           center=True,\n                           scale=True,\n                           activation_fn=None,\n                           reuse=None,\n                           variables_collections=None,\n                           outputs_collections=None,\n                           trainable=True,\n                           scope=None):\n  \"\"\"Weighted instance normalization.\n\n  Can be used as a normalizer function for conv2d.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    weights: 1D tensor.\n    num_categories: int, total number of styles being modeled.\n    center: If True, subtract `beta`. If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    activation_fn: Optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: optional collections for the variables.\n    outputs_collections: collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with tf.variable_scope(scope, 'InstanceNorm', [inputs],\n                         reuse=reuse) as sc:\n    inputs = tf.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' % (\n          inputs.name, params_shape))\n\n    def _weighted_variable(name, initializer, weights, num_categories):\n      \"\"\"Weighting.\"\"\"\n      shape = tf.TensorShape([num_categories]).concatenate(params_shape)\n      var_collections = slim.utils.get_variable_collections(\n          variables_collections, name)\n      var = slim.model_variable(name,\n                                shape=shape,\n                                dtype=dtype,\n                                initializer=initializer,\n                                collections=var_collections,\n                                trainable=trainable)\n      weights = tf.reshape(\n          weights,\n          weights.get_shape().concatenate([1] * params_shape.ndims))\n      conditioned_var = weights * var\n      conditioned_var = tf.reduce_sum(conditioned_var, 0, keep_dims=True)\n      conditioned_var = tf.expand_dims(tf.expand_dims(conditioned_var, 1), 1)\n      return conditioned_var\n\n     Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = _weighted_variable(\n          'beta', tf.zeros_initializer(), weights, num_categories)\n    if scale:\n      gamma = _weighted_variable(\n          'gamma', tf.ones_initializer(), weights, num_categories)\n     Calculate the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n     Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            outputs)\n\n\n@slim.add_arg_scope\ndef conditional_style_norm(inputs,\n                           style_params=None,\n                           activation_fn=None,\n                           reuse=None,\n                           outputs_collections=None,\n                           check_numerics=True,\n                           scope=None):\n  \"\"\"Conditional style normalization.\n\n  Can be used as a normalizer function for conv2d. This method is similar\n  to conditional_instance_norm. But instead of creating the normalization\n  variables (beta and gamma), it gets these values as inputs in\n  style_params dictionary.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    style_params: a dict from the scope names of the variables of this\n         method + beta/gamma to the beta and gamma tensors.\n        eg. {'transformer/expand/conv2/conv/StyleNorm/beta': <tf.Tensor>,\n        'transformer/expand/conv2/conv/StyleNorm/gamma': <tf.Tensor>,\n        'transformer/residual/residual1/conv1/StyleNorm/beta': <tf.Tensor>,\n        'transformer/residual/residual1/conv1/StyleNorm/gamma': <tf.Tensor>}\n    activation_fn: optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    outputs_collections: collections to add the outputs.\n    check_numerics: whether to checks for NAN values in beta and gamma.\n    scope: optional scope for `variable_op_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with variable_scope.variable_scope(\n      scope, 'StyleNorm', [inputs], reuse=reuse) as sc:\n    inputs = framework_ops.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' %\n                       (inputs.name, params_shape))\n\n    def _style_parameters(name):\n      \"\"\"Gets style normalization parameters.\"\"\"\n      var = style_params[('{}/{}'.format(sc.name, name))]\n\n      if check_numerics:\n        var = tf.check_numerics(var, 'NaN/Inf in {}'.format(var.name))\n      if var.get_shape().ndims < 2:\n        var = tf.expand_dims(var, 0)\n      var = tf.expand_dims(tf.expand_dims(var, 1), 1)\n\n      return var\n\n     Allocates parameters for the beta and gamma of the normalization.\n    beta = _style_parameters('beta')\n    gamma = _style_parameters('gamma')\n\n     Calculates the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n\n     Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(inputs, mean, variance, beta, gamma,\n                                        variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope, outputs)\n\n", "comments": "   compound tensorflow operations style transfer        future   import absolute import   future   import division   future   import print function  import tensorflow tf tensorflow python framework import ops framework ops tensorflow python ops import variable scope  slim   tf contrib slim    slim add arg scope def conditional instance norm(inputs                                labels                                num categories                                center true                                scale true                                activation fn none                                reuse none                                variables collections none                                outputs collections none                                trainable true                                scope none)       conditional instance normalization todo(vdumoulin)  add link        a learned representation artistic style       vincent dumoulin  jon shlens  manjunath kudlur    can used normalizer function conv2d     args      inputs  tensor 4 dimensions  the normalization occurs height         width      labels  tensor  style labels condition      num categories  int  total number styles modeled      center  if true  subtract  beta   if false   beta  ignored      scale  if true  multiply  gamma   if false   gamma        used  when next layer linear (also e g   nn relu )        disabled since scaling done next layer      activation fn  optional activation function      reuse  whether layer variables reused  to       able reuse layer scope must given      variables collections  optional collections variables      outputs collections  collections add outputs      trainable  if  true  also add variables graph collection        graphkeys trainable variables  (see tf variable)      scope  optional scope  variable scope      returns      a  tensor  representing output operation     raises      valueerror  rank last dimension  inputs  undefined          input 4 dimensions          tf variable scope(scope   instancenorm    inputs                            reuse reuse) sc      inputs   tf convert tensor(inputs)     inputs shape   inputs get shape()     inputs rank   inputs shape ndims     inputs rank none        raise valueerror( inputs  undefined rank     inputs name)     inputs rank    4        raise valueerror( inputs  4d tensor     inputs name)     dtype   inputs dtype base dtype     axis    1  2      params shape   inputs shape  1       params shape fully defined()        raise valueerror( inputs  undefined last dimension      (           inputs name  params shape))      def  label conditioned variable(name  initializer  labels  num categories)           label conditioning           shape   tf tensorshape( num categories ) concatenate(params shape)       var collections   slim utils get variable collections(           variables collections  name)       var   slim model variable(name                                  shape shape                                  dtype dtype                                  initializer initializer                                  collections var collections                                  trainable trainable)       conditioned var   tf gather(var  labels)       conditioned var   tf expand dims(tf expand dims(conditioned var  1)  1)       return conditioned var        allocate parameters beta gamma normalization      beta  gamma   none  none     center        beta    label conditioned variable(            beta   tf zeros initializer()  labels  num categories)     scale        gamma    label conditioned variable(            gamma   tf ones initializer()  labels  num categories)       calculate moments last axis (instance activations)      mean  variance   tf nn moments(inputs  axis  keep dims true)       compute layer normalization using batch normalization function      variance epsilon   1e 5     outputs   tf nn batch normalization(         inputs  mean  variance  beta  gamma  variance epsilon)     outputs set shape(inputs shape)     activation fn        outputs   activation fn(outputs)     return slim utils collect named outputs(outputs collections                                              sc original name scope                                              outputs)    slim add arg scope def weighted instance norm(inputs                             weights                             num categories                             center true                             scale true                             activation fn none                             reuse none                             variables collections none                             outputs collections none                             trainable true                             scope none)       weighted instance normalization     can used normalizer function conv2d     args      inputs  tensor 4 dimensions  the normalization occurs height         width      weights  1d tensor      num categories  int  total number styles modeled      center  if true  subtract  beta   if false   beta  ignored      scale  if true  multiply  gamma   if false   gamma        used  when next layer linear (also e g   nn relu )        disabled since scaling done next layer      activation fn  optional activation function      reuse  whether layer variables reused  to       able reuse layer scope must given      variables collections  optional collections variables      outputs collections  collections add outputs      trainable  if  true  also add variables graph collection        graphkeys trainable variables  (see tf variable)      scope  optional scope  variable scope      returns      a  tensor  representing output operation     raises      valueerror  rank last dimension  inputs  undefined          input 4 dimensions          tf variable scope(scope   instancenorm    inputs                            reuse reuse) sc      inputs   tf convert tensor(inputs)     inputs shape   inputs get shape()     inputs rank   inputs shape ndims     inputs rank none        raise valueerror( inputs  undefined rank     inputs name)     inputs rank    4        raise valueerror( inputs  4d tensor     inputs name)     dtype   inputs dtype base dtype     axis    1  2      params shape   inputs shape  1       params shape fully defined()        raise valueerror( inputs  undefined last dimension      (           inputs name  params shape))      def  weighted variable(name  initializer  weights  num categories)           weighting           shape   tf tensorshape( num categories ) concatenate(params shape)       var collections   slim utils get variable collections(           variables collections  name)       var   slim model variable(name                                  shape shape                                  dtype dtype                                  initializer initializer                                  collections var collections                                  trainable trainable)       weights   tf reshape(           weights            weights get shape() concatenate( 1    params shape ndims))       conditioned var   weights   var       conditioned var   tf reduce sum(conditioned var  0  keep dims true)       conditioned var   tf expand dims(tf expand dims(conditioned var  1)  1)       return conditioned var        allocate parameters beta gamma normalization      beta  gamma   none  none     center        beta    weighted variable(            beta   tf zeros initializer()  weights  num categories)     scale        gamma    weighted variable(            gamma   tf ones initializer()  weights  num categories)       calculate moments last axis (instance activations)      mean  variance   tf nn moments(inputs  axis  keep dims true)       compute layer normalization using batch normalization function      variance epsilon   1e 5     outputs   tf nn batch normalization(         inputs  mean  variance  beta  gamma  variance epsilon)     outputs set shape(inputs shape)     activation fn        outputs   activation fn(outputs)     return slim utils collect named outputs(outputs collections                                              sc original name scope                                              outputs)    slim add arg scope def conditional style norm(inputs                             style params none                             activation fn none                             reuse none                             outputs collections none                             check numerics true                             scope none)       conditional style normalization     can used normalizer function conv2d  this method similar   conditional instance norm  but instead creating normalization   variables (beta gamma)  gets values inputs   style params dictionary     args      inputs  tensor 4 dimensions  the normalization occurs height         width      style params  dict scope names variables          method   beta gamma beta gamma tensors          eg    transformer expand conv2 conv stylenorm beta    tf tensor            transformer expand conv2 conv stylenorm gamma    tf tensor            transformer residual residual1 conv1 stylenorm beta    tf tensor            transformer residual residual1 conv1 stylenorm gamma    tf tensor       activation fn  optional activation function      reuse  whether layer variables reused  to       able reuse layer scope must given      outputs collections  collections add outputs      check numerics  whether checks nan values beta gamma      scope  optional scope  variable op scope      returns      a  tensor  representing output operation     raises      valueerror  rank last dimension  inputs  undefined          input 4 dimensions          variable scope variable scope(       scope   stylenorm    inputs   reuse reuse) sc      inputs   framework ops convert tensor(inputs)     inputs shape   inputs get shape()     inputs rank   inputs shape ndims     inputs rank none        raise valueerror( inputs  undefined rank     inputs name)     inputs rank    4        raise valueerror( inputs  4d tensor     inputs name)     axis    1  2      params shape   inputs shape  1       params shape fully defined()        raise valueerror( inputs  undefined last dimension                             (inputs name  params shape))      def  style parameters(name)           gets style normalization parameters        copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license          http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     allocate parameters beta gamma normalization     calculate moments last axis (instance activations)     compute layer normalization using batch normalization function     allocate parameters beta gamma normalization     calculate moments last axis (instance activations)     compute layer normalization using batch normalization function     allocates parameters beta gamma normalization     calculates moments last axis (instance activations)     compute layer normalization using batch normalization function  ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Compound TensorFlow operations for style transfer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops as framework_ops\nfrom tensorflow.python.ops import variable_scope\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef conditional_instance_norm(inputs,\n                              labels,\n                              num_categories,\n                              center=True,\n                              scale=True,\n                              activation_fn=None,\n                              reuse=None,\n                              variables_collections=None,\n                              outputs_collections=None,\n                              trainable=True,\n                              scope=None):\n  \"\"\"Conditional instance normalization from TODO(vdumoulin): add link.\n\n    \"A Learned Representation for Artistic Style\"\n\n    Vincent Dumoulin, Jon Shlens, Manjunath Kudlur\n\n  Can be used as a normalizer function for conv2d.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    labels: tensor, style labels to condition on.\n    num_categories: int, total number of styles being modeled.\n    center: If True, subtract `beta`. If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    activation_fn: Optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: optional collections for the variables.\n    outputs_collections: collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with tf.variable_scope(scope, 'InstanceNorm', [inputs],\n                         reuse=reuse) as sc:\n    inputs = tf.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' % (\n          inputs.name, params_shape))\n\n    def _label_conditioned_variable(name, initializer, labels, num_categories):\n      \"\"\"Label conditioning.\"\"\"\n      shape = tf.TensorShape([num_categories]).concatenate(params_shape)\n      var_collections = slim.utils.get_variable_collections(\n          variables_collections, name)\n      var = slim.model_variable(name,\n                                shape=shape,\n                                dtype=dtype,\n                                initializer=initializer,\n                                collections=var_collections,\n                                trainable=trainable)\n      conditioned_var = tf.gather(var, labels)\n      conditioned_var = tf.expand_dims(tf.expand_dims(conditioned_var, 1), 1)\n      return conditioned_var\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = _label_conditioned_variable(\n          'beta', tf.zeros_initializer(), labels, num_categories)\n    if scale:\n      gamma = _label_conditioned_variable(\n          'gamma', tf.ones_initializer(), labels, num_categories)\n    # Calculate the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n    # Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            outputs)\n\n\n@slim.add_arg_scope\ndef weighted_instance_norm(inputs,\n                           weights,\n                           num_categories,\n                           center=True,\n                           scale=True,\n                           activation_fn=None,\n                           reuse=None,\n                           variables_collections=None,\n                           outputs_collections=None,\n                           trainable=True,\n                           scope=None):\n  \"\"\"Weighted instance normalization.\n\n  Can be used as a normalizer function for conv2d.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    weights: 1D tensor.\n    num_categories: int, total number of styles being modeled.\n    center: If True, subtract `beta`. If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    activation_fn: Optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: optional collections for the variables.\n    outputs_collections: collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with tf.variable_scope(scope, 'InstanceNorm', [inputs],\n                         reuse=reuse) as sc:\n    inputs = tf.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' % (\n          inputs.name, params_shape))\n\n    def _weighted_variable(name, initializer, weights, num_categories):\n      \"\"\"Weighting.\"\"\"\n      shape = tf.TensorShape([num_categories]).concatenate(params_shape)\n      var_collections = slim.utils.get_variable_collections(\n          variables_collections, name)\n      var = slim.model_variable(name,\n                                shape=shape,\n                                dtype=dtype,\n                                initializer=initializer,\n                                collections=var_collections,\n                                trainable=trainable)\n      weights = tf.reshape(\n          weights,\n          weights.get_shape().concatenate([1] * params_shape.ndims))\n      conditioned_var = weights * var\n      conditioned_var = tf.reduce_sum(conditioned_var, 0, keep_dims=True)\n      conditioned_var = tf.expand_dims(tf.expand_dims(conditioned_var, 1), 1)\n      return conditioned_var\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = _weighted_variable(\n          'beta', tf.zeros_initializer(), weights, num_categories)\n    if scale:\n      gamma = _weighted_variable(\n          'gamma', tf.ones_initializer(), weights, num_categories)\n    # Calculate the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n    # Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            outputs)\n\n\n@slim.add_arg_scope\ndef conditional_style_norm(inputs,\n                           style_params=None,\n                           activation_fn=None,\n                           reuse=None,\n                           outputs_collections=None,\n                           check_numerics=True,\n                           scope=None):\n  \"\"\"Conditional style normalization.\n\n  Can be used as a normalizer function for conv2d. This method is similar\n  to conditional_instance_norm. But instead of creating the normalization\n  variables (beta and gamma), it gets these values as inputs in\n  style_params dictionary.\n\n  Args:\n    inputs: a tensor with 4 dimensions. The normalization occurs over height\n        and width.\n    style_params: a dict from the scope names of the variables of this\n         method + beta/gamma to the beta and gamma tensors.\n        eg. {'transformer/expand/conv2/conv/StyleNorm/beta': <tf.Tensor>,\n        'transformer/expand/conv2/conv/StyleNorm/gamma': <tf.Tensor>,\n        'transformer/residual/residual1/conv1/StyleNorm/beta': <tf.Tensor>,\n        'transformer/residual/residual1/conv1/StyleNorm/gamma': <tf.Tensor>}\n    activation_fn: optional activation function.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    outputs_collections: collections to add the outputs.\n    check_numerics: whether to checks for NAN values in beta and gamma.\n    scope: optional scope for `variable_op_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: if rank or last dimension of `inputs` is undefined, or if the\n        input doesn't have 4 dimensions.\n  \"\"\"\n  with variable_scope.variable_scope(\n      scope, 'StyleNorm', [inputs], reuse=reuse) as sc:\n    inputs = framework_ops.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError('Inputs %s has undefined rank.' % inputs.name)\n    if inputs_rank != 4:\n      raise ValueError('Inputs %s is not a 4D tensor.' % inputs.name)\n    axis = [1, 2]\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError('Inputs %s has undefined last dimension %s.' %\n                       (inputs.name, params_shape))\n\n    def _style_parameters(name):\n      \"\"\"Gets style normalization parameters.\"\"\"\n      var = style_params[('{}/{}'.format(sc.name, name))]\n\n      if check_numerics:\n        var = tf.check_numerics(var, 'NaN/Inf in {}'.format(var.name))\n      if var.get_shape().ndims < 2:\n        var = tf.expand_dims(var, 0)\n      var = tf.expand_dims(tf.expand_dims(var, 1), 1)\n\n      return var\n\n    # Allocates parameters for the beta and gamma of the normalization.\n    beta = _style_parameters('beta')\n    gamma = _style_parameters('gamma')\n\n    # Calculates the moments on the last axis (instance activations).\n    mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n\n    # Compute layer normalization using the batch_normalization function.\n    variance_epsilon = 1E-5\n    outputs = tf.nn.batch_normalization(inputs, mean, variance, beta, gamma,\n                                        variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope, outputs)\n\n", "description": "Magenta: Music and Art Generation with Machine Intelligence", "file_name": "ops.py", "id": "8b9dce798fbfa118ef65d57f3101c73b", "language": "Python", "project_name": "magenta", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-magenta/tensorflow-magenta-ca73164/magenta/models/image_stylization/ops.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:00:14Z", "url": "https://github.com/tensorflow/magenta", "wiki": false}