{"author": "deepfakes", "code": "\n\n\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.activations import relu\nfrom keras.initializers import RandomNormal\nfrom keras.applications import *\nfrom keras.optimizers import Adam\n\nfrom lib.PixelShuffler import PixelShuffler\nfrom .instance_normalization import InstanceNormalization\n\nfrom keras.utils import multi_gpu_model\n\nnetGAH5 = 'netGA_GAN128.h5'\nnetGBH5 = 'netGB_GAN128.h5'\nnetDAH5 = 'netDA_GAN128.h5'\nnetDBH5 = 'netDB_GAN128.h5'\n\ndef __conv_init(a):\n    print(\"conv_init\", a)\n    k = RandomNormal(0, 0.02)(a) \n    k.conv_weight = True\n    return k\n\n#def batchnorm():\n#    return BatchNormalization(momentum=0.9, axis=channel_axis, epsilon=1.01e-5, gamma_initializer = gamma_init)\n\ndef inst_norm():\n    return InstanceNormalization()\n\nconv_init = RandomNormal(0, 0.02)\ngamma_init = RandomNormal(1., 0.02) \n\nclass GANModel():\n    img_size = 128\n    channels = 3\n    img_shape = (img_size, img_size, channels)\n    encoded_dim = 1024\n    nc_in = 3 \n    nc_D_inp = 6 \n\n    def __init__(self, model_dir, gpus):\n        self.model_dir = model_dir\n        self.gpus = gpus\n        assert self.gpus == 1, \"Error: GAN128 cannot use multiple gpus right now. See https://github.com/deepfakes/faceswap/issues/287\"\n\n        optimizer = Adam(1e-4, 0.5)\n\n        \n        self.netDA, self.netDB = self.build_discriminator()\n\n        \n        self.netGA, self.netGB = self.build_generator()\n\n    def converter(self, swap):\n        predictor = self.netGB if not swap else self.netGA\n        return lambda img: predictor.predict(img)\n\n    def build_generator(self):\n\n        def conv_block(input_tensor, f, use_instance_norm=True):\n            x = input_tensor\n            x = SeparableConv2D(f, kernel_size=3, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n            if use_instance_norm:\n                x = inst_norm()(x)\n            x = Activation(\"relu\")(x)\n            return x\n\n        def res_block(input_tensor, f, dilation=1):\n            x = input_tensor\n            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\n            x = LeakyReLU(alpha=0.2)(x)\n            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\n            x = add([x, input_tensor])\n            #x = LeakyReLU(alpha=0.2)(x)\n            return x\n\n        def upscale_ps(filters, use_instance_norm=True):\n            def block(x, use_instance_norm=use_instance_norm):\n                x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same')(x)\n                if use_instance_norm:\n                    x = inst_norm()(x)\n                x = LeakyReLU(0.1)(x)\n                x = PixelShuffler()(x)\n                return x\n            return block\n\n        def Encoder(nc_in=3, input_size=128):\n            inp = Input(shape=(input_size, input_size, nc_in))\n            x = Conv2D(32, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(inp)\n            x = conv_block(x,64, use_instance_norm=False)\n            x = conv_block(x,128)\n            x = conv_block(x,256)\n            x = conv_block(x,512)\n            x = conv_block(x,1024)\n            x = Dense(1024)(Flatten()(x))\n            x = Dense(4*4*1024)(x)\n            x = Reshape((4, 4, 1024))(x)\n            out = upscale_ps(512)(x)\n            return Model(inputs=inp, outputs=out)\n\n        def Decoder_ps(nc_in=512, input_size=8):\n            input_ = Input(shape=(input_size, input_size, nc_in))\n            x = input_\n            x = upscale_ps(256)(x)\n            x = upscale_ps(128)(x)\n            x = upscale_ps(64)(x)\n            x = res_block(x, 64, dilation=2)\n\n            out64 = Conv2D(64, kernel_size=3, padding='same')(x)\n            out64 = LeakyReLU(alpha=0.1)(out64)\n            out64 = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(out64)\n\n            x = upscale_ps(32)(x)\n            x = res_block(x, 32)\n            x = res_block(x, 32)\n            alpha = Conv2D(1, kernel_size=5, padding='same', activation=\"sigmoid\")(x)\n            rgb = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(x)\n            out = concatenate([alpha, rgb])\n            return Model(input_, [out, out64] )\n\n        encoder = Encoder()\n        decoder_A = Decoder_ps()\n        decoder_B = Decoder_ps()\n        x = Input(shape=self.img_shape)\n        netGA = Model(x, decoder_A(encoder(x)))\n        netGB = Model(x, decoder_B(encoder(x)))\n\n        self.netGA_sm = netGA\n        self.netGB_sm = netGB\n\n        try:\n            netGA.load_weights(str(self.model_dir / netGAH5))\n            netGB.load_weights(str(self.model_dir / netGBH5))\n            print (\"Generator models loaded.\")\n        except:\n            print (\"Generator weights files not found.\")\n            pass\n\n        if self.gpus > 1:\n            netGA = multi_gpu_model( self.netGA_sm , self.gpus)\n            netGB = multi_gpu_model( self.netGB_sm , self.gpus)\n\n        return netGA, netGB\n\n    def build_discriminator(self):\n        def conv_block_d(input_tensor, f, use_instance_norm=True):\n            x = input_tensor\n            x = Conv2D(f, kernel_size=4, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n            if use_instance_norm:\n                x = inst_norm()(x)\n            x = LeakyReLU(alpha=0.2)(x)\n            return x\n\n        def Discriminator(nc_in, input_size=128):\n            inp = Input(shape=(input_size, input_size, nc_in))\n            #x = GaussianNoise(0.05)(inp)\n            x = conv_block_d(inp, 64, False)\n            x = conv_block_d(x, 128, True)\n            x = conv_block_d(x, 256, True)\n            x = conv_block_d(x, 512, True)\n            out = Conv2D(1, kernel_size=4, kernel_initializer=conv_init, use_bias=False, padding=\"same\", activation=\"sigmoid\")(x)\n            return Model(inputs=[inp], outputs=out)\n\n        netDA = Discriminator(self.nc_D_inp)\n        netDB = Discriminator(self.nc_D_inp)\n\n        try:\n            netDA.load_weights(str(self.model_dir / netDAH5))\n            netDB.load_weights(str(self.model_dir / netDBH5))\n            print (\"Discriminator models loaded.\")\n        except:\n            print (\"Discriminator weights files not found.\")\n            pass\n        return netDA, netDB\n\n    def load(self, swapped):\n        if swapped:\n            print(\"swapping not supported on GAN\")\n            \n        return True\n\n    def save_weights(self):\n        if self.gpus > 1:\n            self.netGA_sm.save_weights(str(self.model_dir / netGAH5))\n            self.netGB_sm.save_weights(str(self.model_dir / netGBH5))\n        else:\n            self.netGA.save_weights(str(self.model_dir / netGAH5))\n            self.netGB.save_weights(str(self.model_dir / netGBH5))\n        self.netDA.save_weights(str(self.model_dir / netDAH5))\n        self.netDB.save_weights(str(self.model_dir / netDBH5))\n        print (\"Models saved.\")\n", "comments": "  based https   github com shaoanlu faceswap gan repo    source   https   github com shaoanlu faceswap gan blob master faceswap gan v2 sz128 train ipynbtemp faceswap gan keras ipynb    convolution kernel   def batchnorm()        return batchnormalization(momentum 0 9  axis channel axis  epsilon 1 01e 5  gamma initializer   gamma init)    batch normalization    number input channels generators    number input channels discriminators    build compile discriminator    build compile generator   x   leakyrelu(alpha 0 2)(x)   x   gaussiannoise(0 05)(inp)    todo load done   init      look swap possible ", "content": "# Based on the https://github.com/shaoanlu/faceswap-GAN repo\n# source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb\n\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.activations import relu\nfrom keras.initializers import RandomNormal\nfrom keras.applications import *\nfrom keras.optimizers import Adam\n\nfrom lib.PixelShuffler import PixelShuffler\nfrom .instance_normalization import InstanceNormalization\n\nfrom keras.utils import multi_gpu_model\n\nnetGAH5 = 'netGA_GAN128.h5'\nnetGBH5 = 'netGB_GAN128.h5'\nnetDAH5 = 'netDA_GAN128.h5'\nnetDBH5 = 'netDB_GAN128.h5'\n\ndef __conv_init(a):\n    print(\"conv_init\", a)\n    k = RandomNormal(0, 0.02)(a) # for convolution kernel\n    k.conv_weight = True\n    return k\n\n#def batchnorm():\n#    return BatchNormalization(momentum=0.9, axis=channel_axis, epsilon=1.01e-5, gamma_initializer = gamma_init)\n\ndef inst_norm():\n    return InstanceNormalization()\n\nconv_init = RandomNormal(0, 0.02)\ngamma_init = RandomNormal(1., 0.02) # for batch normalization\n\nclass GANModel():\n    img_size = 128\n    channels = 3\n    img_shape = (img_size, img_size, channels)\n    encoded_dim = 1024\n    nc_in = 3 # number of input channels of generators\n    nc_D_inp = 6 # number of input channels of discriminators\n\n    def __init__(self, model_dir, gpus):\n        self.model_dir = model_dir\n        self.gpus = gpus\n        assert self.gpus == 1, \"Error: GAN128 cannot use multiple gpus right now. See https://github.com/deepfakes/faceswap/issues/287\"\n\n        optimizer = Adam(1e-4, 0.5)\n\n        # Build and compile the discriminator\n        self.netDA, self.netDB = self.build_discriminator()\n\n        # Build and compile the generator\n        self.netGA, self.netGB = self.build_generator()\n\n    def converter(self, swap):\n        predictor = self.netGB if not swap else self.netGA\n        return lambda img: predictor.predict(img)\n\n    def build_generator(self):\n\n        def conv_block(input_tensor, f, use_instance_norm=True):\n            x = input_tensor\n            x = SeparableConv2D(f, kernel_size=3, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n            if use_instance_norm:\n                x = inst_norm()(x)\n            x = Activation(\"relu\")(x)\n            return x\n\n        def res_block(input_tensor, f, dilation=1):\n            x = input_tensor\n            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\n            x = LeakyReLU(alpha=0.2)(x)\n            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\n            x = add([x, input_tensor])\n            #x = LeakyReLU(alpha=0.2)(x)\n            return x\n\n        def upscale_ps(filters, use_instance_norm=True):\n            def block(x, use_instance_norm=use_instance_norm):\n                x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same')(x)\n                if use_instance_norm:\n                    x = inst_norm()(x)\n                x = LeakyReLU(0.1)(x)\n                x = PixelShuffler()(x)\n                return x\n            return block\n\n        def Encoder(nc_in=3, input_size=128):\n            inp = Input(shape=(input_size, input_size, nc_in))\n            x = Conv2D(32, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(inp)\n            x = conv_block(x,64, use_instance_norm=False)\n            x = conv_block(x,128)\n            x = conv_block(x,256)\n            x = conv_block(x,512)\n            x = conv_block(x,1024)\n            x = Dense(1024)(Flatten()(x))\n            x = Dense(4*4*1024)(x)\n            x = Reshape((4, 4, 1024))(x)\n            out = upscale_ps(512)(x)\n            return Model(inputs=inp, outputs=out)\n\n        def Decoder_ps(nc_in=512, input_size=8):\n            input_ = Input(shape=(input_size, input_size, nc_in))\n            x = input_\n            x = upscale_ps(256)(x)\n            x = upscale_ps(128)(x)\n            x = upscale_ps(64)(x)\n            x = res_block(x, 64, dilation=2)\n\n            out64 = Conv2D(64, kernel_size=3, padding='same')(x)\n            out64 = LeakyReLU(alpha=0.1)(out64)\n            out64 = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(out64)\n\n            x = upscale_ps(32)(x)\n            x = res_block(x, 32)\n            x = res_block(x, 32)\n            alpha = Conv2D(1, kernel_size=5, padding='same', activation=\"sigmoid\")(x)\n            rgb = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(x)\n            out = concatenate([alpha, rgb])\n            return Model(input_, [out, out64] )\n\n        encoder = Encoder()\n        decoder_A = Decoder_ps()\n        decoder_B = Decoder_ps()\n        x = Input(shape=self.img_shape)\n        netGA = Model(x, decoder_A(encoder(x)))\n        netGB = Model(x, decoder_B(encoder(x)))\n\n        self.netGA_sm = netGA\n        self.netGB_sm = netGB\n\n        try:\n            netGA.load_weights(str(self.model_dir / netGAH5))\n            netGB.load_weights(str(self.model_dir / netGBH5))\n            print (\"Generator models loaded.\")\n        except:\n            print (\"Generator weights files not found.\")\n            pass\n\n        if self.gpus > 1:\n            netGA = multi_gpu_model( self.netGA_sm , self.gpus)\n            netGB = multi_gpu_model( self.netGB_sm , self.gpus)\n\n        return netGA, netGB\n\n    def build_discriminator(self):\n        def conv_block_d(input_tensor, f, use_instance_norm=True):\n            x = input_tensor\n            x = Conv2D(f, kernel_size=4, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n            if use_instance_norm:\n                x = inst_norm()(x)\n            x = LeakyReLU(alpha=0.2)(x)\n            return x\n\n        def Discriminator(nc_in, input_size=128):\n            inp = Input(shape=(input_size, input_size, nc_in))\n            #x = GaussianNoise(0.05)(inp)\n            x = conv_block_d(inp, 64, False)\n            x = conv_block_d(x, 128, True)\n            x = conv_block_d(x, 256, True)\n            x = conv_block_d(x, 512, True)\n            out = Conv2D(1, kernel_size=4, kernel_initializer=conv_init, use_bias=False, padding=\"same\", activation=\"sigmoid\")(x)\n            return Model(inputs=[inp], outputs=out)\n\n        netDA = Discriminator(self.nc_D_inp)\n        netDB = Discriminator(self.nc_D_inp)\n\n        try:\n            netDA.load_weights(str(self.model_dir / netDAH5))\n            netDB.load_weights(str(self.model_dir / netDBH5))\n            print (\"Discriminator models loaded.\")\n        except:\n            print (\"Discriminator weights files not found.\")\n            pass\n        return netDA, netDB\n\n    def load(self, swapped):\n        if swapped:\n            print(\"swapping not supported on GAN\")\n            # TODO load is done in __init__ => look how to swap if possible\n        return True\n\n    def save_weights(self):\n        if self.gpus > 1:\n            self.netGA_sm.save_weights(str(self.model_dir / netGAH5))\n            self.netGB_sm.save_weights(str(self.model_dir / netGBH5))\n        else:\n            self.netGA.save_weights(str(self.model_dir / netGAH5))\n            self.netGB.save_weights(str(self.model_dir / netGBH5))\n        self.netDA.save_weights(str(self.model_dir / netDAH5))\n        self.netDB.save_weights(str(self.model_dir / netDBH5))\n        print (\"Models saved.\")\n", "description": "Non official project based on original /r/Deepfakes thread. Many thanks to him!", "file_name": "Model.py", "id": "a671623bee6d8696b86c5648885cf9be", "language": "Python", "project_name": "faceswap", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/deepfakes-faceswap/deepfakes-faceswap-6ff64ef/plugins/Model_GAN128/Model.py", "save_time": "", "source": "", "update_at": "2018-03-18T16:27:43Z", "url": "https://github.com/deepfakes/faceswap", "wiki": true}