{"author": "docker", "code": "from __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport unittest\nfrom threading import Lock\n\nimport six\nfrom docker.errors import APIError\n\nfrom compose.parallel import GlobalLimit\nfrom compose.parallel import parallel_execute\nfrom compose.parallel import parallel_execute_iter\nfrom compose.parallel import ParallelStreamWriter\nfrom compose.parallel import UpstreamError\n\n\nweb = 'web'\ndb = 'db'\ndata_volume = 'data_volume'\ncache = 'cache'\n\nobjects = [web, db, data_volume, cache]\n\ndeps = {\n    web: [db, cache],\n    db: [data_volume],\n    data_volume: [],\n    cache: [],\n}\n\n\ndef get_deps(obj):\n    return [(dep, None) for dep in deps[obj]]\n\n\nclass ParallelTest(unittest.TestCase):\n\n    def test_parallel_execute(self):\n        results, errors = parallel_execute(\n            objects=[1, 2, 3, 4, 5],\n            func=lambda x: x * 2,\n            get_name=six.text_type,\n            msg=\"Doubling\",\n        )\n\n        assert sorted(results) == [2, 4, 6, 8, 10]\n        assert errors == {}\n\n    def test_parallel_execute_with_limit(self):\n        limit = 1\n        tasks = 20\n        lock = Lock()\n\n        def f(obj):\n            locked = lock.acquire(False)\n            \n            assert locked\n            lock.release()\n            return None\n\n        results, errors = parallel_execute(\n            objects=list(range(tasks)),\n            func=f,\n            get_name=six.text_type,\n            msg=\"Testing\",\n            limit=limit,\n        )\n\n        assert results == tasks * [None]\n        assert errors == {}\n\n    def test_parallel_execute_with_global_limit(self):\n        GlobalLimit.set_global_limit(1)\n        self.addCleanup(GlobalLimit.set_global_limit, None)\n        tasks = 20\n        lock = Lock()\n\n        def f(obj):\n            locked = lock.acquire(False)\n            \n            assert locked\n            lock.release()\n            return None\n\n        results, errors = parallel_execute(\n            objects=list(range(tasks)),\n            func=f,\n            get_name=six.text_type,\n            msg=\"Testing\",\n        )\n\n        assert results == tasks * [None]\n        assert errors == {}\n\n    def test_parallel_execute_with_deps(self):\n        log = []\n\n        def process(x):\n            log.append(x)\n\n        parallel_execute(\n            objects=objects,\n            func=process,\n            get_name=lambda obj: obj,\n            msg=\"Processing\",\n            get_deps=get_deps,\n        )\n\n        assert sorted(log) == sorted(objects)\n\n        assert log.index(data_volume) < log.index(db)\n        assert log.index(db) < log.index(web)\n        assert log.index(cache) < log.index(web)\n\n    def test_parallel_execute_with_upstream_errors(self):\n        log = []\n\n        def process(x):\n            if x is data_volume:\n                raise APIError(None, None, \"Something went wrong\")\n            log.append(x)\n\n        parallel_execute(\n            objects=objects,\n            func=process,\n            get_name=lambda obj: obj,\n            msg=\"Processing\",\n            get_deps=get_deps,\n        )\n\n        assert log == [cache]\n\n        events = [\n            (obj, result, type(exception))\n            for obj, result, exception\n            in parallel_execute_iter(objects, process, get_deps, None)\n        ]\n\n        assert (cache, None, type(None)) in events\n        assert (data_volume, None, APIError) in events\n        assert (db, None, UpstreamError) in events\n        assert (web, None, UpstreamError) in events\n\n\ndef test_parallel_execute_alignment(capsys):\n    ParallelStreamWriter.instance = None\n    results, errors = parallel_execute(\n        objects=[\"short\", \"a very long name\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Aligning\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    a, b = err.split('\\n')[:2]\n    assert a.index('...') == b.index('...')\n\n\ndef test_parallel_execute_ansi(capsys):\n    ParallelStreamWriter.instance = None\n    ParallelStreamWriter.set_noansi(value=False)\n    results, errors = parallel_execute(\n        objects=[\"something\", \"something more\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Control characters\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    assert \"\\x1b\" in err\n\n\ndef test_parallel_execute_noansi(capsys):\n    ParallelStreamWriter.instance = None\n    ParallelStreamWriter.set_noansi()\n    results, errors = parallel_execute(\n        objects=[\"something\", \"something more\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Control characters\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    assert \"\\x1b\" not in err\n", "comments": "  always get lock thread running    always get lock thread running ", "content": "from __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport unittest\nfrom threading import Lock\n\nimport six\nfrom docker.errors import APIError\n\nfrom compose.parallel import GlobalLimit\nfrom compose.parallel import parallel_execute\nfrom compose.parallel import parallel_execute_iter\nfrom compose.parallel import ParallelStreamWriter\nfrom compose.parallel import UpstreamError\n\n\nweb = 'web'\ndb = 'db'\ndata_volume = 'data_volume'\ncache = 'cache'\n\nobjects = [web, db, data_volume, cache]\n\ndeps = {\n    web: [db, cache],\n    db: [data_volume],\n    data_volume: [],\n    cache: [],\n}\n\n\ndef get_deps(obj):\n    return [(dep, None) for dep in deps[obj]]\n\n\nclass ParallelTest(unittest.TestCase):\n\n    def test_parallel_execute(self):\n        results, errors = parallel_execute(\n            objects=[1, 2, 3, 4, 5],\n            func=lambda x: x * 2,\n            get_name=six.text_type,\n            msg=\"Doubling\",\n        )\n\n        assert sorted(results) == [2, 4, 6, 8, 10]\n        assert errors == {}\n\n    def test_parallel_execute_with_limit(self):\n        limit = 1\n        tasks = 20\n        lock = Lock()\n\n        def f(obj):\n            locked = lock.acquire(False)\n            # we should always get the lock because we're the only thread running\n            assert locked\n            lock.release()\n            return None\n\n        results, errors = parallel_execute(\n            objects=list(range(tasks)),\n            func=f,\n            get_name=six.text_type,\n            msg=\"Testing\",\n            limit=limit,\n        )\n\n        assert results == tasks * [None]\n        assert errors == {}\n\n    def test_parallel_execute_with_global_limit(self):\n        GlobalLimit.set_global_limit(1)\n        self.addCleanup(GlobalLimit.set_global_limit, None)\n        tasks = 20\n        lock = Lock()\n\n        def f(obj):\n            locked = lock.acquire(False)\n            # we should always get the lock because we're the only thread running\n            assert locked\n            lock.release()\n            return None\n\n        results, errors = parallel_execute(\n            objects=list(range(tasks)),\n            func=f,\n            get_name=six.text_type,\n            msg=\"Testing\",\n        )\n\n        assert results == tasks * [None]\n        assert errors == {}\n\n    def test_parallel_execute_with_deps(self):\n        log = []\n\n        def process(x):\n            log.append(x)\n\n        parallel_execute(\n            objects=objects,\n            func=process,\n            get_name=lambda obj: obj,\n            msg=\"Processing\",\n            get_deps=get_deps,\n        )\n\n        assert sorted(log) == sorted(objects)\n\n        assert log.index(data_volume) < log.index(db)\n        assert log.index(db) < log.index(web)\n        assert log.index(cache) < log.index(web)\n\n    def test_parallel_execute_with_upstream_errors(self):\n        log = []\n\n        def process(x):\n            if x is data_volume:\n                raise APIError(None, None, \"Something went wrong\")\n            log.append(x)\n\n        parallel_execute(\n            objects=objects,\n            func=process,\n            get_name=lambda obj: obj,\n            msg=\"Processing\",\n            get_deps=get_deps,\n        )\n\n        assert log == [cache]\n\n        events = [\n            (obj, result, type(exception))\n            for obj, result, exception\n            in parallel_execute_iter(objects, process, get_deps, None)\n        ]\n\n        assert (cache, None, type(None)) in events\n        assert (data_volume, None, APIError) in events\n        assert (db, None, UpstreamError) in events\n        assert (web, None, UpstreamError) in events\n\n\ndef test_parallel_execute_alignment(capsys):\n    ParallelStreamWriter.instance = None\n    results, errors = parallel_execute(\n        objects=[\"short\", \"a very long name\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Aligning\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    a, b = err.split('\\n')[:2]\n    assert a.index('...') == b.index('...')\n\n\ndef test_parallel_execute_ansi(capsys):\n    ParallelStreamWriter.instance = None\n    ParallelStreamWriter.set_noansi(value=False)\n    results, errors = parallel_execute(\n        objects=[\"something\", \"something more\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Control characters\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    assert \"\\x1b\" in err\n\n\ndef test_parallel_execute_noansi(capsys):\n    ParallelStreamWriter.instance = None\n    ParallelStreamWriter.set_noansi()\n    results, errors = parallel_execute(\n        objects=[\"something\", \"something more\"],\n        func=lambda x: x,\n        get_name=six.text_type,\n        msg=\"Control characters\",\n    )\n\n    assert errors == {}\n\n    _, err = capsys.readouterr()\n    assert \"\\x1b\" not in err\n", "description": "Define and run multi-container applications with Docker", "file_name": "parallel_test.py", "id": "2281946738ca1d1428bf10323df7ce78", "language": "Python", "project_name": "compose", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/docker-compose/docker-compose-867ad15/tests/unit/parallel_test.py", "save_time": "", "source": "", "update_at": "2018-03-13T22:03:14Z", "url": "https://github.com/docker/compose", "wiki": false}