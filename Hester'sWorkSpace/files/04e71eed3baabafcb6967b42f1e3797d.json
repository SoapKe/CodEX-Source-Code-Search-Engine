{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Post-process embeddings from VGGish.\"\"\"\n\nimport numpy as np\n\nimport vggish_params\n\n\nclass Postprocessor(object):\n  \"\"\"Post-processes VGGish embeddings.\n\n  The initial release of AudioSet included 128-D VGGish embeddings for each\n  segment of AudioSet. These released embeddings were produced by applying\n  a PCA transformation (technically, a whitening transform is included as well)\n  and 8-bit quantization to the raw embedding output from VGGish, in order to\n  stay compatible with the YouTube-8M project which provides visual embeddings\n  in the same format for a large set of YouTube videos. This class implements\n  the same PCA (with whitening) and quantization transformations.\n  \"\"\"\n\n  def __init__(self, pca_params_npz_path):\n    \"\"\"Constructs a postprocessor.\n\n    Args:\n      pca_params_npz_path: Path to a NumPy-format .npz file that\n        contains the PCA parameters used in postprocessing.\n    \"\"\"\n    params = np.load(pca_params_npz_path)\n    self._pca_matrix = params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n     Load means into a column vector for easier broadcasting later.\n    self._pca_means = params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n    assert self._pca_matrix.shape == (\n        vggish_params.EMBEDDING_SIZE, vggish_params.EMBEDDING_SIZE), (\n            'Bad PCA matrix shape: %r' % (self._pca_matrix.shape,))\n    assert self._pca_means.shape == (vggish_params.EMBEDDING_SIZE, 1), (\n        'Bad PCA means shape: %r' % (self._pca_means.shape,))\n\n  def postprocess(self, embeddings_batch):\n    \"\"\"Applies postprocessing to a batch of embeddings.\n\n    Args:\n      embeddings_batch: An nparray of shape [batch_size, embedding_size]\n        containing output from the embedding layer of VGGish.\n\n    Returns:\n      An nparray of the same shape as the input but of type uint8,\n      containing the PCA-transformed and quantized version of the input.\n    \"\"\"\n    assert len(embeddings_batch.shape) == 2, (\n        'Expected 2-d batch, got %r' % (embeddings_batch.shape,))\n    assert embeddings_batch.shape[1] == vggish_params.EMBEDDING_SIZE, (\n        'Bad batch shape: %r' % (embeddings_batch.shape,))\n\n     Apply PCA.\n     - Embeddings come in as [batch_size, embedding_size].\n     - Transpose to [embedding_size, batch_size].\n     - Subtract pca_means column vector from each column.\n     - Premultiply by PCA matrix of shape [output_dims, input_dims]\n       where both are are equal to embedding_size in our case.\n     - Transpose result back to [batch_size, embedding_size].\n    pca_applied = np.dot(self._pca_matrix,\n                         (embeddings_batch.T - self._pca_means)).T\n\n     Quantize by:\n     - clipping to [min, max] range\n    clipped_embeddings = np.clip(\n        pca_applied, vggish_params.QUANTIZE_MIN_VAL,\n        vggish_params.QUANTIZE_MAX_VAL)\n     - convert to 8-bit in range [0.0, 255.0]\n    quantized_embeddings = (\n        (clipped_embeddings - vggish_params.QUANTIZE_MIN_VAL) *\n        (255.0 /\n         (vggish_params.QUANTIZE_MAX_VAL - vggish_params.QUANTIZE_MIN_VAL)))\n     - cast 8-bit float to uint8\n    quantized_embeddings = quantized_embeddings.astype(np.uint8)\n\n    return quantized_embeddings\n", "comments": "   post process embeddings vggish      import numpy np  import vggish params   class postprocessor(object)       post processes vggish embeddings     the initial release audioset included 128 d vggish embeddings   segment audioset  these released embeddings produced applying   pca transformation (technically  whitening transform included well)   8 bit quantization raw embedding output vggish  order   stay compatible youtube 8m project provides visual embeddings   format large set youtube videos  this class implements   pca (with whitening) quantization transformations           def   init  (self  pca params npz path)         constructs postprocessor       args        pca params npz path  path numpy format  npz file         contains pca parameters used postprocessing              params   np load(pca params npz path)     self  pca matrix   params vggish params pca eigen vectors name        load means column vector easier broadcasting later      self  pca means   params vggish params pca means name  reshape( 1  1)     assert self  pca matrix shape    (         vggish params embedding size  vggish params embedding size)  (              bad pca matrix shape   r    (self  pca matrix shape ))     assert self  pca means shape    (vggish params embedding size  1)  (          bad pca means shape   r    (self  pca means shape ))    def postprocess(self  embeddings batch)         applies postprocessing batch embeddings       args        embeddings batch  an nparray shape  batch size  embedding size          containing output embedding layer vggish       returns        an nparray shape input type uint8        containing pca transformed quantized version input             copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       load means column vector easier broadcasting later     apply pca       embeddings come  batch size  embedding size        transpose  embedding size  batch size        subtract pca means column vector column       premultiply pca matrix shape  output dims  input dims       equal embedding size case       transpose result back  batch size  embedding size      quantize       clipping  min  max  range      convert 8 bit range  0 0  255 0       cast 8 bit float uint8 ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Post-process embeddings from VGGish.\"\"\"\n\nimport numpy as np\n\nimport vggish_params\n\n\nclass Postprocessor(object):\n  \"\"\"Post-processes VGGish embeddings.\n\n  The initial release of AudioSet included 128-D VGGish embeddings for each\n  segment of AudioSet. These released embeddings were produced by applying\n  a PCA transformation (technically, a whitening transform is included as well)\n  and 8-bit quantization to the raw embedding output from VGGish, in order to\n  stay compatible with the YouTube-8M project which provides visual embeddings\n  in the same format for a large set of YouTube videos. This class implements\n  the same PCA (with whitening) and quantization transformations.\n  \"\"\"\n\n  def __init__(self, pca_params_npz_path):\n    \"\"\"Constructs a postprocessor.\n\n    Args:\n      pca_params_npz_path: Path to a NumPy-format .npz file that\n        contains the PCA parameters used in postprocessing.\n    \"\"\"\n    params = np.load(pca_params_npz_path)\n    self._pca_matrix = params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n    # Load means into a column vector for easier broadcasting later.\n    self._pca_means = params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n    assert self._pca_matrix.shape == (\n        vggish_params.EMBEDDING_SIZE, vggish_params.EMBEDDING_SIZE), (\n            'Bad PCA matrix shape: %r' % (self._pca_matrix.shape,))\n    assert self._pca_means.shape == (vggish_params.EMBEDDING_SIZE, 1), (\n        'Bad PCA means shape: %r' % (self._pca_means.shape,))\n\n  def postprocess(self, embeddings_batch):\n    \"\"\"Applies postprocessing to a batch of embeddings.\n\n    Args:\n      embeddings_batch: An nparray of shape [batch_size, embedding_size]\n        containing output from the embedding layer of VGGish.\n\n    Returns:\n      An nparray of the same shape as the input but of type uint8,\n      containing the PCA-transformed and quantized version of the input.\n    \"\"\"\n    assert len(embeddings_batch.shape) == 2, (\n        'Expected 2-d batch, got %r' % (embeddings_batch.shape,))\n    assert embeddings_batch.shape[1] == vggish_params.EMBEDDING_SIZE, (\n        'Bad batch shape: %r' % (embeddings_batch.shape,))\n\n    # Apply PCA.\n    # - Embeddings come in as [batch_size, embedding_size].\n    # - Transpose to [embedding_size, batch_size].\n    # - Subtract pca_means column vector from each column.\n    # - Premultiply by PCA matrix of shape [output_dims, input_dims]\n    #   where both are are equal to embedding_size in our case.\n    # - Transpose result back to [batch_size, embedding_size].\n    pca_applied = np.dot(self._pca_matrix,\n                         (embeddings_batch.T - self._pca_means)).T\n\n    # Quantize by:\n    # - clipping to [min, max] range\n    clipped_embeddings = np.clip(\n        pca_applied, vggish_params.QUANTIZE_MIN_VAL,\n        vggish_params.QUANTIZE_MAX_VAL)\n    # - convert to 8-bit in range [0.0, 255.0]\n    quantized_embeddings = (\n        (clipped_embeddings - vggish_params.QUANTIZE_MIN_VAL) *\n        (255.0 /\n         (vggish_params.QUANTIZE_MAX_VAL - vggish_params.QUANTIZE_MIN_VAL)))\n    # - cast 8-bit float to uint8\n    quantized_embeddings = quantized_embeddings.astype(np.uint8)\n\n    return quantized_embeddings\n", "description": "Models and examples built with TensorFlow", "file_name": "vggish_postprocess.py", "id": "04e71eed3baabafcb6967b42f1e3797d", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/audioset/vggish_postprocess.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}