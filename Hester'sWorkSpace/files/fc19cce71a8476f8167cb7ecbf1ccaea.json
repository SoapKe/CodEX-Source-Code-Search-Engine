{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"High-level code for creating and running FIVO-related Tensorflow graphs.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport bounds\nfrom data import datasets\nfrom models import vrnn\n\n\ndef create_dataset_and_model(config, split, shuffle, repeat):\n  \"\"\"Creates the dataset and model for a given config.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. This function expects the properties\n      batch_size, dataset_path, dataset_type, and latent_size to be defined.\n    split: The dataset split to load.\n    shuffle: If true, shuffle the dataset randomly.\n    repeat: If true, repeat the dataset endlessly.\n  Returns:\n    inputs: A batch of input sequences represented as a dense Tensor of shape\n      [time, batch_size, data_dimension].\n    targets: A batch of target sequences represented as a dense Tensor of\n      shape [time, batch_size, data_dimension].\n    lens: An int Tensor of shape [batch_size] representing the lengths of each\n      sequence in the batch.\n    model: A vrnn.VRNNCell model object.\n  \"\"\"\n  if config.dataset_type == \"pianoroll\":\n    inputs, targets, lengths, mean = datasets.create_pianoroll_dataset(\n        config.dataset_path, split, config.batch_size, shuffle=shuffle,\n        repeat=repeat)\n     Convert the mean of the training set to logit space so it can be used to\n     initialize the bias of the generative distribution.\n    generative_bias_init = -tf.log(\n        1. / tf.clip_by_value(mean, 0.0001, 0.9999) - 1)\n    generative_distribution_class = vrnn.ConditionalBernoulliDistribution\n  elif config.dataset_type == \"speech\":\n    inputs, targets, lengths = datasets.create_speech_dataset(\n        config.dataset_path, config.batch_size,\n        samples_per_timestep=config.data_dimension, prefetch_buffer_size=1,\n        shuffle=False, repeat=False)\n    generative_bias_init = None\n    generative_distribution_class = vrnn.ConditionalNormalDistribution\n  model = vrnn.create_vrnn(inputs.get_shape().as_list()[2],\n                           config.latent_size,\n                           generative_distribution_class,\n                           generative_bias_init=generative_bias_init,\n                           raw_sigma_bias=0.5)\n  return inputs, targets, lengths, model\n\n\ndef restore_checkpoint_if_exists(saver, sess, logdir):\n  \"\"\"Looks for a checkpoint and restores the session from it if found.\n\n  Args:\n    saver: A tf.train.Saver for restoring the session.\n    sess: A TensorFlow session.\n    logdir: The directory to look for checkpoints in.\n  Returns:\n    True if a checkpoint was found and restored, False otherwise.\n  \"\"\"\n  checkpoint = tf.train.get_checkpoint_state(logdir)\n  if checkpoint:\n    checkpoint_name = os.path.basename(checkpoint.model_checkpoint_path)\n    full_checkpoint_path = os.path.join(logdir, checkpoint_name)\n    saver.restore(sess, full_checkpoint_path)\n    return True\n  return False\n\n\ndef wait_for_checkpoint(saver, sess, logdir):\n  \"\"\"Loops until the session is restored from a checkpoint in logdir.\n\n  Args:\n    saver: A tf.train.Saver for restoring the session.\n    sess: A TensorFlow session.\n    logdir: The directory to look for checkpoints in.\n  \"\"\"\n  while True:\n    if restore_checkpoint_if_exists(saver, sess, logdir):\n      break\n    else:\n      tf.logging.info(\"Checkpoint not found in %s, sleeping for 60 seconds.\"\n                      % logdir)\n      time.sleep(60)\n\n\ndef run_train(config):\n  \"\"\"Runs training for a sequential latent variable model.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. For a list of expected properties and their\n      meaning see the flags defined in fivo.py.\n  \"\"\"\n\n  def create_logging_hook(step, bound_value):\n    \"\"\"Creates a logging hook that prints the bound value periodically.\"\"\"\n    bound_label = config.bound + \" bound\"\n    if config.normalize_by_seq_len:\n      bound_label += \" per timestep\"\n    else:\n      bound_label += \" per sequence\"\n    def summary_formatter(log_dict):\n      return \"Step %d, %s: %f\" % (\n          log_dict[\"step\"], bound_label, log_dict[\"bound_value\"])\n    logging_hook = tf.train.LoggingTensorHook(\n        {\"step\": step, \"bound_value\": bound_value},\n        every_n_iter=config.summarize_every,\n        formatter=summary_formatter)\n    return logging_hook\n\n  def create_loss():\n    \"\"\"Creates the loss to be optimized.\n\n    Returns:\n      bound: A float Tensor containing the value of the bound that is\n        being optimized.\n      loss: A float Tensor that when differentiated yields the gradients\n        to apply to the model. Should be optimized via gradient descent.\n    \"\"\"\n    inputs, targets, lengths, model = create_dataset_and_model(\n        config, split=\"train\", shuffle=True, repeat=True)\n     Compute lower bounds on the log likelihood.\n    if config.bound == \"elbo\":\n      ll_per_seq, _, _, _ = bounds.iwae(\n          model, (inputs, targets), lengths, num_samples=1)\n    elif config.bound == \"iwae\":\n      ll_per_seq, _, _, _ = bounds.iwae(\n          model, (inputs, targets), lengths, num_samples=config.num_samples)\n    elif config.bound == \"fivo\":\n      ll_per_seq, _, _, _, _ = bounds.fivo(\n          model, (inputs, targets), lengths, num_samples=config.num_samples,\n          resampling_criterion=bounds.ess_criterion)\n     Compute loss scaled by number of timesteps.\n    ll_per_t = tf.reduce_mean(ll_per_seq / tf.to_float(lengths))\n    ll_per_seq = tf.reduce_mean(ll_per_seq)\n\n    tf.summary.scalar(\"train_ll_per_seq\", ll_per_seq)\n    tf.summary.scalar(\"train_ll_per_t\", ll_per_t)\n\n    if config.normalize_by_seq_len:\n      return ll_per_t, -ll_per_t\n    else:\n      return ll_per_seq, -ll_per_seq\n\n  def create_graph():\n    \"\"\"Creates the training graph.\"\"\"\n    global_step = tf.train.get_or_create_global_step()\n    bound, loss = create_loss()\n    opt = tf.train.AdamOptimizer(config.learning_rate)\n    grads = opt.compute_gradients(loss, var_list=tf.trainable_variables())\n    train_op = opt.apply_gradients(grads, global_step=global_step)\n    return bound, train_op, global_step\n\n  device = tf.train.replica_device_setter(ps_tasks=config.ps_tasks)\n  with tf.Graph().as_default():\n    if config.random_seed: tf.set_random_seed(config.random_seed)\n    with tf.device(device):\n      bound, train_op, global_step = create_graph()\n      log_hook = create_logging_hook(global_step, bound)\n      start_training = not config.stagger_workers\n      with tf.train.MonitoredTrainingSession(\n          master=config.master,\n          is_chief=config.task == 0,\n          hooks=[log_hook],\n          checkpoint_dir=config.logdir,\n          save_checkpoint_secs=120,\n          save_summaries_steps=config.summarize_every,\n          log_step_count_steps=config.summarize_every) as sess:\n        cur_step = -1\n        while True:\n          if sess.should_stop() or cur_step > config.max_steps: break\n          if config.task > 0 and not start_training:\n            cur_step = sess.run(global_step)\n            tf.logging.info(\"task %d not active yet, sleeping at step %d\" %\n                            (config.task, cur_step))\n            time.sleep(30)\n            if cur_step >= config.task * 1000:\n              start_training = True\n          else:\n            _, cur_step = sess.run([train_op, global_step])\n\n\ndef run_eval(config):\n  \"\"\"Runs evaluation for a sequential latent variable model.\n\n  This method runs only one evaluation over the dataset, writes summaries to\n  disk, and then terminates. It does not loop indefinitely.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. For a list of expected properties and their\n      meaning see the flags defined in fivo.py.\n  \"\"\"\n\n  def create_graph():\n    \"\"\"Creates the evaluation graph.\n\n    Returns:\n      lower_bounds: A tuple of float Tensors containing the values of the 3\n        evidence lower bounds, summed across the batch.\n      total_batch_length: The total number of timesteps in the batch, summed\n        across batch examples.\n      batch_size: The batch size.\n      global_step: The global step the checkpoint was loaded from.\n    \"\"\"\n    global_step = tf.train.get_or_create_global_step()\n    inputs, targets, lengths, model = create_dataset_and_model(\n        config, split=config.split, shuffle=False, repeat=False)\n     Compute lower bounds on the log likelihood.\n    elbo_ll_per_seq, _, _, _ = bounds.iwae(\n        model, (inputs, targets), lengths, num_samples=1)\n    iwae_ll_per_seq, _, _, _ = bounds.iwae(\n        model, (inputs, targets), lengths, num_samples=config.num_samples)\n    fivo_ll_per_seq, _, _, _, _ = bounds.fivo(\n        model, (inputs, targets), lengths, num_samples=config.num_samples,\n        resampling_criterion=bounds.ess_criterion)\n    elbo_ll = tf.reduce_sum(elbo_ll_per_seq)\n    iwae_ll = tf.reduce_sum(iwae_ll_per_seq)\n    fivo_ll = tf.reduce_sum(fivo_ll_per_seq)\n    batch_size = tf.shape(lengths)[0]\n    total_batch_length = tf.reduce_sum(lengths)\n    return ((elbo_ll, iwae_ll, fivo_ll), total_batch_length, batch_size,\n            global_step)\n\n  def average_bounds_over_dataset(lower_bounds, total_batch_length, batch_size,\n                                  sess):\n    \"\"\"Computes the values of the bounds, averaged over the datset.\n\n    Args:\n      lower_bounds: Tuple of float Tensors containing the values of the bounds\n        evaluated on a single batch.\n      total_batch_length: Integer Tensor that represents the total number of\n        timesteps in the current batch.\n      batch_size: Integer Tensor containing the batch size. This can vary if the\n        requested batch_size does not evenly divide the size of the dataset.\n      sess: A TensorFlow Session object.\n    Returns:\n      ll_per_t: A length 3 numpy array of floats containing each bound's average\n        value, normalized by the total number of timesteps in the datset. Can\n        be interpreted as a lower bound on the average log likelihood per\n        timestep in the dataset.\n      ll_per_seq: A length 3 numpy array of floats containing each bound's\n        average value, normalized by the number of sequences in the dataset.\n        Can be interpreted as a lower bound on the average log likelihood per\n        sequence in the datset.\n    \"\"\"\n    total_ll = np.zeros(3, dtype=np.float64)\n    total_n_elems = 0.0\n    total_length = 0.0\n    while True:\n      try:\n        outs = sess.run([lower_bounds, batch_size, total_batch_length])\n      except tf.errors.OutOfRangeError:\n        break\n      total_ll += outs[0]\n      total_n_elems += outs[1]\n      total_length += outs[2]\n    ll_per_t = total_ll / total_length\n    ll_per_seq = total_ll / total_n_elems\n    return ll_per_t, ll_per_seq\n\n  def summarize_lls(lls_per_t, lls_per_seq, summary_writer, step):\n    \"\"\"Creates log-likelihood lower bound summaries and writes them to disk.\n\n    Args:\n      lls_per_t: An array of 3 python floats, contains the values of the\n        evaluated bounds normalized by the number of timesteps.\n      lls_per_seq: An array of 3 python floats, contains the values of the\n        evaluated bounds normalized by the number of sequences.\n      summary_writer: A tf.SummaryWriter.\n      step: The current global step.\n    \"\"\"\n    def scalar_summary(name, value):\n      value = tf.Summary.Value(tag=name, simple_value=value)\n      return tf.Summary(value=[value])\n\n    for i, bound in enumerate([\"elbo\", \"iwae\", \"fivo\"]):\n      per_t_summary = scalar_summary(\"%s/%s_ll_per_t\" % (config.split, bound),\n                                     lls_per_t[i])\n      per_seq_summary = scalar_summary(\"%s/%s_ll_per_seq\" %\n                                       (config.split, bound),\n                                       lls_per_seq[i])\n      summary_writer.add_summary(per_t_summary, global_step=step)\n      summary_writer.add_summary(per_seq_summary, global_step=step)\n    summary_writer.flush()\n\n  with tf.Graph().as_default():\n    if config.random_seed: tf.set_random_seed(config.random_seed)\n    lower_bounds, total_batch_length, batch_size, global_step = create_graph()\n    summary_dir = config.logdir + \"/\" + config.split\n    summary_writer = tf.summary.FileWriter(\n        summary_dir, flush_secs=15, max_queue=100)\n    saver = tf.train.Saver()\n    with tf.train.SingularMonitoredSession() as sess:\n      wait_for_checkpoint(saver, sess, config.logdir)\n      step = sess.run(global_step)\n      tf.logging.info(\"Model restored from step %d, evaluating.\" % step)\n      ll_per_t, ll_per_seq = average_bounds_over_dataset(\n          lower_bounds, total_batch_length, batch_size, sess)\n      summarize_lls(ll_per_t, ll_per_seq, summary_writer, step)\n      tf.logging.info(\"%s elbo ll/t: %f, iwae ll/t: %f fivo ll/t: %f\",\n                      config.split, ll_per_t[0], ll_per_t[1], ll_per_t[2])\n      tf.logging.info(\"%s elbo ll/seq: %f, iwae ll/seq: %f fivo ll/seq: %f\",\n                      config.split, ll_per_seq[0], ll_per_seq[1], ll_per_seq[2])\n", "comments": "   high level code creating running fivo related tensorflow graphs         future   import absolute import   future   import division   future   import print function   import os import time  import numpy np import tensorflow tf  import bounds data import datasets models import vrnn   def create dataset model(config  split  shuffle  repeat)       creates dataset model given config     args      config  a configuration object config values accessible properties        most likely flags object  this function expects properties       batch size  dataset path  dataset type  latent size defined      split  the dataset split load      shuffle  if true  shuffle dataset randomly      repeat  if true  repeat dataset endlessly    returns      inputs  a batch input sequences represented dense tensor shape        time  batch size  data dimension       targets  a batch target sequences represented dense tensor       shape  time  batch size  data dimension       lens  an int tensor shape  batch size  representing lengths       sequence batch      model  a vrnn vrnncell model object          config dataset type     pianoroll       inputs  targets  lengths  mean   datasets create pianoroll dataset(         config dataset path  split  config batch size  shuffle shuffle          repeat repeat)       convert mean training set logit space used       initialize bias generative distribution      generative bias init    tf log(         1    tf clip value(mean  0 0001  0 9999)   1)     generative distribution class   vrnn conditionalbernoullidistribution   elif config dataset type     speech       inputs  targets  lengths   datasets create speech dataset(         config dataset path  config batch size          samples per timestep config data dimension  prefetch buffer size 1          shuffle false  repeat false)     generative bias init   none     generative distribution class   vrnn conditionalnormaldistribution   model   vrnn create vrnn(inputs get shape() list() 2                              config latent size                             generative distribution class                             generative bias init generative bias init                             raw sigma bias 0 5)   return inputs  targets  lengths  model   def restore checkpoint exists(saver  sess  logdir)       looks checkpoint restores session found     args      saver  a tf train saver restoring session      sess  a tensorflow session      logdir  the directory look checkpoints    returns      true checkpoint found restored  false otherwise          checkpoint   tf train get checkpoint state(logdir)   checkpoint      checkpoint name   os path basename(checkpoint model checkpoint path)     full checkpoint path   os path join(logdir  checkpoint name)     saver restore(sess  full checkpoint path)     return true   return false   def wait checkpoint(saver  sess  logdir)       loops session restored checkpoint logdir     args      saver  a tf train saver restoring session      sess  a tensorflow session      logdir  the directory look checkpoints          true      restore checkpoint exists(saver  sess  logdir)        break     else        tf logging info( checkpoint found   sleeping 60 seconds                           logdir)       time sleep(60)   def run train(config)       runs training sequential latent variable model     args      config  a configuration object config values accessible properties        most likely flags object  for list expected properties       meaning see flags defined fivo py           def create logging hook(step  bound value)         creates logging hook prints bound value periodically         bound label   config bound     bound      config normalize seq len        bound label      per timestep      else        bound label      per sequence      def summary formatter(log dict)        return  step      f    (           log dict  step    bound label  log dict  bound value  )     logging hook   tf train loggingtensorhook(           step   step   bound value   bound value           every n iter config summarize every          formatter summary formatter)     return logging hook    def create loss()         creates loss optimized       returns        bound  a float tensor containing value bound         optimized        loss  a float tensor differentiated yields gradients         apply model  should optimized via gradient descent              inputs  targets  lengths  model   create dataset model(         config  split  train   shuffle true  repeat true)       compute lower bounds log likelihood      config bound     elbo         per seq            bounds iwae(           model  (inputs  targets)  lengths  num samples 1)     elif config bound     iwae         per seq            bounds iwae(           model  (inputs  targets)  lengths  num samples config num samples)     elif config bound     fivo         per seq               bounds fivo(           model  (inputs  targets)  lengths  num samples config num samples            resampling criterion bounds ess criterion)       compute loss scaled number timesteps      per   tf reduce mean(ll per seq   tf float(lengths))     per seq   tf reduce mean(ll per seq)      tf summary scalar( train per seq   per seq)     tf summary scalar( train per   per t)      config normalize seq len        return per   per     else        return per seq   per seq    def create graph()         creates training graph         global step   tf train get create global step()     bound  loss   create loss()     opt   tf train adamoptimizer(config learning rate)     grads   opt compute gradients(loss  var list tf trainable variables())     train op   opt apply gradients(grads  global step global step)     return bound  train op  global step    device   tf train replica device setter(ps tasks config ps tasks)   tf graph() default()      config random seed  tf set random seed(config random seed)     tf device(device)        bound  train op  global step   create graph()       log hook   create logging hook(global step  bound)       start training   config stagger workers       tf train monitoredtrainingsession(           master config master            chief config task    0            hooks  log hook             checkpoint dir config logdir            save checkpoint secs 120            save summaries steps config summarize every            log step count steps config summarize every) sess          cur step    1         true            sess stop() cur step   config max steps  break           config task   0 start training              cur step   sess run(global step)             tf logging info( task  active yet  sleeping step                                 (config task  cur step))             time sleep(30)             cur step    config task   1000                start training   true           else                 cur step   sess run( train op  global step )   def run eval(config)       runs evaluation sequential latent variable model     this method runs one evaluation dataset  writes summaries   disk  terminates  it loop indefinitely     args      config  a configuration object config values accessible properties        most likely flags object  for list expected properties       meaning see flags defined fivo py           def create graph()         creates evaluation graph       returns        lower bounds  a tuple float tensors containing values 3         evidence lower bounds  summed across batch        total batch length  the total number timesteps batch  summed         across batch examples        batch size  the batch size        global step  the global step checkpoint loaded              global step   tf train get create global step()     inputs  targets  lengths  model   create dataset model(         config  split config split  shuffle false  repeat false)       compute lower bounds log likelihood      elbo per seq            bounds iwae(         model  (inputs  targets)  lengths  num samples 1)     iwae per seq            bounds iwae(         model  (inputs  targets)  lengths  num samples config num samples)     fivo per seq               bounds fivo(         model  (inputs  targets)  lengths  num samples config num samples          resampling criterion bounds ess criterion)     elbo   tf reduce sum(elbo per seq)     iwae   tf reduce sum(iwae per seq)     fivo   tf reduce sum(fivo per seq)     batch size   tf shape(lengths) 0      total batch length   tf reduce sum(lengths)     return ((elbo  iwae  fivo ll)  total batch length  batch size              global step)    def average bounds dataset(lower bounds  total batch length  batch size                                    sess)         computes values bounds  averaged datset       args        lower bounds  tuple float tensors containing values bounds         evaluated single batch        total batch length  integer tensor represents total number         timesteps current batch        batch size  integer tensor containing batch size  this vary         requested batch size evenly divide size dataset        sess  a tensorflow session object      returns        per  a length 3 numpy array floats containing bound average         value  normalized total number timesteps datset  can         interpreted lower bound average log likelihood per         timestep dataset        per seq  a length 3 numpy array floats containing bound         average value  normalized number sequences dataset          can interpreted lower bound average log likelihood per         sequence datset              total   np zeros(3  dtype np float64)     total n elems   0 0     total length   0 0     true        try          outs   sess run( lower bounds  batch size  total batch length )       except tf errors outofrangeerror          break       total    outs 0        total n elems    outs 1        total length    outs 2      per   total   total length     per seq   total   total n elems     return per  per seq    def summarize lls(lls per  lls per seq  summary writer  step)         creates log likelihood lower bound summaries writes disk       args        lls per  an array 3 python floats  contains values         evaluated bounds normalized number timesteps        lls per seq  an array 3 python floats  contains values         evaluated bounds normalized number sequences        summary writer  a tf summarywriter        step  the current global step             copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       convert mean training set logit space used    initialize bias generative distribution     compute lower bounds log likelihood     compute loss scaled number timesteps     compute lower bounds log likelihood  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"High-level code for creating and running FIVO-related Tensorflow graphs.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport bounds\nfrom data import datasets\nfrom models import vrnn\n\n\ndef create_dataset_and_model(config, split, shuffle, repeat):\n  \"\"\"Creates the dataset and model for a given config.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. This function expects the properties\n      batch_size, dataset_path, dataset_type, and latent_size to be defined.\n    split: The dataset split to load.\n    shuffle: If true, shuffle the dataset randomly.\n    repeat: If true, repeat the dataset endlessly.\n  Returns:\n    inputs: A batch of input sequences represented as a dense Tensor of shape\n      [time, batch_size, data_dimension].\n    targets: A batch of target sequences represented as a dense Tensor of\n      shape [time, batch_size, data_dimension].\n    lens: An int Tensor of shape [batch_size] representing the lengths of each\n      sequence in the batch.\n    model: A vrnn.VRNNCell model object.\n  \"\"\"\n  if config.dataset_type == \"pianoroll\":\n    inputs, targets, lengths, mean = datasets.create_pianoroll_dataset(\n        config.dataset_path, split, config.batch_size, shuffle=shuffle,\n        repeat=repeat)\n    # Convert the mean of the training set to logit space so it can be used to\n    # initialize the bias of the generative distribution.\n    generative_bias_init = -tf.log(\n        1. / tf.clip_by_value(mean, 0.0001, 0.9999) - 1)\n    generative_distribution_class = vrnn.ConditionalBernoulliDistribution\n  elif config.dataset_type == \"speech\":\n    inputs, targets, lengths = datasets.create_speech_dataset(\n        config.dataset_path, config.batch_size,\n        samples_per_timestep=config.data_dimension, prefetch_buffer_size=1,\n        shuffle=False, repeat=False)\n    generative_bias_init = None\n    generative_distribution_class = vrnn.ConditionalNormalDistribution\n  model = vrnn.create_vrnn(inputs.get_shape().as_list()[2],\n                           config.latent_size,\n                           generative_distribution_class,\n                           generative_bias_init=generative_bias_init,\n                           raw_sigma_bias=0.5)\n  return inputs, targets, lengths, model\n\n\ndef restore_checkpoint_if_exists(saver, sess, logdir):\n  \"\"\"Looks for a checkpoint and restores the session from it if found.\n\n  Args:\n    saver: A tf.train.Saver for restoring the session.\n    sess: A TensorFlow session.\n    logdir: The directory to look for checkpoints in.\n  Returns:\n    True if a checkpoint was found and restored, False otherwise.\n  \"\"\"\n  checkpoint = tf.train.get_checkpoint_state(logdir)\n  if checkpoint:\n    checkpoint_name = os.path.basename(checkpoint.model_checkpoint_path)\n    full_checkpoint_path = os.path.join(logdir, checkpoint_name)\n    saver.restore(sess, full_checkpoint_path)\n    return True\n  return False\n\n\ndef wait_for_checkpoint(saver, sess, logdir):\n  \"\"\"Loops until the session is restored from a checkpoint in logdir.\n\n  Args:\n    saver: A tf.train.Saver for restoring the session.\n    sess: A TensorFlow session.\n    logdir: The directory to look for checkpoints in.\n  \"\"\"\n  while True:\n    if restore_checkpoint_if_exists(saver, sess, logdir):\n      break\n    else:\n      tf.logging.info(\"Checkpoint not found in %s, sleeping for 60 seconds.\"\n                      % logdir)\n      time.sleep(60)\n\n\ndef run_train(config):\n  \"\"\"Runs training for a sequential latent variable model.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. For a list of expected properties and their\n      meaning see the flags defined in fivo.py.\n  \"\"\"\n\n  def create_logging_hook(step, bound_value):\n    \"\"\"Creates a logging hook that prints the bound value periodically.\"\"\"\n    bound_label = config.bound + \" bound\"\n    if config.normalize_by_seq_len:\n      bound_label += \" per timestep\"\n    else:\n      bound_label += \" per sequence\"\n    def summary_formatter(log_dict):\n      return \"Step %d, %s: %f\" % (\n          log_dict[\"step\"], bound_label, log_dict[\"bound_value\"])\n    logging_hook = tf.train.LoggingTensorHook(\n        {\"step\": step, \"bound_value\": bound_value},\n        every_n_iter=config.summarize_every,\n        formatter=summary_formatter)\n    return logging_hook\n\n  def create_loss():\n    \"\"\"Creates the loss to be optimized.\n\n    Returns:\n      bound: A float Tensor containing the value of the bound that is\n        being optimized.\n      loss: A float Tensor that when differentiated yields the gradients\n        to apply to the model. Should be optimized via gradient descent.\n    \"\"\"\n    inputs, targets, lengths, model = create_dataset_and_model(\n        config, split=\"train\", shuffle=True, repeat=True)\n    # Compute lower bounds on the log likelihood.\n    if config.bound == \"elbo\":\n      ll_per_seq, _, _, _ = bounds.iwae(\n          model, (inputs, targets), lengths, num_samples=1)\n    elif config.bound == \"iwae\":\n      ll_per_seq, _, _, _ = bounds.iwae(\n          model, (inputs, targets), lengths, num_samples=config.num_samples)\n    elif config.bound == \"fivo\":\n      ll_per_seq, _, _, _, _ = bounds.fivo(\n          model, (inputs, targets), lengths, num_samples=config.num_samples,\n          resampling_criterion=bounds.ess_criterion)\n    # Compute loss scaled by number of timesteps.\n    ll_per_t = tf.reduce_mean(ll_per_seq / tf.to_float(lengths))\n    ll_per_seq = tf.reduce_mean(ll_per_seq)\n\n    tf.summary.scalar(\"train_ll_per_seq\", ll_per_seq)\n    tf.summary.scalar(\"train_ll_per_t\", ll_per_t)\n\n    if config.normalize_by_seq_len:\n      return ll_per_t, -ll_per_t\n    else:\n      return ll_per_seq, -ll_per_seq\n\n  def create_graph():\n    \"\"\"Creates the training graph.\"\"\"\n    global_step = tf.train.get_or_create_global_step()\n    bound, loss = create_loss()\n    opt = tf.train.AdamOptimizer(config.learning_rate)\n    grads = opt.compute_gradients(loss, var_list=tf.trainable_variables())\n    train_op = opt.apply_gradients(grads, global_step=global_step)\n    return bound, train_op, global_step\n\n  device = tf.train.replica_device_setter(ps_tasks=config.ps_tasks)\n  with tf.Graph().as_default():\n    if config.random_seed: tf.set_random_seed(config.random_seed)\n    with tf.device(device):\n      bound, train_op, global_step = create_graph()\n      log_hook = create_logging_hook(global_step, bound)\n      start_training = not config.stagger_workers\n      with tf.train.MonitoredTrainingSession(\n          master=config.master,\n          is_chief=config.task == 0,\n          hooks=[log_hook],\n          checkpoint_dir=config.logdir,\n          save_checkpoint_secs=120,\n          save_summaries_steps=config.summarize_every,\n          log_step_count_steps=config.summarize_every) as sess:\n        cur_step = -1\n        while True:\n          if sess.should_stop() or cur_step > config.max_steps: break\n          if config.task > 0 and not start_training:\n            cur_step = sess.run(global_step)\n            tf.logging.info(\"task %d not active yet, sleeping at step %d\" %\n                            (config.task, cur_step))\n            time.sleep(30)\n            if cur_step >= config.task * 1000:\n              start_training = True\n          else:\n            _, cur_step = sess.run([train_op, global_step])\n\n\ndef run_eval(config):\n  \"\"\"Runs evaluation for a sequential latent variable model.\n\n  This method runs only one evaluation over the dataset, writes summaries to\n  disk, and then terminates. It does not loop indefinitely.\n\n  Args:\n    config: A configuration object with config values accessible as properties.\n      Most likely a FLAGS object. For a list of expected properties and their\n      meaning see the flags defined in fivo.py.\n  \"\"\"\n\n  def create_graph():\n    \"\"\"Creates the evaluation graph.\n\n    Returns:\n      lower_bounds: A tuple of float Tensors containing the values of the 3\n        evidence lower bounds, summed across the batch.\n      total_batch_length: The total number of timesteps in the batch, summed\n        across batch examples.\n      batch_size: The batch size.\n      global_step: The global step the checkpoint was loaded from.\n    \"\"\"\n    global_step = tf.train.get_or_create_global_step()\n    inputs, targets, lengths, model = create_dataset_and_model(\n        config, split=config.split, shuffle=False, repeat=False)\n    # Compute lower bounds on the log likelihood.\n    elbo_ll_per_seq, _, _, _ = bounds.iwae(\n        model, (inputs, targets), lengths, num_samples=1)\n    iwae_ll_per_seq, _, _, _ = bounds.iwae(\n        model, (inputs, targets), lengths, num_samples=config.num_samples)\n    fivo_ll_per_seq, _, _, _, _ = bounds.fivo(\n        model, (inputs, targets), lengths, num_samples=config.num_samples,\n        resampling_criterion=bounds.ess_criterion)\n    elbo_ll = tf.reduce_sum(elbo_ll_per_seq)\n    iwae_ll = tf.reduce_sum(iwae_ll_per_seq)\n    fivo_ll = tf.reduce_sum(fivo_ll_per_seq)\n    batch_size = tf.shape(lengths)[0]\n    total_batch_length = tf.reduce_sum(lengths)\n    return ((elbo_ll, iwae_ll, fivo_ll), total_batch_length, batch_size,\n            global_step)\n\n  def average_bounds_over_dataset(lower_bounds, total_batch_length, batch_size,\n                                  sess):\n    \"\"\"Computes the values of the bounds, averaged over the datset.\n\n    Args:\n      lower_bounds: Tuple of float Tensors containing the values of the bounds\n        evaluated on a single batch.\n      total_batch_length: Integer Tensor that represents the total number of\n        timesteps in the current batch.\n      batch_size: Integer Tensor containing the batch size. This can vary if the\n        requested batch_size does not evenly divide the size of the dataset.\n      sess: A TensorFlow Session object.\n    Returns:\n      ll_per_t: A length 3 numpy array of floats containing each bound's average\n        value, normalized by the total number of timesteps in the datset. Can\n        be interpreted as a lower bound on the average log likelihood per\n        timestep in the dataset.\n      ll_per_seq: A length 3 numpy array of floats containing each bound's\n        average value, normalized by the number of sequences in the dataset.\n        Can be interpreted as a lower bound on the average log likelihood per\n        sequence in the datset.\n    \"\"\"\n    total_ll = np.zeros(3, dtype=np.float64)\n    total_n_elems = 0.0\n    total_length = 0.0\n    while True:\n      try:\n        outs = sess.run([lower_bounds, batch_size, total_batch_length])\n      except tf.errors.OutOfRangeError:\n        break\n      total_ll += outs[0]\n      total_n_elems += outs[1]\n      total_length += outs[2]\n    ll_per_t = total_ll / total_length\n    ll_per_seq = total_ll / total_n_elems\n    return ll_per_t, ll_per_seq\n\n  def summarize_lls(lls_per_t, lls_per_seq, summary_writer, step):\n    \"\"\"Creates log-likelihood lower bound summaries and writes them to disk.\n\n    Args:\n      lls_per_t: An array of 3 python floats, contains the values of the\n        evaluated bounds normalized by the number of timesteps.\n      lls_per_seq: An array of 3 python floats, contains the values of the\n        evaluated bounds normalized by the number of sequences.\n      summary_writer: A tf.SummaryWriter.\n      step: The current global step.\n    \"\"\"\n    def scalar_summary(name, value):\n      value = tf.Summary.Value(tag=name, simple_value=value)\n      return tf.Summary(value=[value])\n\n    for i, bound in enumerate([\"elbo\", \"iwae\", \"fivo\"]):\n      per_t_summary = scalar_summary(\"%s/%s_ll_per_t\" % (config.split, bound),\n                                     lls_per_t[i])\n      per_seq_summary = scalar_summary(\"%s/%s_ll_per_seq\" %\n                                       (config.split, bound),\n                                       lls_per_seq[i])\n      summary_writer.add_summary(per_t_summary, global_step=step)\n      summary_writer.add_summary(per_seq_summary, global_step=step)\n    summary_writer.flush()\n\n  with tf.Graph().as_default():\n    if config.random_seed: tf.set_random_seed(config.random_seed)\n    lower_bounds, total_batch_length, batch_size, global_step = create_graph()\n    summary_dir = config.logdir + \"/\" + config.split\n    summary_writer = tf.summary.FileWriter(\n        summary_dir, flush_secs=15, max_queue=100)\n    saver = tf.train.Saver()\n    with tf.train.SingularMonitoredSession() as sess:\n      wait_for_checkpoint(saver, sess, config.logdir)\n      step = sess.run(global_step)\n      tf.logging.info(\"Model restored from step %d, evaluating.\" % step)\n      ll_per_t, ll_per_seq = average_bounds_over_dataset(\n          lower_bounds, total_batch_length, batch_size, sess)\n      summarize_lls(ll_per_t, ll_per_seq, summary_writer, step)\n      tf.logging.info(\"%s elbo ll/t: %f, iwae ll/t: %f fivo ll/t: %f\",\n                      config.split, ll_per_t[0], ll_per_t[1], ll_per_t[2])\n      tf.logging.info(\"%s elbo ll/seq: %f, iwae ll/seq: %f fivo ll/seq: %f\",\n                      config.split, ll_per_seq[0], ll_per_seq[1], ll_per_seq[2])\n", "description": "Models and examples built with TensorFlow", "file_name": "runners.py", "id": "fc19cce71a8476f8167cb7ecbf1ccaea", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/fivo/runners.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}