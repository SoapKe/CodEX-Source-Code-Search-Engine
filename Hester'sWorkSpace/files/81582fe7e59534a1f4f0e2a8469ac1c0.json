{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n ==============================================================================\nfrom __future__ import print_function\n\nimport os\nimport h5py\nimport numpy as np\nfrom six.moves import xrange\n\nfrom synthetic_data_utils import generate_data, generate_rnn\nfrom synthetic_data_utils import get_train_n_valid_inds\nfrom synthetic_data_utils import nparray_and_transpose\nfrom synthetic_data_utils import spikify_data, split_list_by_inds\nimport tensorflow as tf\nfrom utils import write_datasets\n\nDATA_DIR = \"rnn_synth_data_v1.0\"\n\nflags = tf.app.flags\nflags.DEFINE_string(\"save_dir\", \"/tmp/\" + DATA_DIR + \"/\",\n                    \"Directory for saving data.\")\nflags.DEFINE_string(\"datafile_name\", \"conditioned_rnn_data\",\n                    \"Name of data file for input case.\")\nflags.DEFINE_integer(\"synth_data_seed\", 5, \"Random seed for RNN generation.\")\nflags.DEFINE_float(\"T\", 1.0, \"Time in seconds to generate.\")\nflags.DEFINE_integer(\"C\", 400, \"Number of conditions\")\nflags.DEFINE_integer(\"N\", 50, \"Number of units for the RNN\")\nflags.DEFINE_float(\"train_percentage\", 4.0/5.0,\n                   \"Percentage of train vs validation trials\")\nflags.DEFINE_integer(\"nreplications\", 10,\n                     \"Number of spikifications of the same underlying rates.\")\nflags.DEFINE_float(\"g\", 1.5, \"Complexity of dynamics\")\nflags.DEFINE_float(\"x0_std\", 1.0,\n                   \"Volume from which to pull initial conditions (affects diversity of dynamics.\")\nflags.DEFINE_float(\"tau\", 0.025, \"Time constant of RNN\")\nflags.DEFINE_float(\"dt\", 0.010, \"Time bin\")\nflags.DEFINE_float(\"max_firing_rate\", 30.0, \"Map 1.0 of RNN to a spikes per second\")\nFLAGS = flags.FLAGS\n\nrng = np.random.RandomState(seed=FLAGS.synth_data_seed)\nrnn_rngs = [np.random.RandomState(seed=FLAGS.synth_data_seed+1),\n            np.random.RandomState(seed=FLAGS.synth_data_seed+2)]\nT = FLAGS.T\nC = FLAGS.C\nN = FLAGS.N\nnreplications = FLAGS.nreplications\nE = nreplications * C\ntrain_percentage = FLAGS.train_percentage\nntimesteps = int(T / FLAGS.dt)\n\nrnn_a = generate_rnn(rnn_rngs[0], N, FLAGS.g, FLAGS.tau, FLAGS.dt,\n                     FLAGS.max_firing_rate)\nrnn_b = generate_rnn(rnn_rngs[1], N, FLAGS.g, FLAGS.tau, FLAGS.dt,\n                     FLAGS.max_firing_rate)\nrnns = [rnn_a, rnn_b]\n\n pick which RNN is used on each trial\nrnn_to_use = rng.randint(2, size=E)\next_input = np.repeat(np.expand_dims(rnn_to_use, axis=1), ntimesteps, axis=1)\next_input = np.expand_dims(ext_input, axis=2)   these are \"a's\" in the paper\n\nx0s = []\ncondition_labels = []\ncondition_number = 0\nfor c in range(C):\n  x0 = FLAGS.x0_std * rng.randn(N, 1)\n  x0s.append(np.tile(x0, nreplications))\n  for ns in range(nreplications):\n    condition_labels.append(condition_number)\n  condition_number += 1\nx0s = np.concatenate(x0s, axis=1)\n\nP_nxn = rng.randn(N, N) / np.sqrt(N)\n\n generate trials for both RNNs\nrates_a, x0s_a, _ = generate_data(rnn_a, T=T, E=E, x0s=x0s, P_sxn=P_nxn,\n                                  input_magnitude=0.0, input_times=None)\nspikes_a = spikify_data(rates_a, rng, rnn_a['dt'], rnn_a['max_firing_rate'])\n\nrates_b, x0s_b, _ = generate_data(rnn_b, T=T, E=E, x0s=x0s, P_sxn=P_nxn,\n                                  input_magnitude=0.0, input_times=None)\nspikes_b = spikify_data(rates_b, rng, rnn_b['dt'], rnn_b['max_firing_rate'])\n\n not the best way to do this but E is small enough\nrates = []\nspikes = []\nfor trial in xrange(E):\n  if rnn_to_use[trial] == 0:\n    rates.append(rates_a[trial])\n    spikes.append(spikes_a[trial])\n  else:\n    rates.append(rates_b[trial])\n    spikes.append(spikes_b[trial])\n\n split into train and validation sets\ntrain_inds, valid_inds = get_train_n_valid_inds(E, train_percentage,\n                                                nreplications)\n\nrates_train, rates_valid = split_list_by_inds(rates, train_inds, valid_inds)\nspikes_train, spikes_valid = split_list_by_inds(spikes, train_inds, valid_inds)\ncondition_labels_train, condition_labels_valid = split_list_by_inds(\n    condition_labels, train_inds, valid_inds)\next_input_train, ext_input_valid = split_list_by_inds(\n    ext_input, train_inds, valid_inds)\n\nrates_train = nparray_and_transpose(rates_train)\nrates_valid = nparray_and_transpose(rates_valid)\nspikes_train = nparray_and_transpose(spikes_train)\nspikes_valid = nparray_and_transpose(spikes_valid)\n\n add train_ext_input and valid_ext input\ndata = {'train_truth': rates_train,\n        'valid_truth': rates_valid,\n        'train_data' : spikes_train,\n        'valid_data' : spikes_valid,\n        'train_ext_input' : np.array(ext_input_train),\n        'valid_ext_input': np.array(ext_input_valid),\n        'train_percentage' : train_percentage,\n        'nreplications' : nreplications,\n        'dt' : FLAGS.dt,\n        'P_sxn' : P_nxn,\n        'condition_labels_train' : condition_labels_train,\n        'condition_labels_valid' : condition_labels_valid,\n        'conversion_factor': 1.0 / rnn_a['conversion_factor']}\n\n just one dataset here\ndatasets = {}\ndataset_name = 'dataset_N' + str(N)\ndatasets[dataset_name] = data\n\n write out the dataset\nwrite_datasets(FLAGS.save_dir, FLAGS.datafile_name, datasets)\nprint ('Saved to ', os.path.join(FLAGS.save_dir,\n                                 FLAGS.datafile_name + '_' + dataset_name))\n", "comments": "  copyright 2017 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                          pick rnn used trial      paper    generate trials rnns    best way e small enough    split train validation sets    add train ext input valid ext input    one dataset    write dataset ", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\nfrom __future__ import print_function\n\nimport os\nimport h5py\nimport numpy as np\nfrom six.moves import xrange\n\nfrom synthetic_data_utils import generate_data, generate_rnn\nfrom synthetic_data_utils import get_train_n_valid_inds\nfrom synthetic_data_utils import nparray_and_transpose\nfrom synthetic_data_utils import spikify_data, split_list_by_inds\nimport tensorflow as tf\nfrom utils import write_datasets\n\nDATA_DIR = \"rnn_synth_data_v1.0\"\n\nflags = tf.app.flags\nflags.DEFINE_string(\"save_dir\", \"/tmp/\" + DATA_DIR + \"/\",\n                    \"Directory for saving data.\")\nflags.DEFINE_string(\"datafile_name\", \"conditioned_rnn_data\",\n                    \"Name of data file for input case.\")\nflags.DEFINE_integer(\"synth_data_seed\", 5, \"Random seed for RNN generation.\")\nflags.DEFINE_float(\"T\", 1.0, \"Time in seconds to generate.\")\nflags.DEFINE_integer(\"C\", 400, \"Number of conditions\")\nflags.DEFINE_integer(\"N\", 50, \"Number of units for the RNN\")\nflags.DEFINE_float(\"train_percentage\", 4.0/5.0,\n                   \"Percentage of train vs validation trials\")\nflags.DEFINE_integer(\"nreplications\", 10,\n                     \"Number of spikifications of the same underlying rates.\")\nflags.DEFINE_float(\"g\", 1.5, \"Complexity of dynamics\")\nflags.DEFINE_float(\"x0_std\", 1.0,\n                   \"Volume from which to pull initial conditions (affects diversity of dynamics.\")\nflags.DEFINE_float(\"tau\", 0.025, \"Time constant of RNN\")\nflags.DEFINE_float(\"dt\", 0.010, \"Time bin\")\nflags.DEFINE_float(\"max_firing_rate\", 30.0, \"Map 1.0 of RNN to a spikes per second\")\nFLAGS = flags.FLAGS\n\nrng = np.random.RandomState(seed=FLAGS.synth_data_seed)\nrnn_rngs = [np.random.RandomState(seed=FLAGS.synth_data_seed+1),\n            np.random.RandomState(seed=FLAGS.synth_data_seed+2)]\nT = FLAGS.T\nC = FLAGS.C\nN = FLAGS.N\nnreplications = FLAGS.nreplications\nE = nreplications * C\ntrain_percentage = FLAGS.train_percentage\nntimesteps = int(T / FLAGS.dt)\n\nrnn_a = generate_rnn(rnn_rngs[0], N, FLAGS.g, FLAGS.tau, FLAGS.dt,\n                     FLAGS.max_firing_rate)\nrnn_b = generate_rnn(rnn_rngs[1], N, FLAGS.g, FLAGS.tau, FLAGS.dt,\n                     FLAGS.max_firing_rate)\nrnns = [rnn_a, rnn_b]\n\n# pick which RNN is used on each trial\nrnn_to_use = rng.randint(2, size=E)\next_input = np.repeat(np.expand_dims(rnn_to_use, axis=1), ntimesteps, axis=1)\next_input = np.expand_dims(ext_input, axis=2)  # these are \"a's\" in the paper\n\nx0s = []\ncondition_labels = []\ncondition_number = 0\nfor c in range(C):\n  x0 = FLAGS.x0_std * rng.randn(N, 1)\n  x0s.append(np.tile(x0, nreplications))\n  for ns in range(nreplications):\n    condition_labels.append(condition_number)\n  condition_number += 1\nx0s = np.concatenate(x0s, axis=1)\n\nP_nxn = rng.randn(N, N) / np.sqrt(N)\n\n# generate trials for both RNNs\nrates_a, x0s_a, _ = generate_data(rnn_a, T=T, E=E, x0s=x0s, P_sxn=P_nxn,\n                                  input_magnitude=0.0, input_times=None)\nspikes_a = spikify_data(rates_a, rng, rnn_a['dt'], rnn_a['max_firing_rate'])\n\nrates_b, x0s_b, _ = generate_data(rnn_b, T=T, E=E, x0s=x0s, P_sxn=P_nxn,\n                                  input_magnitude=0.0, input_times=None)\nspikes_b = spikify_data(rates_b, rng, rnn_b['dt'], rnn_b['max_firing_rate'])\n\n# not the best way to do this but E is small enough\nrates = []\nspikes = []\nfor trial in xrange(E):\n  if rnn_to_use[trial] == 0:\n    rates.append(rates_a[trial])\n    spikes.append(spikes_a[trial])\n  else:\n    rates.append(rates_b[trial])\n    spikes.append(spikes_b[trial])\n\n# split into train and validation sets\ntrain_inds, valid_inds = get_train_n_valid_inds(E, train_percentage,\n                                                nreplications)\n\nrates_train, rates_valid = split_list_by_inds(rates, train_inds, valid_inds)\nspikes_train, spikes_valid = split_list_by_inds(spikes, train_inds, valid_inds)\ncondition_labels_train, condition_labels_valid = split_list_by_inds(\n    condition_labels, train_inds, valid_inds)\next_input_train, ext_input_valid = split_list_by_inds(\n    ext_input, train_inds, valid_inds)\n\nrates_train = nparray_and_transpose(rates_train)\nrates_valid = nparray_and_transpose(rates_valid)\nspikes_train = nparray_and_transpose(spikes_train)\nspikes_valid = nparray_and_transpose(spikes_valid)\n\n# add train_ext_input and valid_ext input\ndata = {'train_truth': rates_train,\n        'valid_truth': rates_valid,\n        'train_data' : spikes_train,\n        'valid_data' : spikes_valid,\n        'train_ext_input' : np.array(ext_input_train),\n        'valid_ext_input': np.array(ext_input_valid),\n        'train_percentage' : train_percentage,\n        'nreplications' : nreplications,\n        'dt' : FLAGS.dt,\n        'P_sxn' : P_nxn,\n        'condition_labels_train' : condition_labels_train,\n        'condition_labels_valid' : condition_labels_valid,\n        'conversion_factor': 1.0 / rnn_a['conversion_factor']}\n\n# just one dataset here\ndatasets = {}\ndataset_name = 'dataset_N' + str(N)\ndatasets[dataset_name] = data\n\n# write out the dataset\nwrite_datasets(FLAGS.save_dir, FLAGS.datafile_name, datasets)\nprint ('Saved to ', os.path.join(FLAGS.save_dir,\n                                 FLAGS.datafile_name + '_' + dataset_name))\n", "description": "Models and examples built with TensorFlow", "file_name": "generate_labeled_rnn_data.py", "id": "81582fe7e59534a1f4f0e2a8469ac1c0", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/lfads/synth_data/generate_labeled_rnn_data.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}