{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Build the Inception v3 network on ImageNet data set.\n\nThe Inception v3 architecture is described in http://arxiv.org/abs/1512.00567\n\nSummary of available functions:\n inference: Compute inference on the model inputs to make a prediction\n loss: Compute the loss of the prediction with respect to the labels\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\nfrom inception.slim import slim\n\nFLAGS = tf.app.flags.FLAGS\n\n If a model is trained using multiple GPUs, prefix all Op names with tower_name\n to differentiate the operations. Note that this prefix is removed from the\n names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\n Batch normalization. Constant governing the exponential moving average of\n the 'global' mean and variance for all activations.\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\n\n The decay to use for the moving average.\nMOVING_AVERAGE_DECAY = 0.9999\n\n\ndef inference(images, num_classes, for_training=False, restore_logits=True,\n              scope=None):\n  \"\"\"Build Inception v3 model architecture.\n\n  See here for reference: http://arxiv.org/abs/1512.00567\n\n  Args:\n    images: Images returned from inputs() or distorted_inputs().\n    num_classes: number of classes\n    for_training: If set to `True`, build the inference model for training.\n      Kernels that operate differently for inference during training\n      e.g. dropout, are appropriately configured.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: optional prefix string identifying the ImageNet tower.\n\n  Returns:\n    Logits. 2-D float Tensor.\n    Auxiliary Logits. 2-D float Tensor of side-head. Used for training only.\n  \"\"\"\n   Parameters for BatchNorm.\n  batch_norm_params = {\n       Decay for the moving averages.\n      'decay': BATCHNORM_MOVING_AVERAGE_DECAY,\n       epsilon to prevent 0s in variance.\n      'epsilon': 0.001,\n  }\n   Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n    with slim.arg_scope([slim.ops.conv2d],\n                        stddev=0.1,\n                        activation=tf.nn.relu,\n                        batch_norm_params=batch_norm_params):\n      logits, endpoints = slim.inception.inception_v3(\n          images,\n          dropout_keep_prob=0.8,\n          num_classes=num_classes,\n          is_training=for_training,\n          restore_logits=restore_logits,\n          scope=scope)\n\n   Add summaries for viewing model statistics on TensorBoard.\n  _activation_summaries(endpoints)\n\n   Grab the logits associated with the side head. Employed during training.\n  auxiliary_logits = endpoints['aux_logits']\n\n  return logits, auxiliary_logits\n\n\ndef loss(logits, labels, batch_size=None):\n  \"\"\"Adds all losses for the model.\n\n  Note the final loss is not returned. Instead, the list of losses are collected\n  by slim.losses. The losses are accumulated in tower_loss() and summed to\n  calculate the total loss.\n\n  Args:\n    logits: List of logits from inference(). Each entry is a 2-D float Tensor.\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n    batch_size: integer\n  \"\"\"\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n   Reshape the labels into a dense Tensor of\n   shape [FLAGS.batch_size, num_classes].\n  sparse_labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(batch_size), [batch_size, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  num_classes = logits[0].get_shape()[-1].value\n  dense_labels = tf.sparse_to_dense(concated,\n                                    [batch_size, num_classes],\n                                    1.0, 0.0)\n\n   Cross entropy loss for the main softmax prediction.\n  slim.losses.cross_entropy_loss(logits[0],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=1.0)\n\n   Cross entropy loss for the auxiliary softmax head.\n  slim.losses.cross_entropy_loss(logits[1],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=0.4,\n                                 scope='aux_loss')\n\n\ndef _activation_summary(x):\n  \"\"\"Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measure the sparsity of activations.\n\n  Args:\n    x: Tensor\n  \"\"\"\n   Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n   session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n  tf.summary.histogram(tensor_name + '/activations', x)\n  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n\n\ndef _activation_summaries(endpoints):\n  with tf.name_scope('summaries'):\n    for act in endpoints.values():\n      _activation_summary(act)\n", "comments": "   build inception v3 network imagenet data set   the inception v3 architecture described http   arxiv org abs 1512 00567  summary available functions   inference  compute inference model inputs make prediction  loss  compute loss prediction respect labels       future   import absolute import   future   import division   future   import print function  import  import tensorflow tf  inception slim import slim  flags   tf app flags flags    if model trained using multiple gpus  prefix op names tower name   differentiate operations  note prefix removed   names summaries visualizing model  tower name    tower     batch normalization  constant governing exponential moving average    global  mean variance activations  batchnorm moving average decay   0 9997    the decay use moving average  moving average decay   0 9999   def inference(images  num classes  training false  restore logits true                scope none)       build inception v3 model architecture     see reference  http   arxiv org abs 1512 00567    args      images  images returned inputs() distorted inputs()      num classes  number classes     training  if set  true   build inference model training        kernels operate differently inference training       e g  dropout  appropriately configured      restore logits  whether logits layers restored        useful fine tuning model different num classes      scope  optional prefix string identifying imagenet tower     returns      logits  2 d float tensor      auxiliary logits  2 d float tensor side head  used training            parameters batchnorm    batch norm params             decay moving averages         decay   batchnorm moving average decay          epsilon prevent 0s variance         epsilon   0 001          set weight decay weights conv fc layers    slim arg scope( slim ops conv2d  slim ops fc   weight decay 0 00004)      slim arg scope( slim ops conv2d                           stddev 0 1                          activation tf nn relu                          batch norm params batch norm params)        logits  endpoints   slim inception inception v3(           images            dropout keep prob 0 8            num classes num classes            training training            restore logits restore logits            scope scope)      add summaries viewing model statistics tensorboard     activation summaries(endpoints)      grab logits associated side head  employed training    auxiliary logits   endpoints  aux logits      return logits  auxiliary logits   def loss(logits  labels  batch size none)       adds losses model     note final loss returned  instead  list losses collected   slim losses  the losses accumulated tower loss() summed   calculate total loss     args      logits  list logits inference()  each entry 2 d float tensor      labels  labels distorted inputs inputs()  1 d tensor             shape  batch size      batch size  integer         batch size      batch size   flags batch size      reshape labels dense tensor     shape  flags batch size  num classes     sparse labels   tf reshape(labels   batch size  1 )   indices   tf reshape(tf range(batch size)   batch size  1 )   concated   tf concat(axis 1  values  indices  sparse labels )   num classes   logits 0  get shape()  1  value   dense labels   tf sparse dense(concated                                       batch size  num classes                                       1 0  0 0)      cross entropy loss main softmax prediction    slim losses cross entropy loss(logits 0                                    dense labels                                   label smoothing 0 1                                   weight 1 0)      cross entropy loss auxiliary softmax head    slim losses cross entropy loss(logits 1                                    dense labels                                   label smoothing 0 1                                   weight 0 4                                   scope  aux loss )   def  activation summary(x)       helper create summaries activations     creates summary provides histogram activations    creates summary measure sparsity activations     args      x  tensor          copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       if model trained using multiple gpus  prefix op names tower name    differentiate operations  note prefix removed    names summaries visualizing model     batch normalization  constant governing exponential moving average     global  mean variance activations     the decay use moving average     parameters batchnorm     decay moving averages     epsilon prevent 0s variance     set weight decay weights conv fc layers     add summaries viewing model statistics tensorboard     grab logits associated side head  employed training     reshape labels dense tensor    shape  flags batch size  num classes      cross entropy loss main softmax prediction     cross entropy loss auxiliary softmax head     remove  tower  0 9    name case multi gpu training    session  this helps clarity presentation tensorboard  ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Build the Inception v3 network on ImageNet data set.\n\nThe Inception v3 architecture is described in http://arxiv.org/abs/1512.00567\n\nSummary of available functions:\n inference: Compute inference on the model inputs to make a prediction\n loss: Compute the loss of the prediction with respect to the labels\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\nfrom inception.slim import slim\n\nFLAGS = tf.app.flags.FLAGS\n\n# If a model is trained using multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\n# Batch normalization. Constant governing the exponential moving average of\n# the 'global' mean and variance for all activations.\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\n\n# The decay to use for the moving average.\nMOVING_AVERAGE_DECAY = 0.9999\n\n\ndef inference(images, num_classes, for_training=False, restore_logits=True,\n              scope=None):\n  \"\"\"Build Inception v3 model architecture.\n\n  See here for reference: http://arxiv.org/abs/1512.00567\n\n  Args:\n    images: Images returned from inputs() or distorted_inputs().\n    num_classes: number of classes\n    for_training: If set to `True`, build the inference model for training.\n      Kernels that operate differently for inference during training\n      e.g. dropout, are appropriately configured.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: optional prefix string identifying the ImageNet tower.\n\n  Returns:\n    Logits. 2-D float Tensor.\n    Auxiliary Logits. 2-D float Tensor of side-head. Used for training only.\n  \"\"\"\n  # Parameters for BatchNorm.\n  batch_norm_params = {\n      # Decay for the moving averages.\n      'decay': BATCHNORM_MOVING_AVERAGE_DECAY,\n      # epsilon to prevent 0s in variance.\n      'epsilon': 0.001,\n  }\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n    with slim.arg_scope([slim.ops.conv2d],\n                        stddev=0.1,\n                        activation=tf.nn.relu,\n                        batch_norm_params=batch_norm_params):\n      logits, endpoints = slim.inception.inception_v3(\n          images,\n          dropout_keep_prob=0.8,\n          num_classes=num_classes,\n          is_training=for_training,\n          restore_logits=restore_logits,\n          scope=scope)\n\n  # Add summaries for viewing model statistics on TensorBoard.\n  _activation_summaries(endpoints)\n\n  # Grab the logits associated with the side head. Employed during training.\n  auxiliary_logits = endpoints['aux_logits']\n\n  return logits, auxiliary_logits\n\n\ndef loss(logits, labels, batch_size=None):\n  \"\"\"Adds all losses for the model.\n\n  Note the final loss is not returned. Instead, the list of losses are collected\n  by slim.losses. The losses are accumulated in tower_loss() and summed to\n  calculate the total loss.\n\n  Args:\n    logits: List of logits from inference(). Each entry is a 2-D float Tensor.\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n    batch_size: integer\n  \"\"\"\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Reshape the labels into a dense Tensor of\n  # shape [FLAGS.batch_size, num_classes].\n  sparse_labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(batch_size), [batch_size, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  num_classes = logits[0].get_shape()[-1].value\n  dense_labels = tf.sparse_to_dense(concated,\n                                    [batch_size, num_classes],\n                                    1.0, 0.0)\n\n  # Cross entropy loss for the main softmax prediction.\n  slim.losses.cross_entropy_loss(logits[0],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=1.0)\n\n  # Cross entropy loss for the auxiliary softmax head.\n  slim.losses.cross_entropy_loss(logits[1],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=0.4,\n                                 scope='aux_loss')\n\n\ndef _activation_summary(x):\n  \"\"\"Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measure the sparsity of activations.\n\n  Args:\n    x: Tensor\n  \"\"\"\n  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n  # session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n  tf.summary.histogram(tensor_name + '/activations', x)\n  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n\n\ndef _activation_summaries(endpoints):\n  with tf.name_scope('summaries'):\n    for act in endpoints.values():\n      _activation_summary(act)\n", "description": "Models and examples built with TensorFlow", "file_name": "inception_model.py", "id": "dfa6ac3077133694e2643c1c25664ba0", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/tensorflow-models/tensorflow-models-7e4c66b/research/inception/inception/inception_model.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:59:36Z", "url": "https://github.com/tensorflow/models", "wiki": true}