{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\"\"\"Author: aneelakantan (Arvind Neelakantan)\n\"\"\"\n\nimport tensorflow as tf\n\ndef get_embedding(word, utility, params):\n  return tf.nn.embedding_lookup(params[\"word\"], word)\n\n\ndef apply_dropout(x, dropout_rate, mode):\n  if (dropout_rate > 0.0):\n    if (mode == \"train\"):\n      x = tf.nn.dropout(x, dropout_rate)\n    else:\n      x = x\n  return x\n\n\ndef LSTMCell(x, mprev, cprev, key, params):\n  \"\"\"Create an LSTM cell.\n\n  Implements the equations in pg.2 from\n  \"Long Short-Term Memory Based Recurrent Neural Network Architectures\n  For Large Vocabulary Speech Recognition\",\n  Hasim Sak, Andrew Senior, Francoise Beaufays.\n\n  Args:\n    w: A dictionary of the weights and optional biases as returned\n      by LSTMParametersSplit().\n    x: Inputs to this cell.\n    mprev: m_{t-1}, the recurrent activations (same as the output)\n      from the previous cell.\n    cprev: c_{t-1}, the cell activations from the previous cell.\n    keep_prob: Keep probability on the input and the outputs of a cell.\n\n  Returns:\n    m: Outputs of this cell.\n    c: Cell Activations.\n    \"\"\"\n\n  i = tf.matmul(x, params[key + \"_ix\"]) + tf.matmul(mprev, params[key + \"_im\"])\n  i = tf.nn.bias_add(i, params[key + \"_i\"])\n  f = tf.matmul(x, params[key + \"_fx\"]) + tf.matmul(mprev, params[key + \"_fm\"])\n  f = tf.nn.bias_add(f, params[key + \"_f\"])\n  c = tf.matmul(x, params[key + \"_cx\"]) + tf.matmul(mprev, params[key + \"_cm\"])\n  c = tf.nn.bias_add(c, params[key + \"_c\"])\n  o = tf.matmul(x, params[key + \"_ox\"]) + tf.matmul(mprev, params[key + \"_om\"])\n  o = tf.nn.bias_add(o, params[key + \"_o\"])\n  i = tf.sigmoid(i, name=\"i_gate\")\n  f = tf.sigmoid(f, name=\"f_gate\")\n  o = tf.sigmoid(o, name=\"o_gate\")\n  c = f * cprev + i * tf.tanh(c)\n  m = o * c\n  return m, c\n", "comments": "   author  aneelakantan (arvind neelakantan)      import tensorflow tf  def get embedding(word  utility  params)    return tf nn embedding lookup(params  word    word)   def apply dropout(x  dropout rate  mode)    (dropout rate   0 0)      (mode     train )        x   tf nn dropout(x  dropout rate)     else        x   x   return x   def lstmcell(x  mprev  cprev  key  params)       create lstm cell     implements equations pg 2    long short term memory based recurrent neural network architectures   for large vocabulary speech recognition     hasim sak  andrew senior  francoise beaufays     args      w  a dictionary weights optional biases returned       lstmparameterssplit()      x  inputs cell      mprev   1   recurrent activations (same output)       previous cell      cprev  c  1   cell activations previous cell      keep prob  keep probability input outputs cell     returns       outputs cell      c  cell activations             copyright 2016 google inc  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license       http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                    ", "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Author: aneelakantan (Arvind Neelakantan)\n\"\"\"\n\nimport tensorflow as tf\n\ndef get_embedding(word, utility, params):\n  return tf.nn.embedding_lookup(params[\"word\"], word)\n\n\ndef apply_dropout(x, dropout_rate, mode):\n  if (dropout_rate > 0.0):\n    if (mode == \"train\"):\n      x = tf.nn.dropout(x, dropout_rate)\n    else:\n      x = x\n  return x\n\n\ndef LSTMCell(x, mprev, cprev, key, params):\n  \"\"\"Create an LSTM cell.\n\n  Implements the equations in pg.2 from\n  \"Long Short-Term Memory Based Recurrent Neural Network Architectures\n  For Large Vocabulary Speech Recognition\",\n  Hasim Sak, Andrew Senior, Francoise Beaufays.\n\n  Args:\n    w: A dictionary of the weights and optional biases as returned\n      by LSTMParametersSplit().\n    x: Inputs to this cell.\n    mprev: m_{t-1}, the recurrent activations (same as the output)\n      from the previous cell.\n    cprev: c_{t-1}, the cell activations from the previous cell.\n    keep_prob: Keep probability on the input and the outputs of a cell.\n\n  Returns:\n    m: Outputs of this cell.\n    c: Cell Activations.\n    \"\"\"\n\n  i = tf.matmul(x, params[key + \"_ix\"]) + tf.matmul(mprev, params[key + \"_im\"])\n  i = tf.nn.bias_add(i, params[key + \"_i\"])\n  f = tf.matmul(x, params[key + \"_fx\"]) + tf.matmul(mprev, params[key + \"_fm\"])\n  f = tf.nn.bias_add(f, params[key + \"_f\"])\n  c = tf.matmul(x, params[key + \"_cx\"]) + tf.matmul(mprev, params[key + \"_cm\"])\n  c = tf.nn.bias_add(c, params[key + \"_c\"])\n  o = tf.matmul(x, params[key + \"_ox\"]) + tf.matmul(mprev, params[key + \"_om\"])\n  o = tf.nn.bias_add(o, params[key + \"_o\"])\n  i = tf.sigmoid(i, name=\"i_gate\")\n  f = tf.sigmoid(f, name=\"f_gate\")\n  o = tf.sigmoid(o, name=\"o_gate\")\n  c = f * cprev + i * tf.tanh(c)\n  m = o * c\n  return m, c\n", "description": "Models and examples built with TensorFlow", "file_name": "nn_utils.py", "id": "d11ddfbb7c2cecdad4a413343e443e2c", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/neural_programmer/nn_utils.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}