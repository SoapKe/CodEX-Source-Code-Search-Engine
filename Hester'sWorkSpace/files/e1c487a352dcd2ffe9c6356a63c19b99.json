{"author": "donnemartin", "code": "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\ndef maybe_download(filename, work_directory):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          'Invalid magic number %d in MNIST image file: %s' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          'Invalid magic number %d in MNIST label file: %s' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \"images.shape: %s labels.shape: %s\" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      \n      self._epochs_completed += 1\n      \n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      \n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets", "comments": "   functions downloading reading mnist data       future   import print function import gzip import os import urllib import numpy source url    http   yann lecun com exdb mnist   def maybe download(filename  work directory)       download data yann website  unless already       os path exists(work directory)      os mkdir(work directory)   filepath   os path join(work directory  filename)   os path exists(filepath)      filepath      urllib urlretrieve(source url   filename  filepath)     statinfo   os stat(filepath)     print( succesfully downloaded   filename  statinfo st size   bytes  )   return filepath def  read32(bytestream)    dt   numpy dtype(numpy uint32) newbyteorder(   )   return numpy frombuffer(bytestream read(4)  dtype dt) def extract images(filename)       extract images 4d uint8 numpy array  index   x  depth        print( extracting   filename)   gzip open(filename) bytestream      magic    read32(bytestream)     magic    2051        raise valueerror(            invalid magic number  mnist image file                (magic  filename))     num images    read32(bytestream)     rows    read32(bytestream)     cols    read32(bytestream)     buf   bytestream read(rows   cols   num images)     data   numpy frombuffer(buf  dtype numpy uint8)     data   data reshape(num images  rows  cols  1)     return data def dense one hot(labels dense  num classes 10)       convert class labels scalars one hot vectors       num labels   labels dense shape 0    index offset   numpy arange(num labels)   num classes   labels one hot   numpy zeros((num labels  num classes))   labels one hot flat index offset   labels dense ravel()    1   return labels one hot def extract labels(filename  one hot false)       extract labels 1d uint8 numpy array  index        print( extracting   filename)   gzip open(filename) bytestream      magic    read32(bytestream)     magic    2049        raise valueerror(            invalid magic number  mnist label file                (magic  filename))     num items    read32(bytestream)     buf   bytestream read(num items)     labels   numpy frombuffer(buf  dtype numpy uint8)     one hot        return dense one hot(labels)     return labels class dataset(object)    def   init  (self  images  labels  fake data false)      fake data        self  num examples   10000     else        assert images shape 0     labels shape 0   (            images shape   labels shape      (images shape                                                   labels shape))       self  num examples   images shape 0          convert shape  num examples  rows  columns  depth           num examples  rows columns  (assuming depth    1)       assert images shape 3     1       images   images reshape(images shape 0                                 images shape 1    images shape 2 )         convert  0  255      0 0  1 0         images   images astype(numpy float32)       images   numpy multiply(images  1 0   255 0)     self  images   images     self  labels   labels     self  epochs completed   0     self  index epoch   0    property   def images(self)      return self  images    property   def labels(self)      return self  labels    property   def num examples(self)      return self  num examples    property   def epochs completed(self)      return self  epochs completed   def next batch(self  batch size  fake data false)         return next  batch size  examples data set        convert shape  num examples  rows  columns  depth      num examples  rows columns  (assuming depth    1)    convert  0  255      0 0  1 0      finished epoch    shuffle data    start next epoch ", "content": "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\ndef maybe_download(filename, work_directory):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          'Invalid magic number %d in MNIST image file: %s' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          'Invalid magic number %d in MNIST label file: %s' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \"images.shape: %s labels.shape: %s\" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets", "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.", "file_name": "input_data.py", "id": "e1c487a352dcd2ffe9c6356a63c19b99", "language": "Python", "project_name": "data-science-ipython-notebooks", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/donnemartin-data-science-ipython-notebooks/donnemartin-data-science-ipython-notebooks-a876e34/deep-learning/tensor-flow-examples/input_data.py", "save_time": "", "source": "", "update_at": "2018-03-18T12:16:56Z", "url": "https://github.com/donnemartin/data-science-ipython-notebooks", "wiki": true}