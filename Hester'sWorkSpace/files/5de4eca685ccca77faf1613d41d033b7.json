{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\nr\"\"\"A simple demonstration of running VGGish in training mode.\n\nThis is intended as a toy example that demonstrates how to use the VGGish model\ndefinition within a larger model that adds more layers on top, and then train\nthe larger model. If you let VGGish train as well, then this allows you to\nfine-tune the VGGish model parameters for your application. If you don't let\nVGGish train, then you use VGGish as a feature extractor for the layers above\nit.\n\nFor this toy task, we are training a classifier to distinguish between three\nclasses: sine waves, constant signals, and white noise. We generate synthetic\nwaveforms from each of these classes, convert into shuffled batches of log mel\nspectrogram examples with associated labels, and feed the batches into a model\nthat includes VGGish at the bottom and a couple of additional layers on top. We\nalso plumb in labels that are associated with the examples, which feed a label\nloss used for training.\n\nUsage:\n   Run training for 100 steps using a model checkpoint in the default\n   location (vggish_model.ckpt in the current directory). Allow VGGish\n   to get fine-tuned.\n  $ python vggish_train_demo.py --num_batches 100\n\n   Same as before but run for fewer steps and don't change VGGish parameters\n   and use a checkpoint in a different location\n  $ python vggish_train_demo.py --num_batches 50 \\\n                                --train_vggish=False \\\n                                --checkpoint /path/to/model/checkpoint\n\"\"\"\n\nfrom __future__ import print_function\n\nfrom random import shuffle\n\nimport numpy as np\nimport tensorflow as tf\n\nimport vggish_input\nimport vggish_params\nimport vggish_slim\n\nflags = tf.app.flags\nslim = tf.contrib.slim\n\nflags.DEFINE_integer(\n    'num_batches', 30,\n    'Number of batches of examples to feed into the model. Each batch is of '\n    'variable size and contains shuffled examples of each class of audio.')\n\nflags.DEFINE_boolean(\n    'train_vggish', True,\n    'If Frue, allow VGGish parameters to change during training, thus '\n    'fine-tuning VGGish. If False, VGGish parameters are fixed, thus using '\n    'VGGish as a fixed feature extractor.')\n\nflags.DEFINE_string(\n    'checkpoint', 'vggish_model.ckpt',\n    'Path to the VGGish checkpoint file.')\n\nFLAGS = flags.FLAGS\n\n_NUM_CLASSES = 3\n\n\ndef _get_examples_batch():\n  \"\"\"Returns a shuffled batch of examples of all audio classes.\n\n  Note that this is just a toy function because this is a simple demo intended\n  to illustrate how the training code might work.\n\n  Returns:\n    a tuple (features, labels) where features is a NumPy array of shape\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n    suitable for feeding VGGish, while labels is a NumPy array of shape\n    [batch_size, num_classes] where each row is a multi-hot label vector that\n    provides the labels for corresponding rows in features.\n  \"\"\"\n   Make a waveform for each class.\n  num_seconds = 5\n  sr = 44100   Sampling rate.\n  t = np.linspace(0, num_seconds, int(num_seconds * sr))   Time axis.\n   Random sine wave.\n  freq = np.random.uniform(100, 1000)\n  sine = np.sin(2 * np.pi * freq * t)\n   Random constant signal.\n  magnitude = np.random.uniform(-1, 1)\n  const = magnitude * t\n   White noise.\n  noise = np.random.normal(-1, 1, size=t.shape)\n\n   Make examples of each signal and corresponding labels.\n   Sine is class index 0, Const class index 1, Noise class index 2.\n  sine_examples = vggish_input.waveform_to_examples(sine, sr)\n  sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n  const_examples = vggish_input.waveform_to_examples(const, sr)\n  const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n  noise_examples = vggish_input.waveform_to_examples(noise, sr)\n  noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n\n   Shuffle (example, label) pairs across all classes.\n  all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n  all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n  labeled_examples = list(zip(all_examples, all_labels))\n  shuffle(labeled_examples)\n\n   Separate and return the features and labels.\n  features = [example for (example, _) in labeled_examples]\n  labels = [label for (_, label) in labeled_examples]\n  return (features, labels)\n\n\ndef main(_):\n  with tf.Graph().as_default(), tf.Session() as sess:\n     Define VGGish.\n    embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n\n     Define a shallow classification model and associated training ops on top\n     of VGGish.\n    with tf.variable_scope('mymodel'):\n       Add a fully connected layer with 100 units.\n      num_units = 100\n      fc = slim.fully_connected(embeddings, num_units)\n\n       Add a classifier layer at the end, consisting of parallel logistic\n       classifiers, one per class. This allows for multi-class tasks.\n      logits = slim.fully_connected(\n          fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n      tf.sigmoid(logits, name='prediction')\n\n       Add training ops.\n      with tf.variable_scope('train'):\n        global_step = tf.Variable(\n            0, name='global_step', trainable=False,\n            collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n                         tf.GraphKeys.GLOBAL_STEP])\n\n         Labels are assumed to be fed as a batch multi-hot vectors, with\n         a 1 in the position of each positive class label, and 0 elsewhere.\n        labels = tf.placeholder(\n            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n\n         Cross-entropy label loss.\n        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=logits, labels=labels, name='xent')\n        loss = tf.reduce_mean(xent, name='loss_op')\n        tf.summary.scalar('loss', loss)\n\n         We use the same optimizer and hyperparameters as used to train VGGish.\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=vggish_params.LEARNING_RATE,\n            epsilon=vggish_params.ADAM_EPSILON)\n        optimizer.minimize(loss, global_step=global_step, name='train_op')\n\n     Initialize all variables in the model, and then load the pre-trained\n     VGGish checkpoint.\n    sess.run(tf.global_variables_initializer())\n    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n\n     Locate all the tensors and ops we need for the training loop.\n    features_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.INPUT_TENSOR_NAME)\n    labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n    global_step_tensor = sess.graph.get_tensor_by_name(\n        'mymodel/train/global_step:0')\n    loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n    train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n\n     The training loop.\n    for _ in range(FLAGS.num_batches):\n      (features, labels) = _get_examples_batch()\n      [num_steps, loss, _] = sess.run(\n          [global_step_tensor, loss_tensor, train_op],\n          feed_dict={features_tensor: features, labels_tensor: labels})\n      print('Step %d: loss %g' % (num_steps, loss))\n\nif __name__ == '__main__':\n  tf.app.run()\n", "comments": "   a simple demonstration running vggish training mode   this intended toy example demonstrates use vggish model definition within larger model adds layers top  train larger model  if let vggish train well  allows fine tune vggish model parameters application  if let vggish train  use vggish feature extractor layers   for toy task  training classifier distinguish three classes  sine waves  constant signals  white noise  we generate synthetic waveforms classes  convert shuffled batches log mel spectrogram examples associated labels  feed batches model includes vggish bottom couple additional layers top  we also plumb labels associated examples  feed label loss used training   usage      run training 100 steps using model checkpoint default     location (vggish model ckpt current directory)  allow vggish     get fine tuned      python vggish train demo py   num batches 100      same run fewer steps change vggish parameters     use checkpoint different location     python vggish train demo py   num batches 50                                     train vggish false                                     checkpoint  path model checkpoint        future   import print function  random import shuffle  import numpy np import tensorflow tf  import vggish input import vggish params import vggish slim  flags   tf app flags slim   tf contrib slim  flags define integer(      num batches   30       number batches examples feed model  each batch        variable size contains shuffled examples class audio  )  flags define boolean(      train vggish   true       if frue  allow vggish parameters change training  thus        fine tuning vggish  if false  vggish parameters fixed  thus using        vggish fixed feature extractor  )  flags define string(      checkpoint    vggish model ckpt        path vggish checkpoint file  )  flags   flags flags   num classes   3   def  get examples batch()       returns shuffled batch examples audio classes     note toy function simple demo intended   illustrate training code might work     returns      tuple (features  labels) features numpy array shape      batch size  num frames  num bands  batch size variable     row log mel spectrogram patch shape  num frames  num bands      suitable feeding vggish  labels numpy array shape      batch size  num classes  row multi hot label vector     provides labels corresponding rows features           copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       run training 100 steps using model checkpoint default    location (vggish model ckpt current directory)  allow vggish    get fine tuned     same run fewer steps change vggish parameters    use checkpoint different location    make waveform class     sampling rate     time axis     random sine wave     random constant signal     white noise     make examples signal corresponding labels     sine class index 0  const class index 1  noise class index 2     shuffle (example  label) pairs across classes     separate return features labels     define vggish     define shallow classification model associated training ops top    vggish     add fully connected layer 100 units     add classifier layer end  consisting parallel logistic    classifiers  one per class  this allows multi class tasks     add training ops     labels assumed fed batch multi hot vectors     1 position positive class label  0 elsewhere     cross entropy label loss     we use optimizer hyperparameters used train vggish     initialize variables model  load pre trained    vggish checkpoint     locate tensors ops need training loop     the training loop  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr\"\"\"A simple demonstration of running VGGish in training mode.\n\nThis is intended as a toy example that demonstrates how to use the VGGish model\ndefinition within a larger model that adds more layers on top, and then train\nthe larger model. If you let VGGish train as well, then this allows you to\nfine-tune the VGGish model parameters for your application. If you don't let\nVGGish train, then you use VGGish as a feature extractor for the layers above\nit.\n\nFor this toy task, we are training a classifier to distinguish between three\nclasses: sine waves, constant signals, and white noise. We generate synthetic\nwaveforms from each of these classes, convert into shuffled batches of log mel\nspectrogram examples with associated labels, and feed the batches into a model\nthat includes VGGish at the bottom and a couple of additional layers on top. We\nalso plumb in labels that are associated with the examples, which feed a label\nloss used for training.\n\nUsage:\n  # Run training for 100 steps using a model checkpoint in the default\n  # location (vggish_model.ckpt in the current directory). Allow VGGish\n  # to get fine-tuned.\n  $ python vggish_train_demo.py --num_batches 100\n\n  # Same as before but run for fewer steps and don't change VGGish parameters\n  # and use a checkpoint in a different location\n  $ python vggish_train_demo.py --num_batches 50 \\\n                                --train_vggish=False \\\n                                --checkpoint /path/to/model/checkpoint\n\"\"\"\n\nfrom __future__ import print_function\n\nfrom random import shuffle\n\nimport numpy as np\nimport tensorflow as tf\n\nimport vggish_input\nimport vggish_params\nimport vggish_slim\n\nflags = tf.app.flags\nslim = tf.contrib.slim\n\nflags.DEFINE_integer(\n    'num_batches', 30,\n    'Number of batches of examples to feed into the model. Each batch is of '\n    'variable size and contains shuffled examples of each class of audio.')\n\nflags.DEFINE_boolean(\n    'train_vggish', True,\n    'If Frue, allow VGGish parameters to change during training, thus '\n    'fine-tuning VGGish. If False, VGGish parameters are fixed, thus using '\n    'VGGish as a fixed feature extractor.')\n\nflags.DEFINE_string(\n    'checkpoint', 'vggish_model.ckpt',\n    'Path to the VGGish checkpoint file.')\n\nFLAGS = flags.FLAGS\n\n_NUM_CLASSES = 3\n\n\ndef _get_examples_batch():\n  \"\"\"Returns a shuffled batch of examples of all audio classes.\n\n  Note that this is just a toy function because this is a simple demo intended\n  to illustrate how the training code might work.\n\n  Returns:\n    a tuple (features, labels) where features is a NumPy array of shape\n    [batch_size, num_frames, num_bands] where the batch_size is variable and\n    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n    suitable for feeding VGGish, while labels is a NumPy array of shape\n    [batch_size, num_classes] where each row is a multi-hot label vector that\n    provides the labels for corresponding rows in features.\n  \"\"\"\n  # Make a waveform for each class.\n  num_seconds = 5\n  sr = 44100  # Sampling rate.\n  t = np.linspace(0, num_seconds, int(num_seconds * sr))  # Time axis.\n  # Random sine wave.\n  freq = np.random.uniform(100, 1000)\n  sine = np.sin(2 * np.pi * freq * t)\n  # Random constant signal.\n  magnitude = np.random.uniform(-1, 1)\n  const = magnitude * t\n  # White noise.\n  noise = np.random.normal(-1, 1, size=t.shape)\n\n  # Make examples of each signal and corresponding labels.\n  # Sine is class index 0, Const class index 1, Noise class index 2.\n  sine_examples = vggish_input.waveform_to_examples(sine, sr)\n  sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n  const_examples = vggish_input.waveform_to_examples(const, sr)\n  const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n  noise_examples = vggish_input.waveform_to_examples(noise, sr)\n  noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n\n  # Shuffle (example, label) pairs across all classes.\n  all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n  all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n  labeled_examples = list(zip(all_examples, all_labels))\n  shuffle(labeled_examples)\n\n  # Separate and return the features and labels.\n  features = [example for (example, _) in labeled_examples]\n  labels = [label for (_, label) in labeled_examples]\n  return (features, labels)\n\n\ndef main(_):\n  with tf.Graph().as_default(), tf.Session() as sess:\n    # Define VGGish.\n    embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n\n    # Define a shallow classification model and associated training ops on top\n    # of VGGish.\n    with tf.variable_scope('mymodel'):\n      # Add a fully connected layer with 100 units.\n      num_units = 100\n      fc = slim.fully_connected(embeddings, num_units)\n\n      # Add a classifier layer at the end, consisting of parallel logistic\n      # classifiers, one per class. This allows for multi-class tasks.\n      logits = slim.fully_connected(\n          fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n      tf.sigmoid(logits, name='prediction')\n\n      # Add training ops.\n      with tf.variable_scope('train'):\n        global_step = tf.Variable(\n            0, name='global_step', trainable=False,\n            collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n                         tf.GraphKeys.GLOBAL_STEP])\n\n        # Labels are assumed to be fed as a batch multi-hot vectors, with\n        # a 1 in the position of each positive class label, and 0 elsewhere.\n        labels = tf.placeholder(\n            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n\n        # Cross-entropy label loss.\n        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=logits, labels=labels, name='xent')\n        loss = tf.reduce_mean(xent, name='loss_op')\n        tf.summary.scalar('loss', loss)\n\n        # We use the same optimizer and hyperparameters as used to train VGGish.\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=vggish_params.LEARNING_RATE,\n            epsilon=vggish_params.ADAM_EPSILON)\n        optimizer.minimize(loss, global_step=global_step, name='train_op')\n\n    # Initialize all variables in the model, and then load the pre-trained\n    # VGGish checkpoint.\n    sess.run(tf.global_variables_initializer())\n    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n\n    # Locate all the tensors and ops we need for the training loop.\n    features_tensor = sess.graph.get_tensor_by_name(\n        vggish_params.INPUT_TENSOR_NAME)\n    labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n    global_step_tensor = sess.graph.get_tensor_by_name(\n        'mymodel/train/global_step:0')\n    loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n    train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n\n    # The training loop.\n    for _ in range(FLAGS.num_batches):\n      (features, labels) = _get_examples_batch()\n      [num_steps, loss, _] = sess.run(\n          [global_step_tensor, loss_tensor, train_op],\n          feed_dict={features_tensor: features, labels_tensor: labels})\n      print('Step %d: loss %g' % (num_steps, loss))\n\nif __name__ == '__main__':\n  tf.app.run()\n", "description": "Models and examples built with TensorFlow", "file_name": "vggish_train_demo.py", "id": "5de4eca685ccca77faf1613d41d033b7", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/audioset/vggish_train_demo.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}