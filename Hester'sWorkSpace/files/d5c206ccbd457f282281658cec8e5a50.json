{"author": "openai", "code": "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'swimmer.xml', 4)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        ctrl_cost_coeff = 0.0001\n        xposbefore = self.sim.data.qpos[0]\n        self.do_simulation(a, self.frame_skip)\n        xposafter = self.sim.data.qpos[0]\n        reward_fwd = (xposafter - xposbefore) / self.dt\n        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()\n        reward = reward_fwd + reward_ctrl\n        ob = self._get_obs()\n        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)\n\n    def _get_obs(self):\n        qpos = self.sim.data.qpos\n        qvel = self.sim.data.qvel\n        return np.concatenate([qpos.flat[2:], qvel.flat])\n\n    def reset_model(self):\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)\n        )\n        return self._get_obs()\n", "comments": "", "content": "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'swimmer.xml', 4)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        ctrl_cost_coeff = 0.0001\n        xposbefore = self.sim.data.qpos[0]\n        self.do_simulation(a, self.frame_skip)\n        xposafter = self.sim.data.qpos[0]\n        reward_fwd = (xposafter - xposbefore) / self.dt\n        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()\n        reward = reward_fwd + reward_ctrl\n        ob = self._get_obs()\n        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)\n\n    def _get_obs(self):\n        qpos = self.sim.data.qpos\n        qvel = self.sim.data.qvel\n        return np.concatenate([qpos.flat[2:], qvel.flat])\n\n    def reset_model(self):\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)\n        )\n        return self._get_obs()\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "swimmer.py", "id": "d5c206ccbd457f282281658cec8e5a50", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/mujoco/swimmer.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}