{"author": "chiphuyen", "code": "\"\"\" starter code for word2vec skip-gram model with NCE loss\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 04\n\"\"\"\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nfrom tensorflow.contrib.tensorboard.plugins import projector\nimport tensorflow as tf\n\nimport utils\nimport word2vec_utils\n\n\nVOCAB_SIZE = 50000\nBATCH_SIZE = 128\nEMBED_SIZE = 128            \nSKIP_WINDOW = 1             \nNUM_SAMPLED = 64            \nLEARNING_RATE = 1.0\nNUM_TRAIN_STEPS = 100000\nVISUAL_FLD = 'visualization'\nSKIP_STEP = 5000\n\n\nDOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\nEXPECTED_BYTES = 31344016\nNUM_VISUALIZE = 3000        \n\n\ndef word2vec(dataset):\n    \"\"\" Build the graph for word2vec model and train it \"\"\"\n    \n    with tf.name_scope('data'):\n        iterator = dataset.make_initializable_iterator()\n        center_words, target_words = iterator.get_next()\n\n    \"\"\" Step 2 + 3: define weights and embedding lookup.\n    In word2vec, it's actually the weights that we care about \n    \"\"\"\n    with tf.name_scope('embed'):\n        embed_matrix = tf.get_variable('embed_matrix', \n                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n                                        initializer=tf.random_uniform_initializer())\n        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n\n    \n    with tf.name_scope('loss'):\n        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n\n        \n        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n                                            biases=nce_bias, \n                                            labels=target_words, \n                                            inputs=embed, \n                                            num_sampled=NUM_SAMPLED, \n                                            num_classes=VOCAB_SIZE), name='loss')\n\n    \n    with tf.name_scope('optimizer'):\n        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n    \n    utils.safe_mkdir('checkpoints')\n\n    with tf.Session() as sess:\n        sess.run(iterator.initializer)\n        sess.run(tf.global_variables_initializer())\n\n        total_loss = 0.0 \n        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n\n        for index in range(NUM_TRAIN_STEPS):\n            try:\n                loss_batch, _ = sess.run([loss, optimizer])\n                total_loss += loss_batch\n                if (index + 1) % SKIP_STEP == 0:\n                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n                    total_loss = 0.0\n            except tf.errors.OutOfRangeError:\n                sess.run(iterator.initializer)\n        writer.close()\n\ndef gen():\n    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n\ndef main():\n    dataset = tf.data.Dataset.from_generator(gen, \n                                (tf.int32, tf.int32), \n                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n    word2vec(dataset)\n\nif __name__ == '__main__':\n    main()\n", "comments": "    starter code word2vec skip gram model nce loss cs 20   tensorflow deep learning research  cs20 stanford edu chip huyen (chiphuyen cs stanford edu) lecture 04      import os os environ  tf cpp min log level    2   import numpy np tensorflow contrib tensorboard plugins import projector import tensorflow tf  import utils import word2vec utils    model hyperparameters vocab size   50000 batch size   128 embed size   128              dimension word embedding vectors skip window   1               context window num sampled   64              number negative examples sample learning rate   1 0 num train steps   100000 visual fld    visualization  skip step   5000    parameters downloading data download url    http   mattmahoney net dc text8 zip  expected bytes   31344016 num visualize   3000          number tokens visualize   def word2vec(dataset)          build graph word2vec model train           step 1  get input  output dataset     tf name scope( data )          iterator   dataset make initializable iterator()         center words  target words   iterator get next()          step 2   3  define weights embedding lookup      in word2vec  actually weights care             model hyperparameters    dimension word embedding vectors    context window    number negative examples sample    parameters downloading data    number tokens visualize    step 1  get input  output dataset    step 4  construct variables nce loss define loss function    define loss function nce loss function    step 5  define optimizer    use calculate late average loss last skip step steps ", "content": "\"\"\" starter code for word2vec skip-gram model with NCE loss\nCS 20: \"TensorFlow for Deep Learning Research\"\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 04\n\"\"\"\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport numpy as np\nfrom tensorflow.contrib.tensorboard.plugins import projector\nimport tensorflow as tf\n\nimport utils\nimport word2vec_utils\n\n# Model hyperparameters\nVOCAB_SIZE = 50000\nBATCH_SIZE = 128\nEMBED_SIZE = 128            # dimension of the word embedding vectors\nSKIP_WINDOW = 1             # the context window\nNUM_SAMPLED = 64            # number of negative examples to sample\nLEARNING_RATE = 1.0\nNUM_TRAIN_STEPS = 100000\nVISUAL_FLD = 'visualization'\nSKIP_STEP = 5000\n\n# Parameters for downloading data\nDOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\nEXPECTED_BYTES = 31344016\nNUM_VISUALIZE = 3000        # number of tokens to visualize\n\n\ndef word2vec(dataset):\n    \"\"\" Build the graph for word2vec model and train it \"\"\"\n    # Step 1: get input, output from the dataset\n    with tf.name_scope('data'):\n        iterator = dataset.make_initializable_iterator()\n        center_words, target_words = iterator.get_next()\n\n    \"\"\" Step 2 + 3: define weights and embedding lookup.\n    In word2vec, it's actually the weights that we care about \n    \"\"\"\n    with tf.name_scope('embed'):\n        embed_matrix = tf.get_variable('embed_matrix', \n                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n                                        initializer=tf.random_uniform_initializer())\n        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n\n    # Step 4: construct variables for NCE loss and define loss function\n    with tf.name_scope('loss'):\n        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n\n        # define loss function to be NCE loss function\n        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n                                            biases=nce_bias, \n                                            labels=target_words, \n                                            inputs=embed, \n                                            num_sampled=NUM_SAMPLED, \n                                            num_classes=VOCAB_SIZE), name='loss')\n\n    # Step 5: define optimizer\n    with tf.name_scope('optimizer'):\n        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n    \n    utils.safe_mkdir('checkpoints')\n\n    with tf.Session() as sess:\n        sess.run(iterator.initializer)\n        sess.run(tf.global_variables_initializer())\n\n        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n\n        for index in range(NUM_TRAIN_STEPS):\n            try:\n                loss_batch, _ = sess.run([loss, optimizer])\n                total_loss += loss_batch\n                if (index + 1) % SKIP_STEP == 0:\n                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n                    total_loss = 0.0\n            except tf.errors.OutOfRangeError:\n                sess.run(iterator.initializer)\n        writer.close()\n\ndef gen():\n    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n\ndef main():\n    dataset = tf.data.Dataset.from_generator(gen, \n                                (tf.int32, tf.int32), \n                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n    word2vec(dataset)\n\nif __name__ == '__main__':\n    main()\n", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "04_word2vec.py", "id": "4ec0a6597fd8a17834c5a74e07c5ae1d", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/04_word2vec.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}