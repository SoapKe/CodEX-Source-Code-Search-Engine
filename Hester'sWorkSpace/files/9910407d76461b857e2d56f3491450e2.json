{"author": "tornadoweb", "code": "\n\nimport time\nfrom datetime import timedelta\n\ntry:\n    from HTMLParser import HTMLParser\n    from urlparse import urljoin, urldefrag\nexcept ImportError:\n    from html.parser import HTMLParser\n    from urllib.parse import urljoin, urldefrag\n\nfrom tornado import httpclient, gen, ioloop, queues\n\nbase_url = 'http://www.tornadoweb.org/en/stable/'\nconcurrency = 10\n\n\n@gen.coroutine\ndef get_links_from_url(url):\n    \n    try:\n        response = yield httpclient.AsyncHTTPClient().fetch(url)\n        print('fetched %s' % url)\n\n        html = response.body if isinstance(response.body, str) \\\n            else response.body.decode(errors='ignore')\n        urls = [urljoin(url, remove_fragment(new_url))\n                for new_url in get_links(html)]\n    except Exception as e:\n        print('Exception: %s %s' % (e, url))\n        raise gen.Return([])\n\n    raise gen.Return(urls)\n\n\ndef remove_fragment(url):\n    pure_url, frag = urldefrag(url)\n    return pure_url\n\n\ndef get_links(html):\n    class URLSeeker(HTMLParser):\n        def __init__(self):\n            HTMLParser.__init__(self)\n            self.urls = []\n\n        def handle_starttag(self, tag, attrs):\n            href = dict(attrs).get('href')\n            if href and tag == 'a':\n                self.urls.append(href)\n\n    url_seeker = URLSeeker()\n    url_seeker.feed(html)\n    return url_seeker.urls\n\n\n@gen.coroutine\ndef main():\n    q = queues.Queue()\n    start = time.time()\n    fetching, fetched = set(), set()\n\n    @gen.coroutine\n    def fetch_url():\n        current_url = yield q.get()\n        try:\n            if current_url in fetching:\n                return\n\n            print('fetching %s' % current_url)\n            fetching.add(current_url)\n            urls = yield get_links_from_url(current_url)\n            fetched.add(current_url)\n\n            for new_url in urls:\n                \n                if new_url.startswith(base_url):\n                    yield q.put(new_url)\n\n        finally:\n            q.task_done()\n\n    @gen.coroutine\n    def worker():\n        while True:\n            yield fetch_url()\n\n    q.put(base_url)\n\n    \n    for _ in range(concurrency):\n        worker()\n    yield q.join(timeout=timedelta(seconds=300))\n    assert fetching == fetched\n    print('Done in %d seconds, fetched %s URLs.' % (\n        time.time() - start, len(fetched)))\n\n\nif __name__ == '__main__':\n    io_loop = ioloop.IOLoop.current()\n    io_loop.run_sync(main)\n", "comments": "   download page  url  parse links       returned links fragment     removed  made     absolute  e g  url  gen html tornado gen coroutine  becomes      http   www tornadoweb org en stable gen html               usr bin env python    only follow links beneath base url    start workers  wait work queue empty  ", "content": "#!/usr/bin/env python\n\nimport time\nfrom datetime import timedelta\n\ntry:\n    from HTMLParser import HTMLParser\n    from urlparse import urljoin, urldefrag\nexcept ImportError:\n    from html.parser import HTMLParser\n    from urllib.parse import urljoin, urldefrag\n\nfrom tornado import httpclient, gen, ioloop, queues\n\nbase_url = 'http://www.tornadoweb.org/en/stable/'\nconcurrency = 10\n\n\n@gen.coroutine\ndef get_links_from_url(url):\n    \"\"\"Download the page at `url` and parse it for links.\n\n    Returned links have had the fragment after `#` removed, and have been made\n    absolute so, e.g. the URL 'gen.html#tornado.gen.coroutine' becomes\n    'http://www.tornadoweb.org/en/stable/gen.html'.\n    \"\"\"\n    try:\n        response = yield httpclient.AsyncHTTPClient().fetch(url)\n        print('fetched %s' % url)\n\n        html = response.body if isinstance(response.body, str) \\\n            else response.body.decode(errors='ignore')\n        urls = [urljoin(url, remove_fragment(new_url))\n                for new_url in get_links(html)]\n    except Exception as e:\n        print('Exception: %s %s' % (e, url))\n        raise gen.Return([])\n\n    raise gen.Return(urls)\n\n\ndef remove_fragment(url):\n    pure_url, frag = urldefrag(url)\n    return pure_url\n\n\ndef get_links(html):\n    class URLSeeker(HTMLParser):\n        def __init__(self):\n            HTMLParser.__init__(self)\n            self.urls = []\n\n        def handle_starttag(self, tag, attrs):\n            href = dict(attrs).get('href')\n            if href and tag == 'a':\n                self.urls.append(href)\n\n    url_seeker = URLSeeker()\n    url_seeker.feed(html)\n    return url_seeker.urls\n\n\n@gen.coroutine\ndef main():\n    q = queues.Queue()\n    start = time.time()\n    fetching, fetched = set(), set()\n\n    @gen.coroutine\n    def fetch_url():\n        current_url = yield q.get()\n        try:\n            if current_url in fetching:\n                return\n\n            print('fetching %s' % current_url)\n            fetching.add(current_url)\n            urls = yield get_links_from_url(current_url)\n            fetched.add(current_url)\n\n            for new_url in urls:\n                # Only follow links beneath the base URL\n                if new_url.startswith(base_url):\n                    yield q.put(new_url)\n\n        finally:\n            q.task_done()\n\n    @gen.coroutine\n    def worker():\n        while True:\n            yield fetch_url()\n\n    q.put(base_url)\n\n    # Start workers, then wait for the work queue to be empty.\n    for _ in range(concurrency):\n        worker()\n    yield q.join(timeout=timedelta(seconds=300))\n    assert fetching == fetched\n    print('Done in %d seconds, fetched %s URLs.' % (\n        time.time() - start, len(fetched)))\n\n\nif __name__ == '__main__':\n    io_loop = ioloop.IOLoop.current()\n    io_loop.run_sync(main)\n", "description": "Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.", "file_name": "webspider.py", "id": "9910407d76461b857e2d56f3491450e2", "language": "Python", "project_name": "tornado", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tornadoweb-tornado/tornadoweb-tornado-62e43f6/demos/webspider/webspider.py", "save_time": "", "source": "", "update_at": "2018-03-13T23:11:42Z", "url": "https://github.com/tornadoweb/tornado", "wiki": true}