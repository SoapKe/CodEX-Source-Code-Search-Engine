{"author": "aws", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\"). You\n may not use this file except in compliance with the License. A copy of\n the License is located at\n\n     http://aws.amazon.com/apache2.0/\n\n or in the \"license\" file accompanying this file. This file is\n distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n ANY KIND, either express or implied. See the License for the specific\n language governing permissions and limitations under the License.\nfrom awscli.testutils import set_invalid_utime\nfrom mock import patch\nimport os\n\nfrom awscli.compat import six\nfrom tests.functional.s3 import BaseS3TransferCommandTest\n\n\nclass TestSyncCommand(BaseS3TransferCommandTest):\n\n    prefix = 's3 sync '\n\n    def test_website_redirect_ignore_paramfile(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt --website-redirect %s' % \\\n            (self.prefix, self.files.rootdir, 'http://someserver')\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n\n         The only operations we should have called are ListObjects/PutObject.\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'PutObject')\n         Make sure that the specified web address is used as opposed to the\n         contents of the web address when uploading the object\n        self.assertEqual(\n            self.operations_called[1][1]['WebsiteRedirectLocation'],\n            'http://someserver'\n        )\n\n    def test_no_recursive_option(self):\n        cmdline = '. s3://mybucket --recursive'\n         Return code will be 2 for invalid parameter ``--recursive``\n        self.run_cmd(cmdline, expected_rc=2)\n\n    def test_sync_from_non_existant_directory(self):\n        non_existant_directory = os.path.join(self.files.rootdir, 'fakedir')\n        cmdline = '%s %s s3://bucket/' % (self.prefix, non_existant_directory)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []}\n        ]\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=255)\n        self.assertIn('does not exist', stderr)\n\n    def test_sync_to_non_existant_directory(self):\n        key = 'foo.txt'\n        non_existant_directory = os.path.join(self.files.rootdir, 'fakedir')\n        cmdline = '%s s3://bucket/ %s' % (self.prefix, non_existant_directory)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": [\n                {\"Key\": key, \"Size\": 3,\n                 \"LastModified\": \"2014-01-09T20:45:49.000Z\"}]},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222-\"',\n             'Body': six.BytesIO(b'foo')}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n         Make sure the file now exists.\n        self.assertTrue(\n            os.path.exists(os.path.join(non_existant_directory, key)))\n\n    def test_glacier_sync_with_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/foo %s --force-glacier-transfer' % (\n            self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_handles_glacier_incompatible_operations(self):\n        self.parsed_responses = [\n            {'Contents': [\n                {'Key': 'foo', 'Size': 100,\n                 'LastModified': '00:00:00Z', 'StorageClass': 'GLACIER'}]}\n        ]\n        cmdline = '%s s3://bucket/ %s' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n         There should not have been a download attempted because the\n         operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertIn('GLACIER', stderr)\n\n    def test_turn_off_glacier_warnings(self):\n        self.parsed_responses = [\n            {'Contents': [\n                {'Key': 'foo', 'Size': 100,\n                 'LastModified': '00:00:00Z', 'StorageClass': 'GLACIER'}]}\n        ]\n        cmdline = '%s s3://bucket/ %s --ignore-glacier-warnings' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=0)\n         There should not have been a download attempted because the\n         operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual('', stderr)\n\n    def test_warning_on_invalid_timestamp(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n\n         Set the update time to a value that will raise a ValueError when\n         converting to datetime\n        set_invalid_utime(full_path)\n        cmdline = '%s %s s3://bucket/key.txt' % \\\n                  (self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=2)\n\n         We should still have put the object\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'PutObject')\n\n    def test_sync_with_delete_on_downloads(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s s3://bucket %s --delete' % (\n            self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n\n         The only operations we should have called are ListObjects.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n        self.assertFalse(os.path.exists(full_path))\n\n     When a file has been deleted after listing,\n     awscli.customizations.s3.utils.get_file_stat may raise either some kind\n     of OSError, or a ValueError, depending on the environment. In both cases,\n     the behaviour should be the same: skip the file and emit a warning.\n    \n     This test covers the case where a ValueError is emitted.\n    def test_sync_skips_over_files_deleted_between_listing_and_transfer_valueerror(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (\n            self.prefix, self.files.rootdir)\n\n         FileGenerator.list_files should skip over files that cause an\n         IOError to be raised because they are missing when we try to\n         get their stats. This IOError is translated to a ValueError in\n         awscli.customizations.s3.utils.get_file_stat.\n        def side_effect(_):\n            os.remove(full_path)\n            raise ValueError()\n        with patch(\n                'awscli.customizations.s3.filegenerator.get_file_stat',\n                side_effect=side_effect\n                ):\n            self.run_cmd(cmdline, expected_rc=2)\n\n         We should not call PutObject because the file was deleted\n         before we could transfer it\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n     This test covers the case where an OSError is emitted.\n    def test_sync_skips_over_files_deleted_between_listing_and_transfer_oserror(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (\n            self.prefix, self.files.rootdir)\n\n         FileGenerator.list_files should skip over files that cause an\n         OSError to be raised because they are missing when we try to\n         get their stats.\n        def side_effect(_):\n            os.remove(full_path)\n            raise OSError()\n        with patch(\n                'awscli.customizations.s3.filegenerator.get_file_stat',\n                side_effect=side_effect\n                ):\n            self.run_cmd(cmdline, expected_rc=2)\n\n         We should not call PutObject because the file was deleted\n         before we could transfer it\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n    def test_request_payer(self):\n        cmdline = '%s s3://sourcebucket/ s3://mybucket --request-payer' % (\n            self.prefix)\n        self.parsed_responses = [\n             Response for ListObjects on source bucket\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n             Response for ListObjects on destination bucket\n            {\n                'Contents': [],\n                'CommonPrefixes': []\n            },\n             Response from copy object\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_request_payer_with_deletes(self):\n        cmdline = '%s s3://sourcebucket/ s3://mybucket' % self.prefix\n        cmdline += ' --request-payer'\n        cmdline += ' --delete'\n        self.parsed_responses = [\n             Response for ListObjects on source bucket\n            {\n                'Contents': [],\n                'CommonPrefixes': []\n            },\n             Response for ListObjects on destination bucket\n            {\n                'Contents': [\n                    {'Key': 'key-to-delete',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n             Response from copy object\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('DeleteObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'key-to-delete',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n", "comments": "  copyright 2013 amazon com  inc  affiliates  all rights reserved        licensed apache license  version 2 0 (the  license )  you    may use file except compliance license  a copy    license located           http   aws amazon com apache2 0         license  file accompanying file  this file    distributed  as is  basis  without warranties or conditions of    any kind  either express implied  see license specific    language governing permissions limitations license     the operations called listobjects putobject     make sure specified web address used opposed    contents web address uploading object    return code 2 invalid parameter     recursive      make sure file exists     there download attempted    operation skipped glacier incompatible     there download attempted    operation skipped glacier incompatible     set update time value raise valueerror    converting datetime    we still put object    the operations called listobjects     when file deleted listing     awscli customizations s3 utils get file stat may raise either kind    oserror  valueerror  depending environment  in cases     behaviour  skip file emit warning        this test covers case valueerror emitted     filegenerator list files skip files cause    ioerror raised missing try    get stats  this ioerror translated valueerror    awscli customizations s3 utils get file stat     we call putobject file deleted    could transfer    this test covers case oserror emitted     filegenerator list files skip files cause    oserror raised missing try    get stats     we call putobject file deleted    could transfer    response listobjects source bucket    response listobjects destination bucket    response copy object    response listobjects source bucket    response listobjects destination bucket    response copy object ", "content": "# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom awscli.testutils import set_invalid_utime\nfrom mock import patch\nimport os\n\nfrom awscli.compat import six\nfrom tests.functional.s3 import BaseS3TransferCommandTest\n\n\nclass TestSyncCommand(BaseS3TransferCommandTest):\n\n    prefix = 's3 sync '\n\n    def test_website_redirect_ignore_paramfile(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/key.txt --website-redirect %s' % \\\n            (self.prefix, self.files.rootdir, 'http://someserver')\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n\n        # The only operations we should have called are ListObjects/PutObject.\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'PutObject')\n        # Make sure that the specified web address is used as opposed to the\n        # contents of the web address when uploading the object\n        self.assertEqual(\n            self.operations_called[1][1]['WebsiteRedirectLocation'],\n            'http://someserver'\n        )\n\n    def test_no_recursive_option(self):\n        cmdline = '. s3://mybucket --recursive'\n        # Return code will be 2 for invalid parameter ``--recursive``\n        self.run_cmd(cmdline, expected_rc=2)\n\n    def test_sync_from_non_existant_directory(self):\n        non_existant_directory = os.path.join(self.files.rootdir, 'fakedir')\n        cmdline = '%s %s s3://bucket/' % (self.prefix, non_existant_directory)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []}\n        ]\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=255)\n        self.assertIn('does not exist', stderr)\n\n    def test_sync_to_non_existant_directory(self):\n        key = 'foo.txt'\n        non_existant_directory = os.path.join(self.files.rootdir, 'fakedir')\n        cmdline = '%s s3://bucket/ %s' % (self.prefix, non_existant_directory)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": [\n                {\"Key\": key, \"Size\": 3,\n                 \"LastModified\": \"2014-01-09T20:45:49.000Z\"}]},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222-\"',\n             'Body': six.BytesIO(b'foo')}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        # Make sure the file now exists.\n        self.assertTrue(\n            os.path.exists(os.path.join(non_existant_directory, key)))\n\n    def test_glacier_sync_with_force_glacier(self):\n        self.parsed_responses = [\n            {\n                'Contents': [\n                    {'Key': 'foo/bar.txt', 'ContentLength': '100',\n                     'LastModified': '00:00:00Z',\n                     'StorageClass': 'GLACIER',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            {'ETag': '\"foo-1\"', 'Body': six.BytesIO(b'foo')},\n        ]\n        cmdline = '%s s3://bucket/foo %s --force-glacier-transfer' % (\n            self.prefix, self.files.rootdir)\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'GetObject')\n\n    def test_handles_glacier_incompatible_operations(self):\n        self.parsed_responses = [\n            {'Contents': [\n                {'Key': 'foo', 'Size': 100,\n                 'LastModified': '00:00:00Z', 'StorageClass': 'GLACIER'}]}\n        ]\n        cmdline = '%s s3://bucket/ %s' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=2)\n        # There should not have been a download attempted because the\n        # operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertIn('GLACIER', stderr)\n\n    def test_turn_off_glacier_warnings(self):\n        self.parsed_responses = [\n            {'Contents': [\n                {'Key': 'foo', 'Size': 100,\n                 'LastModified': '00:00:00Z', 'StorageClass': 'GLACIER'}]}\n        ]\n        cmdline = '%s s3://bucket/ %s --ignore-glacier-warnings' % (\n            self.prefix, self.files.rootdir)\n        _, stderr, _ = self.run_cmd(cmdline, expected_rc=0)\n        # There should not have been a download attempted because the\n        # operation was skipped because it is glacier incompatible.\n        self.assertEqual(len(self.operations_called), 1)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual('', stderr)\n\n    def test_warning_on_invalid_timestamp(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n\n        # Set the update time to a value that will raise a ValueError when\n        # converting to datetime\n        set_invalid_utime(full_path)\n        cmdline = '%s %s s3://bucket/key.txt' % \\\n                  (self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=2)\n\n        # We should still have put the object\n        self.assertEqual(len(self.operations_called), 2, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n        self.assertEqual(self.operations_called[1][0].name, 'PutObject')\n\n    def test_sync_with_delete_on_downloads(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s s3://bucket %s --delete' % (\n            self.prefix, self.files.rootdir)\n        self.parsed_responses = [\n            {\"CommonPrefixes\": [], \"Contents\": []},\n            {'ETag': '\"c8afdb36c52cf4727836669019e69222\"'}\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n\n        # The only operations we should have called are ListObjects.\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n        self.assertFalse(os.path.exists(full_path))\n\n    # When a file has been deleted after listing,\n    # awscli.customizations.s3.utils.get_file_stat may raise either some kind\n    # of OSError, or a ValueError, depending on the environment. In both cases,\n    # the behaviour should be the same: skip the file and emit a warning.\n    #\n    # This test covers the case where a ValueError is emitted.\n    def test_sync_skips_over_files_deleted_between_listing_and_transfer_valueerror(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (\n            self.prefix, self.files.rootdir)\n\n        # FileGenerator.list_files should skip over files that cause an\n        # IOError to be raised because they are missing when we try to\n        # get their stats. This IOError is translated to a ValueError in\n        # awscli.customizations.s3.utils.get_file_stat.\n        def side_effect(_):\n            os.remove(full_path)\n            raise ValueError()\n        with patch(\n                'awscli.customizations.s3.filegenerator.get_file_stat',\n                side_effect=side_effect\n                ):\n            self.run_cmd(cmdline, expected_rc=2)\n\n        # We should not call PutObject because the file was deleted\n        # before we could transfer it\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n    # This test covers the case where an OSError is emitted.\n    def test_sync_skips_over_files_deleted_between_listing_and_transfer_oserror(self):\n        full_path = self.files.create_file('foo.txt', 'mycontent')\n        cmdline = '%s %s s3://bucket/' % (\n            self.prefix, self.files.rootdir)\n\n        # FileGenerator.list_files should skip over files that cause an\n        # OSError to be raised because they are missing when we try to\n        # get their stats.\n        def side_effect(_):\n            os.remove(full_path)\n            raise OSError()\n        with patch(\n                'awscli.customizations.s3.filegenerator.get_file_stat',\n                side_effect=side_effect\n                ):\n            self.run_cmd(cmdline, expected_rc=2)\n\n        # We should not call PutObject because the file was deleted\n        # before we could transfer it\n        self.assertEqual(len(self.operations_called), 1, self.operations_called)\n        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')\n\n    def test_request_payer(self):\n        cmdline = '%s s3://sourcebucket/ s3://mybucket --request-payer' % (\n            self.prefix)\n        self.parsed_responses = [\n            # Response for ListObjects on source bucket\n            {\n                'Contents': [\n                    {'Key': 'mykey',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            # Response for ListObjects on destination bucket\n            {\n                'Contents': [],\n                'CommonPrefixes': []\n            },\n            # Response from copy object\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('CopyObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'mykey',\n                    'CopySource': 'sourcebucket/mykey',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n\n    def test_request_payer_with_deletes(self):\n        cmdline = '%s s3://sourcebucket/ s3://mybucket' % self.prefix\n        cmdline += ' --request-payer'\n        cmdline += ' --delete'\n        self.parsed_responses = [\n            # Response for ListObjects on source bucket\n            {\n                'Contents': [],\n                'CommonPrefixes': []\n            },\n            # Response for ListObjects on destination bucket\n            {\n                'Contents': [\n                    {'Key': 'key-to-delete',\n                     'LastModified': '00:00:00Z',\n                     'Size': 100},\n                ],\n                'CommonPrefixes': []\n            },\n            # Response from copy object\n            {},\n        ]\n        self.run_cmd(cmdline, expected_rc=0)\n        self.assert_operations_called(\n            [\n                ('ListObjects', {\n                    'Bucket': 'sourcebucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('ListObjects', {\n                    'Bucket': 'mybucket',\n                    'Prefix': '',\n                    'EncodingType': 'url',\n                    'RequestPayer': 'requester',\n                }),\n                ('DeleteObject', {\n                    'Bucket': 'mybucket',\n                    'Key': 'key-to-delete',\n                    'RequestPayer': 'requester',\n                })\n            ]\n        )\n", "description": "Universal Command Line Interface for Amazon Web Services", "file_name": "test_sync_command.py", "id": "c33d7012c984a6a9820e5ab9d189cd3c", "language": "Python", "project_name": "aws-cli", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/aws-aws-cli/aws-aws-cli-d705c60/tests/functional/s3/test_sync_command.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:33:26Z", "url": "https://github.com/aws/aws-cli", "wiki": false}