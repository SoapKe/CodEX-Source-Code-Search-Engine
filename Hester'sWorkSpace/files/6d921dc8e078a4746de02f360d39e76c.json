{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\nr\"\"\"Script to preprocesses data from the Kepler space telescope.\n\nThis script produces training, validation and test sets of labeled Kepler\nThreshold Crossing Events (TCEs). A TCE is a detected periodic event on a\nparticular Kepler target star that may or may not be a transiting planet. Each\nTCE in the output contains local and global views of its light curve; auxiliary\nfeatures such as period and duration; and a label indicating whether the TCE is\nconsistent with being a transiting planet. The data sets produced by this script\ncan be used to train and evaluate models that classify Kepler TCEs.\n\nThe input TCEs and their associated labels are specified by the DR24 TCE Table,\nwhich can be downloaded in CSV format from the NASA Exoplanet Archive at:\n\n  https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce\n\nThe downloaded CSV file should contain at least the following column names:\n  rowid: Integer ID of the row in the TCE table.\n  kepid: Kepler ID of the target star.\n  tce_plnt_num: TCE number within the target star.\n  tce_period: Orbital period of the detected event, in days.\n  tce_time0bk: The time corresponding to the center of the first detected\n      traisit in Barycentric Julian Day (BJD) minus a constant offset of\n      2,454,833.0 days.\n  tce_duration: Duration of the detected transit, in hours.\n  av_training_set: Autovetter training set label; one of PC (planet candidate),\n      AFP (astrophysical false positive), NTP (non-transiting phenomenon),\n      UNK (unknown).\n\nThe Kepler light curves can be downloaded from the Mikulski Archive for Space\nTelescopes (MAST) at:\n\n  http://archive.stsci.edu/pub/kepler/lightcurves.\n\nThe Kepler data is assumed to reside in a directory with the same structure as\nthe MAST archive. Specifically, the file names for a particular Kepler target\nstar should have the following format:\n\n    .../${kep_id:0:4}/${kep_id}/kplr${kep_id}-${quarter_prefix}_${type}.fits,\n\nwhere:\n  kep_id is the Kepler id left-padded with zeros to length 9;\n  quarter_prefix is the file name quarter prefix;\n  type is one of \"llc\" (long cadence light curve) or \"slc\" (short cadence light\n    curve).\n\nThe output TFRecord file contains one serialized tensorflow.train.Example\nprotocol buffer for each TCE in the input CSV file. Each Example contains the\nfollowing light curve representations:\n  global_view: Vector of length 2001; the Global View of the TCE.\n  local_view: Vector of length 201; the Local View of the TCE.\n\nIn addition, each Example contains the value of each column in the input TCE CSV\nfile. Some of these features may be useful as auxiliary features to the model.\nThe columns include:\n  rowid: Integer ID of the row in the TCE table.\n  kepid: Kepler ID of the target star.\n  tce_plnt_num: TCE number within the target star.\n  av_training_set: Autovetter training set label.\n  tce_period: Orbital period of the detected event, in days.\n  ...\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport multiprocessing\nimport os\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom astronet.data import preprocess\n\n\nparser = argparse.ArgumentParser()\n\n_DR24_TCE_URL = (\"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/\"\n                 \"nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce\")\n\nparser.add_argument(\n    \"--input_tce_csv_file\",\n    type=str,\n    required=True,\n    help=\"CSV file containing the Q1-Q17 DR24 Kepler TCE table. Must contain \"\n    \"columns: rowid, kepid, tce_plnt_num, tce_period, tce_duration, \"\n    \"tce_time0bk. Download from: %s\" % _DR24_TCE_URL)\n\nparser.add_argument(\n    \"--kepler_data_dir\",\n    type=str,\n    required=True,\n    help=\"Base folder containing Kepler data.\")\n\nparser.add_argument(\n    \"--output_dir\",\n    type=str,\n    required=True,\n    help=\"Directory in which to save the output.\")\n\nparser.add_argument(\n    \"--num_train_shards\",\n    type=int,\n    default=8,\n    help=\"Number of file shards to divide the training set into.\")\n\nparser.add_argument(\n    \"--num_worker_processes\",\n    type=int,\n    default=5,\n    help=\"Number of subprocesses for processing the TCEs in parallel.\")\n\n Name and values of the column in the input CSV file to use as training labels.\n_LABEL_COLUMN = \"av_training_set\"\n_ALLOWED_LABELS = {\"PC\", \"AFP\", \"NTP\"}\n\n\ndef _set_float_feature(ex, name, value):\n  \"\"\"Sets the value of a float feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].float_list.value.extend([float(v) for v in value])\n\n\ndef _set_bytes_feature(ex, name, value):\n  \"\"\"Sets the value of a bytes feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].bytes_list.value.extend([str(v) for v in value])\n\n\ndef _set_int64_feature(ex, name, value):\n  \"\"\"Sets the value of an int64 feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].int64_list.value.extend([int(v) for v in value])\n\n\ndef _process_tce(tce):\n  \"\"\"Processes the light curve for a Kepler TCE and returns an Example proto.\n\n  Args:\n    tce: Row of the input TCE table.\n\n  Returns:\n    A tensorflow.train.Example proto containing TCE features.\n\n  Raises:\n    IOError: If the light curve files for this Kepler ID cannot be found.\n  \"\"\"\n   Read and process the light curve.\n  time, flux = preprocess.read_and_process_light_curve(tce.kepid,\n                                                       FLAGS.kepler_data_dir)\n  time, flux = preprocess.phase_fold_and_sort_light_curve(\n      time, flux, tce.tce_period, tce.tce_time0bk)\n\n   Generate the local and global views.\n  global_view = preprocess.global_view(time, flux, tce.tce_period)\n  local_view = preprocess.local_view(time, flux, tce.tce_period,\n                                     tce.tce_duration)\n\n   Make output proto.\n  ex = tf.train.Example()\n\n   Set time series features.\n  _set_float_feature(ex, \"global_view\", global_view)\n  _set_float_feature(ex, \"local_view\", local_view)\n\n   Set other columns.\n  for col_name, value in tce.iteritems():\n    if np.issubdtype(type(value), np.integer):\n      _set_int64_feature(ex, col_name, [value])\n    else:\n      try:\n        _set_float_feature(ex, col_name, [float(value)])\n      except ValueError:\n        _set_bytes_feature(ex, col_name, [str(value)])\n\n  return ex\n\n\ndef _process_file_shard(tce_table, file_name):\n  \"\"\"Processes a single file shard.\n\n  Args:\n    tce_table: A Pandas DateFrame containing the TCEs in the shard.\n    file_name: The output TFRecord file.\n  \"\"\"\n  process_name = multiprocessing.current_process().name\n  shard_name = os.path.basename(file_name)\n  shard_size = len(tce_table)\n  tf.logging.info(\"%s: Processing %d items in shard %s\", process_name,\n                  shard_size, shard_name)\n\n  with tf.python_io.TFRecordWriter(file_name) as writer:\n    num_processed = 0\n    for _, tce in tce_table.iterrows():\n      example = _process_tce(tce)\n      if example is not None:\n        writer.write(example.SerializeToString())\n\n      num_processed += 1\n      if not num_processed % 10:\n        tf.logging.info(\"%s: Processed %d/%d items in shard %s\", process_name,\n                        num_processed, shard_size, shard_name)\n\n  tf.logging.info(\"%s: Wrote %d items in shard %s\", process_name, shard_size,\n                  shard_name)\n\n\ndef main(argv):\n  del argv   Unused.\n\n   Make the output directory if it doesn't already exist.\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n\n   Read CSV file of Kepler KOIs.\n  tce_table = pd.read_csv(\n      FLAGS.input_tce_csv_file, index_col=\"rowid\", comment=\"\")\n  tce_table[\"tce_duration\"] /= 24   Convert hours to days.\n  tf.logging.info(\"Read TCE CSV file with %d rows.\", len(tce_table))\n\n   Filter TCE table to allowed labels.\n  allowed_tces = tce_table[_LABEL_COLUMN].apply(lambda l: l in _ALLOWED_LABELS)\n  tce_table = tce_table[allowed_tces]\n  num_tces = len(tce_table)\n  tf.logging.info(\"Filtered to %d TCEs with labels in %s.\", num_tces,\n                  list(_ALLOWED_LABELS))\n\n   Randomly shuffle the TCE table.\n  np.random.seed(123)\n  tce_table = tce_table.iloc[np.random.permutation(num_tces)]\n  tf.logging.info(\"Randomly shuffled TCEs.\")\n\n   Partition the TCE table as follows:\n     train_tces = 80% of TCEs\n     val_tces = 10% of TCEs (for validation during training)\n     test_tces = 10% of TCEs (for final evaluation)\n  train_cutoff = int(0.80 * num_tces)\n  val_cutoff = int(0.90 * num_tces)\n  train_tces = tce_table[0:train_cutoff]\n  val_tces = tce_table[train_cutoff:val_cutoff]\n  test_tces = tce_table[val_cutoff:]\n  tf.logging.info(\n      \"Partitioned %d TCEs into training (%d), validation (%d) and test (%d)\",\n      num_tces, len(train_tces), len(val_tces), len(test_tces))\n\n   Further split training TCEs into file shards.\n  file_shards = []   List of (tce_table_shard, file_name).\n  boundaries = np.linspace(0, len(train_tces),\n                           FLAGS.num_train_shards + 1).astype(np.int)\n  for i in range(FLAGS.num_train_shards):\n    start = boundaries[i]\n    end = boundaries[i + 1]\n    file_shards.append((train_tces[start:end], os.path.join(\n        FLAGS.output_dir, \"train-%.5d-of-%.5d\" % (i, FLAGS.num_train_shards))))\n\n   Validation and test sets each have a single shard.\n  file_shards.append((val_tces, os.path.join(FLAGS.output_dir,\n                                             \"val-00000-of-00001\")))\n  file_shards.append((test_tces, os.path.join(FLAGS.output_dir,\n                                              \"test-00000-of-00001\")))\n  num_file_shards = len(file_shards)\n\n   Launch subprocesses for the file shards.\n  num_processes = min(num_file_shards, FLAGS.num_worker_processes)\n  tf.logging.info(\"Launching %d subprocesses for %d total file shards\",\n                  num_processes, num_file_shards)\n\n  pool = multiprocessing.Pool(processes=num_processes)\n  async_results = [\n      pool.apply_async(_process_file_shard, file_shard)\n      for file_shard in file_shards\n  ]\n  pool.close()\n\n   Instead of pool.join(), we call async_result.get() to ensure any exceptions\n   raised by the worker processes are also raised here.\n  for async_result in async_results:\n    async_result.get()\n\n  tf.logging.info(\"Finished processing %d total file shards\", num_file_shards)\n\n\nif __name__ == \"__main__\":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "comments": "   script preprocesses data kepler space telescope   this script produces training  validation test sets labeled kepler threshold crossing events (tces)  a tce detected periodic event particular kepler target star may may transiting planet  each tce output contains local global views light curve  auxiliary features period duration  label indicating whether tce consistent transiting planet  the data sets produced script used train evaluate models classify kepler tces   the input tces associated labels specified dr24 tce table  downloaded csv format nasa exoplanet archive     https   exoplanetarchive ipac caltech edu cgi bin tblview nph tblview app exotbls config q1 q17 dr24 tce  the downloaded csv file contain least following column names    rowid  integer id row tce table    kepid  kepler id target star    tce plnt num  tce number within target star    tce period  orbital period detected event  days    tce time0bk  the time corresponding center first detected       traisit barycentric julian day (bjd) minus constant offset       2 454 833 0 days    tce duration  duration detected transit  hours    av training set  autovetter training set label  one pc (planet candidate)        afp (astrophysical false positive)  ntp (non transiting phenomenon)        unk (unknown)   the kepler light curves downloaded mikulski archive space telescopes (mast)     http   archive stsci edu pub kepler lightcurves   the kepler data assumed reside directory structure mast archive  specifically  file names particular kepler target star following format             kep id 0 4    kep id  kplr  kep id    quarter prefix    type  fits      kep id kepler id left padded zeros length 9    quarter prefix file name quarter prefix    type one  llc  (long cadence light curve)  slc  (short cadence light     curve)   the output tfrecord file contains one serialized tensorflow train example protocol buffer tce input csv file  each example contains following light curve representations    global view  vector length 2001  global view tce    local view  vector length 201  local view tce   in addition  example contains value column input tce csv file  some features may useful auxiliary features model  the columns include    rowid  integer id row tce table    kepid  kepler id target star    tce plnt num  tce number within target star    av training set  autovetter training set label    tce period  orbital period detected event  days              future   import absolute import   future   import division   future   import print function  import argparse import multiprocessing import os import sys  import numpy np import pandas pd import tensorflow tf  astronet data import preprocess   parser   argparse argumentparser()   dr24 tce url   ( https   exoplanetarchive ipac caltech edu cgi bin tblview                     nph tblview app exotbls config q1 q17 dr24 tce )  parser add argument(        input tce csv file       type str      required true      help  csv file containing q1 q17 dr24 kepler tce table  must contain        columns  rowid  kepid  tce plnt num  tce period  tce duration         tce time0bk  download       dr24 tce url)  parser add argument(        kepler data dir       type str      required true      help  base folder containing kepler data  )  parser add argument(        output dir       type str      required true      help  directory save output  )  parser add argument(        num train shards       type int      default 8      help  number file shards divide training set  )  parser add argument(        num worker processes       type int      default 5      help  number subprocesses processing tces parallel  )    name values column input csv file use training labels   label column    av training set   allowed labels     pc    afp    ntp     def  set float feature(ex  name  value)       sets value float feature tensorflow train example proto       assert name ex features feature   duplicate feature      name   ex features feature name  float list value extend( float(v) v value )   def  set bytes feature(ex  name  value)       sets value bytes feature tensorflow train example proto       assert name ex features feature   duplicate feature      name   ex features feature name  bytes list value extend( str(v) v value )   def  set int64 feature(ex  name  value)       sets value int64 feature tensorflow train example proto       assert name ex features feature   duplicate feature      name   ex features feature name  int64 list value extend( int(v) v value )   def  process tce(tce)       processes light curve kepler tce returns example proto     args      tce  row input tce table     returns      a tensorflow train example proto containing tce features     raises      ioerror  if light curve files kepler id cannot found            read process light curve    time  flux   preprocess read process light curve(tce kepid                                                         flags kepler data dir)   time  flux   preprocess phase fold sort light curve(       time  flux  tce tce period  tce tce time0bk)      generate local global views    global view   preprocess global view(time  flux  tce tce period)   local view   preprocess local view(time  flux  tce tce period                                       tce tce duration)      make output proto    ex   tf train example()      set time series features     set float feature(ex   global view   global view)    set float feature(ex   local view   local view)      set columns    col name  value tce iteritems()      np issubdtype(type(value)  np integer)         set int64 feature(ex  col name   value )     else        try           set float feature(ex  col name   float(value) )       except valueerror           set bytes feature(ex  col name   str(value) )    return ex   def  process file shard(tce table  file name)       processes single file shard     args      tce table  a pandas dateframe containing tces shard      file name  the output tfrecord file           copyright 2018 the tensorflow authors        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license     name values column input csv file use training labels     read process light curve     generate local global views     make output proto     set time series features     set columns     unused     make output directory already exist     read csv file kepler kois     convert hours days     filter tce table allowed labels     randomly shuffle tce table     partition tce table follows       train tces   80  tces      val tces   10  tces (for validation training)      test tces   10  tces (for final evaluation)    further split training tces file shards     list (tce table shard  file name)     validation test sets single shard     launch subprocesses file shards     instead pool join()  call async result get() ensure exceptions    raised worker processes also raised  ", "content": "# Copyright 2018 The TensorFlow Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr\"\"\"Script to preprocesses data from the Kepler space telescope.\n\nThis script produces training, validation and test sets of labeled Kepler\nThreshold Crossing Events (TCEs). A TCE is a detected periodic event on a\nparticular Kepler target star that may or may not be a transiting planet. Each\nTCE in the output contains local and global views of its light curve; auxiliary\nfeatures such as period and duration; and a label indicating whether the TCE is\nconsistent with being a transiting planet. The data sets produced by this script\ncan be used to train and evaluate models that classify Kepler TCEs.\n\nThe input TCEs and their associated labels are specified by the DR24 TCE Table,\nwhich can be downloaded in CSV format from the NASA Exoplanet Archive at:\n\n  https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce\n\nThe downloaded CSV file should contain at least the following column names:\n  rowid: Integer ID of the row in the TCE table.\n  kepid: Kepler ID of the target star.\n  tce_plnt_num: TCE number within the target star.\n  tce_period: Orbital period of the detected event, in days.\n  tce_time0bk: The time corresponding to the center of the first detected\n      traisit in Barycentric Julian Day (BJD) minus a constant offset of\n      2,454,833.0 days.\n  tce_duration: Duration of the detected transit, in hours.\n  av_training_set: Autovetter training set label; one of PC (planet candidate),\n      AFP (astrophysical false positive), NTP (non-transiting phenomenon),\n      UNK (unknown).\n\nThe Kepler light curves can be downloaded from the Mikulski Archive for Space\nTelescopes (MAST) at:\n\n  http://archive.stsci.edu/pub/kepler/lightcurves.\n\nThe Kepler data is assumed to reside in a directory with the same structure as\nthe MAST archive. Specifically, the file names for a particular Kepler target\nstar should have the following format:\n\n    .../${kep_id:0:4}/${kep_id}/kplr${kep_id}-${quarter_prefix}_${type}.fits,\n\nwhere:\n  kep_id is the Kepler id left-padded with zeros to length 9;\n  quarter_prefix is the file name quarter prefix;\n  type is one of \"llc\" (long cadence light curve) or \"slc\" (short cadence light\n    curve).\n\nThe output TFRecord file contains one serialized tensorflow.train.Example\nprotocol buffer for each TCE in the input CSV file. Each Example contains the\nfollowing light curve representations:\n  global_view: Vector of length 2001; the Global View of the TCE.\n  local_view: Vector of length 201; the Local View of the TCE.\n\nIn addition, each Example contains the value of each column in the input TCE CSV\nfile. Some of these features may be useful as auxiliary features to the model.\nThe columns include:\n  rowid: Integer ID of the row in the TCE table.\n  kepid: Kepler ID of the target star.\n  tce_plnt_num: TCE number within the target star.\n  av_training_set: Autovetter training set label.\n  tce_period: Orbital period of the detected event, in days.\n  ...\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport multiprocessing\nimport os\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom astronet.data import preprocess\n\n\nparser = argparse.ArgumentParser()\n\n_DR24_TCE_URL = (\"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/\"\n                 \"nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce\")\n\nparser.add_argument(\n    \"--input_tce_csv_file\",\n    type=str,\n    required=True,\n    help=\"CSV file containing the Q1-Q17 DR24 Kepler TCE table. Must contain \"\n    \"columns: rowid, kepid, tce_plnt_num, tce_period, tce_duration, \"\n    \"tce_time0bk. Download from: %s\" % _DR24_TCE_URL)\n\nparser.add_argument(\n    \"--kepler_data_dir\",\n    type=str,\n    required=True,\n    help=\"Base folder containing Kepler data.\")\n\nparser.add_argument(\n    \"--output_dir\",\n    type=str,\n    required=True,\n    help=\"Directory in which to save the output.\")\n\nparser.add_argument(\n    \"--num_train_shards\",\n    type=int,\n    default=8,\n    help=\"Number of file shards to divide the training set into.\")\n\nparser.add_argument(\n    \"--num_worker_processes\",\n    type=int,\n    default=5,\n    help=\"Number of subprocesses for processing the TCEs in parallel.\")\n\n# Name and values of the column in the input CSV file to use as training labels.\n_LABEL_COLUMN = \"av_training_set\"\n_ALLOWED_LABELS = {\"PC\", \"AFP\", \"NTP\"}\n\n\ndef _set_float_feature(ex, name, value):\n  \"\"\"Sets the value of a float feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].float_list.value.extend([float(v) for v in value])\n\n\ndef _set_bytes_feature(ex, name, value):\n  \"\"\"Sets the value of a bytes feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].bytes_list.value.extend([str(v) for v in value])\n\n\ndef _set_int64_feature(ex, name, value):\n  \"\"\"Sets the value of an int64 feature in a tensorflow.train.Example proto.\"\"\"\n  assert name not in ex.features.feature, \"Duplicate feature: %s\" % name\n  ex.features.feature[name].int64_list.value.extend([int(v) for v in value])\n\n\ndef _process_tce(tce):\n  \"\"\"Processes the light curve for a Kepler TCE and returns an Example proto.\n\n  Args:\n    tce: Row of the input TCE table.\n\n  Returns:\n    A tensorflow.train.Example proto containing TCE features.\n\n  Raises:\n    IOError: If the light curve files for this Kepler ID cannot be found.\n  \"\"\"\n  # Read and process the light curve.\n  time, flux = preprocess.read_and_process_light_curve(tce.kepid,\n                                                       FLAGS.kepler_data_dir)\n  time, flux = preprocess.phase_fold_and_sort_light_curve(\n      time, flux, tce.tce_period, tce.tce_time0bk)\n\n  # Generate the local and global views.\n  global_view = preprocess.global_view(time, flux, tce.tce_period)\n  local_view = preprocess.local_view(time, flux, tce.tce_period,\n                                     tce.tce_duration)\n\n  # Make output proto.\n  ex = tf.train.Example()\n\n  # Set time series features.\n  _set_float_feature(ex, \"global_view\", global_view)\n  _set_float_feature(ex, \"local_view\", local_view)\n\n  # Set other columns.\n  for col_name, value in tce.iteritems():\n    if np.issubdtype(type(value), np.integer):\n      _set_int64_feature(ex, col_name, [value])\n    else:\n      try:\n        _set_float_feature(ex, col_name, [float(value)])\n      except ValueError:\n        _set_bytes_feature(ex, col_name, [str(value)])\n\n  return ex\n\n\ndef _process_file_shard(tce_table, file_name):\n  \"\"\"Processes a single file shard.\n\n  Args:\n    tce_table: A Pandas DateFrame containing the TCEs in the shard.\n    file_name: The output TFRecord file.\n  \"\"\"\n  process_name = multiprocessing.current_process().name\n  shard_name = os.path.basename(file_name)\n  shard_size = len(tce_table)\n  tf.logging.info(\"%s: Processing %d items in shard %s\", process_name,\n                  shard_size, shard_name)\n\n  with tf.python_io.TFRecordWriter(file_name) as writer:\n    num_processed = 0\n    for _, tce in tce_table.iterrows():\n      example = _process_tce(tce)\n      if example is not None:\n        writer.write(example.SerializeToString())\n\n      num_processed += 1\n      if not num_processed % 10:\n        tf.logging.info(\"%s: Processed %d/%d items in shard %s\", process_name,\n                        num_processed, shard_size, shard_name)\n\n  tf.logging.info(\"%s: Wrote %d items in shard %s\", process_name, shard_size,\n                  shard_name)\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  # Make the output directory if it doesn't already exist.\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  # Read CSV file of Kepler KOIs.\n  tce_table = pd.read_csv(\n      FLAGS.input_tce_csv_file, index_col=\"rowid\", comment=\"#\")\n  tce_table[\"tce_duration\"] /= 24  # Convert hours to days.\n  tf.logging.info(\"Read TCE CSV file with %d rows.\", len(tce_table))\n\n  # Filter TCE table to allowed labels.\n  allowed_tces = tce_table[_LABEL_COLUMN].apply(lambda l: l in _ALLOWED_LABELS)\n  tce_table = tce_table[allowed_tces]\n  num_tces = len(tce_table)\n  tf.logging.info(\"Filtered to %d TCEs with labels in %s.\", num_tces,\n                  list(_ALLOWED_LABELS))\n\n  # Randomly shuffle the TCE table.\n  np.random.seed(123)\n  tce_table = tce_table.iloc[np.random.permutation(num_tces)]\n  tf.logging.info(\"Randomly shuffled TCEs.\")\n\n  # Partition the TCE table as follows:\n  #   train_tces = 80% of TCEs\n  #   val_tces = 10% of TCEs (for validation during training)\n  #   test_tces = 10% of TCEs (for final evaluation)\n  train_cutoff = int(0.80 * num_tces)\n  val_cutoff = int(0.90 * num_tces)\n  train_tces = tce_table[0:train_cutoff]\n  val_tces = tce_table[train_cutoff:val_cutoff]\n  test_tces = tce_table[val_cutoff:]\n  tf.logging.info(\n      \"Partitioned %d TCEs into training (%d), validation (%d) and test (%d)\",\n      num_tces, len(train_tces), len(val_tces), len(test_tces))\n\n  # Further split training TCEs into file shards.\n  file_shards = []  # List of (tce_table_shard, file_name).\n  boundaries = np.linspace(0, len(train_tces),\n                           FLAGS.num_train_shards + 1).astype(np.int)\n  for i in range(FLAGS.num_train_shards):\n    start = boundaries[i]\n    end = boundaries[i + 1]\n    file_shards.append((train_tces[start:end], os.path.join(\n        FLAGS.output_dir, \"train-%.5d-of-%.5d\" % (i, FLAGS.num_train_shards))))\n\n  # Validation and test sets each have a single shard.\n  file_shards.append((val_tces, os.path.join(FLAGS.output_dir,\n                                             \"val-00000-of-00001\")))\n  file_shards.append((test_tces, os.path.join(FLAGS.output_dir,\n                                              \"test-00000-of-00001\")))\n  num_file_shards = len(file_shards)\n\n  # Launch subprocesses for the file shards.\n  num_processes = min(num_file_shards, FLAGS.num_worker_processes)\n  tf.logging.info(\"Launching %d subprocesses for %d total file shards\",\n                  num_processes, num_file_shards)\n\n  pool = multiprocessing.Pool(processes=num_processes)\n  async_results = [\n      pool.apply_async(_process_file_shard, file_shard)\n      for file_shard in file_shards\n  ]\n  pool.close()\n\n  # Instead of pool.join(), we call async_result.get() to ensure any exceptions\n  # raised by the worker processes are also raised here.\n  for async_result in async_results:\n    async_result.get()\n\n  tf.logging.info(\"Finished processing %d total file shards\", num_file_shards)\n\n\nif __name__ == \"__main__\":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "description": "Models and examples built with TensorFlow", "file_name": "generate_input_records.py", "id": "6d921dc8e078a4746de02f360d39e76c", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/astronet/astronet/data/generate_input_records.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}