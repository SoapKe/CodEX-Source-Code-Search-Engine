{"author": "yunjey", "code": "from __future__ import division\nfrom torch.backends import cudnn\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\nimport argparse\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\n\n\nuse_cuda = torch.cuda.is_available()\ndtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n\n\n\ndef load_image(image_path, transform=None, max_size=None, shape=None):\n    image = Image.open(image_path)\n    \n    if max_size is not None:\n        scale = max_size / max(image.size)\n        size = np.array(image.size) * scale\n        image = image.resize(size.astype(int), Image.ANTIALIAS)\n    \n    if shape is not None:\n        image = image.resize(shape, Image.LANCZOS)\n    \n    if transform is not None:\n        image = transform(image).unsqueeze(0)\n    \n    return image.type(dtype)\n\n\nclass VGGNet(nn.Module):\n    def __init__(self):\n        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n        super(VGGNet, self).__init__()\n        self.select = ['0', '5', '10', '19', '28'] \n        self.vgg = models.vgg19(pretrained=True).features\n        \n    def forward(self, x):\n        \"\"\"Extract 5 conv activation maps from an input image.\n        \n        Args:\n            x: 4D tensor of shape (1, 3, height, width).\n        \n        Returns:\n            features: a list containing 5 conv activation maps.\n        \"\"\"\n        features = []\n        for name, layer in self.vgg._modules.items():\n            x = layer(x)\n            if name in self.select:\n                features.append(x)\n        return features\n\n\ndef main(config):\n    \n    \n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    \n    # make content.size() == style.size() \n    content = load_image(config.content, transform, max_size=config.max_size)\n    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n    \n    \n    target = Variable(content.clone(), requires_grad=True)\n    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n    \n    vgg = VGGNet()\n    if use_cuda:\n        vgg.cuda()\n    \n    for step in range(config.total_step):\n        \n        # Extract multiple(5) conv feature vectors\n        target_features = vgg(target)\n        content_features = vgg(Variable(content))\n        style_features = vgg(Variable(style))\n\n        style_loss = 0\n        content_loss = 0\n        for f1, f2, f3 in zip(target_features, content_features, style_features):\n            # Compute content loss (target and content image)\n            content_loss += torch.mean((f1 - f2)**2)\n\n            \n            _, c, h, w = f1.size()\n            f1 = f1.view(c, h * w)\n            f3 = f3.view(c, h * w)\n\n            \n            f1 = torch.mm(f1, f1.t())\n            f3 = torch.mm(f3, f3.t())\n\n            # Compute style loss (target and style image)\n            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n\n        \n        loss = content_loss + config.style_weight * style_loss \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (step+1) % config.log_step == 0:\n            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f' \n                   %(step+1, config.total_step, content_loss.data[0], style_loss.data[0]))\n    \n        if (step+1) % config.sample_step == 0:\n            \n            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n            img = target.clone().cpu().squeeze()\n            img = denorm(img.data).clamp_(0, 1)\n            torchvision.utils.save_image(img, 'output-%d.png' %(step+1))\n\n        \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--content', type=str, default='./png/content.png')\n    parser.add_argument('--style', type=str, default='./png/style.png')\n    parser.add_argument('--max_size', type=int, default=400)\n    parser.add_argument('--total_step', type=int, default=5000)\n    parser.add_argument('--log_step', type=int, default=10)\n    parser.add_argument('--sample_step', type=int, default=1000)\n    parser.add_argument('--style_weight', type=float, default=100)\n    parser.add_argument('--lr', type=float, default=0.003)\n    config = parser.parse_args()\n    print(config)\n    main(config)", "comments": "   select conv1 1   conv5 1 activation maps             super(vggnet  self)   init  ()         self select     0    5    10    19    28            self vgg   models vgg19(pretrained true) features              def forward(self  x)             extract 5 conv activation maps input image                   args              x  4d tensor shape (1  3  height  width)                   returns              features  list containing 5 conv activation maps                 load image file convert variable    unsqueeze make 4d tensor perform conv arithmetic    pretrained vggnet     image preprocessing    for normalization  see https   github com pytorch vision models    load content style images    make content size()    style size()     initialization optimizer    extract multiple(5) conv feature vectors    compute content loss (target content image)    reshape conv features    compute gram matrix      compute style loss (target style image)    compute total loss  backprop optimize    save generated image ", "content": "from __future__ import division\nfrom torch.backends import cudnn\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\nimport argparse\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\n\n\nuse_cuda = torch.cuda.is_available()\ndtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n\n# Load image file and convert it into variable\n# unsqueeze for make the 4D tensor to perform conv arithmetic\ndef load_image(image_path, transform=None, max_size=None, shape=None):\n    image = Image.open(image_path)\n    \n    if max_size is not None:\n        scale = max_size / max(image.size)\n        size = np.array(image.size) * scale\n        image = image.resize(size.astype(int), Image.ANTIALIAS)\n    \n    if shape is not None:\n        image = image.resize(shape, Image.LANCZOS)\n    \n    if transform is not None:\n        image = transform(image).unsqueeze(0)\n    \n    return image.type(dtype)\n\n# Pretrained VGGNet \nclass VGGNet(nn.Module):\n    def __init__(self):\n        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n        super(VGGNet, self).__init__()\n        self.select = ['0', '5', '10', '19', '28'] \n        self.vgg = models.vgg19(pretrained=True).features\n        \n    def forward(self, x):\n        \"\"\"Extract 5 conv activation maps from an input image.\n        \n        Args:\n            x: 4D tensor of shape (1, 3, height, width).\n        \n        Returns:\n            features: a list containing 5 conv activation maps.\n        \"\"\"\n        features = []\n        for name, layer in self.vgg._modules.items():\n            x = layer(x)\n            if name in self.select:\n                features.append(x)\n        return features\n\n\ndef main(config):\n    \n    # Image preprocessing\n    # For normalization, see https://github.com/pytorch/vision#models\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load content and style images\n    # make content.size() == style.size() \n    content = load_image(config.content, transform, max_size=config.max_size)\n    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n    \n    # Initialization and optimizer\n    target = Variable(content.clone(), requires_grad=True)\n    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n    \n    vgg = VGGNet()\n    if use_cuda:\n        vgg.cuda()\n    \n    for step in range(config.total_step):\n        \n        # Extract multiple(5) conv feature vectors\n        target_features = vgg(target)\n        content_features = vgg(Variable(content))\n        style_features = vgg(Variable(style))\n\n        style_loss = 0\n        content_loss = 0\n        for f1, f2, f3 in zip(target_features, content_features, style_features):\n            # Compute content loss (target and content image)\n            content_loss += torch.mean((f1 - f2)**2)\n\n            # Reshape conv features\n            _, c, h, w = f1.size()\n            f1 = f1.view(c, h * w)\n            f3 = f3.view(c, h * w)\n\n            # Compute gram matrix  \n            f1 = torch.mm(f1, f1.t())\n            f3 = torch.mm(f3, f3.t())\n\n            # Compute style loss (target and style image)\n            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n\n        # Compute total loss, backprop and optimize\n        loss = content_loss + config.style_weight * style_loss \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (step+1) % config.log_step == 0:\n            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f' \n                   %(step+1, config.total_step, content_loss.data[0], style_loss.data[0]))\n    \n        if (step+1) % config.sample_step == 0:\n            # Save the generated image\n            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n            img = target.clone().cpu().squeeze()\n            img = denorm(img.data).clamp_(0, 1)\n            torchvision.utils.save_image(img, 'output-%d.png' %(step+1))\n\n        \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--content', type=str, default='./png/content.png')\n    parser.add_argument('--style', type=str, default='./png/style.png')\n    parser.add_argument('--max_size', type=int, default=400)\n    parser.add_argument('--total_step', type=int, default=5000)\n    parser.add_argument('--log_step', type=int, default=10)\n    parser.add_argument('--sample_step', type=int, default=1000)\n    parser.add_argument('--style_weight', type=float, default=100)\n    parser.add_argument('--lr', type=float, default=0.003)\n    config = parser.parse_args()\n    print(config)\n    main(config)", "description": "PyTorch Tutorial for Deep Learning Researchers", "file_name": "main.py", "id": "7015e9db9b639c05f3ed31f2ccf820c0", "language": "Python", "project_name": "pytorch-tutorial", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/yunjey-pytorch-tutorial/yunjey-pytorch-tutorial-6c785eb/tutorials/03-advanced/neural_style_transfer/main.py", "save_time": "", "source": "", "update_at": "2018-03-18T14:24:45Z", "url": "https://github.com/yunjey/pytorch-tutorial", "wiki": true}