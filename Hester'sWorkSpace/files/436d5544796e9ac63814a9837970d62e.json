{"author": "chiphuyen", "code": "\"\"\" Examples to demonstrate variable sharing\nCS 20: 'TensorFlow for Deep Learning Research'\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 05\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport tensorflow as tf\n\nx1 = tf.truncated_normal([200, 100], name='x1')\nx2 = tf.truncated_normal([200, 100], name='x2')\n\ndef two_hidden_layers(x):\n    assert x.shape.as_list() == [200, 100]\n    w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights')\n    b1 = tf.Variable(tf.zeros([50]), name='h1_biases')\n    h1 = tf.matmul(x, w1) + b1\n    assert h1.shape.as_list() == [200, 50]  \n    w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights')\n    b2 = tf.Variable(tf.zeros([10]), name='2_biases')\n    logits = tf.matmul(h1, w2) + b2\n    return logits\n\ndef two_hidden_layers_2(x):\n    assert x.shape.as_list() == [200, 100]\n    w1 = tf.get_variable('h1_weights', [100, 50], initializer=tf.random_normal_initializer())\n    b1 = tf.get_variable('h1_biases', [50], initializer=tf.constant_initializer(0.0))\n    h1 = tf.matmul(x, w1) + b1\n    assert h1.shape.as_list() == [200, 50]  \n    w2 = tf.get_variable('h2_weights', [50, 10], initializer=tf.random_normal_initializer())\n    b2 = tf.get_variable('h2_biases', [10], initializer=tf.constant_initializer(0.0))\n    logits = tf.matmul(h1, w2) + b2\n    return logits\n\n# logits1 = two_hidden_layers(x1)\n# logits2 = two_hidden_layers(x2)\n\n# logits1 = two_hidden_layers_2(x1)\n# logits2 = two_hidden_layers_2(x2)\n\n# with tf.variable_scope('two_layers') as scope:\n#     logits1 = two_hidden_layers_2(x1)\n()\n#     logits2 = two_hidden_layers_2(x2)\n\n# with tf.variable_scope('two_layers') as scope:\n#     logits1 = two_hidden_layers_2(x1)\n()\n#     logits2 = two_hidden_layers_2(x2)\n\ndef fully_connected(x, output_dim, scope):\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n        w = tf.get_variable('weights', [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n        return tf.matmul(x, w) + b\n\ndef two_hidden_layers(x):\n    h1 = fully_connected(x, 50, 'h1')\n    h2 = fully_connected(h1, 10, 'h2')\n\nwith tf.variable_scope('two_layers') as scope:\n    logits1 = two_hidden_layers(x1)\n    ()\n    logits2 = two_hidden_layers(x2)\n\nwriter = tf.summary.FileWriter('./graphs/cool_variables', tf.get_default_graph())\nwriter.close()", "comments": "    examples demonstrate variable sharing cs 20   tensorflow deep learning research  cs20 stanford edu chip huyen (chiphuyen cs stanford edu) lecture 05        logits1   two hidden layers(x1)    logits2   two hidden layers(x2)    logits1   two hidden layers 2(x1)    logits2   two hidden layers 2(x2)    tf variable scope( two layers ) scope         logits1   two hidden layers 2(x1)        scope reuse variables()        logits2   two hidden layers 2(x2)    tf variable scope( two layers ) scope         logits1   two hidden layers 2(x1)        scope reuse variables()        logits2   two hidden layers 2(x2)    scope reuse variables() ", "content": "\"\"\" Examples to demonstrate variable sharing\nCS 20: 'TensorFlow for Deep Learning Research'\ncs20.stanford.edu\nChip Huyen (chiphuyen@cs.stanford.edu)\nLecture 05\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport tensorflow as tf\n\nx1 = tf.truncated_normal([200, 100], name='x1')\nx2 = tf.truncated_normal([200, 100], name='x2')\n\ndef two_hidden_layers(x):\n    assert x.shape.as_list() == [200, 100]\n    w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights')\n    b1 = tf.Variable(tf.zeros([50]), name='h1_biases')\n    h1 = tf.matmul(x, w1) + b1\n    assert h1.shape.as_list() == [200, 50]  \n    w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights')\n    b2 = tf.Variable(tf.zeros([10]), name='2_biases')\n    logits = tf.matmul(h1, w2) + b2\n    return logits\n\ndef two_hidden_layers_2(x):\n    assert x.shape.as_list() == [200, 100]\n    w1 = tf.get_variable('h1_weights', [100, 50], initializer=tf.random_normal_initializer())\n    b1 = tf.get_variable('h1_biases', [50], initializer=tf.constant_initializer(0.0))\n    h1 = tf.matmul(x, w1) + b1\n    assert h1.shape.as_list() == [200, 50]  \n    w2 = tf.get_variable('h2_weights', [50, 10], initializer=tf.random_normal_initializer())\n    b2 = tf.get_variable('h2_biases', [10], initializer=tf.constant_initializer(0.0))\n    logits = tf.matmul(h1, w2) + b2\n    return logits\n\n# logits1 = two_hidden_layers(x1)\n# logits2 = two_hidden_layers(x2)\n\n# logits1 = two_hidden_layers_2(x1)\n# logits2 = two_hidden_layers_2(x2)\n\n# with tf.variable_scope('two_layers') as scope:\n#     logits1 = two_hidden_layers_2(x1)\n#     scope.reuse_variables()\n#     logits2 = two_hidden_layers_2(x2)\n\n# with tf.variable_scope('two_layers') as scope:\n#     logits1 = two_hidden_layers_2(x1)\n#     scope.reuse_variables()\n#     logits2 = two_hidden_layers_2(x2)\n\ndef fully_connected(x, output_dim, scope):\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n        w = tf.get_variable('weights', [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n        return tf.matmul(x, w) + b\n\ndef two_hidden_layers(x):\n    h1 = fully_connected(x, 50, 'h1')\n    h2 = fully_connected(h1, 10, 'h2')\n\nwith tf.variable_scope('two_layers') as scope:\n    logits1 = two_hidden_layers(x1)\n    # scope.reuse_variables()\n    logits2 = two_hidden_layers(x2)\n\nwriter = tf.summary.FileWriter('./graphs/cool_variables', tf.get_default_graph())\nwriter.close()", "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ", "file_name": "05_variable_sharing.py", "id": "436d5544796e9ac63814a9837970d62e", "language": "Python", "project_name": "stanford-tensorflow-tutorials", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/chiphuyen-stanford-tensorflow-tutorials/chiphuyen-stanford-tensorflow-tutorials-54c48f5/examples/05_variable_sharing.py", "save_time": "", "source": "", "update_at": "2018-03-18T15:38:24Z", "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials", "wiki": true}