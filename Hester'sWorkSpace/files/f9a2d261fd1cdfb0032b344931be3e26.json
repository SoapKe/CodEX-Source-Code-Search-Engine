{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n\"\"\"Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 35x35 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n    with tf.variable_scope('Branch_2'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 17x17 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope='Conv2d_0b_1x7')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope='Conv2d_0c_7x1')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 8x8 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope='Conv2d_0b_1x3')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope='Conv2d_0c_3x1')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint='Conv2d_7b_1x1',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after 'PreAuxLogits'.\n  \"\"\"\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError('output_stride must be 8 or 16.')\n\n  padding = 'SAME' if align_feature_maps else 'VALID'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding='SAME'):\n       149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope='Conv2d_1a_3x3')\n      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n\n       147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope='Conv2d_2a_3x3')\n      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n       147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n       73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_3a_3x3')\n      if add_and_check_final('MaxPool_3a_3x3', net): return net, end_points\n       73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope='Conv2d_3b_1x1')\n      if add_and_check_final('Conv2d_3b_1x1', net): return net, end_points\n       71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope='Conv2d_4a_3x3')\n      if add_and_check_final('Conv2d_4a_3x3', net): return net, end_points\n       35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_5a_3x3')\n      if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points\n\n       35 x 35 x 320\n      with tf.variable_scope('Mixed_5b'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope='Conv2d_0b_5x5')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope='Conv2d_0c_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n                                       scope='AvgPool_0a_3x3')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope='Conv2d_0b_1x1')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final('Mixed_5b', net): return net, end_points\n       TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n       17 x 17 x 1088 if output_stride == 8,\n       33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope('Mixed_6a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_6a', net): return net, end_points\n\n       TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final('PreAuxLogits', net): return net, end_points\n\n      if output_stride == 8:\n         TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError('output_stride==8 is only supported up to the '\n                         'PreAuxlogits end_point for now.')\n\n       8 x 8 x 2080\n      with tf.variable_scope('Mixed_7a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_7a', net): return net, end_points\n\n       TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n       8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n      if add_and_check_final('Conv2d_7b_1x1', net): return net, end_points\n\n    raise ValueError('final_endpoint (%s) not recognized', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope='InceptionResnetV2',\n                        create_aux_logits=True):\n  \"\"\"Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse 'scope' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  \"\"\"\n  end_points = {}\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope('AuxLogits'):\n          aux = end_points['PreAuxLogits']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',\n                                scope='Conv2d_1a_3x3')\n          aux = slim.conv2d(aux, 128, 1, scope='Conv2d_1b_1x1')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding='VALID', scope='Conv2d_2a_5x5')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope='Logits')\n          end_points['AuxLogits'] = aux\n\n      with tf.variable_scope('Logits'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n                              scope='AvgPool_1a_8x8')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='Dropout')\n\n        end_points['PreLogitsFlatten'] = net\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope='Logits')\n        end_points['Logits'] = logits\n        end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  \"\"\"Returns the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  \"\"\"\n   Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        'decay': batch_norm_decay,\n        'epsilon': batch_norm_epsilon,\n    }\n     Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n", "comments": "   contains definition inception resnet v2 architecture   as described http   arxiv org abs 1602 07261     inception v4  inception resnet impact residual connections     learning   christian szegedy  sergey ioffe  vincent vanhoucke  alex alemi       future   import absolute import   future   import division   future   import print function   import tensorflow tf  slim   tf contrib slim   def block35(net  scale 1 0  activation fn tf nn relu  scope none  reuse none)       builds 35x35 resnet block       tf variable scope(scope   block35    net   reuse reuse)      tf variable scope( branch 0 )        tower conv   slim conv2d(net  32  1  scope  conv2d 1x1 )     tf variable scope( branch 1 )        tower conv1 0   slim conv2d(net  32  1  scope  conv2d 0a 1x1 )       tower conv1 1   slim conv2d(tower conv1 0  32  3  scope  conv2d 0b 3x3 )     tf variable scope( branch 2 )        tower conv2 0   slim conv2d(net  32  1  scope  conv2d 0a 1x1 )       tower conv2 1   slim conv2d(tower conv2 0  48  3  scope  conv2d 0b 3x3 )       tower conv2 2   slim conv2d(tower conv2 1  64  3  scope  conv2d 0c 3x3 )     mixed   tf concat(axis 3  values  tower conv  tower conv1 1  tower conv2 2 )       slim conv2d(mixed  net get shape() 3   1  normalizer fn none                       activation fn none  scope  conv2d 1x1 )     net    scale       activation fn        net   activation fn(net)   return net   def block17(net  scale 1 0  activation fn tf nn relu  scope none  reuse none)       builds 17x17 resnet block       tf variable scope(scope   block17    net   reuse reuse)      tf variable scope( branch 0 )        tower conv   slim conv2d(net  192  1  scope  conv2d 1x1 )     tf variable scope( branch 1 )        tower conv1 0   slim conv2d(net  128  1  scope  conv2d 0a 1x1 )       tower conv1 1   slim conv2d(tower conv1 0  160   1  7                                     scope  conv2d 0b 1x7 )       tower conv1 2   slim conv2d(tower conv1 1  192   7  1                                     scope  conv2d 0c 7x1 )     mixed   tf concat(axis 3  values  tower conv  tower conv1 2 )       slim conv2d(mixed  net get shape() 3   1  normalizer fn none                       activation fn none  scope  conv2d 1x1 )     net    scale       activation fn        net   activation fn(net)   return net   def block8(net  scale 1 0  activation fn tf nn relu  scope none  reuse none)       builds 8x8 resnet block       tf variable scope(scope   block8    net   reuse reuse)      tf variable scope( branch 0 )        tower conv   slim conv2d(net  192  1  scope  conv2d 1x1 )     tf variable scope( branch 1 )        tower conv1 0   slim conv2d(net  192  1  scope  conv2d 0a 1x1 )       tower conv1 1   slim conv2d(tower conv1 0  224   1  3                                     scope  conv2d 0b 1x3 )       tower conv1 2   slim conv2d(tower conv1 1  256   3  1                                     scope  conv2d 0c 3x1 )     mixed   tf concat(axis 3  values  tower conv  tower conv1 2 )       slim conv2d(mixed  net get shape() 3   1  normalizer fn none                       activation fn none  scope  conv2d 1x1 )     net    scale       activation fn        net   activation fn(net)   return net   def inception resnet v2 base(inputs                               final endpoint  conv2d 7b 1x1                                output stride 16                               align feature maps false                               scope none)       inception model  http   arxiv org abs 1602 07261     constructs inception resnet v2 network inputs given final   endpoint  this method construct network final inception   block conv2d 7b 1x1     args      inputs  tensor size  batch size  height  width  channels       final endpoint  specifies endpoint construct network  it       one   conv2d 1a 3x3    conv2d 2a 3x3    conv2d 2b 3x3          maxpool 3a 3x3    conv2d 3b 1x1    conv2d 4a 3x3    maxpool 5a 3x3          mixed 5b    mixed 6a    preauxlogits    mixed 7a    conv2d 7b 1x1       output stride  a scalar specifies requested ratio input       output spatial resolution  only supports 8 16      align feature maps  when true  changes valid paddings network       same padding feature maps aligned      scope  optional variable scope     returns      tensor  output tensor corresponding final endpoint      end points  set activations external use  example summaries                 losses     raises      valueerror  final endpoint set one predefined values        output stride 8 16  output stride 8       request end point  preauxlogits           output stride    8 output stride    16      raise valueerror( output stride must 8 16  )    padding    same  align feature maps else  valid     end points         def add check final(name  net)      end points name    net     return name    final endpoint    tf variable scope(scope   inceptionresnetv2    inputs )      slim arg scope( slim conv2d  slim max pool2d  slim avg pool2d                           stride 1  padding  same )          149 x 149 x 32       net   slim conv2d(inputs  32  3  stride 2  padding padding                          scope  conv2d 1a 3x3 )       add check final( conv2d 1a 3x3   net)  return net  end points          147 x 147 x 32       net   slim conv2d(net  32  3  padding padding                          scope  conv2d 2a 3x3 )       add check final( conv2d 2a 3x3   net)  return net  end points         147 x 147 x 64       net   slim conv2d(net  64  3  scope  conv2d 2b 3x3 )       add check final( conv2d 2b 3x3   net)  return net  end points         73 x 73 x 64       net   slim max pool2d(net  3  stride 2  padding padding                              scope  maxpool 3a 3x3 )       add check final( maxpool 3a 3x3   net)  return net  end points         73 x 73 x 80       net   slim conv2d(net  80  1  padding padding                          scope  conv2d 3b 1x1 )       add check final( conv2d 3b 1x1   net)  return net  end points         71 x 71 x 192       net   slim conv2d(net  192  3  padding padding                          scope  conv2d 4a 3x3 )       add check final( conv2d 4a 3x3   net)  return net  end points         35 x 35 x 192       net   slim max pool2d(net  3  stride 2  padding padding                              scope  maxpool 5a 3x3 )       add check final( maxpool 5a 3x3   net)  return net  end points          35 x 35 x 320       tf variable scope( mixed 5b )          tf variable scope( branch 0 )            tower conv   slim conv2d(net  96  1  scope  conv2d 1x1 )         tf variable scope( branch 1 )            tower conv1 0   slim conv2d(net  48  1  scope  conv2d 0a 1x1 )           tower conv1 1   slim conv2d(tower conv1 0  64  5                                        scope  conv2d 0b 5x5 )         tf variable scope( branch 2 )            tower conv2 0   slim conv2d(net  64  1  scope  conv2d 0a 1x1 )           tower conv2 1   slim conv2d(tower conv2 0  96  3                                        scope  conv2d 0b 3x3 )           tower conv2 2   slim conv2d(tower conv2 1  96  3                                        scope  conv2d 0c 3x3 )         tf variable scope( branch 3 )            tower pool   slim avg pool2d(net  3  stride 1  padding  same                                          scope  avgpool 0a 3x3 )           tower pool 1   slim conv2d(tower pool  64  1                                       scope  conv2d 0b 1x1 )         net   tf concat(              tower conv  tower conv1 1  tower conv2 2  tower pool 1   3)        add check final( mixed 5b   net)  return net  end points         todo(alemi)  register intermediate endpoints       net   slim repeat(net  10  block35  scale 0 17)          17 x 17 x 1088 output stride    8          33 x 33 x 1088 output stride    16       use atrous   output stride    8        tf variable scope( mixed 6a )          tf variable scope( branch 0 )            tower conv   slim conv2d(net  384  3  stride 1 use atrous else 2                                     padding padding                                     scope  conv2d 1a 3x3 )         tf variable scope( branch 1 )            tower conv1 0   slim conv2d(net  256  1  scope  conv2d 0a 1x1 )           tower conv1 1   slim conv2d(tower conv1 0  256  3                                        scope  conv2d 0b 3x3 )           tower conv1 2   slim conv2d(tower conv1 1  384  3                                        stride 1 use atrous else 2                                        padding padding                                        scope  conv2d 1a 3x3 )         tf variable scope( branch 2 )            tower pool   slim max pool2d(net  3  stride 1 use atrous else 2                                         padding padding                                         scope  maxpool 1a 3x3 )         net   tf concat( tower conv  tower conv1 2  tower pool   3)        add check final( mixed 6a   net)  return net  end points          todo(alemi)  register intermediate endpoints       slim arg scope( slim conv2d   rate 2 use atrous else 1)          net   slim repeat(net  20  block17  scale 0 10)       add check final( preauxlogits   net)  return net  end points        output stride    8            todo(gpapan)  properly support output stride rest net          raise valueerror( output stride  8 supported                             preauxlogits end point  )          8 x 8 x 2080       tf variable scope( mixed 7a )          tf variable scope( branch 0 )            tower conv   slim conv2d(net  256  1  scope  conv2d 0a 1x1 )           tower conv 1   slim conv2d(tower conv  384  3  stride 2                                       padding padding                                       scope  conv2d 1a 3x3 )         tf variable scope( branch 1 )            tower conv1   slim conv2d(net  256  1  scope  conv2d 0a 1x1 )           tower conv1 1   slim conv2d(tower conv1  288  3  stride 2                                        padding padding                                        scope  conv2d 1a 3x3 )         tf variable scope( branch 2 )            tower conv2   slim conv2d(net  256  1  scope  conv2d 0a 1x1 )           tower conv2 1   slim conv2d(tower conv2  288  3                                        scope  conv2d 0b 3x3 )           tower conv2 2   slim conv2d(tower conv2 1  320  3  stride 2                                        padding padding                                        scope  conv2d 1a 3x3 )         tf variable scope( branch 3 )            tower pool   slim max pool2d(net  3  stride 2                                         padding padding                                         scope  maxpool 1a 3x3 )         net   tf concat(              tower conv 1  tower conv1 1  tower conv2 2  tower pool   3)        add check final( mixed 7a   net)  return net  end points          todo(alemi)  register intermediate endpoints       net   slim repeat(net  9  block8  scale 0 20)       net   block8(net  activation fn none)          8 x 8 x 1536       net   slim conv2d(net  1536  1  scope  conv2d 7b 1x1 )       add check final( conv2d 7b 1x1   net)  return net  end points      raise valueerror( final endpoint ( s) recognized   final endpoint)   def inception resnet v2(inputs  num classes 1001  training true                          dropout keep prob 0 8                          reuse none                          scope  inceptionresnetv2                           create aux logits true)       creates inception resnet v2 model     args      inputs  4 d tensor size  batch size  height  width  3       num classes  number predicted classes      training  whether training      dropout keep prob  float  fraction keep final layer      reuse  whether network variables reused  to       able reuse  scope  must given      scope  optional variable scope      create aux logits  whether include auxilliary logits     returns      logits  logits outputs model      end points  set end points inception model          end points         tf variable scope(scope   inceptionresnetv2    inputs  num classes                            reuse reuse) scope      slim arg scope( slim batch norm  slim dropout                           training training)         net  end points   inception resnet v2 base(inputs  scope scope)        create aux logits          tf variable scope( auxlogits )            aux   end points  preauxlogits             aux   slim avg pool2d(aux  5  stride 3  padding  valid                                   scope  conv2d 1a 3x3 )           aux   slim conv2d(aux  128  1  scope  conv2d 1b 1x1 )           aux   slim conv2d(aux  768  aux get shape() 1 3                               padding  valid   scope  conv2d 2a 5x5 )           aux   slim flatten(aux)           aux   slim fully connected(aux  num classes  activation fn none                                       scope  logits )           end points  auxlogits     aux        tf variable scope( logits )          net   slim avg pool2d(net  net get shape() 1 3   padding  valid                                 scope  avgpool 1a 8x8 )         net   slim flatten(net)          net   slim dropout(net  dropout keep prob  training training                             scope  dropout )          end points  prelogitsflatten     net         logits   slim fully connected(net  num classes  activation fn none                                        scope  logits )         end points  logits     logits         end points  predictions     tf nn softmax(logits  name  predictions )      return logits  end points inception resnet v2 default image size   299   def inception resnet v2 arg scope(weight decay 0 00004                                    batch norm decay 0 9997                                    batch norm epsilon 0 001)       returns scope default parameters inception resnet v2     args      weight decay  weight decay weights variables      batch norm decay  decay moving average batch norm momentums      batch norm epsilon  small float added variance avoid dividing zero     returns      arg scope parameters needed inception resnet v2           copyright 2017 the tensorflow authors all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       149 x 149 x 32    147 x 147 x 32    147 x 147 x 64    73 x 73 x 64    73 x 73 x 80    71 x 71 x 192    35 x 35 x 192    35 x 35 x 320    todo(alemi)  register intermediate endpoints    17 x 17 x 1088 output stride    8     33 x 33 x 1088 output stride    16    todo(alemi)  register intermediate endpoints    todo(gpapan)  properly support output stride rest net     8 x 8 x 2080    todo(alemi)  register intermediate endpoints    8 x 8 x 1536    set weight decay weights conv2d fully connected layers     set activation fn parameters batch norm  ", "content": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 35x35 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n    with tf.variable_scope('Branch_2'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 17x17 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope='Conv2d_0b_1x7')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope='Conv2d_0c_7x1')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  \"\"\"Builds the 8x8 resnet block.\"\"\"\n  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope='Conv2d_0b_1x3')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope='Conv2d_0c_3x1')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint='Conv2d_7b_1x1',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after 'PreAuxLogits'.\n  \"\"\"\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError('output_stride must be 8 or 16.')\n\n  padding = 'SAME' if align_feature_maps else 'VALID'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding='SAME'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope='Conv2d_1a_3x3')\n      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope='Conv2d_2a_3x3')\n      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_3a_3x3')\n      if add_and_check_final('MaxPool_3a_3x3', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope='Conv2d_3b_1x1')\n      if add_and_check_final('Conv2d_3b_1x1', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope='Conv2d_4a_3x3')\n      if add_and_check_final('Conv2d_4a_3x3', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_5a_3x3')\n      if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope('Mixed_5b'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope='Conv2d_0b_5x5')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope='Conv2d_0c_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n                                       scope='AvgPool_0a_3x3')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope='Conv2d_0b_1x1')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final('Mixed_5b', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope('Mixed_6a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_6a', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final('PreAuxLogits', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError('output_stride==8 is only supported up to the '\n                         'PreAuxlogits end_point for now.')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope('Mixed_7a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_7a', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n      if add_and_check_final('Conv2d_7b_1x1', net): return net, end_points\n\n    raise ValueError('final_endpoint (%s) not recognized', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope='InceptionResnetV2',\n                        create_aux_logits=True):\n  \"\"\"Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse 'scope' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  \"\"\"\n  end_points = {}\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope('AuxLogits'):\n          aux = end_points['PreAuxLogits']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',\n                                scope='Conv2d_1a_3x3')\n          aux = slim.conv2d(aux, 128, 1, scope='Conv2d_1b_1x1')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding='VALID', scope='Conv2d_2a_5x5')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope='Logits')\n          end_points['AuxLogits'] = aux\n\n      with tf.variable_scope('Logits'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n                              scope='AvgPool_1a_8x8')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='Dropout')\n\n        end_points['PreLogitsFlatten'] = net\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope='Logits')\n        end_points['Logits'] = logits\n        end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  \"\"\"Returns the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  \"\"\"\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        'decay': batch_norm_decay,\n        'epsilon': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n", "description": "Models and examples built with TensorFlow", "file_name": "inception_resnet_v2.py", "id": "f9a2d261fd1cdfb0032b344931be3e26", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/research/adv_imagenet_models/inception_resnet_v2.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}