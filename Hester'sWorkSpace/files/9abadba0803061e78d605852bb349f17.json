{"author": "openai", "code": "import numpy as np\n\nfrom gym.envs.robotics import rotations, robot_env, utils\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass FetchEnv(robot_env.RobotEnv):\n    \"\"\"Superclass for all Fetch environments.\n    \"\"\"\n\n    def __init__(\n        self, model_path, n_substeps, gripper_extra_height, block_gripper,\n        has_object, target_in_the_air, target_offset, obj_range, target_range,\n        distance_threshold, initial_qpos, reward_type,\n    ):\n        \"\"\"Initializes a new Fetch environment.\n\n        Args:\n            model_path (string): path to the environments XML file\n            n_substeps (int): number of substeps the simulation runs on every call to step\n            gripper_extra_height (float): additional height above the table when positioning the gripper\n            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not\n            has_object (boolean): whether or not the environment has an object\n            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface\n            target_offset (float or array with 3 elements): offset of the target\n            obj_range (float): range of a uniform distribution for sampling initial object positions\n            target_range (float): range of a uniform distribution for sampling a target\n            distance_threshold (float): the threshold after which a goal is considered achieved\n            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration\n            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense\n        \"\"\"\n        self.gripper_extra_height = gripper_extra_height\n        self.block_gripper = block_gripper\n        self.has_object = has_object\n        self.target_in_the_air = target_in_the_air\n        self.target_offset = target_offset\n        self.obj_range = obj_range\n        self.target_range = target_range\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        super(FetchEnv, self).__init__(\n            model_path=model_path, n_substeps=n_substeps, n_actions=4,\n            initial_qpos=initial_qpos)\n\n    \n    \n\n    def compute_reward(self, achieved_goal, goal, info):\n        \n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    \n    \n\n    def _step_callback(self):\n        if self.block_gripper:\n            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)\n            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)\n            self.sim.forward()\n\n    def _set_action(self, action):\n        assert action.shape == (4,)\n        action = action.copy()  \n        pos_ctrl, gripper_ctrl = action[:3], action[3]\n\n        pos_ctrl *= 0.05  \n        rot_ctrl = [1., 0., 1., 0.]  \n        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])\n        assert gripper_ctrl.shape == (2,)\n        if self.block_gripper:\n            gripper_ctrl = np.zeros_like(gripper_ctrl)\n        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])\n\n        \n        utils.ctrl_set_action(self.sim, action)\n        utils.mocap_set_action(self.sim, action)\n\n    def _get_obs(self):\n        \n        grip_pos = self.sim.data.get_site_xpos('robot0:grip')\n        dt = self.sim.nsubsteps * self.sim.model.opt.timestep\n        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt\n        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)\n        if self.has_object:\n            object_pos = self.sim.data.get_site_xpos('object0')\n            \n            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))\n            \n            object_velp = self.sim.data.get_site_xvelp('object0') * dt\n            object_velr = self.sim.data.get_site_xvelr('object0') * dt\n            \n            object_rel_pos = object_pos - grip_pos\n            object_velp -= grip_velp\n        else:\n            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)\n        gripper_state = robot_qpos[-2:]\n        gripper_vel = robot_qvel[-2:] * dt  \n\n        if not self.has_object:\n            achieved_goal = grip_pos.copy()\n        else:\n            achieved_goal = np.squeeze(object_pos.copy())\n        obs = np.concatenate([\n            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),\n            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,\n        ])\n\n        return {\n            'observation': obs.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _viewer_setup(self):\n        body_id = self.sim.model.body_name2id('robot0:gripper_link')\n        lookat = self.sim.data.body_xpos[body_id]\n        for idx, value in enumerate(lookat):\n            self.viewer.cam.lookat[idx] = value\n        self.viewer.cam.distance = 2.5\n        self.viewer.cam.azimuth = 132.\n        self.viewer.cam.elevation = -14.\n\n    def _render_callback(self):\n        \n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        site_id = self.sim.model.site_name2id('target0')\n        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]\n        self.sim.forward()\n\n    def _reset_sim(self):\n        self.sim.set_state(self.initial_state)\n\n        \n        if self.has_object:\n            object_xpos = self.initial_gripper_xpos[:2]\n            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:\n                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)\n            object_qpos = self.sim.data.get_joint_qpos('object0:joint')\n            assert object_qpos.shape == (7,)\n            object_qpos[:2] = object_xpos\n            self.sim.data.set_joint_qpos('object0:joint', object_qpos)\n\n        self.sim.forward()\n        return True\n\n    def _sample_goal(self):\n        if self.has_object:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)\n            goal += self.target_offset\n            goal[2] = self.height_offset\n            if self.target_in_the_air and self.np_random.uniform() < 0.5:\n                goal[2] += self.np_random.uniform(0, 0.45)\n        else:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-0.15, 0.15, size=3)\n        return goal.copy()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        utils.reset_mocap_welds(self.sim)\n        self.sim.forward()\n\n        \n        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')\n        gripper_rotation = np.array([1., 0., 1., 0.])\n        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)\n        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)\n        for _ in range(10):\n            self.sim.step()\n\n        \n        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()\n        if self.has_object:\n            self.height_offset = self.sim.data.get_site_xpos('object0')[2]\n", "comments": "   superclass fetch environments               def   init  (         self  model path  n substeps  gripper extra height  block gripper          object  target air  target offset  obj range  target range          distance threshold  initial qpos  reward type      )             initializes new fetch environment           args              model path (string)  path environments xml file             n substeps (int)  number substeps simulation runs every call step             gripper extra height (float)  additional height table positioning gripper             block gripper (boolean)  whether gripper blocked (i e  movable)             object (boolean)  whether environment object             target air (boolean)  whether target air table table surface             target offset (float array 3 elements)  offset target             obj range (float)  range uniform distribution sampling initial object positions             target range (float)  range uniform distribution sampling target             distance threshold (float)  threshold goal considered achieved             initial qpos (dict)  dictionary joint names values define initial configuration             reward type ( sparse   dense )  reward type  e  sparse dense                goalenv methods                                    compute distance goal achieved goal     robotenv methods                                    ensure change action outside scope    limit maximum change position    fixed rotation end effector  expressed quaternion    apply action simulation     positions    rotations    velocities    gripper state    change scalar gripper made symmetric    visualize target     randomize start position object     move end effector position     extract information sampling goals  ", "content": "import numpy as np\n\nfrom gym.envs.robotics import rotations, robot_env, utils\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass FetchEnv(robot_env.RobotEnv):\n    \"\"\"Superclass for all Fetch environments.\n    \"\"\"\n\n    def __init__(\n        self, model_path, n_substeps, gripper_extra_height, block_gripper,\n        has_object, target_in_the_air, target_offset, obj_range, target_range,\n        distance_threshold, initial_qpos, reward_type,\n    ):\n        \"\"\"Initializes a new Fetch environment.\n\n        Args:\n            model_path (string): path to the environments XML file\n            n_substeps (int): number of substeps the simulation runs on every call to step\n            gripper_extra_height (float): additional height above the table when positioning the gripper\n            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not\n            has_object (boolean): whether or not the environment has an object\n            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface\n            target_offset (float or array with 3 elements): offset of the target\n            obj_range (float): range of a uniform distribution for sampling initial object positions\n            target_range (float): range of a uniform distribution for sampling a target\n            distance_threshold (float): the threshold after which a goal is considered achieved\n            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration\n            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense\n        \"\"\"\n        self.gripper_extra_height = gripper_extra_height\n        self.block_gripper = block_gripper\n        self.has_object = has_object\n        self.target_in_the_air = target_in_the_air\n        self.target_offset = target_offset\n        self.obj_range = obj_range\n        self.target_range = target_range\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        super(FetchEnv, self).__init__(\n            model_path=model_path, n_substeps=n_substeps, n_actions=4,\n            initial_qpos=initial_qpos)\n\n    # GoalEnv methods\n    # ----------------------------\n\n    def compute_reward(self, achieved_goal, goal, info):\n        # Compute distance between goal and the achieved goal.\n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _step_callback(self):\n        if self.block_gripper:\n            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)\n            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)\n            self.sim.forward()\n\n    def _set_action(self, action):\n        assert action.shape == (4,)\n        action = action.copy()  # ensure that we don't change the action outside of this scope\n        pos_ctrl, gripper_ctrl = action[:3], action[3]\n\n        pos_ctrl *= 0.05  # limit maximum change in position\n        rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion\n        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])\n        assert gripper_ctrl.shape == (2,)\n        if self.block_gripper:\n            gripper_ctrl = np.zeros_like(gripper_ctrl)\n        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])\n\n        # Apply action to simulation.\n        utils.ctrl_set_action(self.sim, action)\n        utils.mocap_set_action(self.sim, action)\n\n    def _get_obs(self):\n        # positions\n        grip_pos = self.sim.data.get_site_xpos('robot0:grip')\n        dt = self.sim.nsubsteps * self.sim.model.opt.timestep\n        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt\n        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)\n        if self.has_object:\n            object_pos = self.sim.data.get_site_xpos('object0')\n            # rotations\n            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))\n            # velocities\n            object_velp = self.sim.data.get_site_xvelp('object0') * dt\n            object_velr = self.sim.data.get_site_xvelr('object0') * dt\n            # gripper state\n            object_rel_pos = object_pos - grip_pos\n            object_velp -= grip_velp\n        else:\n            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)\n        gripper_state = robot_qpos[-2:]\n        gripper_vel = robot_qvel[-2:] * dt  # change to a scalar if the gripper is made symmetric\n\n        if not self.has_object:\n            achieved_goal = grip_pos.copy()\n        else:\n            achieved_goal = np.squeeze(object_pos.copy())\n        obs = np.concatenate([\n            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),\n            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,\n        ])\n\n        return {\n            'observation': obs.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _viewer_setup(self):\n        body_id = self.sim.model.body_name2id('robot0:gripper_link')\n        lookat = self.sim.data.body_xpos[body_id]\n        for idx, value in enumerate(lookat):\n            self.viewer.cam.lookat[idx] = value\n        self.viewer.cam.distance = 2.5\n        self.viewer.cam.azimuth = 132.\n        self.viewer.cam.elevation = -14.\n\n    def _render_callback(self):\n        # Visualize target.\n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        site_id = self.sim.model.site_name2id('target0')\n        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]\n        self.sim.forward()\n\n    def _reset_sim(self):\n        self.sim.set_state(self.initial_state)\n\n        # Randomize start position of object.\n        if self.has_object:\n            object_xpos = self.initial_gripper_xpos[:2]\n            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:\n                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)\n            object_qpos = self.sim.data.get_joint_qpos('object0:joint')\n            assert object_qpos.shape == (7,)\n            object_qpos[:2] = object_xpos\n            self.sim.data.set_joint_qpos('object0:joint', object_qpos)\n\n        self.sim.forward()\n        return True\n\n    def _sample_goal(self):\n        if self.has_object:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)\n            goal += self.target_offset\n            goal[2] = self.height_offset\n            if self.target_in_the_air and self.np_random.uniform() < 0.5:\n                goal[2] += self.np_random.uniform(0, 0.45)\n        else:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-0.15, 0.15, size=3)\n        return goal.copy()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        utils.reset_mocap_welds(self.sim)\n        self.sim.forward()\n\n        # Move end effector into position.\n        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')\n        gripper_rotation = np.array([1., 0., 1., 0.])\n        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)\n        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)\n        for _ in range(10):\n            self.sim.step()\n\n        # Extract information for sampling goals.\n        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()\n        if self.has_object:\n            self.height_offset = self.sim.data.get_site_xpos('object0')[2]\n", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "file_name": "fetch_env.py", "id": "9abadba0803061e78d605852bb349f17", "language": "Python", "project_name": "gym", "quality": "", "save_path": "/home/ubuntu/test_files/clean/python/openai-gym/openai-gym-6160181/gym/envs/robotics/fetch_env.py", "save_time": "", "source": "", "update_at": "2018-03-18T13:30:35Z", "url": "https://github.com/openai/gym", "wiki": true}